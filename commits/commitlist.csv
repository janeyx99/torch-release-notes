commit_hash,category,topic,title,files_changed,pr_link,author,accepter_1,accepter_2,accepter_3,merge_into
82aaf64422e,autograd_frontend,not user facing,[3/N] Apply py39 ruff fixes (#142115),tools/autograd/gen_annotated_fn_args.py tools/autograd/gen_autograd_functions.py tools/autograd/gen_python_functions.py tools/autograd/gen_trace_type.py tools/autograd/gen_variable_type.py tools/autograd/load_derivatives.py tools/code_analyzer/gen_op_registration_allowlist.py tools/code_coverage/package/tool/print_report.py tools/code_coverage/package/tool/summarize_jsons.py tools/code_coverage/package/util/setting.py tools/extract_scripts.py tools/jit/gen_unboxing.py tools/linter/adapters/no_workflows_on_fork.py tools/linter/adapters/workflow_consistency_linter.py tools/nightly.py tools/nightly_hotpatch.py tools/onnx/gen_diagnostics.py tools/packaging/split_wheel.py tools/pyi/gen_pyi.py tools/setup_helpers/env.py tools/setup_helpers/gen_version_header.py tools/test/test_cmake.py tools/test/test_upload_stats_lib.py tools/testing/clickhouse.py tools/testing/target_determination/heuristics/historical_class_failure_correlation.py tools/testing/target_determination/heuristics/interface.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_run.py tools/testing/test_selections.py tools/testing/update_slow_tests.py tools/testing/upload_artifacts.py,https://github.com/pytorch/pytorch/pull/142115,cyyever,ezyang,,,
fb02b40d277,skip,not user facing,"[BE][accelerator] formalize API name `{current,set}_device_{idx => index}` (#140542)",docs/source/accelerator.rst test/test_accelerator.py torch/accelerator/__init__.py torch/accelerator/_utils.py,https://github.com/pytorch/pytorch/pull/140542,XuehaiPan,albanD,guangyey,,
be27dbf2b80,python_frontend,Untopiced,Enable CPP/CUDAExtension with py_limited_api for python agnosticism (#138088),test/cpp_extensions/open_registration_extension/setup.py test/cpp_extensions/python_agnostic_extension/python_agnostic/__init__.py test/cpp_extensions/python_agnostic_extension/python_agnostic/csrc/ultra_norm.cu test/cpp_extensions/python_agnostic_extension/python_agnostic/ops.py test/cpp_extensions/python_agnostic_extension/setup.py test/run_test.py test/test_cpp_extensions_aot.py torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/138088,janeyx99,albanD,ezyang,,
24a5a2ef258,skip,Untopiced,[14/N] Fix extra warnings brought by clang-tidy-17 (#141644),aten/src/ATen/MatrixRef.h aten/src/ATen/cuda/CUDABlas.cpp test/cpp/api/optim.cpp torch/csrc/CudaIPCTypes.cpp torch/csrc/Generator.cpp torch/csrc/Generator.h torch/csrc/PyInterpreter.cpp torch/csrc/api/include/torch/cuda.h torch/csrc/api/include/torch/data/dataloader/base.h torch/csrc/api/include/torch/nn/modules/container/any_value.h torch/csrc/api/include/torch/optim/adagrad.h torch/csrc/api/include/torch/optim/optimizer.h torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h torch/csrc/api/include/torch/ordered_dict.h torch/csrc/api/src/cuda.cpp torch/csrc/autograd/autograd_not_implemented_fallback.cpp torch/csrc/autograd/custom_function.h torch/csrc/autograd/functions/accumulate_grad.cpp torch/csrc/autograd/functions/comm.cpp torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/utils.cpp torch/csrc/autograd/profiler_python.cpp torch/csrc/autograd/variable.h torch/csrc/distributed/autograd/context/context.h,https://github.com/pytorch/pytorch/pull/141644,cyyever,ezyang,,,
a8fa98ccefc,skip,not user facing,skip test dynamo for aot_dispatch tests on ci (#142185),.ci/pytorch/test.sh test/functorch/test_aotdispatch.py test/run_test.py,https://github.com/pytorch/pytorch/pull/142185,ydwu4,zou3519,,,
c632e297742,skip,not user facing,[hop][dynamo] support torch.SymInt inputs (#141524),test/functorch/test_control_flow.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/141524,ydwu4,zou3519,,,
0ddb33ba229,onnx,improvements,[ONNX] Avoid overwriting overlapped decomposed functions (#142831),torch/onnx/_internal/exporter/_decomp.py,https://github.com/pytorch/pytorch/pull/142831,titaiwangms,justinchuby,,,
e647b6d5902,skip,not user facing,Fix undesired specialization on slice after split. (#142372),test/inductor/test_torchinductor.py torch/_decomp/decompositions.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/142372,ysiraichi,ezyang,,,
42d4eec5f38,skip,not user facing,Don't install lintrunner on S390 (#142876),requirements.txt,https://github.com/pytorch/pytorch/pull/142876,malfet,jeanschmidt,,,
e2283818465,skip,not user facing,[TorchGen] Simplify argument_type_str (#142491),torchgen/api/python.py,https://github.com/pytorch/pytorch/pull/142491,cyyever,ezyang,,,
e5f08c0cbf4,skip,not user facing,[TorchGen] Remove cpp_type_registration_declarations (#142452),torchgen/api/types/types.py torchgen/api/types/types_base.py torchgen/executorch/api/types/types.py torchgen/gen.py,https://github.com/pytorch/pytorch/pull/142452,cyyever,ezyang,,,
0b96413dbfc,releng,not user facing,Upgrade expecttest to 0.3.0 (#142869),.ci/docker/requirements-ci.txt .github/requirements/pip-requirements-macOS.txt .github/workflows/lint.yml .lintrunner.toml requirements.txt,https://github.com/pytorch/pytorch/pull/142869,ezyang,albanD,malfet,,
fd65bd755d3,distributed (torchelastic),docs,[BE] replace incorrect .. note:: invocations (#142868),torch/backends/mha/__init__.py torch/cuda/__init__.py torch/distributed/elastic/multiprocessing/api.py torch/distributed/elastic/utils/distributed.py torch/distributed/fsdp/api.py torch/distributed/launcher/api.py torch/distributed/optim/zero_redundancy_optimizer.py torch/distributed/tensor/debug/_comm_mode.py torch/mtia/__init__.py torch/nn/modules/utils.py,https://github.com/pytorch/pytorch/pull/142868,janeyx99,albanD,,,
688f44824bc,distributed,Untopiced,DistributedDataParallel: add init_sync option to control collectives during initialization (#142824),test/distributed/test_c10d_pypg.py torch/nn/parallel/distributed.py,https://github.com/pytorch/pytorch/pull/142824,d4l3k,H-Huang,fegin,wconstab,
bd199bc7542,releng,not user facing,[EZ] Move slow job from CU12.1 to CU12.4 (#142856),.github/workflows/slow.yml,https://github.com/pytorch/pytorch/pull/142856,malfet,ZainRizvi,atalman,huydhn,
de313f1155b,dynamo,Untopiced,[foreach_map] Initial foreach map HOP impl for inference (#142098),test/inductor/test_foreach.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/trace_rules.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/foreach_map.py torch/_inductor/lowering.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/142098,mlazos,eellison,,,
cd50bd8477c,skip,Untopiced,"Revert ""[BE][accelerator] formalize API name `{current,set}_device_{idx => index}` (#140542)""",docs/source/accelerator.rst test/test_accelerator.py torch/accelerator/__init__.py torch/accelerator/_utils.py,,,,,,
dfe56690767,skip,Untopiced,"Revert ""[RELAND] Add device-agnostic runtime Device/Stream C++ API (#138677)""",aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h torch/csrc/DeviceAccelerator.cpp torch/csrc/utils/device_lazy_init.h,,,,,,
1b3f8b75896,skip,Untopiced,"Revert ""[RELAND] Add UTs for accelerator device-agnostic runtime APIs (#133572)""",test/test_accelerator.py test/test_cuda.py test/test_xpu.py,,,,,,
db81a3f31c5,skip,not user facing,[TorchGen] remove remove_non_owning_ref_types from valuetype_type (#142449),torchgen/api/cpp.py torchgen/executorch/api/et_cpp.py,https://github.com/pytorch/pytorch/pull/142449,cyyever,ezyang,,,
bd7d81db9e8,releng,not user facing,Use validate-docker-images workflow from test-infra (#143081),.github/workflows/docker-release.yml,https://github.com/pytorch/pytorch/pull/143081,atalman,huydhn,,,
1dd6f210293,releng,not user facing,Cuda 12.1 - Remove from trunk tests (#143076),.github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/143076,atalman,huydhn,malfet,,
6cb6e8d790f,releng,not user facing,"Python 3.11, 3.12 Remove tests covered by 3.13 (#143078)",.github/workflows/pull.yml,https://github.com/pytorch/pytorch/pull/143078,atalman,huydhn,malfet,,
d68403df3bc,skip,not user facing,filelock: Make waitcounter variant to use (#139816),test/test_utils_filelock.py torch/_dynamo/pgo.py torch/_inductor/aoti_eager.py torch/_inductor/codecache.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py torch/_inductor/select_algorithm.py torch/utils/_filelock.py,https://github.com/pytorch/pytorch/pull/139816,c00w,ezyang,,,
06075d3d180,inductor,not user facing,[Inductor][CPP] Fix Mask Dtype mismatch (#142103),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/142103,leslie-fang-intel,jgong5,,,
cb354f8b473,distributed,Untopiced,[PGNCCL] Move NCCLComm impl to cpp (#142826),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp,https://github.com/pytorch/pytorch/pull/142826,kwen2501,c-p-i-o,wconstab,,
4d8357e912e,build_frontend,bug fixes,[CD] Use Anaconda cmake for Mac builds (#143054),.ci/wheel/build_wheel.sh,https://github.com/pytorch/pytorch/pull/143054,malfet,Jack-Khuu,atalman,,
f892f9862a6,cuda,not user facing,[ROCM] Enable *_load_dwordx4 ISA for BFloat16 and Half. (#141397),aten/src/ATen/native/cuda/Reduce.cuh aten/src/ATen/native/cuda/ReduceSumProdKernel.cu,https://github.com/pytorch/pytorch/pull/141397,carlobertolli,jeffdaily,,,
2903cf0ad82,distributed,not user facing,Re-enable some C++ warnings (#142332),CMakeLists.txt aten/src/ATen/cuda/detail/CUDAHooks.cpp aten/src/ATen/cuda/detail/LazyNVRTC.cpp aten/src/ATen/test/vec_test_all_types.cpp benchmarks/static_runtime/test_utils.cc caffe2/utils/threadpool/WorkersPool.h cmake/public/utils.cmake functorch/csrc/dim/dim.cpp torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp torch/csrc/lazy/core/ir_dump_util.cpp torch/csrc/profiler/standalone/execution_trace_observer.cpp,https://github.com/pytorch/pytorch/pull/142332,cyyever,albanD,eqy,,
4d0775462eb,skip,not user facing,E2E composability testing (#141398),test/distributed/_composable/test_composability/test_pp_composability.py,https://github.com/pytorch/pytorch/pull/141398,mori360,kwen2501,wconstab,,
fbbafd03200,quantization,not user facing,Turn on AOTAutogradCache by default on open source (#141981),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/dynamo/test_aot_autograd.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/aot_autograd.py torch/_functorch/config.py,https://github.com/pytorch/pytorch/pull/141981,jamesjwu,bdhirsh,oulgen,,
f7b9533c3fd,cpp_frontend,Untopiced,[4/N] Apply bugprone-unchecked-optional-access  (#142832),aten/src/ATen/core/List_test.cpp aten/src/ATen/core/enum_type.h aten/src/ATen/core/function_schema.h aten/src/ATen/core/ivalue.h aten/src/ATen/core/jit_type.h aten/src/ATen/core/tensor_type.cpp aten/src/ATen/core/type.cpp aten/src/ATen/functorch/ADInterpreters.cpp aten/src/ATen/functorch/BatchRulesBinaryOps.cpp aten/src/ATen/functorch/BatchRulesFactory.cpp aten/src/ATen/functorch/BatchRulesHelper.cpp aten/src/ATen/functorch/BatchRulesHelper.h aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp aten/src/ATen/functorch/BatchRulesLoss.cpp aten/src/ATen/functorch/BatchRulesModules.cpp aten/src/ATen/functorch/BatchRulesNorm.cpp aten/src/ATen/functorch/BatchRulesPooling.cpp aten/src/ATen/functorch/BatchRulesRandomness.cpp torch/csrc/Size.cpp torch/csrc/api/include/torch/nn/functional/batchnorm.h torch/csrc/api/include/torch/nn/functional/loss.h torch/csrc/api/include/torch/nn/options/batchnorm.h torch/csrc/api/src/nn/modules/embedding.cpp torch/csrc/api/src/nn/modules/pooling.cpp torch/csrc/api/src/nn/modules/upsampling.cpp torch/csrc/autograd/engine.cpp torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/distributed/c10d/sequence_num.cpp torch/csrc/distributed/rpc/script_call.cpp torch/csrc/dynamo/compiled_autograd.h,https://github.com/pytorch/pytorch/pull/142832,cyyever,albanD,,,
20df80a669f,skip,not user facing,Remove unneeded optional dereference (#141578),aten/src/ATen/native/Convolution.cpp aten/src/ATen/native/Loss.cpp aten/src/ATen/native/LossNLL.cpp aten/src/ATen/native/LossNLL2d.cpp aten/src/ATen/native/cuda/Loss.cu aten/src/ATen/native/layer_norm.cpp,https://github.com/pytorch/pytorch/pull/141578,cyyever,swolchok,,,
fda43c98d1b,quantization,not user facing,Improve implementation of quantized_batch_norm (#141570),aten/src/ATen/native/quantized/cpu/Normalization.cpp,https://github.com/pytorch/pytorch/pull/141570,cyyever,albanD,,,
1e2b841675e,skip,not user facing,[ROCm] Prune old gfx archs gfx900/gfx906 from binaries (#142827),.ci/docker/libtorch/build.sh .ci/docker/manywheel/build.sh,https://github.com/pytorch/pytorch/pull/142827,jithunnair-amd,jeffdaily,,,
79cf8fa7517,skip,not user facing,[Inductor] Use sleef implementation for CPP backend asinh codegen (#142360),aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h aten/src/ATen/cpu/vec/vec256/vec256_double.h aten/src/ATen/cpu/vec/vec256/vec256_float.h aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h aten/src/ATen/cpu/vec/vec512/vec512_double.h aten/src/ATen/cpu/vec/vec512/vec512_float.h aten/src/ATen/cpu/vec/vec_base.h aten/src/ATen/cpu/vec/vec_n.h test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/142360,leslie-fang-intel,jgong5,,,
b25f64b6134,releng,not user facing,Add-o pipefail for all bash scripts (#143050),.ci/pytorch/build.sh .ci/pytorch/common.sh .ci/pytorch/cpp_doc_push_script.sh .ci/pytorch/functorch_doc_push_script.sh .ci/pytorch/install_cache_xla.sh .ci/pytorch/python_doc_push_script.sh .ci/pytorch/test.sh .ci/pytorch/win-build.sh .ci/pytorch/win-test.sh,https://github.com/pytorch/pytorch/pull/143050,Grove-YJX,ezyang,huydhn,,
9701c50bdc8,dynamo,Untopiced,[Dynamo] Add missing tensor builtins to allowed functions (#142841),test/dynamo/test_modes.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/142841,mlazos,yanboliang,,,
357e261b1ed,skip,Untopiced,[Dynamo] only import einops if version is lower than 0.7.0 (#142847),torch/_dynamo/decorators.py,https://github.com/pytorch/pytorch/pull/142847,mlazos,zou3519,,,
5ad7d5304c9,distributed,not user facing,[DTensor][random] add HSDP+TP model init test (#143077),test/distributed/_tensor/test_random_ops.py,https://github.com/pytorch/pytorch/pull/143077,XilunWu,weifengpy,,,
dcb128d4952,linalg_frontend,Untopiced,[ROCm] TunableOp use thread-safe getenv functions (#142274),aten/src/ATen/cuda/tunable/Tunable.cpp test/test_linalg.py,https://github.com/pytorch/pytorch/pull/142274,naromero77amd,eqy,jeffdaily,,
da76e912a4c,skip,not user facing,Hide torch_python symbols (#142214),torch/CMakeLists.txt torch/csrc/Layout.h torch/csrc/QScheme.h torch/csrc/utils/device_lazy_init.h,https://github.com/pytorch/pytorch/pull/142214,cyyever,ezyang,,,
497f89ff832,export,Untopiced,fix dynamo nn module stack fqn (#142823),test/export/test_export.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/142823,avikchaudhuri,pianpwk,zhxchen17,,
a5fb07af271,distributed,Untopiced,[Torch][#142396]Resolve Failure When Uploading To Remote Storage (#143046),torch/distributed/checkpoint/filesystem.py,https://github.com/pytorch/pytorch/pull/143046,Saiteja64,saumishr,,,
18785c1af9b,skip,not user facing,"[BE][accelerator] formalize API name `{current,set}_device_{idx => index}` (#140542)",docs/source/accelerator.rst torch/accelerator/__init__.py torch/accelerator/_utils.py,https://github.com/pytorch/pytorch/pull/140542,XuehaiPan,albanD,guangyey,,
91261107e0b,quantization,Untopiced,debug handler maintain through decomposition (#141612),docs/source/conf.py test/quantization/pt2e/test_numeric_debugger.py torch/ao/quantization/pt2e/_numeric_debugger.py torch/ao/quantization/pt2e/graph_utils.py torch/testing/_internal/common_quantization.py,https://github.com/pytorch/pytorch/pull/141612,Gasoonjia,jerryzh168,,,
30e2b322a1e,fx,not user facing,Add <string> to uninteresting_files (#142984),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/142984,ezyang,IvanKobzarev,albanD,,
cd1b5924d53,skip,Untopiced,"Revert ""[Inductor] Use sleef implementation for CPP backend asinh codegen (#142360)""",aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h aten/src/ATen/cpu/vec/vec256/vec256_double.h aten/src/ATen/cpu/vec/vec256/vec256_float.h aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h aten/src/ATen/cpu/vec/vec512/vec512_double.h aten/src/ATen/cpu/vec/vec512/vec512_float.h aten/src/ATen/cpu/vec/vec_base.h aten/src/ATen/cpu/vec/vec_n.h test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,,,,,,
84f791381a9,releng,not user facing,Python 3.13 CI add crossref test to existing linux-focal-py3_13-clang10-build  (#143074),.github/workflows/pull.yml,https://github.com/pytorch/pytorch/pull/143074,atalman,malfet,,,
7cc3a591c22,inductor,not user facing,[FlexAttention] Fix a few more symbolic shape issues (#142816),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/142816,drisspg,ezyang,yanboliang,,
d83a0492322,releng,not user facing,[EZ] Update lintrunner in CI to 0.12.7 (#143073),.ci/docker/requirements-ci.txt .github/scripts/lintrunner.sh,https://github.com/pytorch/pytorch/pull/143073,malfet,wdvr,,,
725526abc50,inductor,not user facing,Fix scan dtypes (#143048),test/inductor/test_op_dtype_prop.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py,https://github.com/pytorch/pytorch/pull/143048,eellison,arui-meta,blaine-rister,,
0f78be55730,skip,not user facing,Fix search icon (#142808),docs/source/_static/css/custom.css,https://github.com/pytorch/pytorch/pull/142808,svekars,albanD,,,
e3ddc0ca331,inductor,not user facing,Support remote caching requiring redis auth (#141679),torch/_inductor/remote_cache.py,https://github.com/pytorch/pytorch/pull/141679,yasyf,masnesral,,,
30b61e521c6,dynamo,not user facing,[logging] Populate compile_time_autotune_time_us (#143104),test/dynamo/test_utils.py torch/_dynamo/utils.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/143104,masnesral,ezyang,,,
15ee2960e1f,skip,not user facing,[aot] Functionalize aot backward prologue and epilogue wrappers (#142415),torch/_functorch/_aot_autograd/runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/142415,xmfan,bdhirsh,,,
cf538efd0c4,skip,Untopiced,"Revert ""Hide torch_python symbols (#142214)""",torch/CMakeLists.txt torch/csrc/Layout.h torch/csrc/QScheme.h torch/csrc/utils/device_lazy_init.h,,,,,,
520ba556cd4,inductor,not user facing,"[Inductor] Refactor ""r"" reduction prefix to {""r0_"", ""r1_""}. (#142020)",test/inductor/test_coordinate_descent_tuner.py test/inductor/test_padding.py test/inductor/test_torchinductor.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/ir.py torch/_inductor/runtime/coordinate_descent_tuner.py torch/_inductor/runtime/hints.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py torch/utils/_sympy/symbol.py,https://github.com/pytorch/pytorch/pull/142020,blaine-rister,jansel,,,
7667235a23e,cpp_frontend,improvements,c10::optional -> std::optional (#142514),aten/src/ATen/native/mkldnn/xpu/detail/QConv.cpp aten/src/ATen/native/mkldnn/xpu/detail/oneDNN.h aten/src/ATen/native/mkldnn/xpu/qconv.cpp test/cpp/api/autograd.cpp torch/csrc/autograd/engine.cpp torch/csrc/autograd/engine.h,https://github.com/pytorch/pytorch/pull/142514,r-barnes,malfet,,,
dc23f1944ae,skip,Untopiced,Remove unused Python variables in torch/[_-a]* (#133492),torch/_appdirs.py torch/_decomp/decompositions.py torch/_dispatch/python.py torch/_dynamo/backends/distributed.py torch/_dynamo/comptime.py torch/_dynamo/debug_utils.py torch/_dynamo/decorators.py torch/_dynamo/eval_frame.py torch/_dynamo/guards.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/repro/after_aot.py torch/_dynamo/repro/after_dynamo.py torch/_dynamo/symbolic_convert.py torch/_dynamo/test_minifier_common.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/tensor.py torch/_export/__init__.py torch/_export/converter.py torch/_export/db/examples/pytree_flatten.py torch/_export/serde/serialize.py torch/_export/utils.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/functional_utils.py torch/_functorch/_aot_autograd/input_output_analysis.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/_functorch/_aot_autograd/utils.py torch/_functorch/aot_autograd.py torch/_functorch/compilers.py torch/_functorch/eager_transforms.py torch/_functorch/partitioners.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/invoke_subgraph.py torch/_higher_order_ops/prim_hop_base.py torch/_higher_order_ops/scan.py torch/_higher_order_ops/torchbind.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/while_loop.py torch/_inductor/__init__.py torch/_inductor/aoti_eager.py torch/_inductor/autoheuristic/learned_heuristic_controller.py torch/_inductor/autotune_process.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_fx.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/compiler_bisector.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py torch/_inductor/cudagraph_trees.py torch/_inductor/debug.py torch/_inductor/dependencies.py torch/_inductor/fx_passes/ddp_fusion.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/numeric_utils.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/reinplace.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/fx_utils.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/lowering.py torch/_inductor/mkldnn_ir.py torch/_inductor/output_code.py torch/_inductor/runtime/autotune_cache.py torch/_inductor/runtime/halide_helpers.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py torch/_jit_internal.py torch/_library/fake_class_registry.py torch/_library/infer_schema.py torch/_lobpcg.py torch/_logging/_internal.py torch/_lowrank.py torch/_meta_registrations.py torch/_numpy/testing/utils.py torch/_ops.py torch/_prims/__init__.py torch/_refs/__init__.py torch/_strobelight/cli_function_profiler.py torch/_strobelight/examples/cli_function_profiler_example.py torch/_strobelight/examples/compile_time_profile_example.py torch/_subclasses/fake_impls.py torch/_subclasses/fake_tensor.py torch/_subclasses/meta_utils.py torch/_subclasses/schema_check_mode.py torch/_tensor_str.py torch/_utils.py torch/_weights_only_unpickler.py torch/ao/nn/quantizable/modules/rnn.py torch/ao/nn/quantized/dynamic/modules/rnn.py torch/ao/nn/quantized/modules/embedding_ops.py torch/ao/nn/sparse/quantized/dynamic/linear.py torch/ao/nn/sparse/quantized/linear.py torch/ao/ns/_numeric_suite_fx.py torch/ao/ns/fx/graph_passes.py torch/ao/ns/fx/n_shadows_utils.py torch/ao/pruning/sparsifier/base_sparsifier.py torch/ao/quantization/__init__.py torch/ao/quantization/backend_config/_qnnpack_pt2e.py torch/ao/quantization/backend_config/onednn.py torch/ao/quantization/experimental/apot_utils.py torch/ao/quantization/experimental/linear.py torch/ao/quantization/experimental/observer.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/ao/quantization/fx/_model_report/model_report.py torch/ao/quantization/fx/_model_report/model_report_visualizer.py torch/ao/quantization/fx/convert.py torch/ao/quantization/fx/prepare.py torch/ao/quantization/pt2e/representation/rewrite.py torch/ao/quantization/quantize.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py torch/ao/quantization/utils.py torch/autograd/_functions/utils.py torch/autograd/function.py torch/autograd/gradcheck.py torch/autograd/profiler_legacy.py torch/autograd/profiler_util.py,https://github.com/pytorch/pytorch/pull/133492,rec,albanD,,,
2f0fe82f6d0,skip,Untopiced,"Revert ""[14/N] Fix extra warnings brought by clang-tidy-17 (#141644)""",aten/src/ATen/MatrixRef.h aten/src/ATen/cuda/CUDABlas.cpp test/cpp/api/optim.cpp torch/csrc/CudaIPCTypes.cpp torch/csrc/Generator.cpp torch/csrc/Generator.h torch/csrc/PyInterpreter.cpp torch/csrc/api/include/torch/cuda.h torch/csrc/api/include/torch/data/dataloader/base.h torch/csrc/api/include/torch/nn/modules/container/any_value.h torch/csrc/api/include/torch/optim/adagrad.h torch/csrc/api/include/torch/optim/optimizer.h torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h torch/csrc/api/include/torch/ordered_dict.h torch/csrc/api/src/cuda.cpp torch/csrc/autograd/autograd_not_implemented_fallback.cpp torch/csrc/autograd/custom_function.h torch/csrc/autograd/functions/accumulate_grad.cpp torch/csrc/autograd/functions/comm.cpp torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/utils.cpp torch/csrc/autograd/profiler_python.cpp torch/csrc/autograd/variable.h torch/csrc/distributed/autograd/context/context.h,,,,,,
c85323c5e82,skip,Untopiced,"Revert ""Tests Generelization for multiple accelerator devices (#139184)""",test/distributed/fsdp/test_checkpoint_wrapper.py test/distributed/fsdp/test_distributed_checkpoint.py test/distributed/fsdp/test_fsdp_apply.py test/distributed/fsdp/test_fsdp_backward_prefetch.py test/distributed/fsdp/test_fsdp_checkpoint.py test/distributed/fsdp/test_fsdp_clip_grad_norm.py test/distributed/fsdp/test_fsdp_comm.py test/distributed/fsdp/test_fsdp_core.py test/distributed/fsdp/test_fsdp_dtensor_state_dict.py test/distributed/fsdp/test_fsdp_exec_order.py test/distributed/fsdp/test_fsdp_fine_tune.py test/distributed/fsdp/test_fsdp_fx.py test/distributed/fsdp/test_fsdp_input.py test/distributed/fsdp/test_fsdp_memory.py test/distributed/fsdp/test_fsdp_multiple_forward.py test/distributed/fsdp/test_fsdp_multiple_wrapping.py test/distributed/fsdp/test_fsdp_overlap.py test/distributed/fsdp/test_fsdp_pure_fp16.py test/distributed/fsdp/test_fsdp_traversal.py test/distributed/fsdp/test_fsdp_uneven.py test/distributed/fsdp/test_fsdp_unshard_params.py test/distributed/fsdp/test_hsdp_dtensor_state_dict.py test/distributed/fsdp/test_utils.py torch/testing/_internal/common_fsdp.py,,,,,,
7edeb1005af,dynamo,Untopiced,[dynamo][pytree][2/N] make CXX pytree traceable: `tree_flatten` / `tree_unflatten` / `tree_structure` (#137398),test/dynamo/test_misc.py test/test_pytree.py torch/_dynamo/polyfills/pytree.py torch/utils/_cxx_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/137398,XuehaiPan,jansel,,,
d47a80246a6,dynamo,Untopiced,[dynamo][pytree][3/N] make CXX pytree traceable: `tree_map` / `tree_map_` (#137399),test/dynamo/test_misc.py torch/_dynamo/polyfills/pytree.py torch/utils/_cxx_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/137399,XuehaiPan,jansel,,,
5dabe2d4648,skip,not user facing,Fix NJT backward tests (#143072),test/functorch/test_vmap.py test/test_nestedtensor.py torch/testing/_internal/opinfo/core.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/143072,jbschlosser,cpuhrsch,soulitzer,,
ee5bceaee62,export,Untopiced,[sigmoid] Write the new export schema format to archive without breaking compatibility. (#142511),torch/_export/serde/export_schema.thrift torch/_export/serde/gen-cpp2/export_schema_constants.h torch/_export/serde/gen-cpp2/export_schema_types.h torch/_export/serde/schema.thrift torch/_export/serde/schema_check.py,https://github.com/pytorch/pytorch/pull/142511,zhxchen17,desertfire,,,
b0c3d39e0d4,distributed,Untopiced,[pipelining] Update tutorials and documentation (#143045),docs/source/distributed.pipelining.rst torch/distributed/pipelining/_IR.py,https://github.com/pytorch/pytorch/pull/143045,H-Huang,kwen2501,wconstab,,
d48b16a7251,skip,Untopiced,"Revert ""[Dynamo] only import einops if version is lower than 0.7.0 (#142847)""",torch/_dynamo/decorators.py,,,,,,
e3fe5f62b6a,releng,not user facing,Remove Checkout pytorch/builder for Linux Binary Builds (#143125),.github/workflows/_binary-build-linux.yml,https://github.com/pytorch/pytorch/pull/143125,atalman,kit1980,,,
c170248b78a,profiler,Untopiced,[Profiler] Enable Iterative Step without profiler in fbcode (#142077),torch/profiler/__init__.py,https://github.com/pytorch/pytorch/pull/142077,sraikund16,briancoutinho,,,
0b75b7ff2b8,skip,not user facing,[Easy] factor out inductor ophandler decompositions (#142400),torch/_inductor/codegen/common.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/142400,eellison,Chillee,jansel,,
82ce8882730,cpp_frontend,improvements,c10::string_view -> std::string_view in more places (#142517),c10/util/ConstexprCrc.h c10/util/StringUtil.cpp test/cpp/lazy/test_lazy_ops.cpp test/test_overrides.py tools/lite_interpreter/gen_selected_mobile_ops_header.py torch/csrc/api/include/torch/fft.h torch/csrc/api/include/torch/nn/utils/rnn.h torch/csrc/jit/serialization/source_range_serialization.cpp,https://github.com/pytorch/pytorch/pull/142517,r-barnes,malfet,,,
39cacc1d814,releng,not user facing,Fix missing tests on test tool lint job (#143052),.github/workflows/lint.yml tools/testing/test_run.py,https://github.com/pytorch/pytorch/pull/143052,huydhn,ZainRizvi,,,
a7509e98c55,distributed,Untopiced,[pipelining] fix backward_one_chunk when the output of the model is a… (#142237),torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/142237,Adrien-AM,H-Huang,,,
602c86a420b,distributed,Untopiced,[DSD] Fix strict=False case for DDP (#143038),test/distributed/checkpoint/test_state_dict.py torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/143038,fegin,mori360,,,
fb934629040,inductor,Untopiced,[Reopen][Inductor][CPU] Fuse SmoothQuant int8 linear pattern (#142036),aten/src/ATen/native/quantized/cpu/qlinear.cpp test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py,https://github.com/pytorch/pytorch/pull/142036,Xia-Weiwen,jerryzh168,jgong5,,
2533a5a8430,releng,not user facing,upgrade sccache to 0.9.0 (#142854),.ci/docker/common/install_cache.sh,https://github.com/pytorch/pytorch/pull/142854,wdvr,ZainRizvi,malfet,,
9f5ebf3fc60,skip,not user facing,"Clang-format aten/src/ATen/native/Tensor*{cpp,h} (#143089)",.lintrunner.toml aten/src/ATen/native/TensorAdvancedIndexing.cpp aten/src/ATen/native/TensorAdvancedIndexing.h aten/src/ATen/native/TensorAdvancedIndexingUtils.h aten/src/ATen/native/TensorCompare.cpp aten/src/ATen/native/TensorCompare.h aten/src/ATen/native/TensorConversions.cpp aten/src/ATen/native/TensorConversions.h aten/src/ATen/native/TensorDimApply.h aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/TensorFactories.h aten/src/ATen/native/TensorIteratorDynamicCasting.h aten/src/ATen/native/TensorIteratorReduce.cpp aten/src/ATen/native/TensorProperties.cpp aten/src/ATen/native/TensorShape.cpp aten/src/ATen/native/TensorShape.h aten/src/ATen/native/TensorTransformations.cpp aten/src/ATen/native/TensorTransformations.h,https://github.com/pytorch/pytorch/pull/143089,cyyever,albanD,,,
dbe4b69df05,inductor,not user facing,[Inductor] Fix cooperative reduction tests broken in recent refactor (#143135),test/inductor/test_cooperative_reductions.py,https://github.com/pytorch/pytorch/pull/143135,blaine-rister,huydhn,jansel,,
ab04f3aee1d,skip,not user facing,[ca] set autograd graph task state (#143108),test/inductor/test_compiled_autograd.py torch/csrc/autograd/engine.cpp,https://github.com/pytorch/pytorch/pull/143108,xmfan,albanD,jansel,,
ceb664aca68,skip,not user facing,add float_args benchmark (#143143),benchmarks/dynamo/pr_time_benchmarks/benchmarks/float_args.py,https://github.com/pytorch/pytorch/pull/143143,bobrenjc93,laithsakka,,,
b731ced91f5,skip,Untopiced,Prologue Fusion (#134532),test/inductor/test_max_autotune.py torch/_inductor/choices.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/config.py torch/_inductor/ir.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/134532,eellison,jansel,,,
57c46af47a9,inductor,Untopiced,[Inductor][CPU] Add torchao da8w8 pattern with sym quantized act & wgt (#142110),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py,https://github.com/pytorch/pytorch/pull/142110,sanchitintel,jgong5,leslie-fang-intel,,
b7ad52abb01,distributed,Untopiced,Use new group instead of split group on non-CUDA device (#141469),torch/distributed/device_mesh.py torch/distributed/tensor/parallel/fsdp.py,https://github.com/pytorch/pytorch/pull/141469,zhangxiaoli73,albanD,gujinghui,kwen2501,
b5d8d2444ae,skip,not user facing,add README.md for compile time benchmarks (#143145),benchmarks/dynamo/pr_time_benchmarks/README.md,https://github.com/pytorch/pytorch/pull/143145,bobrenjc93,laithsakka,,,
b4f4c75e196,skip,not user facing,[dynamo] Support multiple inheritance for custom dict construction (#142416),test/dynamo/test_misc.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/142416,StrongerXi,jansel,,,
72fd7abb35c,skip,not user facing,[ca] fix flex attention backward HOP capture in initial graph (#143155),test/inductor/test_compiled_autograd.py torch/_higher_order_ops/flex_attention.py,https://github.com/pytorch/pytorch/pull/143155,xmfan,drisspg,,,
075905b7bdf,skip,Untopiced,[14/N] Fix extra warnings brought by clang-tidy-17 (#141644),aten/src/ATen/MatrixRef.h aten/src/ATen/cuda/CUDABlas.cpp test/cpp/api/optim.cpp torch/csrc/CudaIPCTypes.cpp torch/csrc/Generator.cpp torch/csrc/Generator.h torch/csrc/PyInterpreter.cpp torch/csrc/api/include/torch/cuda.h torch/csrc/api/include/torch/data/dataloader/base.h torch/csrc/api/include/torch/nn/modules/container/any_value.h torch/csrc/api/include/torch/optim/adagrad.h torch/csrc/api/include/torch/optim/optimizer.h torch/csrc/api/include/torch/optim/schedulers/lr_scheduler.h torch/csrc/api/include/torch/ordered_dict.h torch/csrc/api/src/cuda.cpp torch/csrc/autograd/autograd_not_implemented_fallback.cpp torch/csrc/autograd/custom_function.h torch/csrc/autograd/functions/accumulate_grad.cpp torch/csrc/autograd/functions/comm.cpp torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/utils.cpp torch/csrc/autograd/profiler_python.cpp torch/csrc/autograd/variable.h torch/csrc/distributed/autograd/context/context.h,https://github.com/pytorch/pytorch/pull/141644,cyyever,ezyang,,,
c37185c76ae,skip,not user facing,[BE] Stop using deprecated APIs in mkldnn_pattern_matcher (#143156),test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/143156,malfet,kit1980,,,
9f90583ca20,releng,not user facing,[CI] Run aarch64 tests on Graviton3 (#143129),.github/workflows/linux-aarch64.yml test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/143129,malfet,digantdesai,,,
3e1f5875143,inductor,bug fixes,[AOTI] Fix an autotune block grid computation issue (#143098),test/inductor/test_aot_inductor.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/143098,desertfire,henrylhtsang,,,
810808d97d2,distributed,not user facing,Enable cutlass-based all-gather matmul when TORCH_SYMM_MEM_ENABLE_NATIVE_ASYNC_TP is set (#142283),test/distributed/test_symmetric_memory.py torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/142283,yifuwang,Chillee,weifengpy,,
04bb82f0976,releng,not user facing,Linux Wheels: Remove triton dependency python < 3.13 constraint (#143162),.circleci/scripts/binary_populate_env.sh,https://github.com/pytorch/pytorch/pull/143162,atalman,kit1980,,,
fbfc530442d,export,Untopiced,[export][ez] Fix forward D67044185 (#143193),torch/_export/serde/gen-cpp2/export_schema_types_custom_protocol.h,https://github.com/pytorch/pytorch/pull/143193,zhxchen17,tugsbayasgalan,,,
da67a6a7bb2,inductor,not user facing,[inductor] Replace set by OrderedSet (#138466),.lintrunner.toml benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_inductor/async_compile.py torch/_inductor/autotune_process.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_flex_attention_template.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/memory_planning.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/rocm/rocm_benchmark_request.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/simd_kernel_features.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/codegen/wrapper.py torch/_inductor/comm_lowering.py torch/_inductor/comms.py torch/_inductor/compile_fx.py torch/_inductor/constant_folding.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/debug.py torch/_inductor/dependencies.py torch/_inductor/dtype_propagation.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/fx_passes/ddp_fusion.py torch/_inductor/fx_passes/dedupe_symint_uses.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/_inductor/fx_passes/misc_patterns.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/numeric_utils.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/pre_grad.py torch/_inductor/fx_passes/reinplace.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/fx_utils.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/mm_common.py torch/_inductor/lowering.py torch/_inductor/memory.py torch/_inductor/metrics.py torch/_inductor/ops_handler.py torch/_inductor/output_code.py torch/_inductor/pattern_matcher.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py torch/_inductor/subgraph_lowering.py torch/_inductor/utils.py torch/_inductor/virtualized.py torch/_inductor/wrapper_benchmark.py,https://github.com/pytorch/pytorch/pull/138466,rec,eellison,,,
7968732f5b8,inductor,not user facing,Fix int8 mm V.ops.mul dispatching (#143127),test/inductor/test_pattern_matcher.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/143127,eellison,drisspg,,,
3f62054de1f,releng,not user facing,[ROCm] upgrade nightly wheels to rocm6.3 - 1 of 2 (docker images) (#142151),.github/workflows/build-libtorch-images.yml .github/workflows/build-manywheel-images.yml,https://github.com/pytorch/pytorch/pull/142151,jithunnair-amd,jeffdaily,,,
82a45d19b45,python_frontend,Untopiced,Expose sharedMemPerMultiprocessor device property to python (#143119),torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/143119,peterbell10,ezyang,,,
23b8ea3094b,dynamo,Untopiced,Allow disabling int specialization on nn.Modules (#142829),test/dynamo/test_modules.py torch/_dynamo/config.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/142829,jhadidjojo,ezyang,yanboliang,,
e0c8abda76c,python_frontend,not user facing,Fix potentially undefined behaviour in index_put sample input (#143116),test/test_mps.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/143116,GeorgeWigley,albanD,,,
63e1f97f4b8,dynamo,not user facing,dynamo tracing perf: don't unnecessarily call getframeinfo on the hot path: 47.26 -> 37.66 (#143066),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/143066,aorenste,jansel,,,
b472d82c96a,dynamo,not user facing,dynamo tracing perf: import in build: 60.48 -> 59.92 (#143056),torch/_dynamo/variables/base.py,https://github.com/pytorch/pytorch/pull/143056,aorenste,jansel,,,
6bcda3a21a6,dynamo,not user facing,dynamo tracing perf: cache on import_source: 52.9 -> 52.58 (#143058),torch/_dynamo/symbolic_convert.py torch/utils/_functools.py,https://github.com/pytorch/pytorch/pull/143058,aorenste,jansel,,,
6178be822dc,dynamo,not user facing,dynamo tracing perf: direct Guard: 52.58 -> 51.76 (#143059),torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/143059,aorenste,jansel,williamwen42,,
bf711a9ccef,cuda,Untopiced,[ROCm] Improve performance of reduce sum for 3D shapes (#143137),aten/src/ATen/native/cuda/Reduce.cuh,https://github.com/pytorch/pytorch/pull/143137,doru1004,eqy,jeffdaily,,
9d05c8110da,skip,not user facing,Require Config to have a default (#143150),torch/testing/_internal/fake_config_module.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/143150,oulgen,ezyang,,,
f1ff8bc1c56,skip,not user facing,Add type to Config (#143151),torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/143151,oulgen,ezyang,,,
1ebdfd56053,skip,not user facing,Migrate compiler config to Config (#143152),torch/compiler/config.py,https://github.com/pytorch/pytorch/pull/143152,oulgen,ezyang,,,
8f404467707,dynamo,bug fixes,Fix precedence of bitwise and/or printing (#143197),test/dynamo/test_repros.py torch/utils/_sympy/functions.py torch/utils/_sympy/printers.py,https://github.com/pytorch/pytorch/pull/143197,ezyang,albanD,williamwen42,,
fe9365f3f57,releng,not user facing,Add check_binary workflow to pytorch/pytorch (#143201),.github/workflows/test-check-binary.yml,https://github.com/pytorch/pytorch/pull/143201,atalman,malfet,seemethere,,
625b4edb975,releng,not user facing,[CD] Test torch.compile on 3.13 (#143207),.ci/pytorch/smoke_test/smoke_test.py,https://github.com/pytorch/pytorch/pull/143207,malfet,ZainRizvi,atalman,,
e87f07d3b8c,skip,Untopiced,"Revert ""Migrate compiler config to Config (#143152)""",torch/compiler/config.py,,,,,,
52f31cc238a,skip,not user facing,dynamo tracing perf: Guard slots: 51.76 -> 51.34 (#143060),torch/_guards.py,https://github.com/pytorch/pytorch/pull/143060,aorenste,jansel,,,
65d0a25289e,skip,not user facing,[associative_scan] patch inductor tests to always run with static shape (#143161),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/143161,ydwu4,eellison,,,
0d6d29af380,skip,not user facing,[CUDA] Follow up to clean up some `set_per_process_memory_fraction` usage in tests (#142811),test/test_cuda.py,https://github.com/pytorch/pytorch/pull/142811,eqy,Skylion007,,,
60c54467dbd,releng,not user facing,[logging] Log runtime autotuning timing to scuba (#141919),.ci/pytorch/common_utils.sh .ci/pytorch/win-test.sh test/dynamo/test_structured_trace.py test/dynamo/test_utils.py torch/_dynamo/metrics_context.py torch/_dynamo/utils.py torch/_guards.py torch/_inductor/codegen/triton.py torch/_inductor/output_code.py torch/_inductor/runtime/triton_heuristics.py torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/141919,masnesral,ezyang,jamesjwu,,
571cd92d7c4,skip,not user facing,[CI] Add Triton 3.13t build (#143212),.github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/143212,malfet,atalman,clee2000,seemethere,
8a040183295,mps,Untopiced,[MPS] Fix conv backward for channels last (cont) (#143196),aten/src/ATen/native/mps/operations/Convolution.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/143196,malfet,manuelcandales,,,
8fae4397b4f,inductor,not user facing,"Add ""inductor_pre_grad_graph"" logging (#142717) (#143126)",test/dynamo/test_logging.py test/dynamo/test_structured_trace.py torch/_inductor/compile_fx.py torch/_logging/_internal.py torch/_logging/_registrations.py,https://github.com/pytorch/pytorch/pull/143126,yushangdi,desertfire,,,
e19f493f02d,dynamo,not user facing,add private config to temporarily preserve old FSDP guard behavior (#142871),torch/_dynamo/config.py torch/_dynamo/guards.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/142871,bdhirsh,yf225,,,
d25e6e623fe,quantization,not user facing,Fix unused Python variables in test/[a-d]* (#134665),test/ao/sparsity/test_kernels.py test/ao/sparsity/test_qlinear_packed_params.py test/ao/sparsity/test_sparsifier.py test/ao/sparsity/test_sparsity_utils.py test/ao/sparsity/test_structured_sparsifier.py test/autograd/test_functional.py test/bottleneck_test/test_cuda.py test/custom_operator/test_custom_ops.py test/distributed/_composable/fsdp/test_fully_shard_clip_grad_norm_.py test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_compile.py test/distributed/_composable/fsdp/test_fully_shard_extensions.py test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fsdp/test_fully_shard_logging.py test/distributed/_composable/fsdp/test_fully_shard_training.py test/distributed/_composable/test_checkpoint.py test/distributed/_composable/test_composability/test_2d_composability.py test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/_composable/test_replicate_with_compiler.py test/distributed/_shard/sharded_tensor/ops/test_binary_cmp.py test/distributed/_shard/sharded_tensor/ops/test_init.py test/distributed/_shard/sharded_tensor/test_sharded_tensor.py test/distributed/_shard/sharding_plan/test_sharding_plan.py test/distributed/_tensor/debug/test_comm_mode.py test/distributed/_tensor/debug/test_comm_mode_features.py test/distributed/_tensor/test_dtensor.py test/distributed/_tensor/test_dtensor_compile.py test/distributed/_tensor/test_random_ops.py test/distributed/_tensor/test_redistribute.py test/distributed/_tensor/test_tensor_ops.py test/distributed/_tensor/test_utils.py test/distributed/_tensor/test_xla_integration.py test/distributed/algorithms/quantization/test_quantization.py test/distributed/checkpoint/e2e/test_e2e_save_and_load.py test/distributed/checkpoint/e2e/test_fine_tuning.py test/distributed/checkpoint/test_checkpoint.py test/distributed/checkpoint/test_fsdp_tp_checkpoint_conversion.py test/distributed/checkpoint/test_hsdp_checkpoint.py test/distributed/checkpoint/test_nested_dict.py test/distributed/checkpoint/test_save_load_api.py test/distributed/checkpoint/test_state_dict.py test/distributed/checkpoint/test_state_dict_utils.py test/distributed/elastic/multiprocessing/api_test.py test/distributed/elastic/rendezvous/c10d_rendezvous_backend_test.py test/distributed/elastic/rendezvous/dynamic_rendezvous_test.py test/distributed/elastic/rendezvous/rendezvous_backend_test.py test/distributed/elastic/timer/file_based_local_timer_test.py test/distributed/fsdp/test_checkpoint_wrapper.py test/distributed/fsdp/test_fsdp_dtensor_state_dict.py test/distributed/fsdp/test_fsdp_fine_tune.py test/distributed/fsdp/test_fsdp_freezing_weights.py test/distributed/fsdp/test_fsdp_memory.py test/distributed/fsdp/test_fsdp_misc.py test/distributed/fsdp/test_fsdp_mixed_precision.py test/distributed/fsdp/test_fsdp_multiple_wrapping.py test/distributed/fsdp/test_fsdp_optim_state.py test/distributed/fsdp/test_fsdp_state_dict.py test/distributed/fsdp/test_fsdp_unshard_params.py test/distributed/fsdp/test_fsdp_use_orig_params.py test/distributed/fsdp/test_utils.py test/distributed/launcher/launch_test.py test/distributed/pipelining/model_registry.py test/distributed/pipelining/test_backward.py test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py test/distributed/pipelining/test_stage.py test/distributed/pipelining/test_unflatten.py test/distributed/tensor/parallel/test_micro_pipeline_tp.py test/distributed/tensor/parallel/test_tp_examples.py test/distributed/tensor/parallel/test_tp_style.py test/distributed/test_c10d_common.py test/distributed/test_c10d_functional_native.py test/distributed/test_c10d_gloo.py test/distributed/test_c10d_nccl.py test/distributed/test_c10d_ops_nccl.py test/distributed/test_c10d_ucc.py test/distributed/test_collective_utils.py test/distributed/test_data_parallel.py test/distributed/test_device_mesh.py test/distributed/test_dynamo_distributed.py test/distributed/test_functional_api.py test/distributed/test_inductor_collectives.py test/distributed/test_launcher.py test/distributed/test_store.py test/distributed/test_symmetric_memory.py test/distributions/test_distributions.py test/dynamo/test_activation_checkpointing.py test/dynamo/test_aot_autograd.py test/dynamo/test_aot_autograd_cache.py test/dynamo/test_autograd_function.py test/dynamo/test_backends.py test/dynamo/test_backward_higher_order_ops.py test/dynamo/test_bytecode_utils.py test/dynamo/test_compiler_bisector.py test/dynamo/test_comptime.py test/dynamo/test_ctx_manager.py test/dynamo/test_cudagraphs.py test/dynamo/test_debug_utils.py test/dynamo/test_decorators.py test/dynamo/test_exc.py test/dynamo/test_exceptions.py test/dynamo/test_export.py test/dynamo/test_frame_init.py test/dynamo/test_functions.py test/dynamo/test_graph_deduplication.py test/dynamo/test_graph_region_tracker.py test/dynamo/test_higher_order_ops.py test/dynamo/test_hooks.py test/dynamo/test_logging.py test/dynamo/test_misc.py test/dynamo/test_modes.py test/dynamo/test_modules.py test/dynamo/test_prim_hop_base.py test/dynamo/test_profiler.py test/dynamo/test_reorder_logs.py test/dynamo/test_repros.py test/dynamo/test_resume.py test/dynamo/test_structured_trace.py test/dynamo/test_subclasses.py test/dynamo/test_subgraphs.py,https://github.com/pytorch/pytorch/pull/134665,rec,albanD,,,
bb574abe73a,quantization,bc breaking,[BC-Breaking]Remove capture_pre_autograd_graph references in quantization (#139505),docs/source/quantization.rst test/quantization/pt2e/test_quantize_pt2e_qat.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantize_pt2e.py,https://github.com/pytorch/pytorch/pull/139505,yushangdi,andrewor14,jerryzh168,tugsbayasgalan,
b29fc52f827,skip,Untopiced,[Profiler] Add Optional Flag to turn off external correlations (#142516),test/profiler/test_profiler.py torch/csrc/autograd/profiler_kineto.cpp torch/csrc/profiler/collection.cpp torch/csrc/profiler/orchestration/observer.cpp torch/csrc/profiler/orchestration/observer.h torch/csrc/profiler/python/init.cpp,https://github.com/pytorch/pytorch/pull/142516,sraikund16,ngimel,,,
ad2faec8bbb,skip,not user facing,Add a pass which analyzes whether a prologue preserves zero mask (#142401),test/inductor/test_max_autotune.py torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/codegen/simd.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/142401,eellison,jansel,,,
f406207af2b,skip,Untopiced,"Revert ""[ROCm] Prune old gfx archs gfx900/gfx906 from binaries (#142827)""",.ci/docker/libtorch/build.sh .ci/docker/manywheel/build.sh,,,,,,
4e0de50eb55,skip,Untopiced,"Revert ""[CI] Add Triton 3.13t build (#143212)""",.github/workflows/build-triton-wheel.yml,,,,,,
8621b9ff0c0,skip,not user facing,Infer whether prologues can be computed without upcasting to fp32 without changing numerics (#142402),test/inductor/test_max_autotune.py torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/codegen/simd.py torch/_inductor/dtype_propagation.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/142402,eellison,jansel,,,
515abb77443,skip,not user facing,[CI] Add Triton 3.13t build (#143212),.github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/143212,malfet,atalman,clee2000,seemethere,
3bfdf6f0633,skip,not user facing,Exclude py 31.3t triton package from PyTorch 3.13t wheel (#143218),.circleci/scripts/binary_populate_env.sh,https://github.com/pytorch/pytorch/pull/143218,atalman,kit1980,,,
8741d72e3cc,skip,not user facing,move function before modifying it (#143202),torch/csrc/autograd/python_variable.cpp,https://github.com/pytorch/pytorch/pull/143202,albanD,ezyang,janeyx99,,
70be7900bb4,python_frontend,bug fixes,Fix Tensor clear to properly clear slots (#143203),test/test_torch.py torch/csrc/autograd/python_variable.cpp,https://github.com/pytorch/pytorch/pull/143203,albanD,colesbury,ezyang,,
d53164880f7,skip,not user facing,dont attempt to fuse in unaligned accesses to mm (#142435),test/inductor/test_max_autotune.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/142435,eellison,jansel,,,
00b02101390,skip,not user facing,[Inductor] Use sleef implementation for CPP backend asinh codegen (#142360),aten/src/ATen/cpu/vec/sve/vec_double.h aten/src/ATen/cpu/vec/sve/vec_float.h aten/src/ATen/cpu/vec/vec128/vec128_float_neon.h aten/src/ATen/cpu/vec/vec128/vec128_reduced_precision_common_neon.h aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h aten/src/ATen/cpu/vec/vec256/vec256_double.h aten/src/ATen/cpu/vec/vec256/vec256_float.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_double_vsx.h aten/src/ATen/cpu/vec/vec256/vsx/vec256_float_vsx.h aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h aten/src/ATen/cpu/vec/vec512/vec512_double.h aten/src/ATen/cpu/vec/vec512/vec512_float.h aten/src/ATen/cpu/vec/vec_base.h aten/src/ATen/cpu/vec/vec_n.h test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/142360,leslie-fang-intel,jgong5,,,
063194aa323,skip,not user facing,add additional CK BMM Instances (2) (#142874),aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_128_128x16x64_16x16_4x1_8x16x1_8x16x1_1x16x1x8_2_Intrawave_v2.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_128_16x128x64_16x16_1x4_8x16x1_8x16x1_1x16x1x8_4_Intrawave_v2.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_128_16x32x64_16x16_1x1_16x8x1_16x8x1_1x16x1x8_4_Intrawave_v1.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_128_16x32x64_16x16_1x1_32x4x1_32x4x1_1x16x1x8_4_Intrawave_v1.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_128_16x32x64_16x16_1x1_8x16x1_8x16x1_1x16x1x8_4_Intrawave_v2.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_128_16x64x64_16x16_1x2_8x16x1_8x16x1_1x16x1x8_4_Intrawave_v2.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_128_32x16x64_16x16_1x1_8x16x1_8x16x1_1x16x1x8_2_Intrawave_v1.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_128_32x16x64_16x16_1x1_8x16x1_8x16x1_1x16x1x8_2_Intrawave_v2.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_128_64x16x64_16x16_2x1_8x16x1_8x16x1_1x16x1x8_2_Intrawave_v2.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_bf16bf16bf16_256_128x128x64_32x32_2x2_8x32x1_8x32x1_1x16x1x16_4_Intrawave_v1.hip aten/src/ATen/native/hip/bgemm_kernels/bgemm_kernel_collection.h,https://github.com/pytorch/pytorch/pull/142874,bradleyhd,mxz297,,,
13233e062d3,cpp_frontend,bug fixes,Fix Apple Clang ICE when building with -march=armv8.6a (#142879),aten/src/ATen/cpu/vec/vec128/vec128_bfloat16_neon.h,https://github.com/pytorch/pytorch/pull/142879,swolchok,malfet,,,
12098ad2424,python_frontend,Untopiced,Add torch.cat tensors type promotion description (#141339),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/141339,zeshengzong,albanD,,,
86c3370bc36,benchmark,Untopiced,operator benchmark: write output to a JSON (#142809),benchmarks/operator_benchmark/benchmark_core.py benchmarks/operator_benchmark/benchmark_runner.py,https://github.com/pytorch/pytorch/pull/142809,apakbin,albanD,,,
bf8d4f5b7a6,skip,not user facing,[Inductor UT] Generalize device-bias code in test_triton_syntax.py. (#143178),test/inductor/test_triton_syntax.py,https://github.com/pytorch/pytorch/pull/143178,etaf,eellison,,,
19f35700006,releng,not user facing,[EZ] Remove `--pre` from numpy installation command (#143237),.ci/pytorch/build.sh,https://github.com/pytorch/pytorch/pull/143237,malfet,janeyx99,kit1980,,
cdc03f99b70,dynamo,not user facing,[ca] add graph id (#141906),torch/_dynamo/__init__.py torch/_dynamo/compiled_autograd.py,https://github.com/pytorch/pytorch/pull/141906,xmfan,jansel,,,
33dee721ae0,skip,not user facing,Reraise worker errors as runtime errors in more cases when the original exception can't be constructed (#140911),torch/_utils.py,https://github.com/pytorch/pytorch/pull/140911,ericphanson,vmoens,,,
e9f6045e80c,quantization,Untopiced,[15/N] Fix extra warnings brought by clang-tidy-17 (#143100),aten/src/ATen/functorch/BatchRulesBinaryOps.cpp aten/src/ATen/functorch/BatchRulesHelper.cpp aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp aten/src/ATen/native/quantized/FakeQuantPerChannelAffine.cpp aten/src/ATen/native/quantized/FakeQuantPerTensorAffine.cpp aten/src/ATen/native/quantized/cpu/conv_serialization.h aten/src/ATen/native/quantized/cpu/fused_obs_fake_quant.cpp torch/csrc/jit/python/pybind_utils.h torch/csrc/profiler/util.cpp,https://github.com/pytorch/pytorch/pull/143100,cyyever,Skylion007,,,
698eefadddb,releng,not user facing,[audio hash update] update the pinned audio hash (#143245),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/143245,pytorchupdatebot,pytorchbot,,,
24f24eebde9,dynamo,not user facing,Get rid of _lazy_import hack (#143213),torch/_dynamo/variables/base.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/143213,ezyang,albanD,aorenste,,
ca973069ed9,skip,not user facing,Update low prec codegen for div/mod (#142350),test/inductor/test_op_dtype_prop.py test/inductor/test_pattern_matcher.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/142350,eellison,blaine-rister,,,
23379e8933b,fx,not user facing,Add torch._compile to uninteresting files (#143209),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/143209,ezyang,albanD,,,
4764303cc66,skip,not user facing,Use static initialization to avoid once_flag in getCUDAHooks (#143198),aten/src/ATen/detail/CUDAHooksInterface.cpp,https://github.com/pytorch/pytorch/pull/143198,cyyever,albanD,,,
96c3b2c388c,python_frontend,new features,Expose remaining sharedMem cudaDeviceProps to python (#143226),torch/csrc/cuda/Module.cpp,https://github.com/pytorch/pytorch/pull/143226,peterbell10,ezyang,,,
c0a39ad35a2,linalg_frontend,Untopiced,[ROCm] Fix TunableOp UTs: Rotating Buffer (#143172),aten/src/ATen/cuda/tunable/README.md aten/src/ATen/cuda/tunable/Tunable.cpp docs/source/cuda.tunable.rst test/test_linalg.py torch/_C/__init__.pyi.in torch/csrc/cuda/Module.cpp torch/cuda/tunable.py,https://github.com/pytorch/pytorch/pull/143172,naromero77amd,jeffdaily,,,
3cc617b6a78,python_frontend,bug fixes,"`__cuda_array_interface__`: Use ""<V2"" for bfloat16. (#143042)",torch/_tensor.py,https://github.com/pytorch/pytorch/pull/143042,heiner,ezyang,,,
be5b3423324,inductor,not user facing,[Inductor] Move peak memory pass and overlap pass to be run at the right place (#142822),test/distributed/_composable/fsdp/test_fully_shard_compile.py torch/_inductor/comms.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/142822,yf225,eellison,,,
7c4d29485ee,skip,not user facing,Add typechecking indirection for Config (#143229),torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/143229,oulgen,aorenste,,,
28d8297712d,skip,not user facing,Migrate compiler config to Config (#143152),torch/compiler/config.py,https://github.com/pytorch/pytorch/pull/143152,oulgen,ezyang,,,
9933e59c2b1,cuda,Untopiced,[torch][cuda] fix race condition in cuda initialization (#143238),torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/143238,suo,ngimel,,,
de484134e47,export,Untopiced,support slicing with symints in non-strict (#143217),test/export/test_export.py torch/_export/non_strict_utils.py,https://github.com/pytorch/pytorch/pull/143217,avikchaudhuri,tugsbayasgalan,,,
91bf2e16deb,distributed,not user facing,[distributed] Remove unused variable in test_composability/test_pp_composability.py (#143191),test/distributed/_composable/test_composability/test_pp_composability.py,https://github.com/pytorch/pytorch/pull/143191,rec,mori360,,,
4273e1a0599,quantization,Untopiced,[5/N] Apply bugprone-unchecked-optional-access  (#143111),aten/src/ATen/core/dispatch/Dispatcher.cpp aten/src/ATen/core/function_schema.h aten/src/ATen/core/tensor_type.cpp aten/src/ATen/functorch/BatchRulesBinaryOps.cpp aten/src/ATen/functorch/BatchRulesConvolution.cpp aten/src/ATen/functorch/BatchRulesModules.cpp aten/src/ATen/functorch/BatchRulesRandomness.cpp aten/src/ATen/functorch/BatchRulesReduceOps.cpp aten/src/ATen/functorch/TensorWrapper.cpp aten/src/ATen/native/CPUFallback.cpp aten/src/ATen/native/cuda/FusedAdamKernel.cu aten/src/ATen/native/cuda/FusedAdamWKernel.cu aten/src/ATen/native/cuda/FusedSgdKernel.cu aten/src/ATen/native/nested/NestedTensorBackward.cpp aten/src/ATen/native/nested/NestedTensorMath.cpp aten/src/ATen/native/nested/NestedTensorMath.h aten/src/ATen/native/nested/NestedTensorUtils.cpp aten/src/ATen/native/nested/cuda/NestedTensorTransformerUtils.cpp aten/src/ATen/native/quantized/cpu/conv_serialization.h torch/csrc/api/src/optim/lbfgs.cpp,https://github.com/pytorch/pytorch/pull/143111,cyyever,Skylion007,,,
dd2d360b7d5,skip,not user facing,[ca] re-enable disabled tests (#143247),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/143247,xmfan,zou3519,,,
9ed045eae92,skip,Untopiced,"Revert ""[Profiler] Add Optional Flag to turn off external correlations (#142516)""",test/profiler/test_profiler.py torch/csrc/autograd/profiler_kineto.cpp torch/csrc/profiler/collection.cpp torch/csrc/profiler/orchestration/observer.cpp torch/csrc/profiler/orchestration/observer.h torch/csrc/profiler/python/init.cpp,,,,,,
5273d8fd2ab,releng,not user facing,[audio hash update] update the pinned audio hash (#143265),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/143265,pytorchupdatebot,pytorchbot,,,
e0bdae7884a,skip,not user facing,[AMD] Turn on TF32 for aten::mm (#139869),aten/src/ATen/Context.cpp aten/src/ATen/cuda/CUDABlas.cpp test/test_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/139869,xw285cornell,leitian,,,
744a303dee0,inductor,not user facing,[FlexAttention] Optimzing learned bias perf to dq calc (#142281),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/142281,drisspg,Chillee,,,
af8789c0565,skip,not user facing,Hide torch_python symbols (#142214),torch/CMakeLists.txt torch/csrc/Layout.h torch/csrc/MemoryFormat.h torch/csrc/Module.cpp torch/csrc/QScheme.h torch/csrc/Size.h torch/csrc/Storage.h torch/csrc/TypeInfo.h torch/csrc/autograd/python_function.h torch/csrc/lazy/ts_backend/ts_backend_impl.h torch/csrc/tensor/python_tensor.h torch/csrc/utils/device_lazy_init.h,https://github.com/pytorch/pytorch/pull/142214,cyyever,ezyang,,,
c1d4d9d3cfc,mps,improvements,[MPS] Support torch.accelerator.synchronize() on mps (#143171),test/test_mps.py torch/csrc/DeviceAccelerator.cpp torch/csrc/utils/device_lazy_init.h,https://github.com/pytorch/pytorch/pull/143171,guangyey,albanD,,,
45ac4ebf153,skip,not user facing,[RELAND] Add UTs for accelerator device-agnostic runtime APIs (#133572),test/test_accelerator.py test/test_cuda.py test/test_xpu.py,https://github.com/pytorch/pytorch/pull/133572,guangyey,EikanWang,albanD,,
9706ada369f,skip,Untopiced,[RELAND] Add device-agnostic runtime Device/Stream C++ API (#138677),aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h torch/csrc/DeviceAccelerator.cpp torch/csrc/utils/device_lazy_init.h,https://github.com/pytorch/pytorch/pull/138677,guangyey,EikanWang,albanD,,
d745b2b5163,distributed,not user facing,remove allow-untyped-defs for distributed/rpc/_testing/__init__.py (#143271),torch/distributed/rpc/_testing/__init__.py,https://github.com/pytorch/pytorch/pull/143271,bobrenjc93,aorenste,,,
e4d2e810869,skip,not user facing,Update slow tests (#143278),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/143278,pytorchupdatebot,pytorchbot,,,
a8cc19bb519,releng,not user facing,[CD] Fix XPU linux CD whl test failure (#143268),.circleci/scripts/binary_linux_test.sh,https://github.com/pytorch/pytorch/pull/143268,chuanqi129,atalman,,,
7ab3177776a,skip,Untopiced,"Revert ""[AMD] Turn on TF32 for aten::mm (#139869)""",aten/src/ATen/Context.cpp aten/src/ATen/cuda/CUDABlas.cpp test/test_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,,,,,,
ccf35af1421,inductor,not user facing,[Inductor] Fix the Index Put lowering with same input of self and values (#139366),test/inductor/test_cpu_repro.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/139366,leslie-fang-intel,eellison,jgong5,,
338835d0d2f,skip,Untopiced,Add support for other backends in get_preferred_device (#132118),torch/distributed/_shard/sharded_tensor/api.py,https://github.com/pytorch/pytorch/pull/132118,jeejakp12,kwen2501,,,
1bf983077f9,skip,not user facing,[reland][dynamo][guards] Consider tensors as immutable for dict tag matches (#141085),test/dynamo/test_modules.py torch/_dynamo/config.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/141085,anijain2305,jansel,,,
1b6b86fad72,dynamo,not user facing,[dynamo] disable eval frame callback around most of _TorchDynamoContext wrapper function (#143211),test/dynamo/test_repros.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/143211,williamwen42,jansel,,,
17b71e5d6a8,skip,not user facing,Add config alias (#142088),test/test_utils_config_module.py torch/testing/_internal/fake_config_module.py torch/testing/_internal/fake_config_module2.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/142088,oulgen,c00w,,,
e885225edaf,inductor,not user facing,Add persistent+TMA version of Triton mm and addmm (#142101),test/inductor/test_fp8.py test/inductor/test_max_autotune.py torch/_inductor/config.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/142101,aakhundov,drisspg,eellison,,
54ed13cdce0,skip,Untopiced,"Revert ""Update low prec codegen for div/mod (#142350)""",test/inductor/test_op_dtype_prop.py test/inductor/test_pattern_matcher.py torch/_inductor/codegen/triton.py,,,,,,
8ad842cda44,dataloader_frontend,not user facing,remove allow-untyped-defs for utils/data/datapipes/dataframe/structures.py (#143273),torch/utils/data/datapipes/dataframe/structures.py,https://github.com/pytorch/pytorch/pull/143273,bobrenjc93,aorenste,,,
678f74988df,onnx,not user facing,Fix a misspelling [ONNX] (#143301),torch/onnx/_internal/exporter/_compat.py,https://github.com/pytorch/pytorch/pull/143301,xadupre,titaiwangms,,,
159b7ad8aa6,inductor,Untopiced,Improve async workers to handle forking for async compile (#142072),torch/_inductor/async_compile.py torch/_inductor/compile_worker/__main__.py torch/_inductor/compile_worker/subproc_pool.py,https://github.com/pytorch/pytorch/pull/142072,aorenste,masnesral,,,
401b1498d2b,distributed,not user facing,[BE] typing for decorators - distributed/_tensor/ops/utils (#142139),torch/distributed/tensor/_ops/_conv_ops.py torch/distributed/tensor/_ops/_embedding_ops.py torch/distributed/tensor/_ops/_experimental_ops.py torch/distributed/tensor/_ops/_math_ops.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/_ops/_random_ops.py torch/distributed/tensor/_ops/_tensor_ops.py torch/distributed/tensor/_ops/_view_ops.py torch/distributed/tensor/_ops/utils.py,https://github.com/pytorch/pytorch/pull/142139,aorenste,Skylion007,kwen2501,,
4e594f4d123,releng,not user facing,"Triton bump for 3.2 cherry-picks (mmav3 segfault fix, gfx950 support) (#143302)",.ci/docker/ci_commit_pins/triton.txt,https://github.com/pytorch/pytorch/pull/143302,bertmaher,atalman,pruthvistony,,
c86383f956e,skip,not user facing,"[BE] Revert ""Add conda to Manylinux Docker images (#139903)"" (#143300)",.ci/docker/manywheel/Dockerfile .ci/docker/manywheel/Dockerfile_2_28,https://github.com/pytorch/pytorch/pull/143300,atalman,seemethere,,,
15aee8e090a,skip,not user facing,update aten bmm CK heuristic (#143294),aten/src/ATen/native/hip/ck_bgemm_bfloat16.hip,https://github.com/pytorch/pytorch/pull/143294,bradleyhd,mxz297,xw285cornell,,
135a2d44830,skip,not user facing,Update low prec codegen for div/mod (#142350),test/inductor/test_op_dtype_prop.py test/inductor/test_pattern_matcher.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/142350,eellison,blaine-rister,,,
6356690b3d2,skip,Untopiced,"Revert ""[BE] Revert ""Add conda to Manylinux Docker images (#139903)"" (#143300)""",.ci/docker/manywheel/Dockerfile .ci/docker/manywheel/Dockerfile_2_28,,,,,,
4c62275325a,skip,Untopiced,Kill capture_pre_autograd_graph API (#143224),torch/_export/__init__.py torch/_utils_internal.py torch/ao/quantization/pt2e/export_utils.py torch/ao/quantization/pt2e/qat_utils.py torch/ao/quantization/pt2e/representation/rewrite.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py,https://github.com/pytorch/pytorch/pull/143224,yushangdi,tugsbayasgalan,,,
557da8014db,autograd_frontend,not user facing,[gen_autograd_functions] rename some variables (#143166),tools/autograd/gen_autograd_functions.py,https://github.com/pytorch/pytorch/pull/143166,zou3519,soulitzer,,,
10df370a77d,skip,not user facing,Add missing IValue overloads for SymInt lists (#143167),aten/src/ATen/core/ivalue.h aten/src/ATen/core/ivalue_inl.h aten/src/ATen/test/ivalue_test.cpp,https://github.com/pytorch/pytorch/pull/143167,zou3519,ezyang,,,
ff373171d0c,profiler,improvements,[Profiler] Add Optional Flag to turn off external correlations v2 (#143314),test/profiler/test_profiler.py torch/csrc/autograd/profiler_kineto.cpp torch/csrc/profiler/collection.cpp torch/csrc/profiler/orchestration/observer.cpp torch/csrc/profiler/orchestration/observer.h torch/csrc/profiler/python/init.cpp,https://github.com/pytorch/pytorch/pull/143314,sraikund16,aaronenyeshi,leitian,,
0178e439497,skip,not user facing,remove allow-untyped-defs for torch/utils/_stats.py (#143319),torch/utils/_stats.py,https://github.com/pytorch/pytorch/pull/143319,bobrenjc93,aorenste,,,
c4ab3e6cebc,skip,not user facing,remove allow-untyped-defs for torch/__config__.py (#143320),torch/__config__.py,https://github.com/pytorch/pytorch/pull/143320,bobrenjc93,aorenste,,,
467970d6834,inductor,improvements,[AOTI] Relax input alignment assertion (#143236),test/inductor/test_aot_inductor.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/143236,desertfire,chenyang78,malfet,,
dd2cd4279e8,skip,not user facing,Create build_directory if it does not exist when generating ninja build file (#143328),torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/143328,atalman,albanD,kit1980,,
90fb7c36abe,distributed,Untopiced,[FSDP2] Clamp `reduce_dtype` in lazy init (#143297),test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py torch/distributed/fsdp/_fully_shard/_fsdp_api.py torch/distributed/fsdp/_fully_shard/_fsdp_param.py,https://github.com/pytorch/pytorch/pull/143297,awgu,weifengpy,,,
b3fb8f8a3a2,skip,Untopiced,FileTimerClient: add retry logic on connect (#143318),torch/distributed/elastic/timer/file_based_local_timer.py,https://github.com/pytorch/pytorch/pull/143318,d4l3k,fegin,,,
9d57a39541e,distributed,Untopiced,[C10D] Update docs for wait() (#143305),docs/source/distributed.rst,https://github.com/pytorch/pytorch/pull/143305,wconstab,LucasLLC,ngimel,,
519d858c31a,skip,Untopiced,"Revert ""Kill capture_pre_autograd_graph API (#143224)""",torch/_export/__init__.py torch/_utils_internal.py torch/ao/quantization/pt2e/export_utils.py torch/ao/quantization/pt2e/qat_utils.py torch/ao/quantization/pt2e/representation/rewrite.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py,,,,,,
6fae60a34a7,distributed,Untopiced,[SymmetricMemory] introduce multimem_all_gather (#142810),test/distributed/test_symmetric_memory.py torch/csrc/distributed/c10d/CUDASymmetricMemoryOps.cu torch/csrc/distributed/c10d/SymmetricMemory.cpp,https://github.com/pytorch/pytorch/pull/142810,yifuwang,weifengpy,,,
286921b39ea,distributed (pipeline),not user facing,[fused_all_gather_matmul] introduce an argument to specify whether the all-gather result needs to be returned (#143159),test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/143159,yifuwang,weifengpy,,,
af190479c8c,distributed,not user facing,[fused_all_gather_matmul] use _multimem_all_gather_matmul for small global Ms (#143160),test/distributed/test_symmetric_memory.py torch/_C/_distributed_c10d.pyi torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/143160,yifuwang,weifengpy,,,
201cb8834f8,distributed,Untopiced,Enable more C++ warnings (#143099),CMakeLists.txt caffe2/CMakeLists.txt torch/csrc/distributed/c10d/socket.cpp,https://github.com/pytorch/pytorch/pull/143099,cyyever,albanD,,,
533d63f83b4,skip,Untopiced,"Revert ""FileTimerClient: add retry logic on connect (#143318)""",torch/distributed/elastic/timer/file_based_local_timer.py,,,,,,
cb4c614ed69,skip,not user facing,[foreach-map] Add tests for backward (#143282),test/inductor/test_foreach.py,https://github.com/pytorch/pytorch/pull/143282,mlazos,eellison,,,
c15638d8035,releng,not user facing,Enable swap on all Linux jobs (#143316),.github/workflows/_linux-build.yml .github/workflows/_linux-test.yml,https://github.com/pytorch/pytorch/pull/143316,huydhn,ZainRizvi,atalman,,
500d02921bc,skip,not user facing,[ROCm] CK Flash Attention Backend (#138947),LICENSE aten/src/ATen/CMakeLists.txt aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/ROCmFABackend.h aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/hip/aotriton_adapter.h aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/bias.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/bwd_blob_list.txt aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_api.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_api.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fwd_blob_list.txt aten/src/ATen/native/transformers/hip/flash_attn/ck/mask.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/rotary.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck_kernel_blob_list.txt aten/src/ATen/native/transformers/hip/flash_attn/flash_api.h aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip aten/src/ATen/native/transformers/hip/flash_attn/flash_common_hip.hpp caffe2/CMakeLists.txt docs/source/backends.rst test/test_transformers.py tools/amd_build/build_amd.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/backends/cuda/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/138947,alugorey,jeffdaily,,,
bcd36921327,inductor,not user facing,[Inductor][Easy] Fix a test failure in loop_ordering_after_fusion (#142474),test/inductor/test_loop_ordering.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/142474,y-sq,eellison,shunting314,sijiac,
e7ec92331e1,jit,not user facing,remove allow-untyped-defs for torch/jit/_ir_utils.py (#143366),torch/jit/_ir_utils.py,https://github.com/pytorch/pytorch/pull/143366,bobrenjc93,aorenste,,,
d2ec7f07560,inductor,not user facing,[FlexAttention] Allow num_warps 8 since when block size >=128 (#143299),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/143299,drisspg,yanboliang,,,
a42ca5a45bd,inductor,not user facing,remove allow-untyped-defs for _inductor/codegen/rocm/rocm_template_buffer.py (#143272),torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py torch/_inductor/codegen/rocm/rocm_template_buffer.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/143272,bobrenjc93,aorenste,,,
297ce776363,skip,not user facing,[Inductor] inplace padding (#140249),test/inductor/test_inplace_padding.py test/inductor/test_torchinductor.py torch/_inductor/config.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/140249,shunting314,eellison,jansel,,
6056efc5ffb,skip,not user facing,non strict sequential slicing (#143298),test/export/test_export.py,https://github.com/pytorch/pytorch/pull/143298,avikchaudhuri,zhxchen17,,,
2531543c5fd,inductor,not user facing,[user triton cache] Dedup user-defined Triton kernels by config in codecache (#143353),test/inductor/test_codecache.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/143353,aakhundov,oulgen,,,
c3f3a6e4d25,composability,not user facing,"Back out ""Fix undesired specialization on slice after split. (#142372)"" (#143356)",test/inductor/test_torchinductor.py torch/_decomp/decompositions.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/143356,laithsakka,oulgen,,,
e2d47a133b9,cpp_frontend,improvements,Disable c10::optional macros (#138912),aten/src/ATen/native/mkldnn/xpu/detail/QConv.cpp aten/src/ATen/native/mkldnn/xpu/qconv.cpp c10/util/Optional.h torch/csrc/api/include/torch/types.h,https://github.com/pytorch/pytorch/pull/138912,r-barnes,Skylion007,malfet,,
cf46eb3bf5f,inductor,Untopiced,[inductor] Include types and size hints in MultiKernel cache key (#142349),torch/_inductor/codegen/multi_kernel.py,https://github.com/pytorch/pytorch/pull/142349,jansel,eellison,shunting314,,
5160a725c84,dynamo,not user facing,[FlexAttention] Fix broken eager tracing (#143344),test/inductor/test_flex_attention.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/flex_attention.py,https://github.com/pytorch/pytorch/pull/143344,drisspg,Chillee,,,
34a0d8b62e3,inductor,not user facing,[inductor] invalidate pointwise dep cache for LOAF (#141160),test/inductor/test_loop_ordering.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/141160,shunting314,vkuzo,,,
6bbbb084585,dynamo,not user facing,[Dynamo] Replace `torch._dynamo.optimize()` with `torch.compile()` [10/N] (#142451),test/dynamo/test_aot_autograd.py test/inductor/test_compiled_optimizers.py test/inductor/test_cpu_repro.py test/inductor/test_cuda_repro.py test/inductor/test_foreach.py test/inductor/test_inductor_freezing.py test/inductor/test_op_dtype_prop.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/142451,shink,bdhirsh,,,
340f02c49bd,skip,not user facing,make it clearer (in docs) one can double decorate with torch.library.impl_* APIs (#137608),torch/library.py,https://github.com/pytorch/pytorch/pull/137608,jackson-tsang578,zou3519,,,
afa313e669e,mps,Untopiced,Extend bmm tiling to work up to 2^32 elem in any single output dim (#143095),aten/src/ATen/native/mps/operations/LinearAlgebra.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/143095,jhavukainen,kulinseth,,,
792f1c47e93,fx,Untopiced,"No actual change, just remove variable contain Tensors from global scope (#143225)",test/jit/test_complexity.py test/test_cpp_api_parity.py test/test_expanded_weights.py test/test_fx_experimental.py test/test_jit.py test/test_nn.py torch/ao/quantization/pt2e/qat_utils.py torch/ao/quantization/pt2e/representation/rewrite.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py torch/testing/_internal/common_nn.py torch/testing/_internal/jit_metaprogramming_utils.py,https://github.com/pytorch/pytorch/pull/143225,albanD,ezyang,,,
4d90c487d83,skip,not user facing,[AOTI] Add is_big_gpu checking to test_conv3d (#143339),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/143339,desertfire,BoyuanFeng,,,
cd7de1f4fa1,skip,not user facing,remove allow-untyped-defs for torch/masked/maskedtensor/creation.py (#143321),torch/masked/maskedtensor/creation.py,https://github.com/pytorch/pytorch/pull/143321,bobrenjc93,laithsakka,,,
969b07b96f4,skip,Untopiced,"Revert ""[ROCm] CK Flash Attention Backend (#138947)""",LICENSE aten/src/ATen/CMakeLists.txt aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/ROCmFABackend.h aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/hip/aotriton_adapter.h aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/bias.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/bwd_blob_list.txt aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_api.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_bf16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d128_fp16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_bf16_b64x64_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d256_fp16_b64x64_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_bf16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d32_fp16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_bf16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_pd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_batch_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_convert_dq_d64_fp16_b64x128_group_o2_psd_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_bf16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_batch_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d128_fp16_group_b16x128x128x16x128x16x32x128x128_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_bf16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_batch_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d256_fp16_group_b16x64x256x16x256x16x32x256x256_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_bf16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_batch_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d32_fp16_group_b32x128x32x32x32x32x64x32x32_r1x4x1_r4x1x1_r2x2x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_bf16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_ps_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_psk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_pskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_batch_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_iglp_pssk_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_alibi_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_d64_fp16_group_b32x128x64x32x64x32x32x64x64_r1x4x1_r4x1x1_r1x4x1_w16x16x32_w16x16x16_o1_kr_ktr_vr_psskddv_mask_dropout_wg16_deterministic.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_bf16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d128_fp16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_bf16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d256_fp16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_bf16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d32_fp16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_bf16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_batch_o2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_batch_o2_pdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_batch_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_batch_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_group_o2_ps.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd_dot_do_o_d64_fp16_group_o2_psdv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_api.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_bf16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_batch_shb_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d128_fp16_group_hbs_b128x128x32x128x32x128_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_bf16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_batch_shb_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d256_fp16_group_hbs_b128x128x32x256x32x256_r4x1x1_r4x1x1_w32x32x16_qr_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_bf16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_batch_shb_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d32_fp16_group_hbs_b128x64x16x32x32x32_r2x1x1_r2x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_bf16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_batch_shb_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_alibi_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd_d64_fp16_group_hbs_b128x64x32x64x32x64_r4x1x1_r4x1x1_w32x32x16_qr_async_vr_psskddv_mask_lse_dropout.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fwd_blob_list.txt aten/src/ATen/native/transformers/hip/flash_attn/ck/mask.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/rotary.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck_kernel_blob_list.txt aten/src/ATen/native/transformers/hip/flash_attn/flash_api.h aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip aten/src/ATen/native/transformers/hip/flash_attn/flash_common_hip.hpp caffe2/CMakeLists.txt docs/source/backends.rst test/test_transformers.py tools/amd_build/build_amd.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/backends/cuda/__init__.py torch/csrc/Module.cpp,,,,,,
487343346e2,dynamo,not user facing,Prevent users from seeing hardcoded print stmt when hypothesis is not installed (#142398),torch/_dynamo/variables/user_defined.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/142398,guilhermeleobas,zou3519,,,
313b9964ae0,skip,not user facing,remove allow-untyped-defs for torch/_C/_lazy.pyi (#143370),torch/_C/_lazy.pyi,https://github.com/pytorch/pytorch/pull/143370,bobrenjc93,aorenste,desertfire,,
ec02ae43451,skip,not user facing,remove allow-untyped-defs for torch/utils/benchmark/examples/simple_timeit.py (#143368),torch/utils/benchmark/examples/simple_timeit.py,https://github.com/pytorch/pytorch/pull/143368,bobrenjc93,aorenste,,,
e3d754419f7,skip,Untopiced,"Revert ""[reland][dynamo][guards] Consider tensors as immutable for dict tag matches (#141085)""",test/dynamo/test_modules.py torch/_dynamo/config.py torch/csrc/dynamo/guards.cpp,,,,,,
bb06fc79fb5,inductor,not user facing,"cpp_builder: handle CUDA lib paths involving ""stubs"" in more circumstances (#142175)",torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/142175,benjaminglass1,desertfire,,,
37a1b9efcc3,export,Untopiced,[export] Serialize all dataclass fields (#142286),torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/142286,benjaminglass1,zhxchen17,,,
74e66a21b42,distributed,not user facing,remove allow-untyped-defs for torch/_C/_distributed_autograd.pyi (#143369),torch/_C/_distributed_autograd.pyi,https://github.com/pytorch/pytorch/pull/143369,bobrenjc93,aorenste,,,
fa4db62968c,releng,not user facing,[CI] Unify the XPU Windows CICD installtion scripts (#143185),.ci/pytorch/win-test-helpers/build_pytorch.bat .ci/pytorch/win-test-helpers/installation-helpers/install_xpu.bat,https://github.com/pytorch/pytorch/pull/143185,chuanqi129,atalman,,,
a3688ead4bc,inductor,docs,[AOTI][doc] Update tutorial (#143390),docs/source/torch.compiler_aot_inductor.rst,https://github.com/pytorch/pytorch/pull/143390,desertfire,yushangdi,,,
bcc93a1e8e6,inductor,not user facing,remove nonowninglayout special case in require strides (#143315),test/inductor/test_cpu_repro.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/143315,eellison,zou3519,,,
53e4d7b6a22,skip,not user facing,remove allow-untyped-defs for torch/_lazy/device_context.py (#143367),torch/_lazy/closure.py torch/_lazy/device_context.py,https://github.com/pytorch/pytorch/pull/143367,bobrenjc93,aorenste,,,
97ca09f6923,dynamo,not user facing,[dynamo] format eval_frame.c (#142117),torch/csrc/dynamo/eval_frame.c,https://github.com/pytorch/pytorch/pull/142117,williamwen42,jansel,,,
c04f0bb7b95,dynamo,not user facing,[dynamo] add benchmark for guard eval (#142430),benchmarks/dynamo/microbenchmarks/dynamo_guard_eval.py,https://github.com/pytorch/pytorch/pull/142430,williamwen42,jansel,,,
18261e9f395,dynamo,not user facing,[dynamo] implement framelocals mapping as c++ object (#140063),test/dynamo/test_guard_manager.py torch/_C/_dynamo/eval_frame.pyi torch/_C/_dynamo/guards.pyi torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/guards.py torch/_dynamo/resume_execution.py torch/_dynamo/testing.py torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/extra_state.cpp torch/csrc/dynamo/extra_state.h torch/csrc/dynamo/framelocals_mapping.cpp torch/csrc/dynamo/framelocals_mapping.h torch/csrc/dynamo/guards.cpp torch/csrc/dynamo/guards.h torch/csrc/dynamo/init.cpp,https://github.com/pytorch/pytorch/pull/140063,williamwen42,jansel,,,
1e9ec514311,skip,not user facing,Fix unused variables in test_serialize_sym_float (#143389),test/export/test_serialize.py,https://github.com/pytorch/pytorch/pull/143389,rec,Skylion007,,,
f3ec59d44c7,inductor,not user facing,Fix non-dense inductor effn attn bias (#141905),test/inductor/test_cuda_repro.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/141905,eellison,drisspg,,,
63cb5e4ade3,fx,Untopiced,Move inner loop of _create_symbolic_sizes_strides_storage_offset into its own method (#138843),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/138843,aorenste,ezyang,,,
e7704f41ca8,fx,Untopiced,Simplify _compute_symbolic_stride() (#138844),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/138844,aorenste,bobrenjc93,,,
5b5d7016c89,skip,not user facing,Remove stable_partition for ARM AOTI Runtimes (#142394),torch/csrc/inductor/aoti_runtime/model_container.h,https://github.com/pytorch/pytorch/pull/142394,oniononion36,frank-wei,,,
e3c53fb1bc1,releng,not user facing,Increase sharding for debug build (#143327),.github/workflows/periodic.yml,https://github.com/pytorch/pytorch/pull/143327,clee2000,huydhn,wdvr,,
b16f020edd2,inductor,not user facing,Add flex attention kernel parameter tuning options (#139639),torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/139639,joydddd,drisspg,,,
de4a555c82f,releng,not user facing,Run inductor-rocm workflow on ciflow/inductor (#143205),.github/workflows/inductor-rocm.yml,https://github.com/pytorch/pytorch/pull/143205,clee2000,huydhn,,,
7c25a55c658,jit,not user facing,clean up type nits on torch/jit/_ir_utils.py (#143371),torch/jit/_ir_utils.py,https://github.com/pytorch/pytorch/pull/143371,bobrenjc93,laithsakka,,,
9283c40ba8e,skip,not user facing,[codemod] Decorate unused variables with `[[maybe_unused]]` (#143381),aten/src/ATen/test/cpu_caching_allocator_test.cpp aten/src/ATen/test/tensor_interop_test.cpp torch/csrc/cuda/nccl.cpp,https://github.com/pytorch/pytorch/pull/143381,r-barnes,malfet,,,
a96387a4816,skip,Untopiced,[Dynamo] only import einops if version is lower than 0.7.0 (#142847),torch/_dynamo/decorators.py,https://github.com/pytorch/pytorch/pull/142847,mlazos,zou3519,,,
0bdc173ab6d,distributed,not user facing,[fr] recognize all_reduce_barrier as a valid op (#143354),test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/components/types.py,https://github.com/pytorch/pytorch/pull/143354,c-p-i-o,wconstab,,,
aabe285aaf9,skip,not user facing,Add 2 more APIs to the exposed public torch python APIs (#143380),torch/csrc/DynamicTypes.h torch/csrc/autograd/python_cpp_function.h torch/csrc/utils/python_arg_parser.h,https://github.com/pytorch/pytorch/pull/143380,manav-a,suo,,,
1e058a8f389,skip,Untopiced,FileTimerClient: add retry logic on connect (#143318),torch/distributed/elastic/timer/file_based_local_timer.py,https://github.com/pytorch/pytorch/pull/143318,d4l3k,fegin,,,
b247f878451,skip,not user facing,tools: Add a tool to build wheels for multiple python versions (#143361),tools/packaging/build_wheel.py,https://github.com/pytorch/pytorch/pull/143361,seemethere,atalman,malfet,,
2642bbc6dce,releng,improvements,[CD] Run smoke tests on MacOS wheel (#143393),.ci/wheel/build_wheel.sh,https://github.com/pytorch/pytorch/pull/143393,malfet,atalman,seemethere,,
a040006da76,releng,not user facing,Force symlink creation when building python on s390x (#143195),.ci/docker/common/install_cpython.sh,https://github.com/pytorch/pytorch/pull/143195,AlekseiNikiforovIBM,ezyang,,,
efe21ee59df,skip,not user facing,[MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage (#143347),docs/source/mtia.memory.rst docs/source/mtia.rst torch/mtia/memory.py,https://github.com/pytorch/pytorch/pull/143347,chaos5958,nautsimon,,,
feb4818bc9c,distributed,Untopiced,[SJD] adding kill logic for current process when killing a worker (#141060),torch/distributed/elastic/timer/file_based_local_timer.py,https://github.com/pytorch/pytorch/pull/141060,felixsu2006,gag1jain,,,
3f99682fbdb,skip,not user facing,NJT linear_backward should not return inner tensor as-is (#143333),torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/143333,soulitzer,jbschlosser,,,
13a5c15ef56,skip,not user facing,Fix sample inputs leaked from subtest (#143415),torch/testing/_internal/opinfo/core.py,https://github.com/pytorch/pytorch/pull/143415,soulitzer,jbschlosser,,,
ac8342f8817,python_frontend,security,Prevent torch.jit.load path in torch.load when weights_only=True (#143326),test/test_serialization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/143326,mikaylagawarecki,albanD,,,
6cd96f069b3,jit,docs,Add warning to torch.jit.load (#143403),torch/jit/_serialization.py,https://github.com/pytorch/pytorch/pull/143403,mikaylagawarecki,albanD,,,
c06b5048ba8,inductor,Untopiced,[Inductor] Fix _can_be_inplace function (#143279),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/mkldnn_fusion.py,https://github.com/pytorch/pytorch/pull/143279,jiayisunx,jansel,jgong5,leslie-fang-intel,
e890d675438,inductor,not user facing,Use process pool for precompilation of triton templates (#142450),torch/_inductor/async_compile.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/142450,eellison,jansel,,,
2400db115c7,releng,not user facing,Use Manylinux 2.28 for nightly build and cxx11-abi (#143423),.github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/143423,atalman,huydhn,malfet,seemethere,
a9de6a68f4a,releng,not user facing,[CD] Test that all PyTorch wheels support OpenMP (#143394),.ci/pytorch/smoke_test/smoke_test.py,https://github.com/pytorch/pytorch/pull/143394,malfet,atalman,,,
17a6d4b8829,export,not user facing,remove allow-untyped-defs for torch/_export/passes/remove_runtime_assertions.py (#143435),torch/_export/passes/remove_runtime_assertions.py,https://github.com/pytorch/pytorch/pull/143435,bobrenjc93,oulgen,,,
c947a7d38ec,skip,not user facing,Fix unused Python variables in test/nn (#143396),test/nn/test_convolution.py test/nn/test_init.py test/nn/test_module_hooks.py test/nn/test_packed_sequence.py test/nn/test_parametrization.py test/nn/test_pooling.py test/nn/test_pruning.py,https://github.com/pytorch/pytorch/pull/143396,rec,mikaylagawarecki,,,
859be14c4e9,cuda,Untopiced,"fix a few int64_t index computations, fix complex128 scan that had to… (#143401)",aten/src/ATen/cuda/cub.cuh test/test_torch.py,https://github.com/pytorch/pytorch/pull/143401,ngimel,eqy,,,
576789197af,python_frontend,Untopiced,Add support for CPU scalar in addcmul (#143264),aten/src/ATen/native/PointwiseOps.cpp aten/src/ATen/native/cuda/PointwiseOpsKernel.cu test/test_torch.py,https://github.com/pytorch/pytorch/pull/143264,EmmettBicker,janeyx99,,,
c17a07ade34,export,Untopiced,Add float8 support in serde schema (#143343),test/export/test_serialize.py torch/_export/serde/export_schema.thrift torch/_export/serde/schema.py torch/_export/serde/schema.yaml torch/_export/serde/schema_check.py torch/_export/serde/serialize.py torch/csrc/utils/generated_serialization_types.h,https://github.com/pytorch/pytorch/pull/143343,yushangdi,yiming0416,,,
6715a8858af,releng,not user facing,Triton bump for 3.2 cherry-picks (device context) (#143409),.ci/docker/ci_commit_pins/triton.txt,https://github.com/pytorch/pytorch/pull/143409,bertmaher,atalman,,,
6829897682b,fx,Untopiced,Remove assert from partitioner.py (#143376),torch/fx/passes/infra/partitioner.py,https://github.com/pytorch/pytorch/pull/143376,digantdesai,tarun292,,,
9275091d6e8,inductor,not user facing,[provenance_tracking] Dump inductor_triton_kernel_to_post_grad_nodes.json info in debug_trace (#143055),test/inductor/test_provenance_tracing.py torch/_inductor/codegen/simd.py torch/_inductor/config.py torch/_inductor/debug.py torch/_inductor/graph.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143055,YUNQIUGUO,chenyang78,,,
90cc43f270c,dynamo,not user facing,Support garbage collection after pt2 compilation (#143364),test/dynamo/test_utils.py torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/143364,qiurc,ezyang,,,
bceedeec2bf,export,Untopiced,fix checking non-trivial input constraints (#143442),test/export/test_export.py torch/_export/utils.py,https://github.com/pytorch/pytorch/pull/143442,avikchaudhuri,tugsbayasgalan,,,
2ea4b56ec87,skip,not user facing,Record min/max of integral tensor in ET (#143088),test/profiler/test_execution_trace.py torch/csrc/profiler/standalone/execution_trace_observer.cpp,https://github.com/pytorch/pytorch/pull/143088,shengfukevin,sanrise,,,
a99536480db,skip,not user facing,[ATen][Native][Special] Hermite polynomial prematurely return NaN if n is high (#141955),aten/src/ATen/native/Math.h aten/src/ATen/native/cuda/Math.cuh torch/testing/_internal/opinfo/definitions/special.py,https://github.com/pytorch/pytorch/pull/141955,Aidyn-A,eqy,malfet,,
03991798ca2,skip,not user facing,remove allow-untyped-defs for torch/nn/parallel/__init__.py (#143437),torch/nn/parallel/__init__.py,https://github.com/pytorch/pytorch/pull/143437,bobrenjc93,oulgen,,,
75fe5a3ef73,fx,not user facing,remove allow-untyped-defs for torch/fx/experimental/debug.py (#143439),torch/fx/experimental/debug.py,https://github.com/pytorch/pytorch/pull/143439,bobrenjc93,oulgen,,,
8dd380803c0,skip,not user facing,remove allow-untyped-defs for torch/_functorch/batch_norm_replacement.py (#143438),torch/_functorch/batch_norm_replacement.py,https://github.com/pytorch/pytorch/pull/143438,bobrenjc93,oulgen,,,
993b2f0ee06,skip,not user facing,Fix unused variables in test/test_transformers.py (#143407),test/test_transformers.py,https://github.com/pytorch/pytorch/pull/143407,rec,drisspg,,,
eb67dd3e2d1,skip,not user facing,[3/N][Memory Profiling] Add memory profiling function for MTIA hooks (#142149),aten/src/ATen/detail/MTIAHooksInterface.h docs/source/mtia.rst torch/_C/__init__.pyi.in torch/csrc/mtia/Module.cpp torch/mtia/__init__.py,https://github.com/pytorch/pytorch/pull/142149,mzzchy,nautsimon,,,
d8ea4ce6312,releng,Untopiced,[reland] Kill capture_pre_autograd_graph API (#143426),.github/ci_commit_pins/xla.txt torch/_export/__init__.py torch/_utils_internal.py torch/ao/quantization/pt2e/export_utils.py torch/ao/quantization/pt2e/qat_utils.py torch/ao/quantization/pt2e/representation/rewrite.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py,https://github.com/pytorch/pytorch/pull/143426,yushangdi,gmagogsfm,,,
84b91ce4a1c,inductor,not user facing,remove allow-untyped-defs for torch/_inductor/test_operators.py (#143436),torch/_inductor/test_operators.py,https://github.com/pytorch/pytorch/pull/143436,bobrenjc93,aorenste,,,
80a42399bb0,skip,not user facing,Various fix for memory leak in test autograd and dataloader (#143323),test/test_autograd.py test/test_utils.py,https://github.com/pytorch/pytorch/pull/143323,albanD,andrewkho,soulitzer,,
f47aac6bc2f,mps,not user facing,Make Context to be Device-agnostic Step by Step (3/N) (#137578),aten/src/ATen/core/GeneratorForPrivateuseone.cpp aten/src/ATen/core/GeneratorForPrivateuseone.h aten/src/ATen/cuda/detail/CUDAHooks.cpp aten/src/ATen/cuda/detail/CUDAHooks.h aten/src/ATen/detail/CUDAHooksInterface.h aten/src/ATen/detail/MPSHooksInterface.h aten/src/ATen/detail/PrivateUse1HooksInterface.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/mps/MPSHooks.mm test/cpp_extensions/open_registration_extension.cpp test/inductor/extension_backends/cpp/extension_device.cpp test/test_cpp_extensions_open_device_registration.py torch/csrc/Generator.cpp,https://github.com/pytorch/pytorch/pull/137578,FFFrog,cyyever,ezyang,,
24a18d76c86,mps,bug fixes,[MPS] Use metal shaders for all view ops (#143375),aten/src/ATen/mps/IndexKernels.h aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/operations/View.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/143375,malfet,albanD,,,
a2092665a94,inductor,not user facing,[AOTI] Refactor path operations in AotCodeCompiler (#143350),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/143350,desertfire,chenyang78,yushangdi,,
0e8013fc1c8,inductor,not user facing,[AOTI] Fix a typo in cpp_builder.py (#143351),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/143351,desertfire,chenyang78,yushangdi,,
ed9931e6eec,skip,not user facing,Add tests for non divisible inputs for flex decoding (#143214),test/inductor/test_flex_decoding.py,https://github.com/pytorch/pytorch/pull/143214,joydddd,drisspg,,,
93e8e327089,skip,not user facing,Remove iOS folder (#143398),ios/.gitignore ios/LibTorch-Lite-Nightly.podspec.template ios/LibTorch-Lite.h ios/LibTorch-Lite.podspec.template ios/LibTorch.h ios/LibTorch.podspec.template ios/METADATA.bzl ios/README.md ios/TestApp/.clang-format ios/TestApp/.gitignore ios/TestApp/Gemfile ios/TestApp/Gemfile.lock ios/TestApp/METADATA.bzl ios/TestApp/README.md ios/TestApp/TestApp.xcodeproj/project.pbxproj ios/TestApp/TestApp.xcodeproj/xcshareddata/xcschemes/TestApp.xcscheme ios/TestApp/TestApp.xcodeproj/xcshareddata/xcschemes/TestAppTests.xcscheme ios/TestApp/TestApp/AppDelegate.h ios/TestApp/TestApp/AppDelegate.m ios/TestApp/TestApp/Assets.xcassets/AppIcon.appiconset/Contents.json ios/TestApp/TestApp/Assets.xcassets/Contents.json ios/TestApp/TestApp/Base.lproj/LaunchScreen.storyboard ios/TestApp/TestApp/Base.lproj/Main.storyboard ios/TestApp/TestApp/Benchmark.h ios/TestApp/TestApp/Benchmark.mm ios/TestApp/TestApp/Info.plist ios/TestApp/TestApp/ViewController.h ios/TestApp/TestApp/ViewController.mm ios/TestApp/TestApp/main.m ios/TestApp/TestAppTests/Info.plist ios/TestApp/TestAppTests/TestFullJIT.mm ios/TestApp/TestAppTests/TestLiteInterpreter.mm ios/TestApp/benchmark/config.json ios/TestApp/benchmark/coreml_backend.py ios/TestApp/benchmark/setup.rb ios/TestApp/benchmark/trace_model.py ios/TestApp/custom_build/custom_build.py ios/TestApp/custom_build/mobilenetv2.yaml ios/TestApp/fastlane/Fastfile ios/TestApp/fastlane/Scanfile ios/TestApp/models/activation_ops.ptl ios/TestApp/models/android_api_module.ptl ios/TestApp/models/blas_lapack_ops.ptl ios/TestApp/models/comparison_ops.ptl ios/TestApp/models/convolution_ops.ptl ios/TestApp/models/distance_function_ops.ptl ios/TestApp/models/dropout_ops.ptl ios/TestApp/models/dynamic_quant_ops.ptl ios/TestApp/models/fused_quant_ops.ptl ios/TestApp/models/general_quant_ops.ptl ios/TestApp/models/linear_ops.ptl ios/TestApp/models/loss_function_ops.ptl ios/TestApp/models/mobilenet_v2.ptl ios/TestApp/models/model_coreml.ptl ios/TestApp/models/model_lite.ptl ios/TestApp/models/nn_utils_ops.ptl ios/TestApp/models/normalization_ops.ptl ios/TestApp/models/other_math_ops.ptl ios/TestApp/models/padding_ops.ptl ios/TestApp/models/pointwise_ops.ptl ios/TestApp/models/pooling_ops.ptl ios/TestApp/models/recurrent_ops.ptl ios/TestApp/models/reduction_ops.ptl ios/TestApp/models/sampling_ops.ptl ios/TestApp/models/shuffle_ops.ptl ios/TestApp/models/sparse_ops.ptl ios/TestApp/models/spectral_ops.ptl ios/TestApp/models/static_quant_ops.ptl ios/TestApp/models/tensor_creation_ops.ptl ios/TestApp/models/tensor_general_ops.ptl ios/TestApp/models/tensor_indexing_ops.ptl ios/TestApp/models/tensor_typing_ops.ptl ios/TestApp/models/tensor_view_ops.ptl ios/TestApp/models/torchscript_builtin_ops.ptl ios/TestApp/models/torchscript_collection_ops.ptl ios/TestApp/models/transformer_ops.ptl ios/TestApp/models/vision_function_ops.ptl ios/TestApp/run_on_aws_devicefarm.py,https://github.com/pytorch/pytorch/pull/143398,msaroufim,albanD,,,
b588a78ca3f,skip,not user facing,add grad_output shape check for adaptive_max_pool2d_backward and adaptive_max_pool3d_backward (#141663),aten/src/ATen/native/AdaptiveMaxPooling2d.cpp aten/src/ATen/native/AdaptiveMaxPooling3d.cpp test/nn/test_pooling.py,https://github.com/pytorch/pytorch/pull/141663,jiayisunx,malfet,mingfeima,,
863e6e4567e,nn_frontend,not user facing,"Improve input dimensions check for reflection_pad1d, reflection_pad2d and reflection_pad3d (#141670)",aten/src/ATen/native/Padding.h test/test_nn.py,https://github.com/pytorch/pytorch/pull/141670,jiayisunx,malfet,mingfeima,,
8d4926e30a9,skip,not user facing,Fix unused variables in test/torch.py (#143399),test/test_torch.py,https://github.com/pytorch/pytorch/pull/143399,rec,albanD,,,
f129bcb5a5b,releng,not user facing,[BE] Refactor argument parsing into its own function (#143395),.ci/pytorch/smoke_test/smoke_test.py,https://github.com/pytorch/pytorch/pull/143395,malfet,atalman,kit1980,seemethere,
255a9774947,skip,not user facing,[1/N] Avoid const_cast (#143169),aten/src/ATen/cuda/jiterator.cu aten/src/ATen/native/Blas.cpp aten/src/ATen/native/BlasKernel.cpp aten/src/ATen/native/EmbeddingBag.cpp aten/src/ATen/native/cuda/CUDAJitLoops.cuh aten/src/ATen/native/cuda/Reduce.cuh aten/src/ATen/native/cuda/jit_utils.cpp aten/src/ATen/native/cuda/jit_utils.h,https://github.com/pytorch/pytorch/pull/143169,cyyever,albanD,,,
894d47b91b9,linalg_frontend,not user facing,[ROCm] Fix unit test: matmul_offline_tunableop (#143322),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/143322,naromero77amd,jeffdaily,,,
1e201422edd,dynamo,Untopiced,[export] add is_exporting flag (#142425),docs/source/torch.compiler_api.rst docs/source/torch.compiler_fine_grain_apis.rst test/export/test_export.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/torch.py torch/_export/utils.py torch/compiler/__init__.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/142425,ydwu4,avikchaudhuri,,,
10b9c5944e8,skip,not user facing,[export] don't decompose custom triton op when exporting (#142426),test/export/test_export.py torch/_library/triton.py,https://github.com/pytorch/pytorch/pull/142426,ydwu4,zou3519,,,
d4ed5941db0,jit,Untopiced,Fix floating point literals in IRPrinter (#142119),test/cpp/tensorexpr/test_ir_printer.cpp torch/csrc/jit/tensorexpr/ir_printer.cpp,https://github.com/pytorch/pytorch/pull/142119,prm-james-hill,jgong5,malfet,,
4717cd1ce9e,skip,not user facing,Skip test_conv2d_linear_add_broadcast_shapes_cpu on fbcode (#143530),test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/143530,huydhn,jansel,,,
d3ff2d42c28,skip,Untopiced,[ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend (#134124),.gitmodules BUILD.bazel CMakeLists.txt WORKSPACE aten/src/ATen/CMakeLists.txt aten/src/ATen/Config.h.in aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cpu/int_mm_kernel.h aten/src/ATen/native/kleidiai/kai_kernels.cpp aten/src/ATen/native/kleidiai/kai_kernels.h aten/src/ATen/native/kleidiai/kai_pack.h aten/src/ATen/native/kleidiai/kai_ukernel_interface.cpp aten/src/ATen/native/kleidiai/kai_ukernel_interface.h aten/src/ATen/native/native_functions.yaml buckbuild.bzl cmake/Dependencies.cmake cmake/Summary.cmake cmake/TorchConfig.cmake.in docs/source/backends.rst setup.py test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_torchinductor.py test/test_linalg.py third_party/kleidiai torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/quantized_lowerings.py torch/_meta_registrations.py torch/backends/__init__.py torch/backends/kleidiai/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_quantization.py torchgen/aoti/fallback_ops.py,https://github.com/pytorch/pytorch/pull/134124,nikhil-arm,digantdesai,malfet,,
d298bd840f1,dynamo,not user facing,[dynamo] add two-point iter test (#143500),test/dynamo/test_functions.py,https://github.com/pytorch/pytorch/pull/143500,williamwen42,StrongerXi,,,
d8c8ba24404,quantization,not user facing,Fix unused Python variables in test/[e-z]* (#136964),test/export/opinfo_schema.py test/export/test_converter.py test/export/test_draft_export.py test/export/test_experimental.py test/export/test_export.py test/export/test_passes.py test/export/test_torchbind.py test/export/test_unflatten.py test/functorch/discover_coverage.py test/functorch/test_ac.py test/functorch/test_aotdispatch.py test/functorch/test_control_flow.py test/functorch/test_dims.py test/functorch/test_eager_transforms.py test/functorch/test_ops.py test/functorch/test_vmap.py test/fx/quantization.py test/fx/test_cse_pass.py test/fx/test_dce_pass.py test/fx/test_fx_split.py test/fx/test_gradual_type.py test/fx/test_matcher_utils.py test/fx/test_subgraph_rewriter.py test/fx/test_z3_gradual_types.py test/higher_order_ops/test_invoke_subgraph.py test/higher_order_ops/test_with_effects.py test/inductor/s429861_repro.py test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_arrayref.py test/inductor/test_aot_inductor_custom_ops.py test/inductor/test_aot_inductor_package.py test/inductor/test_autoheuristic.py test/inductor/test_codecache.py test/inductor/test_compiled_autograd.py test/inductor/test_compiled_optimizers.py test/inductor/test_cpu_repro.py test/inductor/test_cuda_repro.py test/inductor/test_cudacodecache.py test/inductor/test_cudagraph_trees.py test/inductor/test_cutlass_backend.py test/inductor/test_decompose_mem_bound_mm.py test/inductor/test_dependencies.py test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py test/inductor/test_fp8.py test/inductor/test_group_batch_fusion.py test/inductor/test_indexing.py test/inductor/test_inplacing_pass.py test/inductor/test_kernel_benchmark.py test/inductor/test_layout_optim.py test/inductor/test_loop_ordering.py test/inductor/test_max_autotune.py test/inductor/test_move_constructors_to_cuda.py test/inductor/test_multi_kernel.py test/inductor/test_ordered_set.py test/inductor/test_pad_mm.py test/inductor/test_pattern_matcher.py test/inductor/test_perf.py test/inductor/test_profiler.py test/inductor/test_smoke.py test/inductor/test_split_cat_fx_passes.py test/inductor/test_standalone_compile.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_opinfo.py test/inductor/test_torchinductor_strided_blocks.py test/inductor/test_triton_heuristics.py test/inductor/test_triton_kernels.py test/inductor/test_triton_wrapper.py test/inductor/test_unbacked_symints.py test/jit/test_async.py test/jit/test_autodiff.py test/jit/test_autodiff_subgraph_slicing.py test/jit/test_await.py test/jit/test_backends.py test/jit/test_builtins.py test/jit/test_class_type.py test/jit/test_cuda.py test/jit/test_dtype_analysis.py test/jit/test_freezing.py test/jit/test_fuser_common.py test/jit/test_generator.py test/jit/test_hooks_modules.py test/jit/test_ignore_context_manager.py test/jit/test_isinstance.py test/jit/test_jit_utils.py test/jit/test_list_dict.py test/jit/test_logging.py test/jit/test_misc.py test/jit/test_models.py test/jit/test_module_containers.py test/jit/test_module_interface.py test/jit/test_optimize_for_mobile_preserve_debug_info.py test/jit/test_peephole.py test/jit/test_profiler.py test/jit/test_recursive_script.py test/jit/test_remove_mutation.py test/jit/test_save_load_for_op_version.py test/jit/test_symbolic_shape_analysis.py test/jit/test_torchbind.py test/jit/test_tracer.py test/jit/test_types.py test/jit/test_typing.py test/jit/test_union.py test/jit/test_union_pep604.py test/jit/test_with.py test/jit/xnnpack/test_xnnpack_delegate.py test/lazy/test_debug_util.py test/lazy/test_extract_compiled_graph.py test/lazy/test_generator.py test/lazy/test_meta_kernel.py test/lazy/test_reuse_ir.py test/lazy/test_step_closures.py test/lazy/test_ts_opinfo.py test/mobile/model_test/builtin_ops.py test/mobile/model_test/gen_test_model.py test/mobile/model_test/math_ops.py test/mobile/model_test/nn_ops.py test/mobile/model_test/quantization_ops.py test/mobile/model_test/tensor_ops.py test/mobile/test_bytecode.py test/mobile/test_lite_script_module.py test/mobile/test_quantize_fx_lite_script_module.py test/mobile/test_upgraders.py test/nn/test_load_state_dict.py test/nn/test_packed_sequence.py test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/test_autograd_funs.py test/onnx/test_custom_ops.py test/onnx/test_fx_passes.py test/onnx/test_pytorch_onnx_no_runtime.py test/onnx/test_pytorch_onnx_onnxruntime.py test/onnx/test_pytorch_onnx_shape_inference.py test/onnx/test_utility_funs.py test/onnx/verify.py test/optim/test_lrscheduler.py test/optim/test_swa_utils.py test/package/test_directory_reader.py test/package/test_load_bc_packages.py test/package/test_misc.py test/package/test_model.py test/package/test_package_script.py test/package/test_repackage.py test/package/test_save_load.py test/profiler/test_execution_trace.py test/profiler/test_memory_profiler.py test/profiler/test_profiler.py test/profiler/test_record_function.py test/profiler/test_torch_tidy.py test/quantization/bc/test_backward_compatibility.py test/quantization/core/experimental/apot_fx_graph_mode_ptq.py test/quantization/core/experimental/quantization_util.py test/quantization/core/experimental/test_bits.py test/quantization/core/experimental/test_fake_quantize.py test/quantization/core/experimental/test_float8.py test/quantization/core/experimental/test_nonuniform_observer.py test/quantization/core/test_backend_config.py test/quantization/core/test_quantized_module.py test/quantization/core/test_quantized_op.py test/quantization/core/test_quantized_tensor.py test/quantization/core/test_workflow_module.py test/quantization/core/test_workflow_ops.py test/quantization/eager/test_numeric_suite_eager.py test/quantization/eager/test_quantize_eager_ptq.py test/quantization/eager/test_quantize_eager_qat.py test/quantization/fx/test_model_report_fx.py test/quantization/fx/test_numeric_suite_fx.py test/quantization/fx/test_quantize_fx.py test/quantization/fx/test_subgraph_rewriter.py test/quantization/jit/test_deprecated_jit_quant.py test/quantization/jit/test_ondevice_quantization.py test/quantization/jit/test_quantize_jit.py test/quantization/pt2e/test_duplicate_dq.py test/quantization/pt2e/test_graph_utils.py test/quantization/pt2e/test_metadata_porting.py test/quantization/pt2e/test_quantize_pt2e.py test/quantization/pt2e/test_quantize_pt2e_qat.py test/quantization/pt2e/test_representation.py test/quantization/pt2e/test_x86inductor_quantizer.py test/quantization/pt2e/test_xnnpack_quantizer.py test/test_autograd.py test/test_autograd_fallback.py test/test_binary_ufuncs.py test/test_cpp_extensions_aot.py test/test_cpp_extensions_open_device_registration.py test/test_cuda.py test/test_cuda_multigpu.py test/test_cuda_sanitizer.py test/test_cuda_trace.py test/test_custom_ops.py test/test_dataloader.py test/test_datapipe.py test/test_dispatch.py test/test_dlpack.py test/test_dynamic_shapes.py test/test_fake_tensor.py test/test_file_check.py test/test_flop_counter.py test/test_foreach.py test/test_function_schema.py test/test_functional_optim.py test/test_functionalization.py test/test_functionalization_of_rng_ops.py test/test_fx.py test/test_fx_experimental.py test/test_fx_passes.py test/test_fx_reinplace_pass.py test/test_indexing.py test/test_jit.py test/test_jit_autocast.py test/test_jit_fuser.py test/test_jit_fuser_te.py test/test_jit_llga_fuser.py test/test_jiterator.py test/test_legacy_vmap.py test/test_linalg.py test/test_maskedtensor.py test/test_meta.py test/test_metal.py test/test_mkldnn.py test/test_mobile_optimizer.py test/test_module_tracker.py test/test_modules.py test/test_monitor.py test/test_mps.py test/test_multiprocessing.py test/test_multiprocessing_spawn.py test/test_namedtensor.py test/test_nestedtensor.py test/test_nn.py test/test_numba_integration.py test/test_numpy_interop.py test/test_openmp.py test/test_ops.py test/test_ops_jit.py test/test_optim.py test/test_overrides.py test/test_prims.py test/test_proxy_tensor.py test/test_python_dispatch.py test/test_pytree.py test/test_reductions.py test/test_schema_check.py test/test_segment_reductions.py test/test_serialization.py test/test_shape_ops.py test/test_sort_and_select.py test/test_sparse.py test/test_sparse_csr.py test/test_sparse_semi_structured.py test/test_spectral_ops.py test/test_stateless.py test/test_static_runtime.py test/test_subclass.py test/test_tensor_creation_ops.py test/test_tensorboard.py test/test_tensorexpr.py test/test_testing.py test/test_type_hints.py test/test_type_promotion.py test/test_typing.py test/test_unary_ufuncs.py test/test_view_ops.py test/test_vulkan.py test/test_weak.py test/test_xnnpack_integration.py test/test_xpu.py test/torch_np/numpy_tests/core/test_dtype.py test/torch_np/numpy_tests/core/test_einsum.py test/torch_np/numpy_tests/core/test_indexing.py test/torch_np/numpy_tests/core/test_multiarray.py test/torch_np/numpy_tests/core/test_numeric.py test/torch_np/numpy_tests/core/test_scalar_methods.py test/torch_np/numpy_tests/core/test_scalarmath.py test/torch_np/numpy_tests/core/test_shape_base.py test/torch_np/numpy_tests/lib/test_function_base.py test/torch_np/numpy_tests/lib/test_histograms.py test/torch_np/numpy_tests/lib/test_twodim_base.py test/torch_np/numpy_tests/linalg/test_linalg.py test/torch_np/test_basic.py test/torch_np/test_reductions.py test/xpu/test_gemm.py,https://github.com/pytorch/pytorch/pull/136964,rec,albanD,justinchuby,,
5380407af50,dynamo,not user facing,[dynamo] Properly model root frame globals during inlining (#143447),test/dynamo/test_functions.py test/dynamo/test_modules.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/143447,StrongerXi,zou3519,,,
4eafbe52882,dynamo,Untopiced,[Dynamo] Flatten slices during graph deduplication (#143522),test/dynamo/test_graph_deduplication.py torch/_dynamo/graph_deduplication.py,https://github.com/pytorch/pytorch/pull/143522,mlazos,williamwen42,,,
58627fb6bf6,releng,not user facing,[BE] Integrate 5 line build script into template (#143511),.circleci/scripts/binary_macos_build.sh .github/templates/macos_binary_build_workflow.yml.j2 .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/143511,malfet,atalman,kit1980,seemethere,
15a7a0c37ed,quantization,Untopiced,Remove deprecated branch after capture_pre_autograd_graph fully migrate to training IR (#143228),torch/ao/quantization/pt2e/qat_utils.py torch/ao/quantization/pt2e/utils.py,https://github.com/pytorch/pytorch/pull/143228,yushangdi,andrewor14,,,
b23f11c529c,onnx,improvements,[ONNX] Automatically convert dynamic_axes to dynamic_shapes with torch.export.Dim.AUTO (#143158),test/onnx/exporter/test_compat.py test/onnx/exporter/test_hf_models_e2e.py torch/onnx/_internal/exporter/_compat.py,https://github.com/pytorch/pytorch/pull/143158,titaiwangms,shubhambhokare1,xadupre,,
f9da6399508,cpp_frontend,not user facing,[codemod] Fix a few unused-variable issues in pytorch (#143517),test/cpp/profiler/containers.cpp test/cpp/profiler/perf_events.cpp test/cpp/profiler/record_function.cpp,https://github.com/pytorch/pytorch/pull/143517,r-barnes,atalman,mhorowitz,,
5e172ea004c,releng,not user facing,[BE] Get rid of `malfet/checkout@silent-checkout` (#143516),.github/actions/checkout-pytorch/action.yml .github/templates/common.yml.j2 .github/workflows/_binary-build-linux.yml .github/workflows/_binary-test-linux.yml .github/workflows/_mac-test-mps.yml .github/workflows/create_release.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/143516,malfet,ZainRizvi,atalman,huydhn,
55092e1ec51,releng,not user facing,[BE] Delete `install sccache` step from MacBB (#143512),.github/templates/macos_binary_build_workflow.yml.j2 .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/143512,malfet,atalman,kit1980,seemethere,
5c3996cab2e,dynamo,Untopiced,[Dynamo] topologically sort duplicated graph regions (#143523),test/dynamo/test_graph_deduplication.py test/dynamo/test_graph_region_tracker.py torch/_dynamo/graph_region_tracker.py,https://github.com/pytorch/pytorch/pull/143523,mlazos,williamwen42,,,
b4e0e3bfa3d,inductor,Untopiced,Backout D66648013 (#143433),test/inductor/test_cuda_repro.py torch/_inductor/decomposition.py torch/_inductor/graph.py torch/_inductor/lowering.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143433,mlazos,davidberard98,,,
2c48af568a0,sparse_frontend,not user facing,[CUDA][64-bit indexing] Fix some existing problematic `int64_t _ = blockIdx.* * blockDim.*` code (#142010),aten/src/ATen/cuda/detail/KernelUtils.h aten/src/ATen/native/cuda/Bucketization.cu aten/src/ATen/native/cuda/DistanceKernel.cu aten/src/ATen/native/cuda/DistributionTemplates.h aten/src/ATen/native/cuda/FractionalMaxPool3d.cu aten/src/ATen/native/cuda/LossCTC.cu aten/src/ATen/native/cuda/MaxUnpooling.cu aten/src/ATen/native/cuda/ReflectionPad.cu aten/src/ATen/native/cuda/Repeat.cu aten/src/ATen/native/cuda/ReplicationPadding.cu aten/src/ATen/native/cuda/SegmentReduce.cu aten/src/ATen/native/cuda/TensorFactories.cu aten/src/ATen/native/cuda/TensorTransformations.cu aten/src/ATen/native/cuda/UpSampleNearest2d.cu aten/src/ATen/native/cuda/group_norm_kernel.cu aten/src/ATen/native/cuda/layer_norm_kernel.cu aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu,https://github.com/pytorch/pytorch/pull/142010,eqy,ngimel,,,
14fe1f71902,skip,Untopiced,"Revert ""[ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend (#134124)""",.gitmodules BUILD.bazel CMakeLists.txt WORKSPACE aten/src/ATen/CMakeLists.txt aten/src/ATen/Config.h.in aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cpu/int_mm_kernel.h aten/src/ATen/native/kleidiai/kai_kernels.cpp aten/src/ATen/native/kleidiai/kai_kernels.h aten/src/ATen/native/kleidiai/kai_pack.h aten/src/ATen/native/kleidiai/kai_ukernel_interface.cpp aten/src/ATen/native/kleidiai/kai_ukernel_interface.h aten/src/ATen/native/native_functions.yaml buckbuild.bzl cmake/Dependencies.cmake cmake/Summary.cmake cmake/TorchConfig.cmake.in docs/source/backends.rst setup.py test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_torchinductor.py test/test_linalg.py third_party/kleidiai torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/quantized_lowerings.py torch/_meta_registrations.py torch/backends/__init__.py torch/backends/kleidiai/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_quantization.py torchgen/aoti/fallback_ops.py,,,,,,
19d8bbafb24,skip,not user facing,Update release matrix for 2.6 (#143538),RELEASE.md,https://github.com/pytorch/pytorch/pull/143538,kit1980,atalman,,,
deb1da15ccc,inductor,Untopiced,[foreach_map] Add foreach_map Adam impl to compiled optimizer tests (#143454),test/inductor/test_compiled_optimizers.py test/inductor/test_foreach.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/143454,mlazos,Chillee,eellison,,
bf44d5bfb55,inductor,not user facing,[Inductor] move custom pre pass (#143458),torch/_inductor/fx_passes/joint_graph.py,https://github.com/pytorch/pytorch/pull/143458,Valentine233,jansel,jgong5,,
e4301aeaa5a,jit,Untopiced,[ODML] Make the ML feature provider thread safe (#143418),torch/csrc/jit/backends/coreml/objc/PTMCoreMLFeatureProvider.mm,https://github.com/pytorch/pytorch/pull/143418,seanxiaoxiao,shoumikhin,,,
a7ba562ec8b,distributed,not user facing,"[state dict] Change _load_model_state_dict to enable cpu_offload, accept 2 device type and optimize memory (#142845)",test/distributed/checkpoint/test_state_dict.py torch/distributed/_state_dict_utils.py torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/142845,mori360,fegin,,,
fa1a4a91e98,skip,not user facing,add batch_size check for max_pool2d_backward (#141657),aten/src/ATen/native/Pool.h test/nn/test_pooling.py,https://github.com/pytorch/pytorch/pull/141657,jiayisunx,malfet,mingfeima,,
2ffdcab04ce,dynamo,not user facing,[Dynamo] Add DictKeySetVariable to capture dict_keys passed outside of compiled region (#143374),test/dynamo/test_functions.py torch/_dynamo/guards.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/143374,yanboliang,jansel,williamwen42,,
a97c6a78a85,skip,not user facing,Upgrade submodule ideep for bf16f32 matmul changes (#143508),third_party/ideep,https://github.com/pytorch/pytorch/pull/143508,aditew01,jgong5,yanbing-j,,
da06d47bdb9,dynamo,not user facing,dynamo tracing perf: slight improvement on __instancecheck__: 47.77 -> 47.62 (#143064),torch/_dynamo/variables/base.py torch/_dynamo/variables/lazy.py,https://github.com/pytorch/pytorch/pull/143064,aorenste,ezyang,jansel,,
451c2339366,skip,not user facing,leaking c++ singleton specifically (#143509),torch/csrc/monitor/events.cpp,https://github.com/pytorch/pytorch/pull/143509,duduyi2013,c-p-i-o,,,
fd8b217fcd9,python_frontend,bug fixes,Pass allow_rhs_unbacked to the stride test in metadata test too (#143040),test/test_custom_ops.py torch/_prims_common/__init__.py,https://github.com/pytorch/pytorch/pull/143040,ezyang,bobrenjc93,,,
288aa873831,inductor,not user facing,[Inductor][CPU] disable bernoulli_p decomposition (#143460),test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_cpu_repro.py test/test_decomp.py torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/143460,blzheng,jansel,jgong5,leslie-fang-intel,
465f282a24d,skip,not user facing,[reland][dynamo][guards] Consider tensors as immutable for dict tag matches (#141085),test/dynamo/test_modules.py torch/_dynamo/config.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/141085,anijain2305,jansel,,,
171e6a934f7,fx,not user facing,Don't 1 specialize if stride is contiguous (#143365),test/dynamo/test_repros.py torch/_dynamo/variables/builder.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/143365,bobrenjc93,ezyang,,,
61a835ec535,optim,Untopiced,Corrected description of AMSGrad algorithm (#142351),torch/optim/adam.py torch/optim/adamw.py,https://github.com/pytorch/pytorch/pull/142351,Tony-Y,janeyx99,,,
1433bad0e45,export,Untopiced,torch export programming model (#143546),docs/source/export.programming_model.rst docs/source/export.rst,https://github.com/pytorch/pytorch/pull/143546,avikchaudhuri,ydwu4,,,
33c27be017e,mps,bug fixes,Workaround for gather_out in MPS backend (#135543),aten/src/ATen/mps/MPSDevice.h aten/src/ATen/mps/MPSDevice.mm aten/src/ATen/native/mps/operations/ScatterGather.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/135543,jhavukainen,malfet,,,
e1e83015d24,dynamo,Untopiced,"[dynamo, 3.13t] raise error if torch.compile is attempted in 3.13t (nogil) (#143404)",torch/__init__.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/143404,williamwen42,atalman,colesbury,,
2a11472f46c,skip,not user facing,update expected results (#143586),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv,https://github.com/pytorch/pytorch/pull/143586,laithsakka,bobrenjc93,,,
c5ddf5dd904,fx,not user facing,Unbacked SymInt fixes for subclasses + data-dependent slice() bounds (non-dynamic) (#143526),test/inductor/test_unbacked_symints.py torch/_inductor/ir.py torch/fx/passes/runtime_assert.py,https://github.com/pytorch/pytorch/pull/143526,jbschlosser,ezyang,,,
4b82251011f,skip,Untopiced,[ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend (#134124),.gitmodules BUILD.bazel CMakeLists.txt WORKSPACE aten/src/ATen/CMakeLists.txt aten/src/ATen/Config.h.in aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cpu/int_mm_kernel.h aten/src/ATen/native/kleidiai/kai_kernels.cpp aten/src/ATen/native/kleidiai/kai_kernels.h aten/src/ATen/native/kleidiai/kai_pack.h aten/src/ATen/native/kleidiai/kai_ukernel_interface.cpp aten/src/ATen/native/kleidiai/kai_ukernel_interface.h aten/src/ATen/native/native_functions.yaml buckbuild.bzl cmake/Dependencies.cmake cmake/Summary.cmake cmake/TorchConfig.cmake.in docs/source/backends.rst setup.py test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_torchinductor.py test/test_linalg.py third_party/kleidiai torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/quantized_lowerings.py torch/_meta_registrations.py torch/backends/__init__.py torch/backends/kleidiai/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_quantization.py torchgen/aoti/fallback_ops.py,https://github.com/pytorch/pytorch/pull/134124,nikhil-arm,digantdesai,malfet,,
5fa287aa827,dynamo,not user facing,[Dynamo] Rename Dict{View/Keys/Values} to Dict{View/Keys/Values}Variable (#143547),torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/143547,yanboliang,williamwen42,,,
c46cfc245fb,dynamo,not user facing,[Dynamo] Support dict_keys from nested dict object (#143557),test/dynamo/test_functions.py torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/143557,yanboliang,williamwen42,,,
66172578f91,inductor,not user facing,[ROCm] Guard triton backend call around cuda.is_available (#143570),torch/_inductor/kernel/conv.py torch/_inductor/kernel/mm_common.py,https://github.com/pytorch/pytorch/pull/143570,jataylo,atalman,jeffdaily,,
2d150ad29f9,linalg_frontend,Untopiced,[ROCm] Fix unit test: matmul_offline_mgpu_tunableop (#143507),test/test_linalg.py torch/cuda/tunable.py,https://github.com/pytorch/pytorch/pull/143507,naromero77amd,jeffdaily,,,
25172dc0758,quantization,not user facing,remove allow-untyped-defs from torch/ao/quantization/experimental/fake_quantize_function.py (#143582),torch/ao/quantization/experimental/fake_quantize_function.py,https://github.com/pytorch/pytorch/pull/143582,bobrenjc93,XuehaiPan,laithsakka,,
8850a7b62cd,fx,not user facing,add some logging for tensorify (#143391),test/dynamo/test_utils.py torch/_dynamo/metrics_context.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/fx/passes/_tensorify_python_scalars.py,https://github.com/pytorch/pytorch/pull/143391,bobrenjc93,jamesjwu,,,
0a7dba49782,skip,not user facing,[cond] Change Autograd for cond (#142518),test/functorch/test_control_flow.py torch/_higher_order_ops/utils.py,https://github.com/pytorch/pytorch/pull/142518,bohnstingl,ydwu4,,,
8e78345d691,distributed,not user facing,remove allow-untyped-defs from distributed/tensor/experimental/__init__.py (#143583),torch/distributed/tensor/experimental/__init__.py,https://github.com/pytorch/pytorch/pull/143583,bobrenjc93,awgu,,,
629de4da60d,dynamo,not user facing,[dynamo] Add a lint rule to restrict what 3P library one can import (#143312),.lintrunner.toml tools/linter/adapters/import_linter.py,https://github.com/pytorch/pytorch/pull/143312,StrongerXi,zou3519,,,
0b2c47962cc,optim,Untopiced,Add support for differentiable LR in SGD + test v2.0 (#143510),test/optim/test_optim.py torch/optim/sgd.py,https://github.com/pytorch/pytorch/pull/143510,EmmettBicker,janeyx99,,,
fc03c62c568,fx,not user facing,Unbacked SymInt fixes for subclasses + data-dependent slice() bounds (#142062),test/inductor/test_unbacked_symints.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/142062,jbschlosser,ezyang,,,
e9bd74d7636,skip,Untopiced,"Revert ""[export] don't decompose custom triton op when exporting (#142426)""",test/export/test_export.py torch/_library/triton.py,,,,,,
429f4cd1408,inductor,new features,[AOTI] Emit a CMakeLists.txt when package_cpp_only (#143352),test/inductor/test_aot_inductor_package.py torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/143352,desertfire,malfet,,,
2def1f6f744,caffe2,not user facing,[caffe2] Move vectorized templates into a separate file for box_cox operator (#143556),caffe2/perfkernels/batch_box_cox_vec.h,https://github.com/pytorch/pytorch/pull/143556,efiks,hl475,,,
d2b83aa1229,skip,not user facing,add grad_output shape check for fractional_max_pool2d_backward (#141666),aten/src/ATen/native/FractionalMaxPool2d.cpp test/nn/test_pooling.py,https://github.com/pytorch/pytorch/pull/141666,jiayisunx,malfet,mingfeima,,
145fd5bad0c,skip,Untopiced,"Revert ""[Dynamo] only import einops if version is lower than 0.7.0 (#142847)""",torch/_dynamo/decorators.py,,,,,,
8136daff5a1,skip,Untopiced,"Revert ""[ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend (#134124)""",.gitmodules BUILD.bazel CMakeLists.txt WORKSPACE aten/src/ATen/CMakeLists.txt aten/src/ATen/Config.h.in aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cpu/int_mm_kernel.h aten/src/ATen/native/kleidiai/kai_kernels.cpp aten/src/ATen/native/kleidiai/kai_kernels.h aten/src/ATen/native/kleidiai/kai_pack.h aten/src/ATen/native/kleidiai/kai_ukernel_interface.cpp aten/src/ATen/native/kleidiai/kai_ukernel_interface.h aten/src/ATen/native/native_functions.yaml buckbuild.bzl cmake/Dependencies.cmake cmake/Summary.cmake cmake/TorchConfig.cmake.in docs/source/backends.rst setup.py test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_torchinductor.py test/test_linalg.py third_party/kleidiai torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/quantized_lowerings.py torch/_meta_registrations.py torch/backends/__init__.py torch/backends/kleidiai/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_quantization.py torchgen/aoti/fallback_ops.py,,,,,,
544de4008e2,skip,Untopiced,[Inductor] Constrain the shape of other tensor for Conv/Linear + broadcast add fusion. (#141759),aten/src/ATen/native/mkldnn/Linear.cpp test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/mkldnn_fusion.py,https://github.com/pytorch/pytorch/pull/141759,jiayisunx,jansel,jgong5,leslie-fang-intel,
d547fae5b07,caffe2,not user facing,[Codemod][AddExplicitStrictExportArg] caffe2/torch/onnx/_internal/exporter (#143542),torch/onnx/_internal/exporter/_capture_strategies.py,https://github.com/pytorch/pytorch/pull/143542,gmagogsfm,titaiwangms,ydwu4,,
a0cff096bc8,dynamo,Untopiced,Improve cond error messaging (#143595),torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/higher_order_ops.py torch/_export/db/examples/cond_operands.py,https://github.com/pytorch/pytorch/pull/143595,janeyx99,ydwu4,zou3519,,
e1b4635504e,distributed,not user facing,remove allow-untyped-defs from torch/distributed/pipelining/_debug.py (#143606),torch/distributed/pipelining/_debug.py,https://github.com/pytorch/pytorch/pull/143606,bobrenjc93,aorenste,,,
4462cc6375f,skip,Untopiced,"Revert ""[Inductor] inplace padding (#140249)""",test/inductor/test_inplace_padding.py test/inductor/test_torchinductor.py torch/_inductor/config.py torch/_inductor/lowering.py,,,,,,
75661f2036d,distributed,Untopiced,try root fix for FP8 tensor (#143248),torch/distributed/fsdp/_fully_shard/_fsdp_param.py,https://github.com/pytorch/pytorch/pull/143248,mayank31398,fegin,,,
d339f1506ba,sparse_frontend,Untopiced,Add cutlass version guard in prep for upgrade (#143551),aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h,https://github.com/pytorch/pytorch/pull/143551,drisspg,eqy,,,
1c2593f035d,dynamo,not user facing,[dynamo] guard global autocast state (#143592),aten/src/ATen/autocast_mode.h test/dynamo/test_recompiles.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/143592,williamwen42,jansel,,,
fd23cf58480,dynamo,Untopiced,[Dynamo] check node class first for graph dedup (#143609),torch/_dynamo/graph_deduplication.py,https://github.com/pytorch/pytorch/pull/143609,mlazos,williamwen42,,,
dd346dbeab7,distributed (torchelastic),not user facing,remove allow-untyped-defs from torch/distributed/elastic/multiprocessing/errors/handlers.py (#143605),torch/distributed/elastic/multiprocessing/errors/handlers.py,https://github.com/pytorch/pytorch/pull/143605,bobrenjc93,aorenste,,,
cb4e9888df8,quantization,not user facing,remove allow-untyped-defs from torch/ao/quantization/experimental/APoT_tensor.py (#143601),torch/ao/quantization/experimental/APoT_tensor.py,https://github.com/pytorch/pytorch/pull/143601,bobrenjc93,aorenste,,,
78d294379a7,skip,not user facing,remove allow-untyped-defs from torch/_lazy/config.py (#143603),torch/_lazy/config.py,https://github.com/pytorch/pytorch/pull/143603,bobrenjc93,aorenste,,,
9713a6eecaf,fx,not user facing,remove allow-untyped-defs from torch/fx/experimental/refinement_types.py (#143602),torch/fx/experimental/refinement_types.py,https://github.com/pytorch/pytorch/pull/143602,bobrenjc93,aorenste,,,
217a4ddb047,cuda,Untopiced,Add range check embedding_bag on input index >= 0 of cuda device (#140791),aten/src/ATen/native/cuda/EmbeddingBag.cu,https://github.com/pytorch/pytorch/pull/140791,zeshengzong,eqy,,,
2daa6665917,skip,not user facing,update kineto to XPU Windows fixed PR. [submodule kineto] (#143445),third_party/kineto,https://github.com/pytorch/pytorch/pull/143445,xuhancn,sraikund16,,,
4e29e4aa63c,skip,not user facing,[BE] Add a test to ensure grads are never inplaced into accidentally (#143612),test/test_optim.py,https://github.com/pytorch/pytorch/pull/143612,janeyx99,soulitzer,,,
71479a9b9c6,skip,Untopiced,"Revert ""[AOTI] Emit a CMakeLists.txt when package_cpp_only (#143352)""",test/inductor/test_aot_inductor_package.py torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,,,,,,
132fcf4e0d0,skip,not user facing,[user triton] Raise an exception when encountering nested @triton.autotune decorators or @triton.heuristics (#143519),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/143519,SamGinzburg,aakhundov,,,
fe0f20615c5,skip,not user facing,[DynamoBench] Handle accuracy results in benchmark records (#143611),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/143611,huydhn,kit1980,,,
29b586bbad9,skip,not user facing,fix formatting in programming model doc (#143587),docs/source/export.programming_model.rst,https://github.com/pytorch/pytorch/pull/143587,avikchaudhuri,yushangdi,,,
270ad513c8a,skip,Untopiced,[Dynamo] only import einops if version is lower than 0.7.0 (#142847),torch/_dynamo/decorators.py,https://github.com/pytorch/pytorch/pull/142847,mlazos,zou3519,,,
f9f82ca48f9,export,Untopiced,[ts converter] use Dim.AUTO for ts -> export converter (#138273),test/export/test_converter.py torch/_export/converter.py,https://github.com/pytorch/pytorch/pull/138273,pianpwk,avikchaudhuri,,,
b539c61631f,inductor,Untopiced,[Hierarchical Compile] Update NoneAsConstantBuffer to support graph d… (#143531),test/higher_order_ops/test_invoke_subgraph.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/143531,mlazos,eellison,,,
6733045a4aa,skip,not user facing,export AOTI_TORCH_EXPORT on Windows. (#140030),CMakeLists.txt caffe2/CMakeLists.txt torch/CMakeLists.txt torch/csrc/inductor/aoti_torch/c/shim.h,https://github.com/pytorch/pytorch/pull/140030,xuhancn,desertfire,jgong5,malfet,
04b26ee1e89,skip,not user facing,Fix false positive from f-strings in set_linter (#143628),tools/linter/adapters/_linter.py tools/linter/adapters/set_linter.py,https://github.com/pytorch/pytorch/pull/143628,jansel,rec,yanboliang,,
673cc88fd60,dynamo,Untopiced,Add support for `contextmanager` in Dynamo (#136033),aten/src/ATen/Context.cpp aten/src/ATen/Context.h benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv test/dynamo/test_ctx_manager.py test/dynamo/test_functions.py test/dynamo/test_misc.py test/dynamo/test_structured_trace.py torch/_C/__init__.pyi.in torch/_dynamo/codegen.py torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/user_defined.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/136033,guilhermeleobas,zou3519,,,
1c817fe6714,skip,Untopiced,Set `enable_trace_contextlib_contextmanager` flag to True (#140604),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/140604,guilhermeleobas,zou3519,,,
7bf3b7cdc56,skip,not user facing,Rewrite _reparametrize_module to use `contextmanager` (#138203),torch/nn/utils/stateless.py,https://github.com/pytorch/pytorch/pull/138203,guilhermeleobas,zou3519,,,
607884c9afe,inductor,not user facing,[Inductor][CPP] Fix bitwise shift with corner inputs (#143635),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/143635,leslie-fang-intel,jgong5,,,
4f8b7c4272d,fx,not user facing,"Revert ""refactor tensorify restart logic to use sources (#141517)"" (#143623)",torch/_dynamo/output_graph.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py torch/fx/passes/_tensorify_python_scalars.py,https://github.com/pytorch/pytorch/pull/143623,bobrenjc93,mlazos,,,
7ab880bc5e4,skip,not user facing,fix typo in autocast header (#143625),aten/src/ATen/autocast_mode.h torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/143625,williamwen42,mlazos,,,
f5af87c23c5,inductor,not user facing,Make Inductor cpp backend enable_floating_point_contract_flag to take string (#143450),torch/_inductor/config.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/143450,hl475,desertfire,,,
d79fbf6b6d5,skip,not user facing,test/dynamo/test_utils: logging - Stop testing for impossible things. (#143535),test/dynamo/test_utils.py,https://github.com/pytorch/pytorch/pull/143535,c00w,ppanchalia,,,
c7d9f298072,skip,not user facing,"(MTIA) Move ""empty_cache"" API (#143402)",docs/source/mtia.memory.rst torch/mtia/__init__.py torch/mtia/memory.py,https://github.com/pytorch/pytorch/pull/143402,chaos5958,nautsimon,,,
a5ed499f6ac,skip,not user facing,FlexAttention Benchmark (#139665),benchmarks/transformer/score_mod.py,https://github.com/pytorch/pytorch/pull/139665,joydddd,drisspg,,,
5a69c2a6498,sparse_frontend,not user facing,[BE][Sparse] Get rid of gcc-5 workaround (#143653),aten/src/ATen/native/sparse/FlattenIndicesCommon.h aten/src/ATen/native/sparse/SparseBinaryOpIntersectionCommon.h,https://github.com/pytorch/pytorch/pull/143653,malfet,albanD,,,
b5475d334e0,inductor,not user facing,[inductor] Fix an unused variable in cpu_vec_isa.py (#138473),torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/138473,rec,EikanWang,albanD,xuhancn,
94737e8a2a5,skip,Untopiced,[ARM][feat]: Add 4 bit dynamic quantization matmuls & KleidiAI Backend (#134124),.gitmodules BUILD.bazel CMakeLists.txt WORKSPACE aten/src/ATen/CMakeLists.txt aten/src/ATen/Config.h.in aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cpu/int_mm_kernel.h aten/src/ATen/native/kleidiai/kai_kernels.cpp aten/src/ATen/native/kleidiai/kai_kernels.h aten/src/ATen/native/kleidiai/kai_pack.h aten/src/ATen/native/kleidiai/kai_ukernel_interface.cpp aten/src/ATen/native/kleidiai/kai_ukernel_interface.h aten/src/ATen/native/native_functions.yaml buckbuild.bzl cmake/Dependencies.cmake cmake/Summary.cmake cmake/TorchConfig.cmake.in docs/source/backends.rst setup.py test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_torchinductor.py test/test_linalg.py third_party/kleidiai torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/quantized_lowerings.py torch/_meta_registrations.py torch/backends/__init__.py torch/backends/kleidiai/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_quantization.py torchgen/aoti/fallback_ops.py,https://github.com/pytorch/pytorch/pull/134124,nikhil-arm,digantdesai,malfet,,
792e6184c51,skip,not user facing,[GPT-fast] Support run spcific model or micro-benchmark (#143607),benchmarks/gpt_fast/benchmark.py benchmarks/gpt_fast/common.py benchmarks/gpt_fast/generate.py,https://github.com/pytorch/pytorch/pull/143607,yanboliang,BoyuanFeng,huydhn,jerryzh168,
7d4e7fbfc1e,dynamo,not user facing,dynamo tracing perf: no import on hot path: 47.62 -> 47.26 (#143065),torch/_dynamo/guards.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/143065,aorenste,jansel,,,
5feb2d7b410,dynamo,not user facing,dynamo tracing perf: don't call expensive _set_guard_export_info if it's a duplicate guard: 37.66 -> 34.86 (#143067),torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/143067,aorenste,jansel,,,
8ce0bc282a9,dynamo,not user facing,dynamo tracing perf: bytecode_transform improvements: 34.86 -> 33.9 (#143068),torch/_dynamo/bytecode_transformation.py,https://github.com/pytorch/pytorch/pull/143068,aorenste,jansel,,,
a94f259a699,dynamo,not user facing,pgo: Log feature use (#142819),torch/_dynamo/pgo.py,https://github.com/pytorch/pytorch/pull/142819,c00w,ezyang,,,
485497e7275,distributed,Untopiced,[c10d][fr] flight recorder improvements (#143446),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/FlightRecorder.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/143446,c-p-i-o,d4l3k,,,
629de988dfe,cpp_frontend,Untopiced,Fix old-compiler-unfriendly zero init of bfloat16_t array (#143504),aten/src/ATen/cpu/vec/vec128/vec128_bfloat16_neon.h,https://github.com/pytorch/pytorch/pull/143504,swolchok,malfet,,,
3f63b742e61,python_frontend,improvements,Refactor serialization getter/setters into torch.utils.serialization.config (#143324),docs/source/notes/serialization.rst torch/serialization.py torch/utils/serialization/__init__.py torch/utils/serialization/config.py,https://github.com/pytorch/pytorch/pull/143324,mikaylagawarecki,albanD,,,
8e483654cb5,python_frontend,new features,Add config.save.use_pinned_memory_for_d2h to serialization config (#143342),docs/source/notes/serialization.rst test/test_serialization.py torch/serialization.py torch/utils/serialization/config.py,https://github.com/pytorch/pytorch/pull/143342,mikaylagawarecki,albanD,,,
cee06e74eeb,skip,not user facing,Apply clang-format for ATen/core/dispatch headers (#143620),.lintrunner.toml aten/src/ATen/core/dispatch/CppSignature.h aten/src/ATen/core/dispatch/DispatchKeyExtractor.h aten/src/ATen/core/dispatch/Dispatcher.h aten/src/ATen/core/dispatch/OperatorEntry.h aten/src/ATen/core/dispatch/OperatorOptions.h aten/src/ATen/core/dispatch/RegistrationHandleRAII.h,https://github.com/pytorch/pytorch/pull/143620,zeshengzong,malfet,,,
33dd4f187dd,skip,not user facing,[pytorch/et] Allow ET to save additional resources for completing a trace like generated kernels and index tensor data (#143430),test/profiler/test_execution_trace.py torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/143430,sanrise,shengfukevin,sraikund16,,
372b023eb1d,skip,not user facing,Fix test_serialization_zipfile_actually_jit when weights_only is not default (#143668),test/test_serialization.py,https://github.com/pytorch/pytorch/pull/143668,mikaylagawarecki,awgu,,,
a9c753bbc88,skip,not user facing,[logging] A few fixes/updates to record_compilation_metrics (#143332),torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/143332,masnesral,ppanchalia,,,
6e58c375425,distributed,Untopiced,c10d: no call_guard in init (#143598),torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/143598,d4l3k,c-p-i-o,,,
23ca7c2515d,caffe2,Untopiced,Fix unused-variable issues in caffe2 (#143639),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.cpp aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cpp aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,https://github.com/pytorch/pytorch/pull/143639,r-barnes,cyyever,kit1980,malfet,
a3b04d473ea,releng,not user facing,[ROCm] Update setup-rocm for almalinux-based images (#143590),.github/actions/setup-rocm/action.yml .github/workflows/_rocm-test.yml,https://github.com/pytorch/pytorch/pull/143590,amdfaa,atalman,,,
8960cb58091,inductor,Untopiced,Add support for bfloat16 atomic adds in fbcode (#143629),test/inductor/test_cuda_repro.py torch/_inductor/decomposition.py torch/_inductor/graph.py torch/_inductor/lowering.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143629,mlazos,eellison,,,
912d6a28673,linalg_frontend,not user facing,[CUDA] Bump tolerances in `test_svd_lowrank_cuda_float64` (#143049),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/143049,eqy,Skylion007,lezcano,nikitaved,
bf7009d8392,cpp_frontend,not user facing,[rpc] Fix unit test after c10::nullopt removal (#143690),test/cpp/rpc/test_e2e_tensorpipe.cpp,https://github.com/pytorch/pytorch/pull/143690,yf225,XilunWu,c-p-i-o,yifuwang,
ad7ab5ef840,skip,Untopiced,"Revert ""[logging] A few fixes/updates to record_compilation_metrics (#143332)""",torch/_dynamo/utils.py,,,,,,
553031fb9a8,skip,not user facing,[BE] Remove gcc-5 workaround for unused args (#143685),aten/src/ATen/core/dispatch/Dispatcher.h,https://github.com/pytorch/pytorch/pull/143685,malfet,atalman,kit1980,seemethere,
0ce233b8cab,inductor,Untopiced,Support tensor subclass unwrapping (#141941),test/export/test_export.py test/inductor/test_aot_inductor.py torch/_inductor/compile_fx.py torch/export/_trace.py torch/export/exported_program.py torch/export/graph_signature.py torch/testing/_internal/custom_tensor.py,https://github.com/pytorch/pytorch/pull/141941,tugsbayasgalan,bdhirsh,,,
4ee166b82f2,dynamo,not user facing,[ca] add compiled autograd to CompileId (#141907),test/dynamo/test_frame_init.py test/dynamo/test_structured_trace.py torch/_dynamo/cache_size.py torch/_dynamo/compiled_autograd.py torch/_dynamo/convert_frame.py torch/_dynamo/testing.py torch/_guards.py torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/141907,xmfan,ezyang,,,
d88ebbf8228,dynamo,not user facing,cleanup chromium event log on dynamo exit rather than on entry (#143175),torch/_dynamo/convert_frame.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/143175,xmfan,Skylion007,jamesjwu,,
bdeee828221,distributed,Untopiced,unflatten isinstance (#143664),test/export/test_export.py torch/distributed/pipelining/_unflatten.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/143664,avikchaudhuri,tugsbayasgalan,,,
ae3d385fcba,cpp_frontend,improvements,Fix issue with setAttribute and int8_t vs int32_t variables (#143693),aten/src/ATen/cuda/CUDABlas.cpp,https://github.com/pytorch/pytorch/pull/143693,r-barnes,huydhn,,,
ffd1b53f26a,dynamo,Untopiced,[aot] refactor dynamo source and cudagraphs static idx logic (#141748),torch/_dynamo/eval_frame.py torch/_dynamo/output_graph.py torch/_functorch/aot_autograd.py,https://github.com/pytorch/pytorch/pull/141748,xmfan,ezyang,,,
9cb743d1f90,quantization,Untopiced,[easy] Set feature use for aot autograd remote cache (#143674),torch/_functorch/aot_autograd.py,https://github.com/pytorch/pytorch/pull/143674,jamesjwu,bobrenjc93,,,
4627cfd1f99,dynamo,Untopiced,[dynamo] Support user defined dicts (#143548),test/dynamo/test_dicts.py test/dynamo/test_misc.py test/dynamo_expected_failures/TestSerialization.test_serialization_dill test/test_serialization.py torch/_dynamo/side_effects.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/143548,anijain2305,jansel,williamwen42,yanboliang,
0da004f3ddb,dynamo,not user facing,[dynamo] Remove transformers ModelOutput hack (#143567),test/dynamo/test_model_output.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/143567,anijain2305,jansel,williamwen42,,
af0e159740a,inductor,not user facing,[Inductor XPU] Add XPU check for `is_big_gpu()`. (#143491),torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143491,etaf,EikanWang,desertfire,jansel,
b5e159270a3,inductor,not user facing,[AOTI XPU] Replace intel compiler with g++ to build inductor CPP wrapper in runtime. (#142322),torch/_inductor/cpp_builder.py torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/142322,etaf,EikanWang,albanD,desertfire,
fecf03fa3f0,inductor,new features,[AOTI][reland] Emit a CMakeLists.txt when package_cpp_only (#143680),test/inductor/test_aot_inductor_package.py torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/143680,desertfire,huydhn,,,
dabc9566c49,skip,Untopiced,"Revert ""(MTIA) Move ""empty_cache"" API (#143402)""",docs/source/mtia.memory.rst torch/mtia/__init__.py torch/mtia/memory.py,,,,,,
c7d7eff798e,skip,Untopiced,"Revert ""[MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage (#143347)""",docs/source/mtia.memory.rst docs/source/mtia.rst torch/mtia/memory.py,,,,,,
a8953c36f5e,dynamo,Untopiced,[compiled autograd] log compilation time to perfetto (#140964),test/dynamo/test_structured_trace.py torch/_dynamo/compiled_autograd.py,https://github.com/pytorch/pytorch/pull/140964,xmfan,masnesral,,,
b89bfe0bacb,skip,Untopiced,"Revert ""Fix issue with setAttribute and int8_t vs int32_t variables (#143693)""",aten/src/ATen/cuda/CUDABlas.cpp,,,,,,
97990f476d4,skip,Untopiced,"Revert ""Fix unused-variable issues in caffe2 (#143639)""",aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.cpp aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cpp aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,,,,,,
f44310097c8,inductor,not user facing,Reuse partial reductions (#143600),test/inductor/test_pattern_matcher.py torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/143600,eellison,vkuzo,,,
518b5050c0f,caffe2,Untopiced,Fix unused-variable issues in caffe2 (#143639),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.cpp aten/src/ATen/native/sparse/cuda/SparseCUDABlas.cpp aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,https://github.com/pytorch/pytorch/pull/143639,r-barnes,cyyever,kit1980,malfet,
9f3c291bc31,cpp_frontend,improvements,Fix issue with setAttribute and int8_t vs int32_t variables (#143693),aten/src/ATen/cuda/CUDABlas.cpp,https://github.com/pytorch/pytorch/pull/143693,r-barnes,huydhn,,,
47c4e01e71c,releng,not user facing,[audio hash update] update the pinned audio hash (#143694),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/143694,pytorchupdatebot,pytorchbot,,,
bee47b0663a,skip,Untopiced,"Revert ""[pytorch/et] Allow ET to save additional resources for completing a trace like generated kernels and index tensor data (#143430)""",test/profiler/test_execution_trace.py torch/profiler/profiler.py,,,,,,
7b2af25f800,skip,not user facing,[1/n] Support Dynamic Memory Budget in Auto AC (#143539),test/functorch/test_ac_knapsack.py torch/_functorch/_activation_checkpointing/graph_info_provider.py torch/_functorch/_activation_checkpointing/knapsack_evaluator.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/143539,basilwong,jansel,,,
d7e59c2f853,quantization,Untopiced,Fix cppcoreguidelines-pro-type-member-init (#141787),aten/src/ATen/CPUGeneratorImpl.cpp aten/src/ATen/DLConvertor.cpp aten/src/ATen/core/TensorAccessor.h aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_deserialize.cpp aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear_prepack.cpp aten/src/ATen/native/ao_sparse/quantized/cpu/qnnpack_utils.h aten/src/ATen/native/cpu/MultinomialKernel.cpp aten/src/ATen/native/cpu/SoftMaxKernel.cpp aten/src/ATen/native/cpu/SumKernel.cpp aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp aten/src/ATen/native/quantized/cpu/kernels/QuantizedOpKernels.cpp aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp aten/src/ATen/native/xnnpack/Convolution.cpp,https://github.com/pytorch/pytorch/pull/141787,cyyever,albanD,,,
51eacea8c45,export,Untopiced,graph module retracing without preserving MCS (#143676),test/export/test_export.py torch/_export/wrappers.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/143676,avikchaudhuri,tugsbayasgalan,zhxchen17,,
e15442a9b27,skip,Untopiced,"Revert ""export AOTI_TORCH_EXPORT on Windows. (#140030)""",CMakeLists.txt caffe2/CMakeLists.txt torch/CMakeLists.txt torch/csrc/inductor/aoti_torch/c/shim.h,,,,,,
daa3ffe0ebf,skip,Untopiced,Enable more C++ warnings (#143355),.clang-format CMakeLists.txt aten/src/ATen/native/QuantizedLinear.cpp aten/src/ATen/native/RNN.cpp aten/src/ATen/native/quantized/cpu/LinearUnpackImpl.cpp aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp aten/src/ATen/native/quantized/cpu/fbgemm_utils.h aten/src/ATen/native/quantized/cpu/qlinear.cpp aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp aten/src/ATen/native/quantized/cudnn/Conv.cpp aten/src/ATen/native/quantized/cudnn/ConvPrepack.cpp aten/src/ATen/native/quantized/library.cpp aten/src/ATen/native/quantized/qconv_unpack.cpp aten/src/ATen/native/quantized/qlinear_unpack.cpp torch/csrc/distributed/c10d/GlooDeviceFactory.cpp torch/csrc/distributed/rpc/tensorpipe_agent.cpp torch/csrc/distributed/rpc/tensorpipe_cuda.cpp torch/csrc/jit/passes/onnx/constant_map.h torch/csrc/jit/serialization/export.cpp,https://github.com/pytorch/pytorch/pull/143355,cyyever,albanD,,,
e97b97af562,skip,Untopiced,Handle meta tensors in FX quantization (#142262),torch/ao/quantization/quantize.py,https://github.com/pytorch/pytorch/pull/142262,kausv,iamzainhuda,,,
0666347fc45,caffe2,not user facing,[Codemod][AddExplicitStrictExportArg] caffe2/benchmarks/dynamo (#143686),benchmarks/dynamo/common.py benchmarks/dynamo/pr_time_benchmarks/benchmarks/sum_floordiv.py,https://github.com/pytorch/pytorch/pull/143686,gmagogsfm,tugsbayasgalan,,,
197954e14b2,skip,Untopiced,"Revert ""Handle meta tensors in FX quantization (#142262)""",torch/ao/quantization/quantize.py,,,,,,
2293fe10248,releng,not user facing,"[BE][Easy] use `pathlib.Path` instead of `dirname` / `""..""` / `pardir` (#129374)",.circleci/codegen_validation/normalize_yaml_fragment.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_binary_build_matrix.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py docs/source/scripts/build_opsets.py docs/source/scripts/build_quantization_configs.py docs/source/scripts/exportdb/generate_example_rst.py scripts/compile_tests/update_failures.py test/jit/test_backend_nnapi.py test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/onnx_test_common.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py test/quantization/core/test_docs.py test/test_typing.py tools/amd_build/build_amd.py tools/build_libtorch.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/util/setting.py tools/gen_vulkan_spv.py tools/linter/adapters/s3_init.py tools/onnx/update_default_opset_version.py tools/setup_helpers/cmake.py tools/setup_helpers/gen.py tools/setup_helpers/gen_unboxing.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_gen_backend_stubs.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py torch/_inductor/runtime/compile_tasks.py torch/testing/_internal/common_utils.py torchgen/_autoheuristic/mixed_mm/gen_data_mixed_mm.py torchgen/_autoheuristic/mixed_mm/test_mixed_mm.py torchgen/_autoheuristic/mixed_mm/train_decision_mixedmm.py torchgen/_autoheuristic/mm/gen_data_mm.py torchgen/_autoheuristic/mm/train_decision_mm.py torchgen/_autoheuristic/pad_mm/gen_data_pad_mm.py torchgen/_autoheuristic/pad_mm/test_pad_mm.py torchgen/_autoheuristic/pad_mm/train_decision_pad_mm.py torchgen/_autoheuristic/pad_mm/train_pad_mm.py torchgen/_autoheuristic/pad_mm/train_regression_pad_mm.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,https://github.com/pytorch/pytorch/pull/129374,XuehaiPan,justinchuby,malfet,,
f1cbf4b1b5a,quantization,not user facing,Enable ruff's unused variable checking everywhere in pytorch (#136965),benchmarks/transformer/score_mod.py pyproject.toml test/dynamo/test_ctx_manager.py test/higher_order_ops/test_invoke_subgraph.py test/inductor/test_unbacked_symints.py test/test_torch.py test/test_transformers.py tools/linter/adapters/docstring_linter.py tools/test/docstring_linter_testdata/python_code.py.txt.lintrunner torch/_dynamo/symbolic_convert.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/select_algorithm.py torch/ao/quantization/fx/convert.py torch/distributed/elastic/timer/file_based_local_timer.py torch/distributed/tensor/experimental/_attention.py torch/export/_trace.py torch/fx/experimental/symbolic_shapes.py torch/nested/_internal/ops.py torch/testing/_internal/common_methods_invocations.py torch/testing/_internal/distributed/rpc/rpc_test.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/136965,rec,albanD,cyyever,,
f2b744b9cae,dynamo,not user facing,dynamo tracing perf: import_module: 59.92 -> 52.9 (#143057),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/143057,aorenste,jansel,,,
3ec04d30d5e,dynamo,not user facing,dynamo tracing perf: kill import: 50.36 -> 49.12 (#143062),torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/143062,aorenste,jansel,,,
9bf4b1c2e9d,dynamo,not user facing,dynamo tracing perf: c++ strip_function_call: 49.12 -> 47.77 (#143063),torch/_C/_dynamo/__init__.pyi torch/_dynamo/guards.py torch/csrc/dynamo/init.cpp torch/csrc/utils/object_ptr.h,https://github.com/pytorch/pytorch/pull/143063,aorenste,jansel,,,
dc55704b482,dynamo,not user facing,Rename cache limit to recompile limit in configs (#143709),benchmarks/dynamo/microbenchmarks/dynamo_guard_eval.py benchmarks/dynamo/torchao_backend.py benchmarks/transformer/score_mod.py docs/source/torch.compiler_faq.rst docs/source/torch.compiler_troubleshooting.rst docs/source/torch.compiler_troubleshooting_old.rst test/dynamo/test_config.py test/dynamo/test_misc.py test/dynamo/test_modes.py test/dynamo/test_modules.py test/dynamo/test_recompile_ux.py test/dynamo/test_recompiles.py test/functorch/test_control_flow.py test/inductor/test_b2b_gemm.py test/inductor/test_torchinductor.py test/inductor/test_triton_kernels.py test/test_content_store.py torch/__init__.py torch/_dynamo/cache_size.py torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/utils.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/143709,oulgen,jansel,,,
09c950cc872,inductor,not user facing,Remove unused <ATen/core/Array.h> inclusion (#143701),aten/src/ATen/core/DistributionsHelper.h aten/src/ATen/core/PhiloxRNGEngine.h aten/src/ATen/cuda/detail/OffsetCalculator.cuh aten/src/ATen/native/DispatchStub.cpp aten/src/ATen/native/DispatchStub.h aten/src/ATen/native/cpu/int_mm_kernel.h aten/src/ATen/native/cuda/CUDAJitLoops.cuh aten/src/ATen/native/cuda/CUDALoops.cuh aten/src/ATen/native/cuda/IndexKernel.cu aten/src/ATen/native/cuda/MemoryAccess.cuh aten/src/ATen/native/cuda/Reduce.cuh aten/src/ATen/native/cuda/Sort.cu aten/src/ATen/native/cuda/SortStable.cu aten/src/ATen/native/mps/operations/HistogramKernel.mm aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/143701,cyyever,albanD,,,
a316a4581d8,inductor,not user facing,Add mps to GPU_TYPES (#143634),torch/_inductor/scheduler.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143634,malfet,jansel,,,
6425f0779d5,inductor,not user facing,[BE] Update triton repo link (#143429),torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/143429,malfet,jansel,,,
41cdc7f7355,skip,not user facing,[reland][AMD] Turn on TF32 for aten::mm (#143549),aten/src/ATen/Context.cpp aten/src/ATen/cuda/CUDABlas.cpp test/test_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/143549,xw285cornell,eqy,,,
eebc93d41ee,skip,not user facing,Better fix for f-strings in set_linter for py3.12 (#143725),tools/linter/adapters/_linter.py tools/linter/adapters/set_linter.py,https://github.com/pytorch/pytorch/pull/143725,jansel,yanboliang,,,
07fa6e2c8b0,xpu,improvements,Fix torch.accelerator api abort when passing invaild device (#143550),aten/src/ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h aten/src/ATen/mps/MPSGuardImpl.h c10/core/impl/DeviceGuardImplInterface.h c10/core/impl/VirtualGuardImpl.h c10/cuda/impl/CUDAGuardImpl.h c10/xpu/XPUFunctions.h c10/xpu/impl/XPUGuardImpl.h test/test_cuda.py test/test_xpu.py,https://github.com/pytorch/pytorch/pull/143550,guangyey,EikanWang,albanD,dvrogozh,
06b4b96b34e,dynamo,not user facing,dynamo tracing perf: no re in arg_ref: 33.9 -> 33.7 (#143069),torch/_C/_dynamo/__init__.pyi torch/_dynamo/guards.py torch/csrc/dynamo/init.cpp,https://github.com/pytorch/pytorch/pull/143069,aorenste,jansel,,,
448c16ac87f,skip,Untopiced,"Revert ""[reland][AMD] Turn on TF32 for aten::mm (#143549)""",aten/src/ATen/Context.cpp aten/src/ATen/cuda/CUDABlas.cpp test/test_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,,,,,,
01890526b90,skip,not user facing,Add FP8 support for eye (#139974),aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/cpu/TensorCompareKernel.cpp aten/src/ATen/native/cuda/TensorCompare.cu torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/139974,yanbing-j,jgong5,malfet,,
434e0c2104f,inductor,not user facing,Inductor Cutlass backend: Eliminate unused code. (#143723),torch/_inductor/codegen/cuda/cutlass_epilogue_gen.py torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/143723,kadeng,ColinPeppler,,,
7314cf44ae7,python_frontend,not user facing,torch/accelerator: fix device type comparison (#143541),test/test_accelerator.py torch/accelerator/_utils.py,https://github.com/pytorch/pytorch/pull/143541,dvrogozh,albanD,guangyey,,
5c4545f8572,linalg_frontend,not user facing,[BE][Easy] enable PYFMT for `torch/[a-s]*/` (#138447),.lintrunner.toml torch/contrib/_tensorboard_vis.py torch/cuda/_gpu_trace.py torch/cuda/_memory_viz.py torch/fft/__init__.py torch/func/__init__.py torch/futures/__init__.py torch/linalg/__init__.py torch/monitor/__init__.py torch/nested/__init__.py torch/signal/__init__.py torch/signal/windows/__init__.py torch/signal/windows/windows.py torch/special/__init__.py torch/xpu/_gpu_trace.py,https://github.com/pytorch/pytorch/pull/138447,XuehaiPan,ezyang,,,
12662901aa8,releng,Untopiced,[BE] Move Mac BB test to its own step (#143513),.ci/wheel/build_wheel.sh .github/templates/macos_binary_build_workflow.yml.j2 .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/143513,malfet,atalman,huydhn,kit1980,
1519a9e30b6,skip,Untopiced,"Revert ""Add FP8 support for eye (#139974)""",aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/cpu/TensorCompareKernel.cpp aten/src/ATen/native/cuda/TensorCompare.cu torch/testing/_internal/common_methods_invocations.py,,,,,,
a70191da41a,python_frontend,Untopiced,Add torch.topk indices vary description (#143736),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/143736,zeshengzong,zou3519,,,
c042c8a4753,dataloader_frontend,not user facing,Use default_collate from public API (#143616),test/test_dataloader.py,https://github.com/pytorch/pytorch/pull/143616,kit1980,malfet,,,
727ee853b46,skip,not user facing,Apply TorchFix TOR203 fixes (#143691),benchmarks/profiler_benchmark/resnet_memory_profiler.py functorch/benchmarks/per_sample_grads.py functorch/examples/dp_cifar10/cifar10_opacus.py functorch/examples/dp_cifar10/cifar10_transforms.py functorch/examples/maml_omniglot/support/omniglot_loaders.py test/functorch/test_eager_transforms.py,https://github.com/pytorch/pytorch/pull/143691,kit1980,malfet,,,
0ebc6388cf1,skip,Untopiced,"Revert ""Exclude py 31.3t triton package from PyTorch 3.13t wheel (#143218)""",.circleci/scripts/binary_populate_env.sh,,,,,,
75e1f8a227f,releng,not user facing,[ROCm] upgrade nightly wheels to rocm6.3 - 2 of 2 (binaries) (#143613),.ci/manywheel/build_rocm.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/build-triton-wheel.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/143613,jithunnair-amd,jeffdaily,,,
00831f9b221,package/deploy,not user facing,[BE]: Properly forward raise pickle exception with from (#143761),torch/package/_package_pickler.py,https://github.com/pytorch/pytorch/pull/143761,Skylion007,XuehaiPan,albanD,,
2ab698e708c,profiler,Untopiced,allow profiling on all threads via experimentalConfig (#143659),test/profiler/test_profiler.py torch/csrc/autograd/profiler_kineto.cpp torch/csrc/profiler/orchestration/observer.cpp torch/csrc/profiler/orchestration/observer.h torch/csrc/profiler/python/init.cpp,https://github.com/pytorch/pytorch/pull/143659,ngimel,sraikund16,,,
4271a95590a,skip,not user facing,[logging] A few fixes/updates to record_compilation_metrics (#143332),torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/143332,masnesral,ppanchalia,,,
6ccb8ed1868,optim,Untopiced,Refactor AdamW into Adam (heavily inspired by tfsingh) (#143710),test/test_optim.py torch/optim/adam.py torch/optim/adamw.py,https://github.com/pytorch/pytorch/pull/143710,EmmettBicker,janeyx99,,,
bc78b6ea4f8,python_frontend,improvements,Add a warning when a tensor with requires_grad=True is converted to a scalar (#143261),aten/src/ATen/native/Scalar.cpp test/test_torch.py,https://github.com/pytorch/pytorch/pull/143261,joshdavham,albanD,,,
0ca6a47872d,releng,not user facing,Update tag_regex in filter_test_configs.py for workflows such as `inductor-rocm` (#143768),.github/scripts/filter_test_configs.py,https://github.com/pytorch/pytorch/pull/143768,jithunnair-amd,huydhn,,,
67355a12895,python_frontend,Untopiced,"[Easy] Add torch.range, torch.arange params optional description (#143731)",torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/143731,zeshengzong,janeyx99,,,
fe95cbe0182,skip,not user facing,[dynamo] Remove DICT_SUBCLASS_GUARD_MANAGER and use dict.keys (#143722),test/dynamo/test_dicts.py test/dynamo/test_guard_manager.py test/dynamo/test_misc.py test/dynamo/test_modules.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/143722,anijain2305,jansel,,,
7d1c6661397,skip,not user facing,[dynamo] Remove dead code after introducing UserDefinedDictVariable (#143699),torch/_dynamo/guards.py torch/_dynamo/side_effects.py torch/_dynamo/source.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/143699,anijain2305,jansel,williamwen42,yanboliang,
da21fabf34d,python_frontend,docs,[BE] Only print MKL version on x86 platforms (#143763),aten/src/ATen/ParallelCommon.cpp,https://github.com/pytorch/pytorch/pull/143763,malfet,Skylion007,albanD,,
d6a066ead6e,skip,not user facing,Simplify host_softmax (#143251),aten/src/ATen/native/SoftMax.cpp,https://github.com/pytorch/pytorch/pull/143251,cyyever,albanD,,,
49fdc52fd2b,skip,Untopiced,"Revert ""Add a warning when a tensor with requires_grad=True is converted to a scalar (#143261)""",aten/src/ATen/native/Scalar.cpp test/test_torch.py,,,,,,
1feae27ed6e,inductor,Untopiced,[16/N] Fix extra warnings brought by clang-tidy-17 (#143714),.lintrunner.toml aten/src/ATen/SDPBackend.h aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/functorch/BatchRulesModules.cpp torch/_inductor/codegen/cpp_prefix.h torch/csrc/Generator.cpp torch/csrc/api/include/torch/nn/modules/upsampling.h torch/csrc/api/src/nn/modules/upsampling.cpp torch/csrc/dynamo/guards.cpp torch/csrc/inductor/aoti_package/model_package_loader.cpp,https://github.com/pytorch/pytorch/pull/143714,cyyever,Skylion007,albanD,,
dec4286b2d3,inductor,not user facing,[inductor] Fix for extract_target with dots (#143766),torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/143766,jansel,yanboliang,,,
b90a3b7281e,skip,not user facing,[cumsum][CUDA][64-bit indexing] Add 64-bit indexing path for `cumsum` (#143696),aten/src/ATen/native/cuda/ScanUtils.cuh test/test_torch.py,https://github.com/pytorch/pytorch/pull/143696,eqy,malfet,,,
01d60bcf321,skip,not user facing,[Easy] Fix todo by enable tests for cuda (#143637),test/test_tensor_creation_ops.py,https://github.com/pytorch/pytorch/pull/143637,zeshengzong,albanD,,,
60a0d53c13a,dynamo,not user facing,[dynamo] Add test for #143697 (#143764),test/dynamo/test_repros.py,https://github.com/pytorch/pytorch/pull/143764,jansel,Skylion007,,,
ace645a0172,quantization,not user facing,Add support for prototype affine quantization in pt2e flow (#141421),docs/source/quantization-support.rst mypy.ini test/quantization/pt2e/test_quantize_pt2e.py test/test_quantization.py torch/ao/quantization/__init__.py torch/ao/quantization/observer.py torch/ao/quantization/pt2e/_affine_quantization.py torch/testing/_internal/common_quantization.py,https://github.com/pytorch/pytorch/pull/141421,jerryzh168,cccclai,,,
b0c3f48a406,inductor,Untopiced,[inductor] Improve error message for assert_size_stride (#143765),torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/143765,jansel,zou3519,,,
ad750ae3207,skip,not user facing,[Inductor XPU] Support max-autotune on XPU and reuse the corresponding Inductor UT. (#143266),test/inductor/test_kernel_benchmark.py test/inductor/test_max_autotune.py test/inductor/test_select_algorithm.py torch/_inductor/autotune_process.py torch/_inductor/codegen/triton.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143266,etaf,EikanWang,jansel,,
1963fc83a1c,distributed,not user facing,[micro_pipeline_tp] don't pass return_A to fused_all_gather_scaled_matmul (#143782),test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py,https://github.com/pytorch/pytorch/pull/143782,yifuwang,tianyu-l,,,
362ecad9bbb,releng,not user facing,[ROCm] Use `linux.rocm.gpu.2` for 2-GPU and `linux.rocm.gpu.4` for 4-GPU runners (#143769),.github/actionlint.yaml .github/workflows/periodic.yml .github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/143769,jithunnair-amd,jeffdaily,,,
135c7db99d6,releng,not user facing,Use absolute path `path.resolve()` -> `path.absolute()` (#129409),.github/scripts/build_triton_wheel.py .github/scripts/close_nonexistent_disable_issues.py .github/scripts/collect_ciflow_labels.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_ci_workflows.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py .github/scripts/trymerge.py aten/src/ATen/native/transformers/cuda/flash_attn/kernels/generate_kernels.py aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/generate_kernels.py aten/src/ATen/nnapi/codegen.py benchmarks/distributed/rpc/parameter_server/launcher.py docs/source/scripts/build_activation_images.py mypy_plugins/check_mypy_version.py test/allowlist_for_publicAPI.json test/distributed/_tensor/test_dtensor.py test/distributed/elastic/rendezvous/out_of_tree_rendezvous_test.py test/distributed/flight_recorder/test_fr_analysis.py test/dynamo/test_cudagraphs_expandable_segments.py test/inductor/test_cudagraph_trees_expandable_segments.py test/jit/fixtures_srcs/generate_models.py test/jit/test_backend_nnapi.py test/mobile/test_bytecode.py test/mobile/test_upgrader_codegen.py test/mobile/test_upgraders.py test/onnx/error_reproduction.py test/package/generate_bc_packages.py test/package/test_directory_reader.py test/package/test_load_bc_packages.py test/package/test_misc.py test/package/test_save_load.py test/quantization/core/test_docs.py test/run_test.py test/test_cuda_expandable_segments.py test/test_serialization.py test/test_type_hints.py tools/amd_build/build_amd.py tools/build_with_debinfo.py tools/code_coverage/package/util/setting.py tools/generate_torch_version.py tools/jit/gen_unboxing.py tools/linter/adapters/clangtidy_linter.py tools/nvcc_fix_deps.py tools/onnx/update_default_opset_version.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/stats/sccache_stats_to_benchmark_format.py tools/stats/upload_artifacts.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py tools/testing/update_slow_tests.py tools/testing/upload_artifacts.py torch/_inductor/codecache.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/pattern_matcher.py torch/_logging/_internal.py torch/onnx/_internal/exporter/_capture_strategies.py torch/onnx/_internal/exporter/_core.py torch/package/package_exporter.py torch/testing/_internal/common_utils.py torch/utils/cpp_extension.py torch/utils/data/datapipes/gen_pyi.py torchgen/decompositions/gen_jit_decompositions.py torchgen/gen.py torchgen/gen_executorch.py torchgen/operator_versions/gen_mobile_upgraders.py torchgen/shape_functions/gen_jit_shape_functions.py,https://github.com/pytorch/pytorch/pull/129409,XuehaiPan,albanD,,,
060ee147537,inductor,Untopiced,[inductor] Make adaptive_max_pool2d error on int64 (#143762),test/inductor/test_torchinductor.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/143762,jansel,yanboliang,,,
783065637ea,skip,not user facing,Add FP8 support for eye (#139974),aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/cpu/TensorCompareKernel.cpp aten/src/ATen/native/cuda/TensorCompare.cu torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/139974,yanbing-j,jgong5,malfet,,
dbbc81cb347,skip,not user facing,Enabled force_shape_pad for test_pad_mm and test_slice_mm_bandwidth_computation (#141768),test/inductor/test_kernel_benchmark.py test/inductor/test_pad_mm.py,https://github.com/pytorch/pytorch/pull/141768,iupaikov-amd,jeffdaily,,,
b77406a9ece,distributed,not user facing,[BE][CI] bump `ruff` to 0.8.4 (#143753),.ci/aarch64_linux/build_aarch64_wheel.py .ci/pytorch/smoke_test/smoke_test.py .lintrunner.toml benchmarks/distributed/ddp/benchmark.py benchmarks/functional_autograd_benchmark/torchaudio_models.py benchmarks/tensorexpr/__main__.py caffe2/perfkernels/hp_emblookup_codegen.py functorch/examples/maml_regression/evjang.py functorch/examples/maml_regression/evjang_transforms.py functorch/examples/maml_regression/evjang_transforms_module.py test/distributed/test_c10d_common.py test/distributed/test_c10d_gloo.py test/distributed/test_c10d_nccl.py test/distributed/test_c10d_ucc.py test/distributed/test_store.py test/dynamo/test_modules.py test/test_mkldnn.py test/test_multiprocessing_spawn.py test/torch_np/numpy_tests/core/test_multiarray.py test/torch_np/numpy_tests/core/test_scalarmath.py third_party/generate-cpuinfo-wrappers.py third_party/generate-xnnpack-wrappers.py tools/code_coverage/package/util/utils.py torch/_inductor/codegen/cuda/cutlass_lib_extensions/gemm_operation_extensions.py torch/_numpy/testing/utils.py torch/autograd/gradcheck.py torch/distributed/benchmarks/benchmark_ddp_rpc.py torch/distributed/checkpoint/utils.py torch/distributed/fsdp/_common_utils.py torch/distributed/tensor/_dtensor_spec.py torch/multiprocessing/spawn.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/utils.py torch/package/_package_pickler.py torch/testing/_internal/common_utils.py torch/testing/_internal/dist_utils.py torch/testing/_internal/distributed/_shard/sharded_tensor/__init__.py torch/testing/_internal/distributed/distributed_test.py torch/testing/_internal/distributed/rpc/dist_optimizer_test.py torch/testing/_internal/distributed/rpc/rpc_test.py torch/utils/_cxx_pytree.py torch/utils/_freeze.py torch/utils/_pytree.py torch/utils/data/typing.ipynb torchgen/static_runtime/gen_static_runtime_ops.py,https://github.com/pytorch/pytorch/pull/143753,XuehaiPan,Skylion007,,,
aec3b462744,distributed,Untopiced,[DTensor] Add aten.amin/amax to linear_reduction_strategy (#143747),test/distributed/_tensor/test_dtensor_ops.py test/distributed/_tensor/test_math_ops.py test/distributed/flight_recorder/test_fr_analysis.py torch/distributed/tensor/_ops/_math_ops.py,https://github.com/pytorch/pytorch/pull/143747,lw,kwen2501,,,
6c32ef4c5bc,releng,Untopiced,Remove builder repo from workflows and scripts (#143776),.ci/manywheel/build_cuda.sh .ci/pytorch/run_tests.sh .github/actions/test-pytorch-binary/action.yml .github/templates/common.yml.j2 .github/templates/linux_binary_build_workflow.yml.j2 .github/templates/upload.yml.j2 .github/workflows/_binary-build-linux.yml .github/workflows/_binary-test-linux.yml .github/workflows/_binary-upload.yml .github/workflows/build-magma-windows.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-main.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/143776,atalman,huydhn,,,
844e6108f6b,skip,Untopiced,"Revert ""[Inductor XPU] Support max-autotune on XPU and reuse the corresponding Inductor UT. (#143266)""",test/inductor/test_kernel_benchmark.py test/inductor/test_max_autotune.py test/inductor/test_select_algorithm.py torch/_inductor/autotune_process.py torch/_inductor/codegen/triton.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,,,,,,
9e5f3fdfc73,dynamo,Untopiced,[dynamo] Shorten tracebacks for backend compiler errors (#143552),torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/output_graph.py torch/_inductor/exc.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/143552,jansel,yanboliang,,,
3e7f9e2cc4b,inductor,Untopiced,[inductor] Shorten tracebacks for errors inside inductor (by skipping AOTAutograd frames) (#143610),test/inductor/test_cpu_repro.py test/inductor/test_flex_attention.py torch/_inductor/compile_fx.py torch/_inductor/exc.py,https://github.com/pytorch/pytorch/pull/143610,jansel,eellison,,,
9035fb5a7b7,dynamo,not user facing,[dynamo] Add types to exc.py (#143626),torch/_dynamo/exc.py,https://github.com/pytorch/pytorch/pull/143626,jansel,yanboliang,,,
27b0d41f0ab,composability,not user facing,[ROCm] Add miopen_batch_norm to meta_registrations to fix AOTI issue (#143569),torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/143569,jataylo,jeffdaily,,,
7013be0094e,skip,Untopiced,Use random64 in Fischer-Yates algorithm for large N (#143682),aten/src/ATen/native/TensorFactories.cpp test/test_dataloader.py test/test_sparse_csr.py test/test_tensor_creation_ops.py test/torch_np/test_random.py,https://github.com/pytorch/pytorch/pull/143682,ngimel,albanD,eqy,malfet,
c0d710634fc,skip,not user facing,Respect ROCR_VISIBLE_DEVICES on AMD GPU device discovery (#142292),test/test_cuda.py torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/142292,tbennun,huydhn,jataylo,jeffdaily,
a8ac3a6b20b,inductor,not user facing,[inductor] fix the `adaptive_avg_pool` on processing int64 (#143802),test/inductor/test_torchinductor.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/143802,shaoyuyoung,jansel,,,
f42cff4e291,skip,not user facing,[17/N] Fix extra warnings brought by clang-tidy-17 (#143804),aten/src/ATen/MatrixRef.h aten/src/ATen/native/mkldnn/xpu/Conv.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.cpp aten/src/ATen/native/mkldnn/xpu/detail/oneDNNContext.h torch/csrc/Generator.cpp torch/csrc/Generator.h,https://github.com/pytorch/pytorch/pull/143804,cyyever,Skylion007,,,
4bacfd6e117,skip,not user facing,Sort requirements.txt (#143778),requirements.txt,https://github.com/pytorch/pytorch/pull/143778,Raymo111,albanD,,,
e05bfb8ee3e,skip,not user facing,[Submodule] Bump libfmt to 11.1.0 (#143843),third_party/fmt,https://github.com/pytorch/pytorch/pull/143843,cyyever,Skylion007,,,
bf8da4c145f,releng,not user facing,Bump jinja2 from 3.1.4 to 3.1.5 in /.ci/docker (#143844),.ci/docker/requirements-ci.txt,https://github.com/pytorch/pytorch/pull/143844,dependabot,Skylion007,,,
efac5ed81b7,inductor,not user facing,[inductor] Reorder imports in codecache.py (#143813),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/143813,jansel,Skylion007,,,
cf76c05b4dc,inductor,not user facing,[inductor] Refactor conditional triton imports into triton_compat.py (#143814),torch/_inductor/runtime/autotune_cache.py torch/_inductor/runtime/coordinate_descent_tuner.py torch/_inductor/runtime/triton_compat.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/143814,jansel,Skylion007,,,
9255ffc8414,skip,Untopiced,"Revert ""Enable more C++ warnings (#143355)""",.clang-format CMakeLists.txt aten/src/ATen/native/QuantizedLinear.cpp aten/src/ATen/native/RNN.cpp aten/src/ATen/native/quantized/cpu/LinearUnpackImpl.cpp aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp aten/src/ATen/native/quantized/cpu/fbgemm_utils.h aten/src/ATen/native/quantized/cpu/qlinear.cpp aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp aten/src/ATen/native/quantized/cudnn/Conv.cpp aten/src/ATen/native/quantized/cudnn/ConvPrepack.cpp aten/src/ATen/native/quantized/library.cpp aten/src/ATen/native/quantized/qconv_unpack.cpp aten/src/ATen/native/quantized/qlinear_unpack.cpp torch/csrc/distributed/c10d/GlooDeviceFactory.cpp torch/csrc/distributed/rpc/tensorpipe_agent.cpp torch/csrc/distributed/rpc/tensorpipe_cuda.cpp torch/csrc/jit/passes/onnx/constant_map.h torch/csrc/jit/serialization/export.cpp,,,,,,
cc4e70b7c3f,skip,Untopiced,"Revert ""Use absolute path `path.resolve()` -> `path.absolute()` (#129409)""",.github/scripts/build_triton_wheel.py .github/scripts/close_nonexistent_disable_issues.py .github/scripts/collect_ciflow_labels.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_ci_workflows.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py .github/scripts/trymerge.py aten/src/ATen/native/transformers/cuda/flash_attn/kernels/generate_kernels.py aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/generate_kernels.py aten/src/ATen/nnapi/codegen.py benchmarks/distributed/rpc/parameter_server/launcher.py docs/source/scripts/build_activation_images.py mypy_plugins/check_mypy_version.py test/allowlist_for_publicAPI.json test/distributed/_tensor/test_dtensor.py test/distributed/elastic/rendezvous/out_of_tree_rendezvous_test.py test/distributed/flight_recorder/test_fr_analysis.py test/dynamo/test_cudagraphs_expandable_segments.py test/inductor/test_cudagraph_trees_expandable_segments.py test/jit/fixtures_srcs/generate_models.py test/jit/test_backend_nnapi.py test/mobile/test_bytecode.py test/mobile/test_upgrader_codegen.py test/mobile/test_upgraders.py test/onnx/error_reproduction.py test/package/generate_bc_packages.py test/package/test_directory_reader.py test/package/test_load_bc_packages.py test/package/test_misc.py test/package/test_save_load.py test/quantization/core/test_docs.py test/run_test.py test/test_cuda_expandable_segments.py test/test_serialization.py test/test_type_hints.py tools/amd_build/build_amd.py tools/build_with_debinfo.py tools/code_coverage/package/util/setting.py tools/generate_torch_version.py tools/jit/gen_unboxing.py tools/linter/adapters/clangtidy_linter.py tools/nvcc_fix_deps.py tools/onnx/update_default_opset_version.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/stats/sccache_stats_to_benchmark_format.py tools/stats/upload_artifacts.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py tools/testing/update_slow_tests.py tools/testing/upload_artifacts.py torch/_inductor/codecache.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/pattern_matcher.py torch/_logging/_internal.py torch/onnx/_internal/exporter/_capture_strategies.py torch/onnx/_internal/exporter/_core.py torch/package/package_exporter.py torch/testing/_internal/common_utils.py torch/utils/cpp_extension.py torch/utils/data/datapipes/gen_pyi.py torchgen/decompositions/gen_jit_decompositions.py torchgen/gen.py torchgen/gen_executorch.py torchgen/operator_versions/gen_mobile_upgraders.py torchgen/shape_functions/gen_jit_shape_functions.py,,,,,,
475656fd9c3,skip,Untopiced,"Revert ""[BE][Easy] use `pathlib.Path` instead of `dirname` / `""..""` / `pardir` (#129374)""",.circleci/codegen_validation/normalize_yaml_fragment.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_binary_build_matrix.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py docs/source/scripts/build_opsets.py docs/source/scripts/build_quantization_configs.py docs/source/scripts/exportdb/generate_example_rst.py scripts/compile_tests/update_failures.py test/jit/test_backend_nnapi.py test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/onnx_test_common.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py test/quantization/core/test_docs.py test/test_typing.py tools/amd_build/build_amd.py tools/build_libtorch.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/util/setting.py tools/gen_vulkan_spv.py tools/linter/adapters/s3_init.py tools/onnx/update_default_opset_version.py tools/setup_helpers/cmake.py tools/setup_helpers/gen.py tools/setup_helpers/gen_unboxing.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_gen_backend_stubs.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py torch/_inductor/runtime/compile_tasks.py torch/testing/_internal/common_utils.py torchgen/_autoheuristic/mixed_mm/gen_data_mixed_mm.py torchgen/_autoheuristic/mixed_mm/test_mixed_mm.py torchgen/_autoheuristic/mixed_mm/train_decision_mixedmm.py torchgen/_autoheuristic/mm/gen_data_mm.py torchgen/_autoheuristic/mm/train_decision_mm.py torchgen/_autoheuristic/pad_mm/gen_data_pad_mm.py torchgen/_autoheuristic/pad_mm/test_pad_mm.py torchgen/_autoheuristic/pad_mm/train_decision_pad_mm.py torchgen/_autoheuristic/pad_mm/train_pad_mm.py torchgen/_autoheuristic/pad_mm/train_regression_pad_mm.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,,,,,,
78502a58ba2,distributed,Untopiced,Enable FSDP2 on XPU device (#143737),torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py torch/distributed/fsdp/_fully_shard/_fsdp_param.py torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py torch/distributed/fsdp/_fully_shard/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/143737,zhangxiaoli73,awgu,,,
51a7ecde804,skip,not user facing,[Easy] Bump CUDA nightly version to 11.8 / 12.4 / 12.6 in nightly pull tool (#143263),tools/nightly.py,https://github.com/pytorch/pytorch/pull/143263,XuehaiPan,malfet,,,
3df12d38cfe,dynamo,not user facing,dynamo tracing perf: cache cleaned_instructions: 33.7 -> 30.0 (#143070),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_dynamo/bytecode_transformation.py,https://github.com/pytorch/pytorch/pull/143070,aorenste,jansel,,,
96e9a5aeec2,releng,not user facing,[CI] Disable sccache for xpu test (#143851),.ci/pytorch/test.sh,https://github.com/pytorch/pytorch/pull/143851,chuanqi129,malfet,,,
2966fb37084,skip,not user facing,[pytorch/et] Allow ET to save additional resources for completing a trace like generated kernels and index tensor data (#143775),test/profiler/test_execution_trace.py torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/143775,sanrise,shengfukevin,wdvr,,
ee25daef5a4,skip,Untopiced,"Revert ""[dynamo] Remove dead code after introducing UserDefinedDictVariable (#143699)""",torch/_dynamo/guards.py torch/_dynamo/side_effects.py torch/_dynamo/source.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,,,,,,
26364428f5b,skip,Untopiced,"Revert ""[dynamo] Remove DICT_SUBCLASS_GUARD_MANAGER and use dict.keys (#143722)""",test/dynamo/test_dicts.py test/dynamo/test_guard_manager.py test/dynamo/test_misc.py test/dynamo/test_modules.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/csrc/dynamo/guards.cpp,,,,,,
3f80632c802,skip,Untopiced,Add torch._scaled_mm for CPU (#139975),aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/139975,yanbing-j,jgong5,malfet,mingfeima,
1598d458797,quantization,not user facing,remove allow-untyped-defs from torch/ao/__init__.py (#143604),torch/ao/__init__.py,https://github.com/pytorch/pytorch/pull/143604,bobrenjc93,aorenste,,,
8059d56ec36,skip,Untopiced,fix randint distribution for large max (#143787),aten/src/ATen/core/DistributionsHelper.h aten/src/ATen/native/cuda/DistributionTemplates.h aten/src/ATen/test/rng_test.h test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_tensor_creation_ops.py,https://github.com/pytorch/pytorch/pull/143787,ngimel,eqy,,,
c4bff71854d,releng,devs,[Easy] Add ROCm support to nightly pull tool (#141282),.github/scripts/generate_binary_build_matrix.py CONTRIBUTING.md Makefile tools/nightly.py,https://github.com/pytorch/pytorch/pull/141282,XuehaiPan,malfet,,,
373dba35f94,fx,not user facing,remove allow-untyped-defs from fx/experimental/refinement_types.py (#143868),torch/fx/experimental/refinement_types.py,https://github.com/pytorch/pytorch/pull/143868,bobrenjc93,Skylion007,,,
29841b94148,distributed,not user facing,remove allow-untyped-defs from torch/distributed/pipelining/_debug.py (#143871),torch/distributed/pipelining/_debug.py,https://github.com/pytorch/pytorch/pull/143871,bobrenjc93,Skylion007,,,
f3d0f67039b,inductor,not user facing,[inductor] Minor refactor of hip compile_meta (#143815),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/143815,jansel,eellison,,,
be1936804bd,inductor,Untopiced,[inductor] Drop support for pre-ASTSource Triton (#143817),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/143817,jansel,eellison,,,
138efb30021,inductor,not user facing,[inductor] Move GPUTarget backwards compat to triton_compat.py (#143818),torch/_inductor/runtime/triton_compat.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/143818,jansel,eellison,,,
6bdf2addc5e,inductor,not user facing,[inductor] Simplify get_launch_args_* handling (#143835),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/143835,jansel,eellison,shunting314,,
1cd70e7e237,distributed,Untopiced,[fr][c10d] log trace capture enabled or not in flight recorder (#143865),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/logger.hpp,https://github.com/pytorch/pytorch/pull/143865,c-p-i-o,fduwjj,,,
809106a93fa,distributed,not user facing,[fr][c10d] fix flaky test (#143878),test/distributed/flight_recorder/test_fr_analysis.py,https://github.com/pytorch/pytorch/pull/143878,c-p-i-o,fduwjj,,,
43853691bc6,quantization,Untopiced,[Quantization] add an option keep_original_weights in _lower_to_native_backend (#141049),test/quantization/fx/test_quantize_fx.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/ao/quantization/fx/convert.py torch/ao/quantization/fx/lower_to_fbgemm.py torch/ao/quantization/quantize_fx.py,https://github.com/pytorch/pytorch/pull/141049,hl475,jerryzh168,,,
d60282c177f,inductor,not user facing,remove allow-untyped-defs from _inductor/codegen/cpu_device_op_overrides.py (#143881),torch/_inductor/codegen/cpu_device_op_overrides.py,https://github.com/pytorch/pytorch/pull/143881,bobrenjc93,aorenste,,,
e296bab614f,skip,not user facing,[dynamo] Remove DICT_SUBCLASS_GUARD_MANAGER and use dict.keys (#143722),test/dynamo/test_dicts.py test/dynamo/test_guard_manager.py test/dynamo/test_misc.py test/dynamo/test_modules.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/143722,anijain2305,jansel,,,
0f474a960b9,skip,not user facing,[dynamo] Remove dead code after introducing UserDefinedDictVariable (#143699),torch/_dynamo/guards.py torch/_dynamo/side_effects.py torch/_dynamo/source.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/143699,anijain2305,jansel,williamwen42,yanboliang,
fca457b5db7,skip,Untopiced,"Revert ""Add torch._scaled_mm for CPU (#139975)""",aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,,,,,,
379bbef23c1,skip,Untopiced,Enable more C++ warnings (#143355),.clang-format CMakeLists.txt aten/src/ATen/native/QuantizedLinear.cpp aten/src/ATen/native/RNN.cpp aten/src/ATen/native/quantized/cpu/LinearUnpackImpl.cpp aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp aten/src/ATen/native/quantized/cpu/qlinear.cpp aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp aten/src/ATen/native/quantized/cpu/qlinear_prepack.cpp aten/src/ATen/native/quantized/cudnn/Conv.cpp aten/src/ATen/native/quantized/cudnn/ConvPrepack.cpp aten/src/ATen/native/quantized/library.cpp aten/src/ATen/native/quantized/library.h aten/src/ATen/native/quantized/qconv_unpack.cpp aten/src/ATen/native/quantized/qlinear_unpack.cpp torch/csrc/distributed/c10d/GlooDeviceFactory.cpp torch/csrc/distributed/rpc/tensorpipe_agent.cpp torch/csrc/distributed/rpc/tensorpipe_cuda.cpp torch/csrc/jit/passes/onnx/constant_map.h torch/csrc/jit/serialization/export.cpp,https://github.com/pytorch/pytorch/pull/143355,cyyever,albanD,,,
969415885d8,inductor,not user facing,[inductor][invoke_subgraph] Support None/int as input/output of invoke_subgraph (#139373),test/higher_order_ops/test_invoke_subgraph.py torch/_inductor/codegen/wrapper.py torch/_inductor/graph.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/139373,anijain2305,eellison,,,
ba5cacbc17e,quantization,not user facing,[Codemod][AddExplicitStrictExportArg] caffe2/test (#143688),docs/source/scripts/exportdb/generate_example_rst.py test/distributed/_tensor/experimental/test_tp_transform.py test/dynamo/test_export.py test/dynamo/test_fx_passes_pre_grad.py test/dynamo/test_sources.py test/export/test_draft_export.py test/export/test_hop.py test/export/test_pass_infra.py test/export/test_passes.py test/export/test_sparse.py test/export/test_swap.py test/export/test_unflatten.py test/functorch/test_control_flow.py test/fx/test_fx_split.py test/fx/test_fx_traceback.py test/fx/test_source_matcher_utils.py test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_package.py test/onnx/exporter/test_api.py test/onnx/onnx_test_common.py test/onnx/test_fx_passes.py test/onnx/test_fx_to_onnx.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py test/quantization/pt2e/test_metadata_porting.py test/quantization/pt2e/test_numeric_debugger.py test/quantization/pt2e/test_quantize_pt2e.py test/quantization/pt2e/test_quantize_pt2e_qat.py test/test_fake_tensor.py test/test_out_dtype_op.py,https://github.com/pytorch/pytorch/pull/143688,gmagogsfm,tugsbayasgalan,,,
f6801ba4b30,skip,Untopiced,"Revert ""Use random64 in Fischer-Yates algorithm for large N (#143682)""",aten/src/ATen/native/TensorFactories.cpp test/test_dataloader.py test/test_sparse_csr.py test/test_tensor_creation_ops.py test/torch_np/test_random.py,,,,,,
35714767395,skip,Untopiced,"Revert ""fix randint distribution for large max (#143787)""",aten/src/ATen/core/DistributionsHelper.h aten/src/ATen/native/cuda/DistributionTemplates.h aten/src/ATen/test/rng_test.h test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_tensor_creation_ops.py,,,,,,
9e8d84f8631,skip,not user facing,Fix duplicate pattern error (#139321),test/inductor/test_pattern_matcher.py torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/139321,eellison,shunting314,,,
a20765a9c1e,fx,Untopiced,subgraph rewriter supports matched pattern with no users (#143842),test/fx/test_subgraph_rewriter.py torch/fx/subgraph_rewriter.py,https://github.com/pytorch/pytorch/pull/143842,YangQun1,yushangdi,,,
fda9048ca8e,distributed (torchelastic),not user facing,remove allow-untyped-defs from distributed/elastic/multiprocessing/errors/handlers.py (#143869),torch/distributed/elastic/multiprocessing/errors/handlers.py,https://github.com/pytorch/pytorch/pull/143869,bobrenjc93,Skylion007,,,
a87cd5283b8,dynamo,not user facing,[dynamo] Trace through overridden __getattribute__ method (#143888),test/dynamo/test_misc.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/143888,anijain2305,jansel,,,
d2f769476f8,skip,not user facing,[Easy] add quotes to shell activation commands (#143902),tools/nightly.py,https://github.com/pytorch/pytorch/pull/143902,XuehaiPan,Skylion007,malfet,,
1e65dec2b97,dynamo,not user facing,[Dynamo] Add MPSDevice interface (#143891),torch/_dynamo/device_interface.py,https://github.com/pytorch/pytorch/pull/143891,malfet,jansel,,,
228b2284498,nested tensor_frontend,bug fixes,Fix batch-specific attention mod for NJT + Flex (#143866),test/test_nestedtensor.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/143866,jbschlosser,Skylion007,cpuhrsch,,
7a13bfa1ada,releng,not user facing,[EZ] Update jinja2 to 3.1.5 (#143923),.github/requirements-gha-cache.txt,https://github.com/pytorch/pytorch/pull/143923,malfet,Skylion007,,,
b5042cfa58b,skip,Untopiced,"Revert ""remove allow-untyped-defs from torch/ao/__init__.py (#143604)""",torch/ao/__init__.py,,,,,,
fe398de7694,skip,not user facing,[EZ] Update sympy to 1.13.3 (#143908),requirements.txt,https://github.com/pytorch/pytorch/pull/143908,malfet,Skylion007,huydhn,,
af823bd526d,skip,not user facing,remove allow-untyped-defs from utils/tensorboard/_convert_np.py (#143918),torch/utils/tensorboard/_convert_np.py,https://github.com/pytorch/pytorch/pull/143918,bobrenjc93,Skylion007,,,
c0c7f881da7,distributed,not user facing,remove allow-untyped-defs from distributed/pipelining/_unflatten.py (#143915),torch/distributed/pipelining/_unflatten.py,https://github.com/pytorch/pytorch/pull/143915,bobrenjc93,Skylion007,aorenste,malfet,
0de661dc274,optim,Untopiced,Add support for differentiable weight decay (#143679),test/optim/test_optim.py torch/optim/sgd.py,https://github.com/pytorch/pytorch/pull/143679,EmmettBicker,janeyx99,,,
928e01545c9,skip,not user facing,restore 'unused' variable to fix test_cuda_device_memory_allocated (#143885),test/test_cuda_multigpu.py,https://github.com/pytorch/pytorch/pull/143885,dnikolaev-amd,Skylion007,,,
63d6e1f743c,inductor,not user facing,remove allow-untyped-defs from _inductor/codegen/aoti_hipify_utils.py (#143916),torch/_inductor/codegen/aoti_hipify_utils.py,https://github.com/pytorch/pytorch/pull/143916,bobrenjc93,Skylion007,,,
c17d7676863,inductor,not user facing,remove allow-untyped-defs from _inductor/codegen/rocm/rocm_template_buffer.py (#143870),torch/_inductor/codegen/rocm/rocm_template_buffer.py,https://github.com/pytorch/pytorch/pull/143870,bobrenjc93,Skylion007,aorenste,,
b54620f40f6,inductor,not user facing,"[CUTLASS] fix bugs: extra data_ptr() call, wrong size symbol name, bias symbol not added (#143528)",torch/_inductor/codegen/cuda/cuda_kernel.py,https://github.com/pytorch/pytorch/pull/143528,ColinPeppler,henrylhtsang,,,
e3fefdfbf0b,inductor,not user facing,[CUTLASS] fix addmm (#143537),test/inductor/test_cutlass_backend.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/143537,ColinPeppler,chenyang78,,,
88ccf2fa5e4,distributed (torchelastic),not user facing,remove allow-untyped-defs from distributed/elastic/multiprocessing/subprocess_handler/handlers.py (#143917),torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py,https://github.com/pytorch/pytorch/pull/143917,bobrenjc93,Skylion007,,,
7c343a9d680,inductor,not user facing,Fix emulate low precision bool inp (#143657),test/inductor/test_cuda_repro.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/143657,eellison,BoyuanFeng,,,
c3c27aef342,dynamo,not user facing,[dynamo] Remove HFPretrained config hack (#143698),benchmarks/dynamo/ci_expected_accuracy/aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/cu124/dynamic_inductor_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/cu124/inductor_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_huggingface_training.csv torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/143698,anijain2305,jansel,williamwen42,,
ad78edee8ef,quantization,Untopiced,"Add support for list, tuple and dict in numeric debugger (#143882)",test/quantization/pt2e/test_numeric_debugger.py torch/ao/quantization/pt2e/_numeric_debugger.py torch/testing/_internal/common_quantization.py,https://github.com/pytorch/pytorch/pull/143882,jerryzh168,dulinriley,,,
4a7cf0dbff5,inductor,not user facing,[Inductor] Add MPS device op overrides (#143892),torch/_inductor/codegen/common.py torch/_inductor/codegen/mps_device_op_overrides.py,https://github.com/pytorch/pytorch/pull/143892,malfet,jansel,,,
1e246ef05b0,skip,not user facing,[CUDA][CUDA graphs][RNG] Skip replay prologue if `wholegraph_increment` is 0 (#143777),aten/src/ATen/cuda/CUDAGeneratorImpl.cpp,https://github.com/pytorch/pytorch/pull/143777,eqy,eellison,ngimel,,
d3e9133ab21,inductor,not user facing,"Fix separate in process bisector cache, cleanup on exit (#143661)",torch/_inductor/compiler_bisector.py,https://github.com/pytorch/pytorch/pull/143661,eellison,ezyang,,,
cbc4cf3043a,skip,Untopiced,Add torch._scaled_mm for CPU (#139975),aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/139975,yanbing-j,jgong5,malfet,mingfeima,
3fabd10c40c,skip,not user facing,Add AOT inductor support for _scaled_mm for CPU (#141961),test/inductor/test_aot_inductor.py torch/_inductor/graph.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/141961,yanbing-j,malfet,,,
01980cac388,dynamo,not user facing,[dynamo] Make ConstDictKeySource a subclass of ChainedSource (#143924),torch/_dynamo/source.py,https://github.com/pytorch/pytorch/pull/143924,anijain2305,jansel,,,
74028cfd0c3,inductor,not user facing,[Inductor][CPP] Fix Data Type issue of frexp (#143746),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/143746,leslie-fang-intel,jgong5,,,
d8c3900d80c,mps,improvements,[Inductor] Implement primitive Metal compiler (#143893),test/inductor/test_mps_basic.py test/run_test.py torch/_inductor/codegen/common.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/143893,malfet,jansel,,,
8cccc46e334,skip,Untopiced,"Revert ""Add AOT inductor support for _scaled_mm for CPU (#141961)""",test/inductor/test_aot_inductor.py torch/_inductor/graph.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,,,,,,
45a709d9ec5,skip,Untopiced,"Revert ""Add torch._scaled_mm for CPU (#139975)""",aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,,,,,,
3054aae493a,mps,bug fixes,[MPS] Fix fmin/fmax for scalar argument (#143934),aten/src/ATen/native/mps/OperationUtils.h test/test_mps.py,https://github.com/pytorch/pytorch/pull/143934,malfet,Skylion007,,,
e1abbe155ec,quantization,not user facing,remove allow-untyped-defs from ao/nn/qat/dynamic/modules/linear.py (#143919),torch/ao/nn/qat/dynamic/modules/linear.py,https://github.com/pytorch/pytorch/pull/143919,bobrenjc93,Skylion007,malfet,,
85f348578bc,caffe2,not user facing,[Codemod][AddExplicitStrictExportArg] caffe2/test/inductor (#143929),test/inductor/test_aot_inductor_package.py,https://github.com/pytorch/pytorch/pull/143929,gmagogsfm,hl475,,,
3ba6fcd3ff9,skip,not user facing,remove allow-untyped-defs from torch/_size_docs.py (#143942),torch/_size_docs.py,https://github.com/pytorch/pytorch/pull/143942,bobrenjc93,Skylion007,,,
cf0b72c4ab9,inductor,not user facing,remove allow-untyped-defs from _inductor/compile_worker/watchdog.py (#143941),torch/_inductor/compile_worker/watchdog.py,https://github.com/pytorch/pytorch/pull/143941,bobrenjc93,Skylion007,,,
7101b8ca357,onnx,not user facing,remove allow-untyped-defs from onnx/_internal/_lazy_import.py (#143943),torch/onnx/_internal/_lazy_import.py,https://github.com/pytorch/pytorch/pull/143943,bobrenjc93,justinchuby,,,
b6bdb67f820,releng,not user facing,"[BE][Easy] use `pathlib.Path` instead of `dirname` / `""..""` / `pardir` (#129374)",.circleci/codegen_validation/normalize_yaml_fragment.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_binary_build_matrix.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py benchmarks/dynamo/ci_expected_accuracy/update_expected.py docs/source/scripts/build_opsets.py docs/source/scripts/build_quantization_configs.py docs/source/scripts/exportdb/generate_example_rst.py scripts/compile_tests/update_failures.py test/jit/test_backend_nnapi.py test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/onnx_test_common.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py test/quantization/core/test_docs.py test/test_typing.py tools/amd_build/build_amd.py tools/build_libtorch.py tools/code_coverage/package/oss/utils.py tools/code_coverage/package/util/setting.py tools/gen_vulkan_spv.py tools/linter/adapters/s3_init.py tools/onnx/update_default_opset_version.py tools/setup_helpers/cmake.py tools/setup_helpers/gen.py tools/setup_helpers/gen_unboxing.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_gen_backend_stubs.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py torch/_inductor/runtime/compile_tasks.py torch/testing/_internal/common_utils.py torchgen/_autoheuristic/mixed_mm/gen_data_mixed_mm.py torchgen/_autoheuristic/mixed_mm/test_mixed_mm.py torchgen/_autoheuristic/mixed_mm/train_decision_mixedmm.py torchgen/_autoheuristic/mm/gen_data_mm.py torchgen/_autoheuristic/mm/train_decision_mm.py torchgen/_autoheuristic/pad_mm/gen_data_pad_mm.py torchgen/_autoheuristic/pad_mm/test_pad_mm.py torchgen/_autoheuristic/pad_mm/train_decision_pad_mm.py torchgen/_autoheuristic/pad_mm/train_pad_mm.py torchgen/_autoheuristic/pad_mm/train_regression_pad_mm.py torchgen/gen_backend_stubs.py torchgen/gen_lazy_tensor.py,https://github.com/pytorch/pytorch/pull/129374,XuehaiPan,justinchuby,malfet,,
79b354ee37b,skip,not user facing,[inductor] Make generated kernels deterministic (#143951),test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143951,jansel,oulgen,,,
a7915c56f6a,dynamo,Untopiced,Propagate callable parameter types using ParamSpec (#142306) (#143797),torch/_dynamo/external_utils.py torch/_dynamo/testing.py torch/_strobelight/cli_function_profiler.py torch/_subclasses/fake_tensor.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/utils/_stats.py torch/utils/_strobelight/cli_function_profiler.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/143797,kaspell,Skylion007,,,
92d8965082b,optim,Untopiced,"Adding support for differentiable lr, weight_decay, and betas in Adam/AdamW (#143726)",test/optim/test_optim.py torch/optim/adam.py,https://github.com/pytorch/pytorch/pull/143726,EmmettBicker,janeyx99,,,
cf891271372,package,Untopiced,[Torch.package] Add support for UntypedStorage tensors (#143930),test/package/test_save_load.py torch/package/package_importer.py,https://github.com/pytorch/pytorch/pull/143930,henryhu6,henrylhtsang,,,
1b0d19a2cbb,skip,Untopiced,"Revert ""[inductor] Make generated kernels deterministic (#143951)""",test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/utils.py,,,,,,
2ed4d65af0a,skip,not user facing,Update torch-xpu-ops commit pin (#143853),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/143853,xytintel,EikanWang,,,
2fa09853cbd,skip,not user facing,Update slow tests (#143745),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/143745,pytorchupdatebot,pytorchbot,,,
438698b20b5,releng,not user facing,[CD] Remove redundant triton dependency for xpu wheels (#143839),.circleci/scripts/binary_populate_env.sh,https://github.com/pytorch/pytorch/pull/143839,chuanqi129,huydhn,,,
dca443835e5,cpp_frontend,Untopiced,Enable more readability-redundant checks (#143963),.clang-tidy aten/src/ATen/StorageUtils.cpp aten/src/ATen/core/Vitals.cpp aten/src/ATen/core/Vitals.h aten/src/ATen/core/blob.h aten/src/ATen/core/class_type.cpp aten/src/ATen/core/class_type.h aten/src/ATen/core/function.h aten/src/ATen/core/function_schema.h aten/src/ATen/core/jit_type.h aten/src/ATen/core/library.cpp c10/core/Allocator.h c10/cuda/CUDAAllocatorConfig.cpp c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDAGuard.h c10/test/util/string_util_test.cpp c10/util/LeftRight.h c10/util/signal_handler.cpp torch/csrc/CudaIPCTypes.cpp torch/csrc/api/include/torch/detail/TensorDataContainer.h torch/csrc/api/include/torch/nn/modules/container/sequential.h torch/csrc/autograd/function.h torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/profiler_python.cpp torch/csrc/distributed/autograd/engine/dist_engine.cpp torch/csrc/distributed/c10d/FileStore.cpp torch/csrc/distributed/c10d/Work.cpp torch/csrc/distributed/rpc/rref_impl.cpp torch/csrc/dynamo/guards.cpp torch/csrc/inductor/aoti_eager/kernel_holder.h torch/csrc/inductor/aoti_package/model_package_loader.cpp torch/csrc/lazy/core/hash.h torch/csrc/profiler/standalone/execution_trace_observer.cpp torch/csrc/utils/invalid_arguments.cpp torch/csrc/utils/throughput_benchmark.h torch/library.h,https://github.com/pytorch/pytorch/pull/143963,cyyever,albanD,,,
7c1c0730bee,skip,not user facing,[Inductor UT] Generalize newly introduced device-bias hard code in (#143975),test/inductor/test_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/143975,etaf,malfet,,,
beb6c2dea57,mps,bug fixes,[MPS] Fix crash when mm is invoked with mixed dtypes (#143948),aten/src/ATen/native/mps/operations/LinearAlgebra.mm,https://github.com/pytorch/pytorch/pull/143948,malfet,Skylion007,,,
c27c788e35d,mps,Untopiced,"[MPS] Fix `torch.add(x,y, alpha=2)` crash (#143949)",aten/src/ATen/native/mps/operations/BinaryOps.mm test/inductor/test_mps_basic.py test/test_mps.py,https://github.com/pytorch/pytorch/pull/143949,malfet,Skylion007,,,
9d026000de0,quantization,Untopiced,change import relative paths due to internal build failures (#143968),aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp aten/src/ATen/native/quantized/library.cpp,https://github.com/pytorch/pytorch/pull/143968,wdvr,albanD,,,
baee623691a,skip,not user facing,[BE][Ez]: Update fmtlib submodule to 1.11.1 (#143937),third_party/fmt,https://github.com/pytorch/pytorch/pull/143937,Skylion007,albanD,,,
d260bc4476e,inductor,not user facing,cpp_wrapper: minimize pybind11 dependency (#143772),torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/143772,benjaminglass1,Skylion007,,,
d88a8c41d5e,skip,not user facing,"Fix flaky ""Upload test stats"" job (#143991)",tools/stats/check_disabled_tests.py tools/stats/upload_test_stats.py,https://github.com/pytorch/pytorch/pull/143991,benjaminglass1,clee2000,huydhn,,
2da7fb53201,skip,not user facing,[inductor] Make generated kernels deterministic (#143951),test/dynamo/test_structured_trace.py test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143951,jansel,oulgen,,,
d9a6ffb63c2,distributed,Untopiced,[FSDP] Add workaround to fix `buffer_dtype` without root parameters (#143989),test/distributed/fsdp/test_fsdp_mixed_precision.py torch/distributed/fsdp/_runtime_utils.py,https://github.com/pytorch/pytorch/pull/143989,awgu,H-Huang,,,
934eaa503f0,skip,not user facing,[Inductor XPU] Support max-autotune on XPU and reuse the corresponding Inductor UT. (#143266),test/inductor/test_gpu_cpp_wrapper.py test/inductor/test_kernel_benchmark.py test/inductor/test_max_autotune.py test/inductor/test_select_algorithm.py torch/_inductor/autotune_process.py torch/_inductor/codegen/triton.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143266,etaf,EikanWang,jansel,,
af629a81465,cpp_frontend,Untopiced,Enable readability-redundant-declaration (#143982),.clang-tidy c10/core/TensorOptions.h torch/csrc/Size.cpp torch/csrc/utils/python_compat.h torch/custom_class_detail.h,https://github.com/pytorch/pytorch/pull/143982,cyyever,Skylion007,,,
cb24013b5b6,profiler,not user facing,Fix assertion failure in pytorch profiler (#143940),torch/csrc/autograd/profiler_python.cpp,https://github.com/pytorch/pytorch/pull/143940,kadeng,Skylion007,sraikund16,,
11bb94b7eaf,mps,improvements,[MPSInductor] Fix index generation for transpose (#143973),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/143973,malfet,jansel,,,
8df99b6a6e1,jit,Untopiced,Remove unneeded std::make_optional (#143575),torch/csrc/autograd/python_variable.cpp torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/143575,cyyever,Skylion007,,,
f3e5078c277,inductor,not user facing,[Inductor] Relax size constraints for re-inplacing (#143884),test/inductor/test_torchinductor.py torch/_inductor/codegen/wrapper.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/143884,BoyuanFeng,eellison,,,
a2753e376bb,inductor,Untopiced,[Inductor] Support tiling reduction dimensions   (#137243),test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/config.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/137243,blaine-rister,jansel,,,
01034e963c9,inductor,not user facing,[AOTI] Not use AOTI_TORCH_CHECK in non AOTI mode. (#143970),torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/143970,etaf,EikanWang,jansel,,
16a57e232c6,dynamo,deprecation,removed dead code for dynamo flag dead_code_elimination (#140938),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/140938,vinayak912002,zou3519,,,
39450ae6557,xpu,Untopiced,Refine XPU external Stream (#142347),c10/xpu/XPUStream.cpp c10/xpu/XPUStream.h c10/xpu/test/impl/XPUCachingAllocatorTest.cpp c10/xpu/test/impl/XPUStreamTest.cpp,https://github.com/pytorch/pytorch/pull/142347,guangyey,albanD,gujinghui,,
a68c0ca4979,xpu,Untopiced,Add low priority XPU Stream (#141119),c10/xpu/XPUStream.cpp c10/xpu/XPUStream.h c10/xpu/test/impl/XPUStreamTest.cpp torch/xpu/streams.py,https://github.com/pytorch/pytorch/pull/141119,guangyey,albanD,gujinghui,,
8f6c4d17321,xpu,Untopiced,Add get_stream_from_external API for XPU backend (#141123),docs/source/xpu.rst torch/_C/__init__.pyi.in torch/csrc/xpu/Module.cpp torch/xpu/__init__.py,https://github.com/pytorch/pytorch/pull/141123,guangyey,EikanWang,albanD,,
3848de55edf,python_frontend,Untopiced,Add get_stream_from_external API for CUDA backend (#143799),docs/source/cuda.rst test/test_cuda_multigpu.py torch/_C/__init__.pyi.in torch/csrc/cuda/Module.cpp torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/143799,guangyey,EikanWang,albanD,,
09e47ab7ab2,cuda,improvements,Refine CUDA Stream priority (#143849),aten/src/ATen/test/cuda_stream_test.cpp c10/cuda/CUDAStream.cpp torch/cuda/streams.py,https://github.com/pytorch/pytorch/pull/143849,guangyey,EikanWang,albanD,,
f0f09bb3c29,inductor,not user facing,[MPSInductor] Implement minimum and maximum ops (#143977),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/143977,malfet,jansel,,,
5ef0de76151,inductor,not user facing,[MPSInductor] Fix multiple kernel generation (#143998),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/143998,malfet,jansel,ruidazeng,,
eec30916e78,skip,Untopiced,"Revert ""Update low prec codegen for div/mod (#142350)""",test/inductor/test_op_dtype_prop.py test/inductor/test_pattern_matcher.py torch/_inductor/codegen/triton.py,,,,,,
d8a2796fb67,skip,Untopiced,"Revert ""[Inductor UT] Generalize newly introduced device-bias hard code in (#143975)""",test/inductor/test_pattern_matcher.py,,,,,,
a174ee2255a,skip,Untopiced,"Revert ""Fix duplicate pattern error (#139321)""",test/inductor/test_pattern_matcher.py torch/_inductor/pattern_matcher.py,,,,,,
e7ed660233b,inductor,not user facing,[inductor] Add missing py312 xfail (#144006),test/inductor/test_cuda_repro.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/144006,jansel,Skylion007,,,
8d9ff9c8a41,skip,not user facing,Fix a bug for wrong stride in fake tensor (#141427),aten/src/ATen/native/UnaryOps.cpp test/test_ops.py test/test_torch.py,https://github.com/pytorch/pytorch/pull/141427,ywq880611,jansel,,,
dec1a6d0f05,dynamo,not user facing,[dynamo] Separate out GetItemSource and DictGetItemSource (#143926),test/dynamo/test_exc.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/143926,anijain2305,jansel,,,
f242dbb76f9,distributed,Untopiced,[dtensor] add src_data_rank to distribute_tensor API (#143883),test/distributed/_tensor/test_api.py torch/distributed/tensor/_api.py torch/distributed/tensor/_collective_utils.py torch/distributed/tensor/placement_types.py,https://github.com/pytorch/pytorch/pull/143883,wanchaol,XilunWu,tianyu-l,,
0431d47eaaa,distributed,Untopiced,[tp] propagate src_data_rank kwarg in TP API (#144005),test/distributed/tensor/parallel/test_parallelize_api.py torch/distributed/tensor/parallel/api.py torch/distributed/tensor/parallel/style.py,https://github.com/pytorch/pytorch/pull/144005,wanchaol,tianyu-l,,,
a93e75d1e26,mps,Untopiced,[MPS] Handle implicit cpu-scalar-to-gpu transfer (#144055),aten/src/ATen/native/mps/OperationUtils.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/144055,malfet,Skylion007,,,
916b510ff57,skip,not user facing,Enable mkldnn pattern matcher tests for BF16 on AArch64 (#144030),test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/144030,Mousius,malfet,,,
8f3eb843730,releng,not user facing,ROCm: Enable 4 gpu tests for distributed config (#140319),.ci/pytorch/test.sh test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/_shard/sharded_tensor/test_sharded_tensor.py test/distributed/_tensor/test_tensor_ops.py test/distributed/fsdp/test_fsdp_optim_state.py test/distributed/test_symmetric_memory.py,https://github.com/pytorch/pytorch/pull/140319,jagadish-amd,jeffdaily,kwen2501,malfet,
8506a2af9ac,export,not user facing,remove allow-untyped-defs from _export/pass_infra/proxy_value.py (#143944),torch/_export/pass_base.py torch/_export/pass_infra/proxy_value.py,https://github.com/pytorch/pytorch/pull/143944,bobrenjc93,aorenste,,,
a8c98ce175e,sparse_frontend,Untopiced,[cutlass-3] Update third-party/cutlass-3 from 3.4 to 3.5.1 (#143515),aten/src/ATen/native/cuda/MixedDtypesLinear.cu aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/nested/cuda/NestedTensorMatmul.cu aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cu aten/src/ATen/native/sparse/cuda/SparseSemiStructuredLinear.cu aten/src/ATen/native/sparse/cuda/SparseSemiStructuredOps.cu aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_preprocess_kernel.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_kernel.h torch/_inductor/codegen/cuda/cutlass_utils.py torch/csrc/distributed/c10d/cuda/AsyncMM.cu,https://github.com/pytorch/pytorch/pull/143515,drisspg,Skylion007,eqy,,
bb5e439f2d8,skip,not user facing,Add networkx as bazel dep to fix CI failure (#143995),BUILD.bazel tools/build/bazel/requirements.in tools/build/bazel/requirements.txt,https://github.com/pytorch/pytorch/pull/143995,clee2000,huydhn,,,
3eb3f4ed558,releng,not user facing,Upload METADATA file with whl binaries (#143677),.circleci/scripts/binary_upload.sh .lintrunner.toml scripts/release/upload_metadata_file.py,https://github.com/pytorch/pytorch/pull/143677,clee2000,seemethere,,,
496fc909651,releng,not user facing,[CI] Multigpu 1 -> 2 shards (#143992),.ci/pytorch/multigpu-test.sh .github/workflows/periodic.yml,https://github.com/pytorch/pytorch/pull/143992,clee2000,huydhn,,,
28a74fe3aa3,skip,not user facing,remove allow-untyped-defs from torch/mps/event.py (#144092),torch/mps/event.py,https://github.com/pytorch/pytorch/pull/144092,bobrenjc93,aorenste,,,
bdfb40ed29a,skip,not user facing,remove allow-untyped-defs from utils/_import_utils.py (#144089),torch/utils/_import_utils.py,https://github.com/pytorch/pytorch/pull/144089,bobrenjc93,aorenste,,,
0d6db839a75,dataloader_frontend,not user facing,remove allow-untyped-defs from utils/data/datapipes/iter/streamreader.py (#144088),torch/utils/data/datapipes/iter/streamreader.py,https://github.com/pytorch/pytorch/pull/144088,bobrenjc93,aorenste,,,
377e29745f1,distributed (torchelastic),not user facing,remove allow-untyped-defs from distributed/elastic/utils/data/cycling_iterator.py (#144090),torch/distributed/elastic/utils/data/cycling_iterator.py,https://github.com/pytorch/pytorch/pull/144090,bobrenjc93,aorenste,,,
891a86d1ada,quantization,not user facing,remove allow-untyped-defs from ao/quantization/experimental/fake_quantize.py (#144091),torch/ao/quantization/experimental/fake_quantize.py,https://github.com/pytorch/pytorch/pull/144091,bobrenjc93,aorenste,,,
37e9da06871,skip,not user facing,[ROCm][Windows] Disable roctracer-related code (#143329),cmake/Dependencies.cmake torch/CMakeLists.txt torch/csrc/cuda/shared/nvtx.cpp torch/csrc/profiler/stubs/cuda.cpp,https://github.com/pytorch/pytorch/pull/143329,m-gallus,sraikund16,,,
c31912666e6,releng,not user facing,[ROCm] Print amdgpu info on bare metal for CI runners (#144038),.github/actions/setup-rocm/action.yml,https://github.com/pytorch/pytorch/pull/144038,jithunnair-amd,jeffdaily,,,
c5b75f8db14,skip,not user facing,[AOTI] Remove more AOTI_TORCH_EXPORT (#144081),torch/csrc/inductor/aoti_torch/shim_cuda.cpp torch/csrc/inductor/aoti_torch/shim_mkldnn.cpp torch/csrc/inductor/aoti_torch/shim_xpu.cpp,https://github.com/pytorch/pytorch/pull/144081,desertfire,yushangdi,,,
41b5c600df8,linalg_frontend,Untopiced,[ReduceOps] Add dimension checking for cummin()/cummax(). (#143920),aten/src/ATen/native/ReduceOps.cpp test/test_torch.py,https://github.com/pytorch/pytorch/pull/143920,dcci,malfet,,,
48a05ee7735,distributed,Untopiced,[dtensor] improve doc of the DTensor class (#144099),docs/source/distributed.tensor.rst torch/distributed/tensor/_api.py,https://github.com/pytorch/pytorch/pull/144099,wanchaol,awgu,,,
e141cb9c34e,skip,not user facing,export AOTI_TORCH_EXPORT on Windows. (#140030),CMakeLists.txt caffe2/CMakeLists.txt torch/CMakeLists.txt torch/csrc/inductor/aoti_torch/c/shim.h,https://github.com/pytorch/pytorch/pull/140030,xuhancn,desertfire,jgong5,malfet,
00df63f09f0,skip,not user facing,[ROCm] Fix for ld failed to convert GOTPCREL relocation in PyTorch build (#143986),cmake/Dependencies.cmake,https://github.com/pytorch/pytorch/pull/143986,hongxiayang,jeffdaily,,,
fb1beb31d28,dynamo,not user facing,[dynamo][BE] move `dropwhile` polyfill to submodule `polyfills.itertools` (#144066),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/144066,XuehaiPan,jansel,,,
d9507548d83,dynamo,not user facing,[dynamo][BE] move `zip_longest` polyfill to submodule `polyfills.itertools` (#144067),torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/variables/iter.py,https://github.com/pytorch/pytorch/pull/144067,XuehaiPan,yanboliang,,,
c09bf71bd61,inductor,not user facing,[Inductor][CPU] Fix C++ compile error of torch.max on bool type (#143848),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/143848,blzheng,jgong5,leslie-fang-intel,,
55dc61dd521,dataloader_frontend,Untopiced,Dataloader distribute tasks to workers when in_order is False (#142324),test/test_dataloader.py torch/utils/data/dataloader.py,https://github.com/pytorch/pytorch/pull/142324,michael-diggin,andrewkho,,,
a1ae8fadc70,skip,not user facing,[cpu][vec] support reduce ops for add and max (#144065),aten/src/ATen/cpu/vec/vec256/vec256_float.h aten/src/ATen/cpu/vec/vec256/vec256_int.h aten/src/ATen/cpu/vec/vec512/vec512_float.h aten/src/ATen/cpu/vec/vec512/vec512_int.h aten/src/ATen/cpu/vec/vec_base.h,https://github.com/pytorch/pytorch/pull/144065,Valentine233,mingfeima,,,
b336d72dae0,inductor,not user facing,[MPSInductor] Preserve dtype during load (#144051),torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144051,malfet,Skylion007,,,
f7644efa79f,inductor,not user facing,[MPSInductor][EZ] Fix logical_[or|end] ops (#144122),torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144122,malfet,huydhn,,,
e88d06f54ee,releng,not user facing,ir.ExternKernel: correctly handle kwarg default arguments (#141371),.ci/pytorch/test.sh .github/workflows/inductor-unittest.yml torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/141371,benjaminglass1,desertfire,,,
b5b419d6276,releng,not user facing,cpp_wrapper: Use runtime dispatched fallbacks for complex ops (#143223),.ci/pytorch/test.sh test/inductor/test_torchinductor.py torch/_inductor/ir.py torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp,https://github.com/pytorch/pytorch/pull/143223,benjaminglass1,desertfire,,,
b9fbd65dfd5,skip,not user facing,AOTI fallback ops: remove ops that were never codegen'ed (#143421),torchgen/aoti/fallback_ops.py,https://github.com/pytorch/pytorch/pull/143421,benjaminglass1,desertfire,,,
cbdc70ae07f,releng,not user facing,Use the build environment as sccache prefix instead of workflow name (#144112),.github/workflows/_linux-build.yml .github/workflows/_linux-test.yml,https://github.com/pytorch/pytorch/pull/144112,huydhn,malfet,,,
f3968373c12,releng,not user facing,Migrate the rest of CUDA 12.1 jobs to 12.4 (#144118),.ci/docker/build.sh .github/workflows/inductor-micro-benchmark.yml .github/workflows/inductor-perf-compare.yml .github/workflows/inductor-perf-test-nightly.yml .github/workflows/inductor-periodic.yml .github/workflows/periodic.yml,https://github.com/pytorch/pytorch/pull/144118,huydhn,atalman,,,
e93f625d009,inductor,not user facing,[AOTI] don't codegen autotune_at_compile_time for non-Triton kernels (#143990),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cpp_wrapper_gpu.py,https://github.com/pytorch/pytorch/pull/143990,ColinPeppler,chenyang78,desertfire,henrylhtsang,
60fe8a65af5,inductor,not user facing,[Inductor] Generalize tiling algorithm to handle fused reductions (#144041),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/codegen/simd.py,https://github.com/pytorch/pytorch/pull/144041,blaine-rister,jansel,,,
2409b49a33c,skip,Untopiced,"Revert ""Rewrite _reparametrize_module to use `contextmanager` (#138203)""",torch/nn/utils/stateless.py,,,,,,
a450e177fdc,dynamo,not user facing,[dynamo] remove inline inbuilt tests as flag is enabled by default (#144129),test/dynamo/test_inline_inbuilt_nn_modules.py,https://github.com/pytorch/pytorch/pull/144129,anijain2305,williamwen42,,,
732359c6338,dynamo,not user facing,[dynamo][easy] Minor fixes in guards.cpp (#144130),torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/144130,anijain2305,williamwen42,,,
c5c897c3a12,dynamo,not user facing,[dynamo][easy] Miscellaneous fixes (#144141),test/dynamo/test_dicts.py test/dynamo/test_misc.py test/dynamo/test_modules.py torch/_dynamo/side_effects.py,https://github.com/pytorch/pytorch/pull/144141,anijain2305,williamwen42,,,
8d63a4a4092,skip,Untopiced,"Revert ""Set `enable_trace_contextlib_contextmanager` flag to True (#140604)""",benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv torch/_dynamo/config.py,,,,,,
a7b61c5b492,inductor,not user facing,[MPSInductor] Add signbit op support (#144105),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144105,malfet,Skylion007,jansel,,
56f6289f6ae,inductor,Untopiced,[mps/inductor] Add support for atanh(). (#144121),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144121,dcci,jansel,malfet,,
6e8dca9ff36,skip,not user facing,[while_loop][aot] auto-unspecialize int input and output to unbacked symints (#143105),test/functorch/test_control_flow.py torch/_higher_order_ops/while_loop.py,https://github.com/pytorch/pytorch/pull/143105,ydwu4,zou3519,,,
56607098564,dynamo,not user facing,[hop][BE] unify meta checking with check_meta_consistency (#143545),test/dynamo/test_export.py test/functorch/test_control_flow.py test/inductor/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/while_loop.py,https://github.com/pytorch/pytorch/pull/143545,ydwu4,zou3519,,,
c36f94b3738,dynamo,not user facing,[while_loop][dynamo] auto-unspecialize int input and output to unbacked symints (#143106),test/dynamo/test_higher_order_ops.py test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/utils.py torch/_higher_order_ops/while_loop.py,https://github.com/pytorch/pytorch/pull/143106,ydwu4,zou3519,,,
7b69f7b4490,optim,docs,Clarify what we mean by decoupled weight decay in the *AdamWs (#144101),torch/optim/adamw.py torch/optim/nadam.py torch/optim/radam.py,https://github.com/pytorch/pytorch/pull/144101,janeyx99,albanD,,,
8b3479e3611,distributed,not user facing,remove allow-untyped-defs from torch/distributed/fsdp/_dynamo_utils.py (#144131),torch/distributed/fsdp/_dynamo_utils.py,https://github.com/pytorch/pytorch/pull/144131,bobrenjc93,Skylion007,,,
383ff4011c0,skip,not user facing,[ez] Use strip for arg sanitization in upload_metadata_file to improve readability (#144155),scripts/release/upload_metadata_file.py,https://github.com/pytorch/pytorch/pull/144155,clee2000,Skylion007,huydhn,,
52e107a7cab,inductor,not user facing,"[MPSInductor] Add `constant`, `isinf` and `isnan` ops (#144156)",test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144156,malfet,Skylion007,jansel,,
ad093956748,mps,improvements,[MPSInductor] Fix multi rangevar kernel invocation (#144050),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144050,malfet,jansel,,,
e9e18a9617c,export,not user facing,remove allow-untyped-defs from _export/db/logging.py (#144093),torch/_export/db/logging.py,https://github.com/pytorch/pytorch/pull/144093,bobrenjc93,Skylion007,,,
45411d1fc9a,releng,not user facing,Use absolute path `path.resolve()` -> `path.absolute()` (#129409),.github/scripts/build_triton_wheel.py .github/scripts/close_nonexistent_disable_issues.py .github/scripts/collect_ciflow_labels.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_ci_workflows.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py .github/scripts/trymerge.py aten/src/ATen/native/transformers/cuda/flash_attn/kernels/generate_kernels.py aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/generate_kernels.py aten/src/ATen/nnapi/codegen.py benchmarks/distributed/rpc/parameter_server/launcher.py docs/source/scripts/build_activation_images.py mypy_plugins/check_mypy_version.py test/allowlist_for_publicAPI.json test/distributed/_tensor/test_dtensor.py test/distributed/elastic/rendezvous/out_of_tree_rendezvous_test.py test/distributed/flight_recorder/test_fr_analysis.py test/dynamo/test_cudagraphs_expandable_segments.py test/inductor/test_cudagraph_trees_expandable_segments.py test/jit/fixtures_srcs/generate_models.py test/jit/test_backend_nnapi.py test/mobile/test_bytecode.py test/mobile/test_upgrader_codegen.py test/mobile/test_upgraders.py test/onnx/error_reproduction.py test/package/generate_bc_packages.py test/package/test_directory_reader.py test/package/test_load_bc_packages.py test/package/test_misc.py test/package/test_save_load.py test/quantization/core/test_docs.py test/run_test.py test/test_cuda_expandable_segments.py test/test_serialization.py test/test_type_hints.py tools/build_with_debinfo.py tools/code_coverage/package/util/setting.py tools/generate_torch_version.py tools/jit/gen_unboxing.py tools/linter/adapters/clangtidy_linter.py tools/nvcc_fix_deps.py tools/onnx/update_default_opset_version.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/stats/sccache_stats_to_benchmark_format.py tools/stats/upload_artifacts.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py tools/testing/update_slow_tests.py tools/testing/upload_artifacts.py torch/_inductor/codecache.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/pattern_matcher.py torch/_logging/_internal.py torch/onnx/_internal/exporter/_capture_strategies.py torch/onnx/_internal/exporter/_core.py torch/package/package_exporter.py torch/testing/_internal/common_utils.py torch/utils/cpp_extension.py torch/utils/data/datapipes/gen_pyi.py torchgen/decompositions/gen_jit_decompositions.py torchgen/gen.py torchgen/gen_executorch.py torchgen/operator_versions/gen_mobile_upgraders.py torchgen/shape_functions/gen_jit_shape_functions.py,https://github.com/pytorch/pytorch/pull/129409,XuehaiPan,albanD,,,
eb7a303d21c,distributed,Untopiced,[dtensor] expose the __create_chunk_list__ in the doc (#144100),docs/source/distributed.tensor.rst torch/distributed/tensor/_api.py,https://github.com/pytorch/pytorch/pull/144100,wanchaol,awgu,,,
6e09d32c009,jit,not user facing,remove allow-untyped-defs from torch/jit/_passes/_property_propagation.py (#144132),torch/jit/_passes/_property_propagation.py,https://github.com/pytorch/pytorch/pull/144132,bobrenjc93,Skylion007,,,
9b8a4e71410,onnx,not user facing,remove allow-untyped-defs from torch/onnx/operators.py (#144133),torch/onnx/operators.py,https://github.com/pytorch/pytorch/pull/144133,bobrenjc93,Skylion007,,,
64b197b603f,export,not user facing,remove allow-untyped-defs from export/_remove_auto_functionalized_pass.py (#144135),torch/export/_remove_auto_functionalized_pass.py,https://github.com/pytorch/pytorch/pull/144135,bobrenjc93,Skylion007,,,
64bffb3124a,onnx,not user facing,remove allow-untyped-defs onnx/_internal/exporter/_fx_passes.py (#144134),torch/onnx/_internal/exporter/_fx_passes.py,https://github.com/pytorch/pytorch/pull/144134,bobrenjc93,Skylion007,,,
b75f32b8485,onnx,docs,Update TorchDynamo-based ONNX Exporter memory usage example code. (#144139),docs/source/onnx_dynamo.rst docs/source/onnx_dynamo_memory_usage.rst,https://github.com/pytorch/pytorch/pull/144139,fatcat-z,justinchuby,,,
9bf2a9a6167,skip,not user facing,[ScaledMM] Fix NaNs in test for garbage input data (#144042),test/test_matmul_cuda.py,https://github.com/pytorch/pytorch/pull/144042,drisspg,janeyx99,,,
3251171ae82,skip,not user facing,Make whl metadata public readable (#144164),scripts/release/upload_metadata_file.py,https://github.com/pytorch/pytorch/pull/144164,huydhn,clee2000,,,
0a94bb432ed,distributed,not user facing,[ROCm] CK Flash Attention Backend (#143695),LICENSE aten/src/ATen/CMakeLists.txt aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/ROCmFABackend.h aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/hip/aotriton_adapter.h aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/bias.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_00042c36bc588e60a7c8a9ba297a8a25d8ac0660.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0029076f83a3dc695a167beda6fe19230a2b114b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_006c417a52a1bd7c55e45d111483d26f4480caeb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_008f2429c678d13386a06e8d8b15c4b480940ff3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_00a2adbe938d458d51ca5fc4020667a215b672a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_012c0f480917c329f4c3c6c666cf32af2d82b294.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_014c209d5cfc6b965bfd78c64bf132c0154e32be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0153ec18d3ded0f8bdc6459ea5757ebd94d9faf2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01ac1a2ecf9a487809e46faa92e267df2d47de91.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01ca79005067e20e4eed5a72ff9187cde702cd1c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01cb354dddef6e99e4ac843f2adafcddfc58d520.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01d12033d59ce2799a2a024e5d9232325ccf1320.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01d3b034a2d8d0b83c0aefa4faac6c3f28ce737f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01e2428c5447aa9a78f79f73f31cf685c586872d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01e8aedb7b7d77f44a46b2e9b7a826f245aaf4a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01e8f0df0c54ce619e5b66441b3c96a5e18b05d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01ee0083f6df962c4a754cd3295b1a436c590a0e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01f74764c3c3284fdd1b67d0ea781c2261ed0de6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0225857454eaab2eb664aef7a0849ce12c32fdf9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0237c76137df14fb808ade8bd6837045f2aaa5c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0271bd8b7c270e1593871b638288a4923342c446.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_02d88a03cd3966dd0cff550065f58c3ffecfff6c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_02ff94e3c787a7b06ffc90c25777fa74f225e32c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_030a759dcc92028b4c6f317fc230b98cb929e806.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_031b12f9fd94e01aaff2c0da4f35f346822087e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_036887daf6cc092e7422a17882488e59cecfb643.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_037c6c80fcec3eb8b0bef50ad6af6d27bf5447f5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0392491c5a6dfc742c2be483419a40f6a7a7ea56.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_03a71615a088e972c998f9c7cb44566c268c5124.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_03ff035717140f7385282419598cb4fb2881ce8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_041a0718891596ddac1fb0088637029233ccbe60.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_042a156e9eb935555ab14a84461959b466c2fb5b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04641230fe9a50a221047f7a1df8a370f72805b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04c363e11d202c6d2f4bb753661c5a2043edc0ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04caeecbc01667ec6f5599358a0a20423aa9a00b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04f39b453505f68a5091f68b1c3de48369d1e7ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04ffca078cfab8bc6c4ccd1cc8994a1bb4a88ea7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0502e718337eab7d47aa65cea7d3c5f641484520.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0513b2f3bd8ad51315aadb7f63737201898adca8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_053981d9e7af2ebc0f91e61ac5e25cbe68c95bd8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_054fda16133a0d25077967b05425f9128e1fe1a5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05538339c21c92c53d237865d72debaaf2ee5075.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0595316f0dfffda03e5296b959a49ec3f3c48d67.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05dfe927fd64a564c5fad537fb7c41ee9c94c2c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05e60b3ab7477f9edc8576a8bf43e3a62b8d5ef8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05f794c7023cbb7e35f1fd1ae45bd2377bfbc520.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0628931bf5cc1daa6e106cf60bb21fa1aac6b1df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_062c8c3c1cf6c33af4574099e9b6ac54a55ad776.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0682150e93f547e00f13cd8984779bf49b91e50c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_069c663be0267c009be4814e9e4e7c13ec999411.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_06ae52ef937cc27c544e32025ea0dadb7fad982d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_06b74acd9abfbd1c4ec2f4c718eeb92a0bca7bab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_06ba94794a14f0f0022af6f5f3c16e1e16959d4c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_071751b1012b90f7b57f8591cd06ae1fd27d9cd3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0766e7aa4b263a811408b285213e47176ee2bdaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_076b3beb57b30afb30636f948e3989b346b38d20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0789852b0cd3cc030c78b28f2fd5b6b0546382a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_078b96ad691a85eebd18586db0b62b8911016d9c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_07c3fc96d2bebe546dce6ebf46e5c7a519959599.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_07ff04fcc273e469737512893ea3fb5876ac131d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0801c56831b4c6428200db6318638a2129bb197a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0836d5dfc0f939ab9a4064b403339373caf35b56.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0842c4e3aabdf55405b3ce09ce1899245ddf11ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_085722b43cde5f37242edb071f639da7c4a0bd48.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0878b9aa31429d23a93cd953cc6a2fc5f43d0d3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_089a347aef8a920e3b59d5ffe71fc5bfe002609c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_089de13222caec1483207d4a54249f8da4f9c151.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_091cb49c1958fb4342d79f367ea93cf2b472f785.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_093834d4d3fe76e1745e4482c6b51b550c6f3dfc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09513bff5c1da6aadf11d2e8272a422eabff21bc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_096863cd93d1b105a617d0daa1d4f37d7fb6b893.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0968cebd81ade762c2f92fffc0153fa7a2b91eb5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_096e888c52d0f4a5847d7515fcc66208b1ff40d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_097b3e1dae9bfb2e89398706508f8e01966fd4ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09d76cca48b71dbcc9bd96734787209fee4c9a74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09e50367b62bb09071e28b44235a7c112645a706.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09ecb6347009f6a5d5530a6acf90f9f40288cbcf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a2b116fd5065109aae46ee547e4f49ad0e9d6e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a4e76d89b175e1d9fd2e9fb908d5fce1ebb945d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a55ed15ef58c941e06dda890aeb530e28eb7bba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a672fca51de618e3441cf8764e8e83eb782f2c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a68c2f9a3acdd787b81be455cbc7836c8bfd90c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a89417a043556970f72eebd48b4f3e7ac15377a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a92671b6ea99891c0d69b1c793f4d131b9a82ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0aafb881e34a3794970a1282af740b3f19c138b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ace6e29e1d3060c3086c08fe27b471e375f9c75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ad9d68fcee021437e13ffdf94d78252205f5a31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b2647b5982405a48e8c8888552a4b89386ccdd9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b2efefea81036641561bed80c75d77651176f74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b3153af7bcdba33115a0d31f121fd76be2ffbcc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b532fcf26f90c82a792cde7943634f667c1d033.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b90a0186d8b8004e3f19886c7992c8e04d0e066.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b9585ba1c10acf67115c5899b3546608541820d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0bb81407c8a2b3cdc5fecf655b3ad64d5d729cc9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0bc7910aac798f0555e9e505ad7f177c9fbbd92c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0be8cf70c6be969ecfca675782c860b5b75ac089.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0befed50a89d80c22b2c8c3d5ba67d73c3d0190e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c32a2d9701e23dd930119c4ee8089042b5b0ac5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c3b2ec99fa7b09c7f78dcc3142a661d686044ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c8a0bb89a6f05289c0405df5126fa0cc16252e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c93c65e5942a2f43f2e491547add02777dd2eee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c9bd38b8f9009d932ec49204fdea39a52885246.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0caeedaa7d50f1741d618fb6c573529eebb075b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0cdef49859c80c6b3ba18eb2fb4c35c72abc1cf2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0cee6b9427c164d78994150305a47f73954a67c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0d0e0147a92061d32608a34e7b47bd534eb787fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0d13a4c8d169877da6408584dc1f20a6f7c5e3aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0dde401aa76cb5425563cbbdb0362748148da3ca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e007c36231ccdae12f102eacca1f74b0711b9c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e0a2370f2a320484d8f9f21e3197425c2dbe9ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e1dbc9c433ce8ec33ace9e62550261d613db582.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e3f4cd28a4c06cc109f6a0798a77844bcc750b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e661b5f30566d1f159f060c264849c7ae4772f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ebacd06455ab20eba78b389462946716b5819f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ef309b923172f4c0fb38d9b9f5325b33b4877c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ef9b9413697d6f4573c6605bff6f58d027c5016.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0efdaa9266a5a464009297dc59db92504f8bf1a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0f0c699d9c3b0ed62097e38ba05e40e815cf474e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0f588dcb2ef86677ebf84e406eb802e9921d1f1e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fbb0bef3b388867e75d7a8a187b8b4b650a42ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fbddf533661642d84bf5a16149692d5a892182a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fcb7492feb79e27e0bda73e57ef7dab410e2bb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fd4068ea93fcf4df463e3bf3a6898d23b65da7f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_103186dbad604763008e0204a1ea90baecef8877.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1037f1bc50c4a65dac09ba56b701256b701c4322.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_10a055e5c3d6a953d470db5dc21449766248058a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_10c24f1f9009e46afa3a59193784cc2575f79056.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_10ceed95b0a0a01f844678717c88e0426fb503fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1132b11429034d96d82c82dbfdb69e460ad8a564.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_11e7df31541c3aa919e9825ad7dc4432f9a03c0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_11ff174ff2175e9ec22ac3a0fa59dd7713b79643.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1211733062ed30b876f1d63bffa642d77e258dd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_12207f4b6e7fac27d6c16493a5373f448a2aaae8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1241814f76107d74ed069ecec99a248676487eee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_12d5c8a4988efe60ef7943ecd73e18a28a736583.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_12d60c8abecb3bc9b84b0ea7851628ab17d8b0b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_131691f01cc7f29affb88152dd48c7a484315dcd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_131c1fdc4206bb952b2fea675f24e3b09f605eef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_133c51948cf8584900807998da14d788039f53b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_135ea67de101135ed5fe04f5cab1ec1d7b3714bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_137fa6780d9e6bde10aec10a875c039fdbbc652e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1386cd75411e61a8dbbaf2b916e62f4f5f99104f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_13d5f2ec83b3331654e37ea0b44d88cd98abaa37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_13f747525ad31e76c88774fb2208e470da9c2310.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14221590b90c48d3cf259fb4e834ccfaf7f3209b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_144f19363ef26efd36f0436cfa9f84f181a8824c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_146eb8c40e3146e06936f3141b2c4d92a578ddec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14baaaf1e90a075ab802c6e7d97c4b1605c8bd72.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14c4ebd1792c781d219bd21b691b575f64635730.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14d11aad7b666f500f68b264a2fcca6dfc5f1a05.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14d4630876785655bd4950566e81ae0b645c0d3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14f77aeeafe4b28f314fde5ebccfd2a554872781.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14fea611f3c253aebf726af3e5fdb7e63e18e13a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_151a4425b411596c46c7032f6b83d3152a0e0cd4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_153e897098539c3466da9d7a37234daf16476277.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1552dc38d26f6badb7a9bcb5ce9124d54cc45ed3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_155bafb551768855c8c01faa63e44764ebe6c110.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_155c3549d067464d186a99b8205317cc000d4898.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1573e3d855d28c54af612ab950b081302891d56d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_157768cd725813f8111d265cfdfea7f42034e5e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_157b89d8d625b8244b5cceaa4d3e5fc5a09c8989.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_158d5ce564c3ae1eefb54e3d41dde2604560ef4a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_159ee1f1b44d1a8fbaead65d8449413bb616d15e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15b255dde1a9d915e582ee2a83de7d83190c6a24.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15cf7068183421b141ed5d6e7fe902d06b6492a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15dc02ea7e0908cf0bd48034f5a49debfaa36219.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15e8e1ab8c63db96843054bb7a98d708ae6a9c44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15fe3e8f4add16a088fe44458353fa7c0c4f9658.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_16047b5544acef40e39932672cac6f562e200948.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1621507cf219fe608715d4e5bb6e5764022e2d61.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_162b0dfbe3f615b1d164290799b2457437a0044b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_164a947a6c2ba83a5b1cb7074aee0bdac6c9c64e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_165dfb45658df8f1ae8dc0738ac9614740f2576c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_167f5328b035ed59a6f05dfee31edd704c4b07ee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1687ddf65ce4ed2997583e20fee9f201e86633b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_16f94f5c65c37624f5458c165daf83517d9e3c81.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_173c44dd85077e6b12dd06fdcf6b11ba349e1866.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_17b9b96edda151072215502cc2b606bf1f6f0b03.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1847fef2c06ea581b0ab31af1cb0556c572696ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_187963e1969301abfa61d06afc97faea2bb4efb1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1886d4bf54b3a4a9e093360998b2059b3c03d072.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_188a70d526394e254274df95de0727850820326c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1899e28aff2fb168cdc3af7132dd7fd09c2e1ced.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_18a4d71b31c451a50df7996e3db864bc3c3882ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_18b92b4e249195ac3e0c74d246585a4c9e0992fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_18ed7195a9443c84956c3f32839cb3ab9056bdfc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1914250fce818584291c69a5f058a58cfbd83df9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_193699a5daa14ca2def07489e0b563149bc403f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_19af6a7f9e5020e8d0f0ca0f6258001f6ce592c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_19cd9f7b08cec83736605af63d9fcaf463a1aea4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_19df4e13108e043361e9528b71df56f04f696a0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a11dd5ebb989503a1c182684e7f247e2f8cd9c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a236be9da05a07d11cd28034d90cdf89941a172.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a5e18f6333ed2cce509f07cb8bd5868951d66a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a6785392af35e27d6697b584cb6f17a766d3fee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a6bc2762b95d550485aa720edaf71138d94cd07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a8da3e6ab050262b659c801ccf9a14787d7f176.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a96f0ac76f117e66eba97cb990c2350561ec2ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a98bcbe900f8c141136d18c114b02fffbe8bca1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a99b2625adffa8215276bb88fc65bae944b846b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1acf2f892742b1d236d2b31a8185c6869126adad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1b3e7c8969027d3316875f33dc50fe022e05ce37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1be43f8b629e7039f57b95866d5777273377470d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1be746990a2032f0363ad9f9112cc994983f4706.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1bf767e7104cfc8322f26df35907fbf04b8948f3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1c1b0f85e085dd0769c566fb16aafe5ab5952714.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1c2a2d78176e3f0a78e3ad78217e75a4430c0de5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1c65ba6dba01da9caa84ba89453b61d81376763f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1ca3f45d0be2d1119cccd0af042a3e8adeda2ed7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1cbf88db44aa5f884438288a325270d29c7a04b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1cc459e57bfed5ec7f40ea4a4dd9f72f3ad7a709.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d02609fb803ea2697e2c2cef35e6f923d2578cf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d0b822743e0205f60521d38d7c64f589fdf0f58.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d21263e16dafe79b9fe2f998847296e575c14e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d3ef3d5ded0dfe2a0bafb52ea8f841658db35fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d498e418ebbf33bed58b4074d1edf3d9bdd07c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1da23de9604b5d98fe02529075bad995954c12ca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1db03461737f1e359f389a8d297476f9b60faabd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1dc6e599144a093203fd7f92ac6d3c2cd7180d49.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1de2f97d49f015b9af0b186801e939c6f357a0c4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1df893ee660d37fba7eaca452ae65b3e45a73087.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e22f2d99804198c61251b4629a3f18ed3dcd42e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e33ce1fa113b221e5303b4093c2c4e748ce8298.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e42736d4f677a59a172bd6f162616a437696351.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e7d7888480b83c78833214b32e10f37a6e20301.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e9130607a2d24cb0662a47e9cf12c6602143838.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e943fcc2e64c618fc1415b3f1a0db4d70aa8494.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1edaf9d4270d2ac61c299320e06ba73f44730364.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f0cad6ad5b172e51c569e84cd54a19b4eb0ed05.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f13a6d0f8c798c0c4ba4ad202d081899fe081ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f6bc5faf18be193212217788d476ce6fd384bfb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f7faa0b33a9aada86f032174afd40d18efa7715.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f81f8cce0d77dec9f977b9eeb0778b70a13fa75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1fcdcb750f382fc7828a9886585f50efbe5be735.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1fd9fa7c2e13d0bad5fddb2b5a316bbc09d397ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1fda1c96568eab89a8f6498f8bb23c1223cdc7b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2005aca3520b171bb82d10ad70fef44f28c19776.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_204a573ce6b7d2f90aede543939315561cc43177.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_20588bcac681a5d69f252d7523a3681a0c6b6181.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2081430c92864c29bb9f409e7c27caee1de00749.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_20d5c3c86398f6ce55abc90db3e362dbf9f457f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_20f7ea0aabd069362ba4bbd66623cea5b6e1a6bd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_210ef512b7862837f54acbc3b21e135a192647a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2122c973581930ab7a4ebc90b3bf1cdaa229a87f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21411df58165946bf02942b597d94de7dd856987.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_216806a4598c885e517e664fc8280c59ec3cbf11.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2173b7c710d418f44dc2b41bec5905024334eae5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2177d95cdf45f6fec95d1812f2ef183a75259e38.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21828c7d3f5574690f12f841c27f025206e6165b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2184fba2eec5899bb40d49d4508196e6be1ec1b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21e235e31d6955393ac8e825bd69ead70687b7c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21f860d42fdc2cc6bd743d53ba546e332c22fedf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22105635385fbfb5d2f330df83ba6747bcb27f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_224f9af5e5ca519b21b71a54acb49f50b4999c47.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22511de2592b6e350737e44865e1fed6496e3f32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22632f996eb63fbe4bc5748c5897b775087446a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_226662cf1c9900a4334d2cadcc5f5ac3ad355f05.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2273457ac3be01cc1595a015a5f598f8290c77e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22a07ecf1a59f72ec6bef3e970d7f33cf54c5f44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22c142d869ef940ca876c93033ad53b576ed34f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_23047ea90076e3b0a3eb0586d49b9ee74ca6d279.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_230861e81e5acc523fa680534eed757b7b4a4e1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_232f61bf31dbb5de5d7039d5ff2338068a759b68.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_233132e712eba8972ba444c604f89e01c5b84cc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_235bf652702c2976551778b9159e09188575c63c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_236b3eef02b904304348b9d35f715b639d63218f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_238e4c1ca112afec494fbe47a85b553302c43395.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_23914c00690ac5c4f89cdbbaf00732ba66c5c0ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_23c9b46da8774462de8c24e14b12df3ed596eb57.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_242013527a0266ad479715ee3e6ae01c45de29d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_24410fd9a4150c33186a2a365d06d8f6ea621c20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_245d90000b55ab8b6055b1934880fc6c4870b34b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_24643917fc970c043d1c80d8d4b17ec92deeb8a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_249668a3212cd00edaae871758be30a5a1fea589.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_249e6b93baae25dff97a0bc9145a8d328ed3f317.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2543da478310245e19e6c6a0d9ed7ad99540b3bc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_256ef175029a43e64164176d4eb212baf9d27bb9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_258d747083272ea657604ac84867ecea17bd65da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_25938733446b6c0dcd159719f08d04a9aa467967.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_25b3225da1e1842f83592971a1f62a0fe30aa9d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2660282ad39ef034fecbdb74acedfb48620b7dfd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26835ba70606c769e56d19dbfe74061361aa855e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2695783ae8f0034692efd6563f789ef03fd0f4f3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26d77b228420a3ead919474ec9c6fb2800f86890.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26ea90eb5a527434c1740933a1d2dd863eccf14c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26f90358e522d7bb7c76c3a2c6010f0f38788bb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2703018e71d57d3266fc35e2e18a78faa3dd52ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_278639d44a4a8372a627a7c31e9527c8faa26f97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_27c2000d32c230a57a6712f27bc0fba02722f5fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_280bfced8745fbd9266207463fb41476dc23afff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_281d897ad17d7f6db2741b396e6b85a9b8f35286.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_285e61dad8f63fb973cb2eb899c959e400622652.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_288458c5a0720ef152848713119ebce6d76db6d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_289071756e7d0582eb61ce6483fa3c988d2e10b5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28e4d2c757e4b8c366a2c320360e21ff0ef671a8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28f1ef32c4384ec26f3dc5e3af6a74fc8cebae92.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28f2e2b108a53308a0cb6c123c8d318cbc2eadb4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28f7634d29bef11fd466b452a46b0612f38c949b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_290c484c2a366258941ee0051e139ea716a9de2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_291a8bdf9d63b112e7fe5fa7e8835a6789cb8ecf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_292454f2d82184ab0491ea0675750c6ec55d659c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_292b4f995d622826af5d1f2bffa7ba68467c841a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_295a523f815eb822d66162d4feb75fe0bc50b648.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_296c5836ba118969c4ba89ed62a98dffe3105738.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2995d39cd62f20622a31f11a292ed175abb5fdf9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_29bffc159b0bb826ba489ae763dae141bfe8e802.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_29c9e5384809b21f39e78bb2e43af345a9a21d19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_29fe68ba10b3480dddc9866c51ca8b5efe962cc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a3a980a26682d879c3a3425f3ba5be3f5761adf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a45129fc4995abcb8f880692f11c6186fc01641.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a833fc01e88bd8e256ef64ae8251dd0ed10720b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a97c457144cb63a9c6c3d6be613b47bd0df9928.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ad492377add5c8f6d0d2dbf9ee9e4338bbd9f1f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ae344010d49f7f9a6caab2cb84be7f87d2d96bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2af6c5be53732eb1939a2f93232af7dc011dec1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b0bcb241e5a1be1d35366461408d06e095a26ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b3326e055da32cc979892a2fbd0f7b003cb9f98.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b3af90387f1d227119c5dcd4b71362940bbce52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b4050988e5790a28dbe10b4c20e14f10f6cf85c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b49a9b0801a06dd89c7f7182d7590b515df1592.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b50073f6dfeb7ea77d5dce288a1d2f08f8f6362.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b5317b6cde327a842170ebff20c2b03d81379ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b8169ce4b4b9a17ac96fbb232e6a93f22071ab4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b823c3b99e7c8d1cdc39a5dbc7365a383bf9ccb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ba934408c75da5479cc41f96b98ea7d333635ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2bb6da1095bd8669c0e48b5cd808cf0dcefa2674.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c0bda0feaade2b554d648d72f219ac9c389bf09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c2e75e6f659a500dd3cf2cfd65118f111342119.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c77bd7e89ed832cc31b2995566a49bec6e4cb52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c7aede7762a524a7a424cc4dc46e43fdedf73a2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c808da5c2514806c2953bb77d5692e5d7c97aa3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c82e3c4e445e1e02f14435e4ca01a90850139a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c9756060ac0e73dbcfc58a9222a78f0283cd029.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2caba3ab83239e474412fcf89fe0fbef97e51bf1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2cf351fc2c2da4a8e1760a3affc9a5947c6b3bda.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d06f77a4054ca615d96636c0e2eba2a89850142.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d1f2d1e57095f756ddd11e8e9d4f6f253e3ffa3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d23a26e0a59a8323dd97632e610d24624143fbe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d43460c011b8d5e01ea98c9b8ddce962de59a96.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d446754d7000673779d15d3e73039fd3c10a720.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d7b637e0313cb423b22cd8844cc2997b3ff73e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d9a04b7f41dd6f0db017157a44790f35c626e2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d9c659ba43bb907fd4e3e36a50958288bafd1a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2da2b905c4ce32234c2af62328adae6b1f9217a8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2db33b5442d2e0948762b1f2147a321a9d6907be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2dfac5a83def98340c8786d55a30a98ad68b9eed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2e30f50071113dc4ab59468d568ac9deb06b0342.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2e43e401abbfb1b6737e4dc822f68421abbc648a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2e8b4260626beeac76c26dbcee3cba1457b30e99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ea394a09c8691a534ad2219bedf73724b6dd5ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2eba937ff6d0302ab013db7349d4feb914107f1f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2f0247e301a7b076b6ec8a778c3b47e330638963.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2f32f2d658f1f69840fbad511ce8a3851c859d52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2f55a23a0f24ff7062a4c286944f25d2db3e20a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_30024440e780fdf9ec94deccc85216d8bbb5788a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_303b7b04496e4db7c1ba2436485dc7c8a4c88448.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3076a6de0e2612279e0ed64612f7393856bcc9ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_30c8e4d5c761fda50e010da779e8e4730051d403.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_30f0200092b0e18d57a9f5e512d565f1c0229436.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3108502fd29d3a24b32177bcea968121ee809115.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3110540b50e95e99a5cccebe47d9d3a83093c2fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_311104394c8bef8d4ecff35c1409221e723a5a8a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_311731442b756308c0a869f21b7b8b103aa613e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31222e158484773d2257f4a31e3dfbdb68336a8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3163272d25bc2db2ffaa1fea87648b45ee68d408.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_319df310195191895005b30151da8c1afab6c82f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31a968898f0bc6366313e41eddb5e3a3ed12dc98.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31b807c48c472e9b1311a6037cd98e21d6706889.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31c3760f5978baf9780ce4587ae4c768af0e49d1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31c4b866692ba5c3d115482bef4790733863c1fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3206cc121ce8955ed59ea3b12b858ee2e0cf82f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_320a6196b662a1d3dc7441a9536d825dc356b95d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_321500dd4c41e4d68834814a48a639f5ca36a2fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_322a86568f89a5a5a165cfffbae9ca6949f2477e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_32438250078ba2a47345ec4955dafb4e4de78a25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_32527660fa7aeb9a951a9f2fc3c53989bd141c48.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_325fbcb9e503e68fafea08abf86a4951f440850f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_32652a27e8605cef59c8341813b68e7513be23c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_327e27892bc57f3dec0da24f94f2a483d6c9321b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_328a311bafd1c153525393b252e4170f8aafb370.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_33099fcfc218ffdf69edb4f2f0e46121bea9fafc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_33746071156e9ad46f403a539dc237e0a44122a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_33e7c1e5f41a451c7baff54f7238b220f1bdf8a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3400f0af03743dce328486f8fc805dd30bd6da31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3408103188e27b3bc55dce0c1716c0b4d32d6494.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_342d29c85070f488a14b1915f948e5fd69019c99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_344932e2655d7b32704be8de9a63bbd8c3369f02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_345a939a2491166dc520e9a2b9de7e43671e0c2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_345ea796c8d97bfe3b7c9663bf15e2e5e7696235.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_34807a8e90bf1cd839f32fd718afa6469c35a4fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_349241529745bf138552f49d9a93db418663ad65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_34c2db98d8e2e690f499f41cfd5afb831b756f54.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3511c54e6a6f9eec378d8b661121066536195d3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_351425a006aeeff4d69c8570cb6bf1e1427d2c21.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_354121d3bad1d448bd413718fa096f54faa12e95.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_356f83cb96d0313abcdb24955edd4264df72aed7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_357f7e626135cc9176a295f3d1f336a7c3852688.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_358399e756ed5026baf3ab78af17489dc07b9532.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_358d28c958c0a831a615a4811d13279b18db09c4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3642b78913a853a62dbff8b99d9ae3fa458f461d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_366662dccf2f650bcd8123c49006c759cd4c0ef6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_367e58867c46d96c9bbaa96eaaa9f93595c9e099.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_36a0a960541bd8a2dc6741579de685b7c0a5f6d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_377b70f54cb2778b5ce3df936b477f775eea8b3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_378759ae25465c32960487375828e23c5f1ac869.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_378bf438642e5d863e31145ada2a0688059aa5d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_37ad61bf8427a26775969f8a9166fd0bfb7446b4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_37fe04467e87ec2110f60c7aea0cc9bf2ca07481.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38010c9bf7341588f071f889b7a0b4dcc4e7a14c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_381b29d9888365bff0f109d897b508eebfd8a61f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3824e97d5ecba46e06d5ec1a9456c810d80227a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38273a2f8e6bbb42ba0b0871b6c95abb34531f33.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38a5ff72f22e0ad040a281e66b1aca0bf3a2aadb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38abcbeaa4d33d3150f2b0238bb62ebbfe960980.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38b94d76503e13c911781169fbc378517332c42e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38bb367362fe2c4849ded728ec5dd00969ce188f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38e12dad9e3bafe177ed3c27c833825813e18fc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38f8a89468cf9c8606cf12a930db062a83cd0ea0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3937d9dfb68351de2942e32f35e2ca1ce71edfa8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_39422621a00ff79b2f5ec0dafb957c77693537b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3967a8807c9451b09227c0f685c18aafeb062fd2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3992d5df4ba2e999caf6889a852db4e1ba078e65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_39d3071347a0c98f3221104036f477aa13bffa4d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a1dca5feb864e8981387c2d07e62acef1730aa8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a2280997eb6f1d091094fc54cecf42b7c9c3a2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a2643099365d0903c799585f41dc1a525ac9f9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a6b9566559ed2b1c85f2bea1c55e72c41dc47bd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3af86f458fb4dfcceb7db3357fbae0dc15142a15.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3afbb5ac9048a962a60f48886728220ae6c2aeaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b26eafe76cca8e74e819220b6de1f4279d48e43.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b4ecb47f9ebe8c2784976c3e9bbe4834b475cf1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b508b92f7e123b21658f6e17d624ffa87831fee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b5b3c218e4a7b459e54080e24c5b730221eac02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3bb129e6dee6848043dd0e8fa812ae80fec4d014.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3bb3b682eab96e4e173affad75b9d8e73f1dd690.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3be7cea6df8e6dd56194e1172f28943667f1c4ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3bed3aaf24c73073c604a3b23bb4b0358b8e3490.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c1454ffc1418dac641f63671e947d9f550b1f0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c38bb80e9880335faaea81985ed5d0e713ecb08.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c3b7e4b8c1efe59f79a15512716fce2282a79a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c64c33870ebc329921cfa3867d58b1857421f65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cb0cee09d633b6f70febbba63a1e090522cfb4a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cce3baac1e3ca03af0c3f4ee4d0158ad1031e9f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3ccf0a9d5a5451da5dbf6075ccea45e4a140550a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cd7a9ca49c1149d46f6b05b0fefc41ecaeb6ea1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cf45927b6d931e31e2209685d787efa28eed8ba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d1cea88a2277b87d405025ba256272a1720f88d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d289100991d4c8c362f64c8f6c4ba395c2f3495.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d3f3eb2f5eb1f3287879604892b1c230df85f1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d45624dc6e33c477c73a155500b015b6c010de8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d55cb42b0096a8ae338ce100f86e378aa1a04c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3da8c31f6d5bcaacfa4a21aed4d1d3caecb48922.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3dba3cd44f78c950fe7ceaa5f0629dfc607b30f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3dff884e176ec7cff86d17c6afe1ddaa4dd6007d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e143d88eaa0d9cfea856b2f3a57d1275a656627.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e2557f206fd81d82a3b9d59113105040beb891f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e562e6c3af28b8478020ce3c3bf73c036001c93.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e61b019e1398a6a3c36143fb84b5ff22c9f4508.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e839660557dee9d5bcda9b56940ce23236c5f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3eb2ea922daabbba131b90713e06d8caf5f30662.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3ecf565a5a1c4a09887c67ac3b9a019dca427ac0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3f34433b784d1e405ade3378918641372a30bf6b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3f5e01b4f2ca8ea10898c39d6570bd74e85f46ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3f7315955f555768f24585a50d75e216c40f062d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3fad30ff0739ab5dede67a96e859f8c474c245f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3fcc6893456a559c7d22714116022fc69b372266.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4018b1fcee808b6cccd131418b6ae9e8bf900d8f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4018f690b6322588041bb467beabd8a7bc79a2e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_40357c5e9739eae136a7abf92bc38d3ac94753f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4052ca6a3ec02f6559e4bbf1edde42ad2d127c26.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_405e7efa263223148318ae96bd1929b382e994e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_40aa64439b80ff8dd12498b3e5f6b625da16e285.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_40db688a9189e1c47c300d474df946a248a63303.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4118e3ab290263ed2576feaf22a1944bf2ddcb7a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_415b183c50dd2663dabe3eb8b780913b778c54ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4160f6b6d0869740a5a411abd80108f729f810eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_417b1cb14b67dc82f614831550f7deb0895bd7e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_419461cdb5687ebbb7bf0be136071d70420c1619.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_41b68458076e6cb129d3ec793e95b91430a0c8a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_41db3f29d1940e59dadc357c040ea37a6ff208d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4217a48a1677bd26cd48e512f1fc8830a8a551b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_428ce4e14cf94b284ffa735fe03d923cc74c9fe0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_429b82a27571ac91e3631cbdb7e0a58155abf962.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_42e2326066c91452335eac05f25a6311376bd9e5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4306c6c37cf472ad262f53941611b5e60072bdf6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4347e039c003489dd528faf5d710e687321a3fd7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4356b3a2ff49f72b91a6b9c215df285f2798ad47.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4377ac04be3a6cbdbfbe57612a469412812fb5b5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_438e3565f4c720e6c9691b0d33c1392936e2e7ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4395d3c96b3f4556b9765fd0a3b5701b2fb10948.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_43e7c78e8f65be35e2753a0ad5123118555c56b2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_43f2156a04b18bab55af60e9357f28d8a4604e8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4409f2a7deb027e864afdfc9975d3ab93c5dcc9a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4432c5214c4d40c54ca2d02f0d4785c6d6902370.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44462715ed5f192532760d6f4c66ff9d4e20e254.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44564dddf8b492d80be54854abb8d1d831e42679.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_445cd8fa559588f4264ce6192f2de3e3065365ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_445e28a8a51cd435130ded2abc9fc606e522c713.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4462b192a64efb60d5484798526278ac7a0fb9fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4466b6c6b2ec3acb40ac1cda432efa1e4e62d9d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44690e48f30657b0fcfa26fb3b9af3ef76e792e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44c181996532676f2140fd026707135144e9d37b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44cc95831c347212021c0bab7b43acd7daabce42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44d82b58fdc3e5b7a7c20490ce7f5acce4e6ec79.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_451fbbdc2dcf2ec81efce34673ee6c425cc16ca2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4568af1b2f104664fd05d21ad789aed39ecfa42b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_457eaffbff3c58183a656687010daa2c16cfc26e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_458d708d13577f2b92e6d5adfe952a87e0cf7be5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_459c8fb6028991321b09a990c2188d854d940268.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_459ea3713aef9b916e1b38a882a45012930924d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_45b9871c220c0065d74bffeed4021d0304a9625c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_45f4363f50af1e7ccd24751d5f5b181bf32c604f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4601680af41c8738089ff377147e0547dcad114d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_461737a13e24009bf1a5a4b780175043a9f2e33e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4666db0ff7b035e54f2c0e59acedc2131b722a55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_468a5f057fd5cef2df5f919f5102f47e86901e3b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_474fe2d739eca8c93fdcb2c105d4154cee6ca1c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_47548aa042c69bb9c59a8bf706b44028aaa41830.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_47f3ced9b5ddb0dfee8ed5e7df8eca0bbe273047.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_47fe73f04cef91cd2a0682e905483968ff80eadb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_481415463f0316ebe25ff2fda47c68cc54db3359.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4824e1f8cda50f80988857611da766685da94494.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48280c91d7cd8712fd533e246a6b0f758834abc9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_482e34930d11ff493007b1613993e01acc1af78d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48300e0aeabe337785d4c7b41796ce65df6cc42a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_483eaea4096c8f5bee16a64860432f0634a253d8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48435e5dd23e49e19dd313f9891ffec800ce74c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_486f6c7c7655c34b7b9973ff357b0813f0a3fd7c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_487724686efd35731e5335efa949486c93ae26e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_489e7be0f85656d012a6451b65f6c1d2613b187d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48ae3af78583258c4b13c11a442022e0e058bb85.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48d7d145f96aa8958a9208d0c8887742a8c834fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48e9e858abf6f77489f3fadc4ee81edacd26705a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4904c5910a2d0595b39a3f87652a9d1ef4fcbe80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_490a68220a7b621ae9817d7b77f55de239b0a4f3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4911bdd71351610d55916d452495e599960d0a41.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_492fbc418e829f89bcb8d93f8afd2869dd8dfccc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_49d4c005d723cdab9fbc307933c1257d114b539e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_49f5017cc0f5c8c8dc71492e7765cf729c1f225c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a06b5b153ea6e8b1e20d9aad9d4633333fd98f5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a2e6b05e7e4de2cb23d815f8b2c8adf22131c0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a4a00bd6ea27ff20a2903d619e1361b5e27672a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a5dbf601de5754c03a03a1a42395dc0766fb8ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a9f3da698a6103caf25d785928dd9f814ac27b4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ab5d6e8fbfd92e9f7e47bda5cfbb0d4162a6319.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4afd02981f92fbef6277c1985cc479c12bae9239.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b1eaca3c37a82d19f8dc91f06764170069ca3af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b2e7f96b095ebfb66ecc7a75752fba2a63e4f37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b30f472f00bec9da0564ddc40e07112b5f9a117.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b45948f2795293e72530b02669c4f549608ea7f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b4c03c916393d6be7c5181369ebcef949eaa763.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b68e4d00295b294320b94bc777d7d34609127e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b7393d55600c9892558248f4131fc06a6cf3309.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b74439f42140cdda9bb0f78d995d741212a35f4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b76e5dce9af523422782dd25d8dcf6f25edc68f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4baf664bfdf070362bcc91af77d1bc406f744351.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4bc48576f285325345fa1205e5e7e01787b74f71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4bd4d46397a3749646b232b306688e52b8c6e584.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4be4a98f150f3f9ab6f03b5fd0968c5454565c9a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4beca56234ff6fb4f23b9b24822887fd9a3d0df9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4bef4d120e71bfcfe61d67aa44d24ceb907c2b9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4c0c50a1fac82d47dff2357ee3ddbfa0b2c8d487.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4c69d06e3f32e3b6d28d3e54ad764b472741c193.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4c8720923c3452e3aebd7b9c1b4b23f0c35d7e4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4cabdafad0bf803223ba5e8f474cd59233dc48cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4cb1861e31df98bdfd731efc3d335055090d83af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4cd3de43cc1f7588d62a10362f59d113ee818846.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ce03571f1d2779bdeaf0a6a2d617e236d191c11.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ce671f5defd76ca08614a7a1f184c36c0f1e2ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d3b1ae63e127b6e6afe39e354d4995afc5faeaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d5f3cf0f78f73df79665c26b20b0805615e1b04.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d65e58c9f147498ed04dd51fe1393770603a6d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d7dc0f356b630179916f8fc2041b7f1402b46df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4da9e9b7277bc90518ab92860bef2097ba96d982.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4db2e63cfebcf84043f79be0321708cd159c62b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4dbdd9c3f496a27bde68cf86374999ff2dd53505.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4dc87b7d385e7b092e4706c464217b004fd8a6a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4dde56efe17f4fd36a11cc959320a5e43f1dc232.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e0a88ccef04e81b8c684b695f7cb4310e448915.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e15e4f16de26068cba30ef12fc29332d45e460e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e47f8fa40332c6ed12d9971e0b539049a871c34.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e760de14b71a41882ec4a2c7362565af36d1a5d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e79dce18e49ffe024fe4cd0693ad3399f5edaee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e9a933b916285d9580a76df543cfafc88a536cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ec2075f394acfb14fae7b1ef4304fd9b654ba0d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ed6da5357b67cc28aee4afa9523adaf055c4e32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ef35d82ceb4af2e07719c16109c6d72eaedce67.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f0aded9d1baec3125ce8e176248cb146ca580fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f1e1c969b57659e7e1367ac9ba10ed5ef5b69a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f44435491aa68acb3217b0e693232c67641a2db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f4a5d56721bb1a1332a65882132a8c5763932ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f6243c6850c0a2d2b7bf1476e12f95f187257b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fa4d21931b9afcbd70b1567995d3eeb6f9308aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fa883a36a76edb276a66c5d779294f170d6d4b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fd34faa8b168e2ac7862641229e6146d3e28aee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fe530cbf6363a8f08a94728e45e88ecde299e7b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ff20bafbf156fe8fb80bdd84a5d2f3a4a944c1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_501dcf3213efd214cc2ce8c9ba0027f991d241b4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5052b2318dbb78b1a82ef03666a35a623f44481b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5093976cb7b32a8bd28ce92fc13af00a3e21f737.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50e59bd079f4d205b613056f975fd2b4e372ab10.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50e7b11019fc2299d70869253877319b03388244.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50f887556a3540609649744957651ca667b91774.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50f915b4d9bd18a3c25a85917392ea4a5e88b349.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_515128c6978449b33ce0c35b02a9e9aaad65ef7a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_522a2a9435103ed405dc1500d31652f1d431a49d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_523e5bf45ec5008aa3aba4773e68a78e122b2fe7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_52688999141a72e61322140db29043ef9f7fbc3d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_526c89b7a04758b4badbf9695b316f877b8bb053.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_528db08068589c6e4c096054d26a2e5be63285b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_52a89981a05963efcea7ba5c1e967638beeebbbb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_52a8a323414448c50571a334f29bc0a38919b61d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_532a6ffd8a21d3e98342fd401f0247f62ca4e038.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5344427df3ae9392c4fc4c25c232196828e70648.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5382a30dcf702daae19bd6705864bfe36e09502c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_53bd60bd2afee49b30a583c32a45ae9f2076db08.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5403eec1cdd216d5c4a7ba977e2ef92a0d7fcc8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_540bd57333c6839ccf5cf2e928edb996bc60c371.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_541874a7633e5713720b9d084b6d1c6715a51a17.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54208a6e8c5263e38f9ffcb062564ab61d2785ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5435b4651a90e331fcdcf224282457e3dc038a30.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54402a22ceee3b665a3f24edb98b8398c35c6f5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54548ad36fb92d0963893146c8db20f53cbf0c8f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5467aea26852aa9a9e3dae76b906005ddf6fbae1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_548b347672451e8391388a400d016803f4c4cf8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54940ce53998becf9bddf56df7d19894a7658168.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_549b6956eaf678f7eb901567d1a515eddbedae5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54b6e18b10d529eb6b32d7c19c59eaefc7184376.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54ff49018f1c12b9fa31e523ad40b9cc162ba34d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_555ba79201a585bc091ccfc326fd24e851d1eecc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_556cd05288e1666f5c67fb87ad02ce660e4c589c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55b14cf2998a61611d1de2594e926fcdc378999c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55bd9c4f1b7a0621c67f3e964d946ce22fb2fc80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55bf8444c1c26b91fd490c7216f4d0f8aa0a1f1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55cda610c235987e13232e828f8d86fa88030560.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55ea83a47c6299fefa4220ed88f7a8e1dd938215.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_566b4782793c6526bfce7362efbf6bf069928b2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_566e26d4969bc6bbe9b092bedab11cddb3360c0f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56964a17f902257aca9d08c736516a2c67d9a0e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56cc4399c5567a9495f17d54c712cc9e65e57521.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56de9a7dfb1201b56528740e9d8a07b62710fcaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56ffe9e21362afe9c3a407c09d5de186954931a6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5724d91c1fd6290a6cf8d52a3801ac6b921dc7d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_572e68bd619e118292768f0925ccf92cbfa68415.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5732094f5917e9164ee0f973ac6ec47245a69101.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5789f267d34c9961ced63ad07ffea2c6d2911415.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5854f09511778dd1779a839b0b194896070f69ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58679919fcd292a2a69543de0db94e2985c9d364.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58762476c7f2bb05dce92ec22c0acbeb03676746.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_587fc33d02b1932235b8d152e57559060211d591.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58a784fb478ff5b3f1e2da9765a3a777efda92e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58a7ab44bbd9fbc97c7805860d5f6ac81d6ae468.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58eb2edc7738d8d18ac359691da261ceaaf71788.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5919133d2ed892745013b2fc5d503414cf0a4d83.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5939e6610e41aff8d1ccdb66d9e84d3e48e8d379.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_594929c433b049a8cf949ff476309a8faf5c25fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_597a0276ec419f18f060a5186e6bb703ae434ac8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_59901147b7188212b8d8feea15831a11425fe4b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_59beb9cb4e161f9dcff79080149076488d436301.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_59d366421e0b51c90fa53c366d47ed8d51b3a329.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a05b4e7782bd0e29ca9f6d33fc59d4304136d41.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a216f777feec4752f5882677b18168225da4b53.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a29b93cee012c79d4364502f1d90f947c73641d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a85ae0a16e4b293b549bcb6a3ee52df7fccca32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5aba1183efe205af38e79a1b2dccea5fa515d02e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5ace1c9b00f160a17355d4583d49c47887ac33c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5af96b404feac271dac8f4190180754480d3ba80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5b413bdc825ae863d53dab548f2145dc0de8fd37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5b55946ff3c15a44b9c741e9f6bbbcb5bd4c8577.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5b7a4ea3bb8905a22ae97a94c354b1cbe38093bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5ba578c0e7abf1127dd0370f06d7278656c93ab9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5bc803342862aa30e23e5be7d84e611bc571c529.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5be9ed84ad9be1627db7a66af9370679816c0897.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5bead6be6e39ece0e5d44335083336f7f546d2f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5c36fc744dfb0d985c9113175e76c7ec1c935054.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5c742b9ac6749f189d597ac97d46d35189472c50.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5cd03e29403ad53d6d52e5e81182ea6ff5aff2be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5cd41b6f578f3c903eb9d58ebfab62eb296044e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5d707d065ae152450f9def619ddc3dddb9089e88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5d7ed4c885fb32a0b548186e56d64bab98071d30.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5daedab8931f2eefb649b91e80145cb71b63360c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5de27c4081377f59363c2bf2ea8624217566d2d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e0abf4e2b6be3e2c555c2134705b9dcaee617ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e62968de58d9df7d687d671f37d63393f189321.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e735b12d130ebf849ac5d6752e413ecf3e69fbf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e840be0741afa4d41fd4789c8300223fdc63ddc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5ea53f7c6370845fa94aa9b395c52fd1900b62de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5efe77ca5c394a60af0313072cdd132216a52bf3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f20263fd84776f155519b3481be5e2c5b035585.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f3c3bed2b584ea2031debf9f953f5f8f7012171.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f71e663978dbcba859c5114ec675a712e343fd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f8925f929a5b26f3544ca31938aa75b3c59d34d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f954a393b7b5a7131c13d0c4578443f468a738d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fa19223cf296d7fd10e15e2571e63c84a80fbb1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fa7fafd4227918e0c7f0c6ca3b2bd673cd07279.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fb062527121e627871b3f1b2a94b96c42e51205.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fc66c5b53f83bf1e023e81e9d51f0285b3ae731.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6018ab272d7306689c7dc5a6d5326efea1471235.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6049c01db99fce654e9351e711b113cf7424550a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_606f5e0b99814b0a82a731de36f28024bc317801.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_60801d21c14796c08377349ec86a6c800af497b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6082d55544b5280b49b071ea277fb1827193fa2a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_609616f72bf16a060fa50091ac139ddc06bf9d88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_609f68180582384ba81aae2b1d4a4c52dde2c68c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_60efa9c427dc278c0d1bc31189f683cd45e4d873.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61204f6805d5d830aa6fca2a9b5f238ed63c3a73.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61220f6dca850a5b5ccf1f619a267c40c37efeca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_614a9f10ebc51bde3f580ef527c17f89489c12c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_615430cb65d8d540836c7f12b3367abd3c8e63d2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_618031345ea71cc17e458eb97a559b7c94d3ae43.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61896aa9e4e4d7e494c1755b1e77a08e0e264f8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61a44ac409e914c12281f1d26e5b52d8bfd0df75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61a9e92183ba87924e73ff0b5e25bd12d6038e69.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62048a8ae1c0096f3372b0114c15edbe813425fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6214f820b39a8ba81e547a78ed19a909ac13221c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_621da34ee666903307d3a09b7a032f2a70054759.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_628b28f65f19e7d1b22fb3b85b7cf3d09cd54ebc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_629e0b97b3fece7c12504f4c8f1860d611b57269.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62ab710e4acc711430745e05e036dd6a4d6bcdca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62ba7a5a0f3a714eb5f9f2af20f7bfbc82a30350.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62eb2f81e73d65fddce7ff43c397da6529317607.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_634d530731c7ade2c7beecfd1bbbca8583032217.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6360621af3f7e1e81a8be48fea8d2750fdecbbf4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6376eb68c550b50b9aea42a7a2cc3bda186b0e40.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_63c411351ec59bdbed2590c599f9eddf7807b371.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_63f121a3c8928c10a2d86b487cd13fa995da670d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_643b3798f11997d33ccb58d90ed6c10d5411b735.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_649336d59a8b35919e593217b6fd4314a04ea359.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64a0ca185449a49fa485892fde6af745ba758167.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64b3488ddf3bb1a4870371882f0a5d267bdfdf73.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64c3c1e3dac623f07c2dc1b934ccb868cafcb38c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64cf03c0aa3f1b2a7b76b4e3418eb5063b982a29.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64fe2db75cb20428856b02cd1cc8d7b393a6ad9c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_65794d9c185b21f59274ac5d4db10a7abc0be968.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_658552954505a2092662071401e135e84956c4c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_65910c8b7a30acc731948ab58467fdbe4fe32f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_661b49505cfecbe4ec3e5c7371de3aaaa85ac9d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_661ffaf653085dd7f122d603bb3ba4b001e5f3c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_662767e588220d0dc6137b00cc1d8dcc91e97134.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6649f19deeaea20663bee781af7edced7f7a4fc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66968bbf7e210911fcb95ba90c79837230ab1ce3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66a020f728df204ff51e37d2ddc21afb0aad5e7b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66be70b088b20fc8de464167c35745461ddab640.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66f651d3415562206c1049b172261fddba01ea6c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_671828f15eec2a58be23063a1a8132d337cd26de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6767cce35ab784aa42ebcb75af7305bc38a8721a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6785dcec0197fdbb50124ab06efa627f1a2c0567.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_678a4a8210a972bb2ed89d6ac754fb79438ab2da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_67fb736c61088b8dd92fe0371f5c98e23bf9077f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_680e81c3700f130df142c9a37a368944ca548721.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_683e8a33fdb7053760c9c135002b0a94facbe015.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_687f4aaafd1a5b9ee85aadc6fab79ad0c27a2ea2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_688aaa193f332ed13e017e78ec07a7c80e45f6c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6905ba47078abd7a5b6a51eb93b26095517e7f70.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_69214eb450c3b249017480efb8d092b0edad6dc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6979ef43adffdb62100270a62706fb811963925a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_69cbe8eca7e3510f5caa7f13419cfbefbf031754.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a3f42d5c9ccdd3807e488b00f02bc6ab5d8d99a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a4b6226b355bf35d4d07aaef1828091f03ad2ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a66604bb15f97a56847a7c968dbe32d247cbc13.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a7b6781ffff9a42beebb4d73f0d15461ddd4479.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a7eb3d86aa385f9ecffbc5ba10489e56856f918.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a95543aeed81adfb6d847f78212585a36122ae3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6abeb7b50ae6a1fc62535b9a1dabbde6f177a9d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6af23d1460abfe875e71f7911697c42fef0f41c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6af4c15a119e805e4407b184625f57966f8833d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6b0ef67ce0f178aa2863c4909f5bdd7f766c9b2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6b638314efcc4f16aa4a6e58e6caf2fda1711519.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6bad2ed9f91bc1efd89ea66cd5c775fa140cf931.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6cfb7075345704340ff33dc0ef7c04ef127f26ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d07bf9c05e41dcf2416e05dab4bdde17158db76.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d17b92fab5bee7717bf9aff6a6bef7cee3816e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d307974bdeeef95cca0d130ebb7aeb77fb1b6eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d40d762ed576832b3a752453e9881b5fe6d2650.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d470f5c6fb81032fcd7974180297d4bb2a8427d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d5aad18f59e47a3fa3278c7ef1a6372830c33d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6db86621d626722434f2ae9b7b8ab435a8dd8827.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6dd707cf48a17d31abef94215c5720419faa0a39.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e240106c771ebea461fc2a87b6da68e510aba70.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e6a4475ea795935f4cbf2dc0ac156a33d754587.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e7e1d245baabe2f6293e3d85318f9936b333500.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e8cda718e10824956f0ee39bbb0891eafa45a7b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6eca9cd905ea8b0454cf9564643894682b08cb97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6eebd0c2fbfc85f938b10535855c388971129a28.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6ef5803b33d97db72eb8a8528aeb3fc956a938cc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f31b3345893eec8ed1ddf1d8de2512b46ff6187.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f3d098f8bb63133924aab70d26a6ed64018c13b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f8788c537cbf6833c58a6ca15c0a36de33c9fbd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f88527a2cdb5adf51407f4661a254bb32d7de23.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6fa6478cc27e52fd9511fbff38369c921155cfb9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6ff4605d82507fc4bd6e96095eaee5173ea41973.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6ff58a5186d69efd6062f3717bd315394ea6592b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_703246f1f53a988cf252eff88bdf814bd382d3ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_70586668a61ab88bc46b763df8f1c2ea52001ea0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_70c8e45f6ea7cf5dba9eeadd0b19481d9f5defb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_70cf755f1485c065222be4daab84283a9c3d0eb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_714c5369aa848021e020d874289e3ae4e0f74d77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7177f939ac3dae8749cbf4232dcf04d2cf63b48f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71a2d046629a4b65c90d0e18d061c4984062f844.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71b6100efe30d836dab557ea4ac54c4b9d35c6aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71dcbe9f481c92215f3b636bc0e86ce8f65e6472.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71e3980331dc4bcec6ab6f4c345c7b5f71356979.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71e5fb3544dafa9da03fd2de4bb9bd0718f6009f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7237ce5f3cf13ace3efc0b0227ae5a8c1fdfce1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_724d1d4408196d611b2e0535bf8833652acbd6ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7264e378e1ea1d4dd97f6949d66f3492883b663e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_72abb25dba0c48b380b2dabeb6ab7efaa706d180.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7309c38fc8a2d5ad6efd449107dc54a7509624fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7344f96bed2f56793b1c2583485aa161cdf30379.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7393267865f1c2b0aa1a09a586f54cec98eea4ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_73d4901b8ef034590314048de7223a572d61ee0f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_73ec21ed6e040260c4f04ef68ef9307aa86985a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_741401abfbbbdf0dd1d62df8bc3e85371ead71d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_743176ecb1f0bc800c870861585edf56f88d7739.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_744ec604c577a27e0aae5b39711a9e2eb82801b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_745705ae121a1a331527cedfe4d31218a428a0df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_748a3d76e8ab73af9a5d2302d33e3b1d1b866dd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7497eca4d1a18306b406b367653622a8d64095bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_74ba59d347ce8916a22b40e6f22a3c89e13db4d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_74d5f2aef029f2103bb419cc982cae99fd1a9253.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7524904ac5a2040c7ea72aef5942212f291a21bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_758b211174da0f398b2a093e7389905b4f9c4060.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7596c14b8fee751d03f42ca48ea4f66e87fc2e2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7597ce4d2e5264bdeda47487d5bdb55a014c6616.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75a310a6eb86e3e8baac7a930c3ffbef372942b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75c38912947881caa14b3fc7ab7bca317e296dc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75f2010bf6c478d2f0eba77e912697661306c1cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75f21e38ad01fade35b1db40adabd75eb602410c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7601e6aea44b96e94fb019501be6b102c6e6a654.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_761bde840c0c8149b24a8f6f264e963c4e9e8ceb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_765940baaaa2ae6ade43ef4c94a220eaa63702b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76674fc182dfa6329c73a354aa3adf458429444a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76704ca28a4877a1e84022e022614709adabb280.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_768c80fd3ea17813df1bf19a158186834fd00780.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76be322fc072ca19baa82707e260c6eba936ae19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76f884e9ca116ee47b446efe9fc770c178a858d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_770ad1eb1b30ad8f1e7c17df486093129b2d5630.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_77200e875e0ef160b311c7de450c137772312d0d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_772016803aa3ca6ebe785557118365f9be7c4339.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7726be8909f631c04d4395fa4ffd03a736f447f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7728d5bec7941c9b6d5632bee8d67ed92b9c03ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7764814a0de7702f0b7b5ce9dede6440603f4853.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_77a814291d8f01870274149b9d82fb75921d6e20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_77d0223697ed41c4c2fd8830f8df6e5620db547f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7831ce329f2a0812ebb1dd103ea4ba8cb7ba531d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7838849e57ee9cd292e588f587a8079b57becfc8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_783ec08544591a22f59dc12f169b7327b4185a1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_784c35fee4d372123631312f1051c43e1fa12378.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78663faeb0425f45e8a0da0f7b1a5ddbee5e07e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7872c45ba170f2782c4b5b75cfc78ac79a4cf157.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7878e2a4d3b96a552e03d1ffc33debfd50c9f7f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78e1edca5abe1bb3e7aa946eab6484b7bed806a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78e945db4afa1330fe3978bc1bc9ae99828ae287.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78f7e2a2c08cd87702793f91b6935cbe4c22be55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_797750ac0b18b48f56ceb4640256e9bd3a36621a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7993fc08ac5c6ce7a2eceb1227f4e3718dc4cf5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_79a7dce707954e765d97cb22e57d9bd6168860d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_79d0b8053ddf99a4d4447656d733c2da026b3a7c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_79f182ae021e23869d7bebf2a9b4575bdc910ed0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a0ab620e6d62259a559e329460e46e6e3f7c3f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a13d62a715fd717f0d4101f787349cb49cbe70f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a242e5953f44316b6a4f6587ec26283ed6cbcae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a2e032f6500fbc5468183415b6dd1d3e43f0bee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a890b126da2d8cfbf84f048b779cac2dd56b509.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a902ed4ae3cc6558c73b730ff3949778007a230.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7aa14aa94d625b33df1adfa30ef4d91769592608.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ab03a62e064864e1e9c1cd506c1b2e1786a777c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7adf69b51f0a8cc9ae7e250e60df38758230fe4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7afd1a756247b15b078d15a39e350a07c22982da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b2d3680c3578c7292349b58843aef7a82e0087d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b5680f97836be4a369802e8115617a83875703e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b67045d438a7e4b8f3a313a5df5a85f351c1be5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b7fa76609243a8709f349ffc0d9d88157f28dc9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b9a3bf1a9b37e0bd9bae6249609e5994dc0dba1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7bb7b63e8a4c1df4eac4d978e166867195bd6e53.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c19fc90e5a9c422dbf529d2def286f47dea0f50.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c23dde1a386436e9864c8fa5f1706c0d2fbfd0d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c3d8ef4da515960bf40eb1feb04d21950ad5ae5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c4710e8f4e27fae4ae079f1667c3a1879cb6da8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7cbe4562c51d6829ec5942e11035c452fe318b3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7cdc419d4248dfdeeab1f0980aec35fa134e52e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d08373ace7087bdaca4ce8b0bc329f553f88d77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d0f767c17385eb7d756cbe8ed444d7cef72dea5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d12e9cb599d24631c082e3cf65d2c58b6d4d44f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d2f87c021e0b6a27b2d7e30351fd50f06414b5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d5667b27f15a06d4040354fba3601d48bb9c045.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7dac5d4cf103d658e129673549549f1276f134e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7dd260849b86c46b685955cab54ba07d49b47954.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ddd621da88c57798db1e689b93b692b6519ff96.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7dfe21ee27f8a0ca0407ef0dea73cd73ae6940db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e1bdde812c332c9fc58613698568a04771b9fa8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e332a6aeecfb12dcf70c69157fd3137343fb9f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e6129eead18d13a4a6cb9550384fddabc7a2a16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e89f79217037e361bb0909d06534e40f5026b4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e9519dd0d0f940fd5efd61bd32df7528ba7e3fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e9c7feb747241c9c7de2adf3a19933a1c4c0995.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ea9c37d92e344f3cc58cd4d1d00f19167e3623e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ec038393ec329a894aee9bbac078a40f57a4684.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ec04763d635c5bc3e810737b5d948c59f117d5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ee953cb24e28bcdc8f05783894b23cbf83bdf35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f6ccdb3c2d595fffd05bc5e6417b157276547fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f80d44e82e601dc48d4c8b4e710ef7265894b6c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f9403cb91d6aabebf081afae94a8ba397d8d24f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f9bb3486fee7b7c9e24300b8a4e4ce88a11bfc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7fa76fc1b066a15b08dc6c24a7cf33a58b4cb6cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7fe409f4421193fb48a54aa5f26bd6229d23204c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ff65c7abd9b0d8a2df9302d6dc167637b3a72f0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8004763f674dfb3f14b66dfdeb2a046e413ce2cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8007bf7ae1b71bf8ac4a793aa519ad333aa7a7ba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8021fa266c77e6b5bd1af2a9c22c686e5a6eac78.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_802b21f9588d72c3c3e3b9a3b269f19c484d5aa4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8046f566fa7188c92568b277354e8b06ad382544.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_806f9ab9baf631df1d3a8d801e4cf93a102526cf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_807545400aa6e70ff49a5f38ed6a218a180bd87f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80987e2d765efc320eaee813607c94c80ee35aa4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80a72d70d80b66c19e85daa00497308381050048.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80bfb0e6032892cc58cef4dd403f305a5b76851b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80cf0997573f4bcfbaaf75e40f519580a7495a17.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80efc341089a50ed5669b3c86f6ddd9b124d1442.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80f51f0e178c33e6196df1d2e47bd38bf5391cc8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80fb694fce7b4c3c459fca43c89c6002fbfdaef5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_810dd4e870ceda3ba9b5f0084a4b025b2e609d57.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_811db756577b61cde9fe8279d956980db9ee21a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_813e60e8405aca3f7fbed19452ae37574ada9a77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_815918206483d2ae04a45aa67d69dfb986587214.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_816c48e129a0235cb3a19124ddb28cce286fb368.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81acf1d17650712b71a499bb66909bfcfcb6aecb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81bb8f13b6f20a72c9ce6d0b53f81eddbf05f1c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81dd3ea61bb61de02667b14f5a94198f48c7307b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81f6c575c3fa2ccc7e65022f1ba65c8cfc16541e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82048cf91270631f98ac37dc488a1fb2e00ce004.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8250f27341241086515d833aa53ae873d4ece3fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8278845045d68027dcf3bf867ecde2fb12ec51d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82ad0c0580516485ea432d98f53e73f6dfec548c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82c932e6eaaf44861c794539d9caf8b50192fc44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82d7f61e6313930f063758b61102e7a43b118beb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82f0f3d71108dcc49234a258f0f3b21ea2123cc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82f1d7e1a93bf2fa80c409e6827ea88af56c44f0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8301bfc0394936a68fa0098580f06e77c88ebed9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83080406598df6bd3102db70a554e496e29db96a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_830e3532f27b391585d5de90f3bdf97992b67651.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8352031044ef2e4a22e27ad04ab5d2c02121faee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_835a906031a258c6362313eec783678bd8125c91.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_836a308c2d2afd6e0dfbfda61984b631c4ccffc6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83d580a612af85533c87aecdd7b0345c71b75980.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83d920a76114c63156740ba5dd6f3846c4b21c28.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83ddca2c6ecbba4314c434e7471ffb8fa642f936.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83f6a1837a65df12b7c55d25ca28cc939c2a6328.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_843e7888cba5f463d19fcb71aaaab25dc3d2c09d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8441910c34830ad2459fb85c2c14af02da718fdc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8457ea5726149efb8778e6d90798b8e48288fc9a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_847feaf237911478173377a501ee19ee325b012b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84cca7528c7d1bf49ba79625733ff0ae7522c096.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84dc4af43de08130a04bfa06df9799b6e9e96900.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84e8ae99e184013739019c93d07caddce532382b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84fc5e94f89d6a9287cf64662a372784511468dd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8513d96a66a4d9fb8dfc84afba7e1d8c200248a6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85156f2c556c6ef6180608c361b7b35ede71ffea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_854c8003a508ed3f8cbe6967c4ae2635a491c721.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85908fe6dc9c629c82d6953081b10021e64583b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85960fe542635079de5eca3c7785890cd4740005.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85fdde4b25e2fc8cbdd46c2850c19eac8d9af8f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86309c036d96367939ccc3e8922595ac35a3e179.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86513d6e065a44bcb0c789eed1e7e5456e800ab6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_865eb90b1a2d64acc0f6fbe1d807c501fd4be3cd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8689126a7eb09d81baaf8f99dbff8932fbeab3cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86d73393d0d8b769f30222f7817563a955c36dfc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86fa51b8c7a2f3fac5cf4cd2951ed2ede5c35450.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_875b08ca602fe48840c72cd61798acb98540fcd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_876a418fbe6183d0392b7a7d9986d067e323e2b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_877e33463b3bf1853c6d2d2009af8d27bf88abbe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8793dc3217e154b65ebba065aa10ab4dc2374ae8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_87e3a06266deda093bdf28af82d8666066157fc6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8840e8899b4e632714632450bcef001c6070f955.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88ac7f6cbdfca2e397bcb86af4216e87166601c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88c04463f9c5ce565a9daa8c22e16de80fadd707.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88d52c5f70abb525b9c8aa8fc1cb3997c33ed67c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88ea5b5346c87cc4fc1e841c518080df4ab811a2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88ed7f650c958a644c8031aeb88688b1e42458e5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_890aa875ac13957f00b30210477924697abf0c9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_89617bdea526d12d6a33ed42b9b0018c0b173722.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_89a3327da9a3411ff1cddc67eb647083cd947a92.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a1fd28acfe85b3adac859c4bbffa4d28fe634fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a58d4bca33c4c0e79141a56688049237d170d1b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a824621a50cdc3cbadc4b1f9ef18e1325385082.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a980749c6b2a18c80426dd189e5506334343ca4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8adbdcd28cb2f078f89adf9aad2b3d4a0a477823.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8b17c082f249649eca733a8f0cdf9a1205c3e3d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8b9043572cabb65435627a3faf23b18d039bbcd8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8b92990df507e82f96eeb7aa3ec00c01437566fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8bd1a40b12ce927323594fcce61eb9c20cc5e3d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8bd7b8c63a51c8639b3cf27ad09d41ae47c480d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c074afcf33e3f3534ac3577484237fcfd2ca48e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c13c4f3f645a2bb475eb1c55ce1de452f0e2332.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c3bd4e029bba76ebfc79e6522dbc8ca0bba5dd2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c4688cbd23727dd0ea9a36fb977b31aeae98d65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c7970957024de050748d3e31cef434f582d968b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8cdcdeb845e7bcdb89ef70ab2a97157d4db3cb52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8cf1007430da272174d3476d042f398627e83512.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8d079c1eb36db8461fa8b861c56760afcd97cc34.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8d7549e66ef309e32779ddc2a1f14e79bae53754.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8d79fe8a600c3b4e0ec9aa510f8036ba2b608985.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8da8285bd6182355e3164cdc5a983375cdf0a61d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e1b48a28b71c7f4c78eb14321b39951a7c5e903.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e2c587db8bd9f1b551624e0cf8b67a90245d7da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e2d5f979fc4fbd0991581a020a414f9c8656ae2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e431313fe082958d31b68d2fd0d61df0fe56736.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e50ea8dd480012cbe10be392cd26d1870e6ef9b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e675919a6c7758cbbeecb83b7ac6c62f95cdb46.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e812705ae3e452810794fa7caceef2ef6066dfb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e816fcad5e9ecfca94a6491eb2274bcc41e558b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e938d0e3ad30db201880642e57758285b2ec4cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8efb5fc2ace6839eac741c5e6616665845f43566.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8f607ee20c0d92b6dbd0338f139517fdcce98d0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8f6e463eedd3e65b9c79feed3cd92ad8cbc9f036.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8f7166d4bb0c1c9b9999ba16a1adbf09ebfdb6f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fa4c40e244b412a07933d369704bcdaa6d5e74c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fb224b40a7be7db0a9c5c08cc5ab05b526c14e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fb33fc20f2e85e915f1b1529ae87981dfcaf86d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fc08b4f3959a2375ac03f40c4ce12d70cdc2d80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9009b7d39346537aa6c4a4e46b81139f603edb60.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_900d7f81c73b35ea64095d01c5d48d9190839e0a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9068ba8df8b0e977e9769f6acf6cfee6b00b9922.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_906fa8bf5e992ddc25815486ae9c24d8bfba7227.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_90b17d8cba28cceddb3ef907df878aeef0762d15.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_90da0d469cca5c8481504148468460c85a15c559.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_90e5c56e92712d00092ba102a5eb5176a3e5d471.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_910cb8bd09d287a1566265eb1e8894fe68d3cc81.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_915b75db795dbef037b14b003ee073665fe35d3e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9163ae070075f26926a86d39e15c27e6edb1f1cf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91695dea4171747fb3cc6d910459f800608d07c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_919ae177b7a793fa352c4f6bb8e4175f3064d814.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91a6200e36944b1f11106c02f7fcee053f01ee71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91b9e2616c2fe0480096b1ccf0f74d584b220146.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91c916e14198f6d18dc89915e379b01070434e91.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9207a63fc55c411c73e4f93306c5ffed800dd249.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92121fd448b4640a17e1a7fe73bb7b58714c0afb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_921f789d619db6f225e8e9d646e93bbc9dc1a669.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92739f4464512feee083b875e11e11eee4f5b448.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92992be6252f2afdc368bd4baec4b8a55ae0abf8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92b0770fe64e3c60b9e56170aa88bbf74802a813.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92b722cdabcfaa388ccc6ccceb7e42462f3bdcd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92ba64cdf615c1be2865f027a293cb530fc07dc6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92d841e6d783bb46d841aafd9027f92dd1b61b88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92e53359c69bbe4d7405d45261a8a62008eb7d06.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92f9ad0fb65638cfffb3e7786f2cbf01d9585b23.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_93054acb8a9508fd0f0f486367fb62454de47c39.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_931cf8d05cfa45319f4e5bb49334d35a530bffcf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_93728d999ae43ee1b5a16e60b90cf8533c7d303f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_937801fbb43fb6797f0425f08d13926b74d87c4a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_937c48d0b7096ad6c8bc445f13f2c8c1934695ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_93b885d6869400b0dc2ef1b2c2636ddfd21cde31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_942439e4f5644a3a4630481bc7d98834b29b6e1c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_94a94d145e575747c8956ac703810582c819e2e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_94aa519eb57e5797125728492d9330f5c0f0670a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_94f6f9dee9f0c3825d91f4d320a5280070e60ee7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_95061acc6650fc7b79fa1fe5b2b1e083555eec2c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_951343832a5bfd060c8d12da0d8a090f070a717d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9545f95c1093c60f0fb6c794636f79aaeb53b733.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_95530399ad7b43d8ce2c89da24c71056f2146b18.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9583148fd684a7e6a312127e023798278415bd27.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9594816877815bc0294610ca24f986fdccdc7c6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_960ecb3013071fb65f2d5ed4c947c4bf303e5308.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9638c9618dbf2af119e37596f7eb0fd3f8d72748.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_963986150adcd6e1d3886bacf2166de1252e14df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_964f916d3484295b5918e2e4c22c5529588a5662.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9689ecd7bf51bcffe9f5002959bdda41c50a3c8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_968fc75a7d102aca068e3ceb6111728c280fa837.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96c129dd4c798343d6f78ab78056f0faf2f1c9d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96c5e79f54b71677124f555b0ae4bfd27248d099.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96caa2056d99eb67ada498e287b4fae984397691.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96dee49ec6755006d67f0c30c65f50558bba69b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96f1bb85dff8c97846f6b2e8796a6289bcd0d9d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_970073c70133ff2ee4737f803a0ac43801c47242.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_971a08c2e48d805b295d979b24173a04cf58def0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_97246460c21bc66c0f13936d27477a9fca1c44d1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9745b04a8026a01828c5dd606d89d044d3ed1d99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_976cf509d9c2bf86ba6ee5ded544fa8e6717f590.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_977137b371df841993c8d0584be7d83aca6add78.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_97851d5ecbf02f8af623988b1a39c0b91e51533a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9801b25e0f132d647934deb395b62a3f70cc7c88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_987a617fae00fa90a1ba60937b0312c81087c19e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_987f00dd759d9714693e7517dfaa8bb427294d42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9893336a4b00b2a63f23ed7e13ec54c82d9e5063.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_98e484adeddf3394d8d7693b808d83b64c71ee69.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_98f5efcd500ce6b9ffc14bc9877e0ba457539925.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_98f9a4f4d85f292b78123599a2e1798f12aa545b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9990e6ad243a48b84304b5cad0c663c0802aedfd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_99ae680eed89ea93a3a94586bd5a68dbc5439f37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_99e2f290b962f1617b0a9d4fd6d55c43e4439d6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_99f8352674bd6bbe98944a1c0a769a4fc028a623.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a0a70932bd587759df1e5e150b25b0126d7b529.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a20fa19d8d30654602e363806f559113218d66d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a8e04fe9432a60f86ff0369e8c1851821074a04.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a9edbe35a8fac7796f00bde836bd547044770ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ab73ea77ec20ea3bfaf995dacf93a6960ecdca0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ad1f99284aafc8d7908d062f179a056eb314925.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ae866c7db36286876818bfb718ac35204fa3843.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9afe4b6f3b901ff4af81bd4f1cd8ff19f09d0b07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b062dd633645772e4f2caffd111af73184f7657.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b327f0fa1155f2235d76be45cd22e3db5a69429.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b4dcde1ae3446b825dea739d4295c1d1ec5c4be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b6d08e63b9a90f2524cbfa8c5fcf8b82a1d2d36.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b73c92a13757877f34bd8a13c6fb29b60999020.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b841b7cf5da31f0c30ec42c91cc8d5bd3fedd03.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9bcc791049e3ff9ebc1a9085d2d20efcc2f99b71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9bf235679af1ca03a6e601b4cf6cd0416d1c9091.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9c4fc7cda4b560040cec93f63021b529aa1ee3fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ca3b1d36d777213eb381b47871bf15dd163c994.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9cc3ef3d3b36f52089548e9dce522b0448e2c26a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9d3d274058bc0a3d4d35d90669587761fdfbdba1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9d6759d8855c4c6289f1f241a1628cf0406c1b64.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9d69d441f48f9ea346dd8e00376a9a708da3ad87.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9dc424f0e192155e3c4e786e5b87d5a1a3e6c4ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9e51083e13aa4dfa8c969f8f916835a8e5e9ca39.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9eef1b54d5d3841f3fa6b84cca6c7ad33efa2d9f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9f0517550c7a23882b95de451e8099ea2186b4ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9fb389d4b5ba590baa951f17da06f0e53d2bfa55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a017be7b8bcf303b30a147f41346898acc5fab7d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a02a71fdd587e47ee68e0cc76c3c4494ce06c359.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a02f152e9184af0b3d77082d8bdf519dbbfceb2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a046e888e3836b0bd3c49fec8e1872e880798f0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a0874fc5ac87a1ec487c7722bf3b1bdaa924ee09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a094599fb5caf5e7aba728cd4713a8d0c6368a46.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a0a556c9358ddd6db719458c81d2d6d822a895da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a103cd47156a98ad2cf2c325ea00df3f1d67fb72.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a189292c81a18d21a2921ce6740f81ebf4c046ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a1c71e7d33f0597fe090a3524e33e18b2e562680.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a1cba1509c413c870c5d784410855ee1bd737da2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a1d6ad9de7ac7993ae1923a2ef070b7dacb8c563.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a20c91b2f11bb7e5058ca7935b0bda4f5558a9dc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a21f3637624762547af1292e1b85e640b1d329dc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a225c4f1f3c7b271957768bb9235131c67afb48a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a2482a64659c838f3da55f56e3cbbee1dbfe6722.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a25e2aed617e1ff31f93ae7e054313ee0dceee97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a2a715b7e9c1a576f011dfe5769c5b392e984f82.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a2ef5d30a2318ae06430d17f84878800c4ca7364.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3339150d8bf9d073827738527f6cbe15b854607.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3709e4fc53d2254a03ea7660b8c72d2f47cf1ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a388a284f45f711d82a6ed87036d87cef1872eb1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3ac4f93722dc314086f1b7d7b8adc687cd75f82.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3d7aa46528ee74e2bef1e87c1feceacfa55e173.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3dc780b17152f696f9b957432c2eae8fb16e85e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3f9c236d24b30bc9c3fad90cfd6eb00da835de2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3ff8445ba691807caadd9f26e7eb90851875280.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a421c2ed6b295c458071f1988b9d6f7b46e8992c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a4700d87a19a173e84d64e43cffabbed52366e35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a487f617c4b84c6a0328fedac750d41dc3dafe27.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a48843d844f78690c7a45b730652f0f763c595c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a4980becb0d3149fee575bad1fc3b463d08aabf5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a4b7f10440331a8a88ff93ba253217c2832bcf9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a55b47aafc4340e69e300ac61a7601a5c14513b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a55c7dd576e5b1061c059e5e99aeedf4389e2d25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a59423c095db052603d77073d409534bceef425f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5a7833f4597bb03a3e845d5580d677e97421040.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5bdc110955c05c6c6ea236a6f60266a4a6dce5e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5c0109313de1f6245d2a80f8539485b849e9d55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5c4dc0d70c547dbbfb661e879ba7f9adfafc2ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5d4eb673bafd81e3a0ee213da4603d88b8460ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5e5cae764142683b70d3344cf07dd1edb7d69e2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5f2f0cef657ae5e333d65ae4ab20529a43cd7de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5f8b7b2a891aa9f2ab49762eb31d835efdf18b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5fa94bb32a80e81886b711ebfcf2df5f5405866.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a622fa57764ec746e02f6d4bd4846b48c722b807.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a62a2ab489839ea1a1bfd1b24e54a3c232ed934f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a6461d72fb6ba50e81de3f661528c96dcfdc3f3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a64b4cf3f6706e4b4e0af4402e2263b9a1585f9b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a65c43b870705c780d734f9ef063f55cf8b3b52d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a673f35edd69241c6b921d6712dfd064d78ecbad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a71305f191f06cd53b7563971c706e8b71b19e2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a74b0e7dd816ad08eec5a1bba6e227afee9813ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a7784b03ad757d51c234fa86ea9891f055ecd5c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a78fecb9725ceb4bcf2aa037d43bc43efeb1c3fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a7f7553a7d2f6d42fe695cdc64423c85223af440.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a821661d8280c6e9d27f2c9ce1b3c855387b5a76.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a85d35b2fd98742427930eb536e346ffb005edd8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a8a4af070ee46d802cb11086b93daf91538f8a04.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a8a744edfa3a19d1493611df5bd0d4d59b707d43.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a92b43d374642df991edef1f6036dc898bf77cf8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a93324ccf11b273ed20fd960c61df897c8890b1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a93a03b33305b33055273711ab31a5b8d8298d5d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a968df29f5ae1463706b7981b3bde55918e1aa65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a98925d99dc484da41dd55700e151cf545cf821d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9b50c6ebb27986ce5b378d8c39315eb9cb91dea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9d2be18e2d53a5144f97dfdebb225fcb6d611d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9df9ac4ee78e5f4d5bd0567e58a7090907c61e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9f00f270680de81df7737e848e0408cb070e68b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa1041530f794c7b8dc4a8321ea0fcdd338fff35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa522b43c5e5ea69bcabb4c0fe28def2bd081a12.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa6d13b09f85ee62bb5018608812181fb43afc86.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa82d20635e592edbf00439294835f6f39ad54a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa996b9c843200a2ec33ed4319b48106cd7c6384.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aafe891dad43815e635f81225705ff944f990d75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab09941bddfa9d61985b55f9b6bf0edec9bb89f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab0be5a2072b5e87f5ee58149688796b6513219f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab0c3fe9529e24327686070731d0ac3ada76245e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab1ca4ce061f7f69a250356f613cab00d1e2ac71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab1d7f93427095e39bfc1d986b3d7fe54073ec75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab43f4a56c166dad0113f51b337a083f4df7cdb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab56e886d53a1d88fada0f10f00b9f398dc54568.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab6cd5c9242f8278c8f3d9ce57b97d605c7e5a3e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab877ae2a1aab04498bf2b26b3fe99d6488ef151.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_abf6c6412f9853855b74a96e862935ddef66f763.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_abf92a5314fd33491b5eb6ebd2418b7e0d5db774.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ac1ccde31b47e0e56ee0daab6403fed7895208c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ac5e9aee85cd16903bf7b82a4ac10402b0b26e22.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ac9382cf8bb56ffd962c99329bf67da992f8810d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aceb0641213e9a45ba48bcf72bb23845720d8b79.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad091c69d19b27f7ad50ef6311532ad8b642a9c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad82071cc074fd30437f6158b5eb2c6df1f8c587.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad989d2ce769f20e175fa88f4082c1c25fe03062.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad9b99a194b59d3149842c15733394da275b12c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ada016be2bd0e377fbe01fa7adb9bbb8febce100.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adae2d4f8b2dac799e03ea6f279e6ecdf66f5381.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adaef10ff2c5d89530310bdf1d53a194f06a94ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_add29e3e9828911a117dccaa5650e77805730d14.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adda7ad787524e3e47dcc1b65c41b2faea38f55f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_addb6a14043c5a4df0f5042b3770b40c4e90795c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adf160741a4f751d2f15d6eb23d4121cdca62b55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae1ab1f4bbe86bb9bbc22e4774648076c321136f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae1afeb6cfdf860ff08e4c2f11c922fd5bfa621a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae239476d61f48379754b97f29d7a285cc3192de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae4e7253ad4873576052ec0a9400597bb7975753.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae4e80cb185759dd9b3eb3c67c239964b3694caa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae51b30c7e1cd30e550187458350c8db7c59a9ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae7899b1ef159ecbf01f27014601eb79b31b49b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae87b1d5c50606430b544ed650d87df24366e7d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae8d0bdde763e617beafc0365ec4a3cd11df6c55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aebb2441e6cc1ccba4a391566e547402bcf7ced2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aebd5fed34ebceb879ae3dffaf58c7c04ab5fe80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aebff7e6605b273bad844b8f70ef031625bff48e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aec87e65afa93e84d7a947c52f291c1c7360033c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aece14f7a220222eb4ce6783ec2b9fce6fde94b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_af06c0dae15684f83e15722a4c07342af9ea011c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_af6ccfa11add1ae49888337e84d9c446d2f67da4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afadc4f76e237514db0bc0203102297b79730bd0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afc4b47a6fa62a4ca5cff6a7e01c9f6b371d2215.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afcafd07c1f56e74373ccf37db35976023456d50.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afccf699f593c828e11efc053b144044e45b32d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afda8f46b5ded4c2aa9d722fec17b75004b59f7d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afdab954fd111ec48721f25710d61c0c8affd8db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b00e062055933388e37525df5766f3c14cd3538a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b01dc872c24db4db0c9179fc07e17f41060390de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b03ab68e33844f97aa58d463e00037bc11c50da0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b04f14f829eff73afaa57a875f74ebd1e6860979.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b0544a38dfdf4d81dc95894387845f48435e299a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b0dd965d5d9080ed5c6a04b7eea9890f3a264f20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b0f555b74ed36f1bef8f47880b3edc6760f27788.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b1766695dbb790bd614b83dc7569ad449404cc89.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b18a615e66d7cd739ce35412811359a03cb23a8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b192c55f002d8540d5f965cc4df0c2e33f4b9ff9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b19f05f6848403480ba41d37cdbf44ccca1b1f8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b1ad101ce91348266d3885afdf2996a0fdb72135.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b1c5d55d47d6038e9162d32ac968ff58c0942938.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b20c6252863a73341b0010191fad4c834860f884.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b20e314642cf565e4f32bceffdb5c0e653ab627b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b24f91dec2029b25d0d96962528410df55a468ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b285e2f1970b78e18002464eeda63798229bbc3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b298e213f927b518c693660110f08bdd94990ef0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b2af5f5b5ee3ae964824a3e9c7bbeb5bb39c557c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b2f91e937b427ecc932c0cb0c90b2c2378db0be6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3063d06723ac70c5f8802ab49c5c35e1debf56e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b31f56244076c501cb09b4b90975132cae4c4386.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3486244e0b7d6dbcaa1951e8b8883ce441c3f99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b34c1ce348c3d9cdf6bbec9758de9d5fe94c43fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b38a1d3cffae01332a3a9d9472ff1b2c443e82af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3a104733f678193068d8642d6560faa03897258.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3da22d3482738a8474ae15e8e5fca9020c4e195.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b41735d250b5a16967281a5f07873b9cde3df4d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b41a30092e8138877c1f6c25656e0f8ae2c2444e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b41ea5293bc1c56efa2c4b5681d965aa6f2ce6c3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4588379eaa268d79fe8f8e4457b009f204a5fb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b493c99888d82cd2852bfb101f99a2e6a27665b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4a5715b550f67b8870ba66e1e6282a26cc1dbf3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4b037a2e262d11d3ed7d9feeb41b9e05427a739.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4bd2d206ceb237ed2c51f58abb5cbf96e39d07b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4ec377c44ac18527ca6a01bc3b146706a6e1e09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4f12f10d7b968e0d8e7c23f36d3a360de74a905.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b50e6df20a2426abd3d2ff2262a37c009196024c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b513834918d5ea789e2db21abece7c2d3532a7e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5248f443a12d96815c04409a00102923c717023.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5371415448fffffd58bf014dac9f4876153657b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5ac596c636df55e81293228cbc53dcbb3024e5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5ba2e73df35f6e0f7317303823fde92a42b1a35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5bccc85f74f54a2ceb17fe3040b04fe306c53f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5c3131fb8e5a25bd4a14bc9075eb6fa01b61d02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5c7fca1f76a31b0390e92d90d569fab94d4f783.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5db3d5b1d8af89381fc4b8073f84c5fa25fdef5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b60a4e87a7aabfe3c1ce02b408522f3ec862e3d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b6b17ae67adee9e56a022cd2a5514fb9c4e99920.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b72a804bb3c99830653d41ac0bd49943c801b89a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b737410b404a51043fc3bd503c0b107c297e4c9f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b75843bb13058ffe29251e053800c509c7590544.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b774450ebadaacf23e944aaf8ca90eada01e8a5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b779cc0b0380e1e6a2b51fc6216fdd72215b882b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b7a03ab0b7887cc7ed0cb40e56360a8d36c0bb8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b80d0828ba6d24ea3c1a97bd9835ee937b4b32fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b872f9e6ebe330cc1818ea82b53acec79a2f672c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b8fbc6f6e9c515edce3c7a438b3bc308b30d3857.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9385db12001110c42eff6aabad935a69ad3afe2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9559dd36a0a4f5e068a722e285f485137bd5ef0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9627f9c8d0088df0364a64643f2b5dcd951f2bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9a742ceeb6736a2c8f9439d0b05e10d3e0c5c6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9baf70220079e6d4e87eb01a7259923d8a01e29.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9d00ab8373747a5c6b9d2f8dd50ceb14db4163c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9ed0a64deb55616646ea98b21a891c971cd98ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ba145535e53899fe127987aa854f81234a9c51c4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ba8b09f0aaa40a7c9ad5f0458b460d3e328f3c74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bafbef3f13d429ec3e9f4672218998d5669d79f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bb111b7acc269f8d5e70915d3efde4c425aa5f5c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bb28a4e95723e3df380f98b5ac107c4df353850b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bb35c86443cc9ea38c06ebc0656306483c95ef67.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bba10ecb79ede07324e1198a71a95ff26e9eb235.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bbe23201fbebed25781f249e5c77c31e0e7f9ddb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bbfd025488e52b97c04995c4c5faff371b77e4d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc1ae1dddb8cc5d78196da6b26ebe66c1ce7e567.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc238fd2095b26a167b41cdec8280182330b7b25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc4425e30a0b17e8b31726817e8d3177b5c51934.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc4e0f0496a34d2fb43c80ce0162ad4183f29064.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc6ce17223d8d83a64b8c96ac88223e4441a4692.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc744db85d4237ee9640f1658e0caab7648e3bb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc79e255d25744725e2a9db9f90d5cc2b8a0e0c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc897852a4ca992961843144f4ec4f8b86dd5e9d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bcb6f0730fd09b4c6c60913425927dfdb8f83d82.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bcd7ccdceb7baf3b986f2a0248827822a5f72e47.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bcf8836c8cf932cc2748e313885003f0e11a887f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd064e302ff5b983dbdb4ccf51383fb29ddff44f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd28203f47b6a48e9b66302cf8312f3796ca500c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd37f4f7914805a97d5073f1ebf8a8b8c2648d31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd3daa5f99b4522d932334924347353ce2854821.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd6aa39d0ae3c87d011610cdb5e2e317f337c454.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd80a1774d8b7d8bee4e8663392b97cda11dcbf5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd8bf7c572c1984ca3061062cf3c31d993f6762d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd9c47f3305e47db6ab6bc627fb3d80269633074.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bdab172627718278a71a93e3737ef08ad9259a4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bde24a8dbe6add6f2dd2beb48b1280f3a84a9b2a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_be1e1533fc37b41838bd37edc2b6d2f2e76ae1c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_be4dd90ccb2f258029d0156cf23f940b694cf08d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_be8ec1163a01b9cd9a802d8b44669e8770c20234.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_beae876d6da465687f162136231f15767cc7bb14.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_beb9afccc15de7dfcb2e7d898abc0d61201de73e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bec30e7107c5dce3fe6aa87d83ed96da75478da0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bec9e4c0317e8d351f60258ed6611fbf365c4024.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_becc2a4d7ac045365300bf8bd45fc6d3e1e1c8b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bed5a8c5cf683f6dfaefad72c2e2f5c2f2b2732f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bef3bd014a918feddadc98eed92a7734f9bcd890.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bf9cdf86a7944cd690b0fcbbaec235863acd10bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0338fbc05f86270ded7df2bd3e2758a03961b62.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0342686e4efd26413c6719782ed13603479c4e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c063318cb851ccaa923be12d34c84d839bc64bb8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c08095341ca7e3a1debeb780c1878e351692bee2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0a3c4ac0a50bb9b7ad764929dbee98c856b1210.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0f76aff077c28f8afd7b22f284cf2894e08a043.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c112c01d201c366bdd7acccf2e1b18b00f671153.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c11d68fe766fc753c657362673704005b538660b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c137c03bf161b2ec6a9a046fa49d7bbf80ae47b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c197d1f050f42d82e6851fa286db6f81ba197f40.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1b76bc7a17f573c0d52c07ae9ff4302662ae61f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1b94e19d762ddc33cc4e94c6675d93cbde21e3d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1f40c3421b9ad8cf43940530ec50bcf620058f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1f721a330b2d0fac13b22061616d7b10c0f91e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c250ea59ab6e1ee39cce15cbd3f181047cdee31a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2541b6b5cf27de3f45f60671d36602f07ce1783.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c27b3026f1dc3056dee3a3e64bf31c45683607c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c28de8f96c8315877031a2d56261e95fee6aef44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c29110dd501853e87ebc122dd1971b0bb1bcd92f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2940fd05efd52bdf8a3f9aa4b78bde9b5809b34.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2a2856bf9a81544a30d535a13554e3a8107c476.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2b719893a4d8a1e71857966d399f06c0a41749c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2f04447e6a94c94a2315454e71d7d607a9fd0f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2fcced07cc194a8050bc7b2f791453b3f5b2064.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c323a4d1f24d59bddd20ed2f2fb6446627b0ae8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c355189ade9b1a8269230232db754a3881b53168.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c35ea54eb6cd0f3756c462c66d9be956279b46ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c363ee1b087f6b504a3dd3972b96e77db02b0582.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c3cfaf0d53869c373f6d0ec821b008dbb819141a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c3d0eaf9399c863d672e8c08d123739bab837d4b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4015f0d0a7a5173810f6f17c00065e03fc61a89.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c402e84359b2037a29efd1d6ce7213ba7605ab25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c41b6eda4f250da059fe0c428428219ff5a250ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c42ab428503e8f8bfa78c8cb8d9afad9f5185118.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4376ac8d82db1bc25fa273a80dfbf8b71ee5e2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c45a5e40f6a66bc5292a56e0097c69fe37cedfb3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c487a1a9933239270f44b1e08e1cf5323521c089.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4997f79435cf64add10506acb97d0647cfbb3d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4b34d3cb673447773f6da23e9cf52b98e99f718.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4c3425fe683d35dc3335db77d183ad1620b7a92.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4c6c405cefe204824e8fad1b3dd34bba87e796a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4de1bc135191f3c2aff740f4c6bb7e98da42f84.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4dec99707511cebd9188d216ee0a148d729b470.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c538dc4f65d02776875627cbd20a9c794d70b043.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c53e295b68e807774ed31bb914e4bc59312a77d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c56aa150611b0d4800470c1493dc907082a5c23f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c581974c8b6f43f60d0af29c350d850b55c03121.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c59937be2b9a13d6520fdcc922e4e75c9fa085ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c59a22c6efd8bb8815887325aa0b739e260cc754.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c59ab718fa23f24f09a713ac28a339208a7a5802.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c5b440ca9a5196ee1e72c878c87d96934e9273c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c5fcdea177734366d3bf283317a65cc3fffda611.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c5fef330a975002ed15670e8e7b26a10376d3cb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c64f4cdce32189065362a502105c31bd2d9d99a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c6e2da8b791d31f4ba05ef5f833fd6dea9e35f1c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c7568e11e44ce70924d27e683190422cfae5c31d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c7af2bbfac25de2853be344b9f636226c1c0112d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c806d7803d06ef8aac1d5caac9f36aafd47653d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c80dce1a17d073259250ec0c87ade69e639ffa8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c8dbfaffc8a9b573f194f9c63f1175d9725f8950.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c8f6461673882d636772ae4d26e78eabcb568f31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c919b8ed877d4244d01a17ecb948b459e361ff24.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c921a4790f982d48bcaf950123c699647afb739b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9312d7159369d13f3148a6f0882dfad6921ceec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9530e20038eb40c49bc8b045be0cf4e7e6b4eac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c977735a36c325706bd19a12df66ed0839b032b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9ad71883a19b522486706d3705700c012a6fc19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9ba0a3369d4e4eaea1c902a90e6501f232dd57c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9f1e7e478a2208c4d32e2d7e6abebdc16bcc5fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9f28230817c9d9805c41dfcd4e834fe302e1df1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9fb8343e623e46f01893a2b61345d1ca5928671.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9fe51f982abd60e567d4238d3266fb60e45814b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca00cfdc5592b7440d72482a18781e9cf3afb05a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca1992a2634cd6674076611be54197c715ad8271.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca3975efd767ddf7c12e308d948bdcaf0968493a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca3d98ff43fbb80ceb82fc22ab039bee898969b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca4c6ad28aff1976c6dd36974ec3b339aa3090e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca5681d4e5871aacef74bdba9e368445875252d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca920c3239bb5796b1ab2fc75177eb3b820aa784.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cabb7b12cdd9b8b522af577e13232b2459dbd38d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cae6c7efbfc831e2bcfc8c1efa1a486c02627cbf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_caede7a18f3e3d5e24f6c70392413a2cda16ac15.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb10303a0b79f2710eb7c66896d3c1f8b12c04dd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb1a0ce432c27f4cfa51731c3ef181bf60c8a727.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb1b91c16e0255fe7a0a85638b98d94634e143a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb1deea4f4fab0db31d46a91228601f0c272d6e6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb20538073888bdb3174a8e9c32d7449072aa753.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb3d5273945c5d40cc05c2660af2df1fb7a15f3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb4576e8ea5d59d7663f3760009a00a19e1b0667.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cbd571f4fe576fdb17d5f75a558cb6747087c7f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cbe5a98163e878c7697e554758ebd0597c2c1760.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cbf3e4d4d4837a0cb33b78c4f2767b1d93da0850.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cc127a63d56099e08125b16939dac82f0173122b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cc4ac5a18f57f2ebb65f7e356e858ab0d59b2133.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cc54b107e1b557ea36b5cbaf7fe3dfce05415c86.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ccac6c0e61b65c9422c7f30fbd979031698370a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ccd0b777df1328bf24e070ed4cdf8615bb2199fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cd0453a5c3828c1358360f31f5d3b7258e17fdb9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cd4efcdd12184211c74e7b3f2f30fecf1041ca32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cd757a8bbeabd16a44d149ab188430f6d79ddcaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cde0582e1aef74f9209de638b553ec0671476258.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce4714e4f33340859c106a3129993e22652262e2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5064e27ba427cb951f7e1b01328b0beb6b2b7c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5ad502dd40353312d561e9f40aa478c16ef5b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5b5932f6df9a194ceb0d69220fba9596528eec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5c161b725becf059fb4439c668edd454ac77d1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce909cb5f96a4884caa0d2eb8c5e6bc7fa352797.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ceb9544e2a0caae2c9e3dd8bbd2c509e8dca1379.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cee81ab2e2678816c7b516d2d4c50e8cb5874c68.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cf5c6c0bfaf98f6e655fc443246b81fcc730fe97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cf73e1fc0015094861ca0c1c81bacdbe0c5b8f37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cfda56a4eb08b803332f25bda6209932d9624acc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cfec97bdfb6fa95e057eaf5a8138853e1c0884f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d00f65bc99ca08eba66564d34f72f2769bff9491.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d036096f49a89730f8af7e75457c88cb8ae64165.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d049a1b8f4c1c6d37973ce38593efda1de8ce0cd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d04dc4ed02eb42c3fe303342801ed3073a0dcb8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d06ba4c996570ddab77b6ff1e2a0101b638543eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0863830fc5d43dc6d6400280e892bb7de2892d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d090b771a4f9750132f549c82a88b4ab00dce5c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0b09e8513646fbb2a007544a63ec9e2b04dc4c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0daa59f5dce6fc3965193ae37d8c82a3d1834e6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0dd0165ee91c095a19ceddf08789e3576912590.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0de618ff3ea9f67b90f2227fb7fcc74ea34183d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0f63cafbeb445408c884727b473667fb479675e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d137b7b6e04e1caf43a62bd6788a75361cfa98f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1840494c4fa78ff399c0399b3ad7ca3d22d4587.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d18727988e47264b42b4153dc82fc1a750f08db0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1c0dfd19a08d61586758091370acbdc6f267017.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1c25cfc437d8bd803860e39a45b2f3b9fa48393.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1d3eacc320104100bce46235fe656e5a8223c66.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d20d45aa85c0daa299da98c277cee826fe67bd27.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d257148f457557ea80ca56690e525db3a4b0ff55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d25ce4b3e9cc392ceafebc7fe3bcbe05aaad4bbc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2d08c5470a385d0160b2c1441fd1c30fff1c17c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2daccc4b3a0f90bff39cb4597f8b7e484613d9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2dfdb42c1b380e860aa5609302f29698dd27923.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2f4b869ff23874b6bde0aab68c419108b7e69f4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d32c64ef01aa228277d031a74df51363f98aa2b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d34d6cdcd81a456125ab5e0875466c6334d8e5c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d34fcb56caa8f80404789fba0ffac447483a4d84.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3784fb4c0685d7b651f4113f3c71e050881f3a5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3a23ded424200d0c6f06b1dbd0a7b7b0e7b5d9b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3a2edf232786d458e2125f8dfeda8847f842afa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3af8763f289dace1054bdcb4dfeda28b0aefcae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3fce1e11aee2273620e75efe4aa0390fcde9ba5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d40569ae9dbd693c0ab3d6ba69704d31e451011b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d41b6a64dd181f2efa65aaed03a3d229b3566c1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d41cd6b60a97e7071518cbd1a63abb8b910df024.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d43715cce8935439f90172d141050d78c7e76fb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4605b2ad3e3753c5f255678abc1690b949c5abc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4645b713821371161a9925dec8a3d6c157ba1aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4aff499ad527be5fe33b8e92547df57af26d40d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4b99af9a573df50a27fccbec3fa8e350f1854eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4c9f975891087e6eed6393629b41155deafc509.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d50ac8e8a03f8e7ec2c6e993dd39f09f465dab57.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d54ac01458df3f240e0656d82330f9de23ba9651.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d54b3731883a5f8393d60d27487f8d017aedd3f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d5e82799f4452e148c3e02acd6526cf30757eb52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d5edfe3e3dc3008b928c8e6dbd50784b905f189e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d600779c17b7b21c18e1308e6d765fe02a7945d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d6149eea92f2c40c11de3b778102fcf9b6a006b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d623b36cc3f56d1001b2d3abadd8a5628fefd014.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d63c8c746055851217a514321cd735eaf6937263.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d64b8b52f4a98801e185e2f132b2f80c29dd0c37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d66b79c4ebdcfd239cecec58203606bc123bd6bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d66c30148a6fa816937f2f095802264d3dfa0273.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d703eea8075cacec4d41fee7dc4734f593ee79e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d712f23ef88ae5d7b161d36f42d22a5ba53b6354.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d713fe25dc90b3511fc259cebf463376dcb55d84.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7145383e39dec0e346b5094401acf85ef3c2075.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d723b191785c97d284675f700a7baeb52a2eb791.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7290cc4c3036c9205e689cbcc60e7d16b97a7d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d733f4c03e338ea7c6d8f759c1132499bdcea059.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d773df9ccfc1ace90fe3afb5c00976deabedf6f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7adde8780b39f1364c572a19c3bfb19417678e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7bda8157fb27d544e049fd7d2ec735725f1bf44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7fae2c18645d36a181a0bdd2d8ca7a4ac0f6d1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d82773721479613ad72e334510a248f1436b38d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d867098db97b3f26e71a151c63b74260bfab21f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d86e4dcbe9c4cac8f7c8c5d97ce384ae0cbdbfbc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d8901a63986cc28ef24cab012b32114851a8c1ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d9061c204d8a85c974676f4438994a0be9d69a60.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d924ee32b178b6bffa7a71603d6e2818f66177a5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d937609afa8e21a761dad6b01ff3f26346e450fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d95835bc6f000d3a3379bbc38d90e83dcaf867ee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d992eab7de49033f5480c5e86a69e675db0d2a19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d9c23b7f8fcc4e4f4c81f5f00cfd345b98df2e0f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d9c3e27b522320dcca5ee84fa534b03aae2bfea9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da07d8b5666423da30a95e3b2cabd3839d200981.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da29a515d14dac02066bcd4701285b9916b43cf5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da6afccdee4107507a64323e17bf12c46da2b92a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da74887afedbd67928fe4d596709f9ff92530611.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da822ea727fb3543e445e4000f7e6ebb946d6a3b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da9f6e1d59132fe96709490af25bd794f267851c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db0d0cf55d90b3f3c9eecada1db93c420f34b1ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db5016bff9e5dc37184d2b9417eb351c7ea1c322.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db85839ee8d464c5a81b8dad9839f5e0f4b467a8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db8f0bd93b352d28c5b6d78f4332026993f0bea4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbae1670fac6812b2d2cbad973e4b475509ea504.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbb06b43d5d65429e23cc717448cf1fffb0cfd74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbc4135fce01e8731fec7a78d0cc0fdeeae28b90.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbcea8f7b5930abf76eecefce92d0db785d2df5d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbde2ef18e2174ebe13a6e7c8c2a6b05a6612047.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc039d422a57c159ea4dbcc867d766ff1b356a07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc08afbff5def8bcb4e823657ce01f57c9dc77c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc184767d723f4995791848cdc68bd948408204f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc1a7f9b1afeba6690fdc0d0d1755ea89c805573.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc34b6ef496d4e0d8fbbe10731d4a7b1c136c036.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc3d625c5ad3e871f5a727ac946df642d988b9ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc4d27535b9570b8f4b790470a83c1d0a9a2b6ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc5ba6d73f331c76e696953606c5b347b6a46f3f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc62a8db637d32e7dfdb2521cbdae6e1fbbd5fd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc818f3ce244743cb1dbff9aca399df90742a6d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc91797c1474a368e9cb056b50b4629d7736c3cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc9e54273c0ea2358fb573a7d918aa7b09fe07f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dcf815ef540060cc7ed43e1c57a28e1d080c5621.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd10bbf37503bbc92af82bc3487989b41b20ca85.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd11806cd2d3ef1127f676b2d98bf8fff2a1e5ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd35634440edb25cb095800b882c70aaceca1dbb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd67d442001d2b167e70e8730abde4d4461b8569.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd9494d9ac35eba6794a4f9120d2db9932596ef8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dda8d021381083bc48b7fb1840729254dd8e5137.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ddcb1cfea1b0dbe50a02252cba99428fd977527e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dde93ffe7fca311e136e42fbcd12b05c9fc7174c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ddf5339054f47d9ed6cc7f9e66ab21ce3bccf3db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de1ff66d2aeb47d2fdccaa4bb6b9d066b380c99e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de26a187c4db06115072a5132e1166b5b03368b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de36bc309877917a18fd21acb30563c7e2f233c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de5359f0fba3da9dfed06ddbea8fe2a33a9cf40c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de6683d175affaa5ff261ab8503f64172d8eba8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de7eb562a7eff31d589e12945d80233aac202ae2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de85901d66dc04b1143bb6404445baf65693b781.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_deb9ec2cccab94920e40f62a1f0f094acd919d07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df0b2bcba57e77d975ec5304fc50cbd09cddf4bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df4bb75ca79f805a81fbad750ad22f6d22b0d8ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df4c9eb48da49a61957537270d94e56cb4e426be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df5b1c6758d4b8540158299dd0362297083084c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df645b3888dc8d1df50c47c0d75822eebd3eb019.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df66feebc9a0dcc508ce002c255154622875e524.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dfcd68acfca68d1acac94f493e25be0ef20f209f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e02a198f23c409b715761b702d7b0e6e5992701f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e035773419a9b3631698a3d375d829af55f7731e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e088f0f7363804cf5403adef70828ab32d09a02a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e0966fa1ff013e477b1706928de6cb7f8587c154.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e09d9baa269dfbb30b714389d1733be51cc419b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e0e48d7edfe9513f24ad9fae68cac3aa940b17dd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e10f47a44400de385ddbeb99475b717c5646fb41.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e11a3b7d4fdfed64e64f7a95dbc64eff541092d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e13b86fe4e153e0bfa8d1e75f3641fe32b0c5149.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e16075c3a5fcfe63ba12e854bb1fed6873f014ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e16edb824cecf459a8ec51b8dc74b1e06369aceb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e1c1a31a1d8556cbe0b6ea76faacc78855108539.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e1cc934ba7baab1a2eb062df1e4ee5066e9ffbc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e1d85ad2c9d197f501267fe0804e6985802fbd18.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2762543d3380185e304f84749a70db1b8d3dd8c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e28fd64c2f2b27577109a984e6ab82f5f0fcb296.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2b629c37cf94134693ce455b8c88b72a39df7fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2bf6805a489739abb77c13173d57723e9304afa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2c9f955f227430c6224ebc347649386be7f01eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2deafd2f36cee29109fb824e0135407453adcfe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e3015c5d50481547aa5754d042d9d7040cf1c7ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e307a1b0d5a8f94e0a0f4032f401d20b4b643523.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e334e691714f0b99773c2ac515ed82de0f387065.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e34b7e452a4db74189334697e3a240ad68085f0e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e389d0e4442cd8304081892ddc75043e68a6398c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e465193d97d43237c22c04478ca5833011d8dc8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e477abef05ff37ec27705eda51896e2aa3a04966.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e4d9a2396ceccdadab24602f30e9070901a76dc7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e502730dea6987e2c038446c448aa08bdcc23113.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e514c6b4bc75d95a150104a17972abae77cb47ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e52e3053f30f780f346fa6b7a836ad2554cb85df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e56757fb17f5e94a6ba1fb14540a68c36d571159.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e578ec9e09d3b78dca6b5bf0be1538657f02f319.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5935fbda313d3518f142f43d46f56c600f69286.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5b2bb9f8466de1ad5210e4c39ee7b8ecacdffa9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5b65fc519ea7cfcd19f7eddbc3acad6842ff558.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5c5079636a4a31a849ce8a5af89d50330a74628.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5ccd5f7ddc894b2717112cbfc766804e02b7bd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e618fb4e529104fc90069c8779ce5463460bd516.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e638053e01268a4c5883620fc6a9901951e2e01a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e639a1e84faa98477b05df71d363b9ff0f9b2760.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e68a9e05debd456a9975953f7b0d510e7a0f6978.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6973d75297bd2c3432a7c88e8a9ee1c9ae693bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6b53fb8d81148ff384d31a703bb4c2e7a5a33af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6e0ec1db1ea308e226f675e68e29b839e41b252.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6e6b10e73733716e71ebf5a53703fb935fc5e02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7153f9a9b0b7c54ddf2debbe297efcffbb4fcfa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e73a776ae4ba68c23acab1a5a6381684051738ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e75c757c67aa23cb88e1aced6fcf36b7b28391db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e75d492ac3a6ab75648056bcf26250a4aa929cfd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e76879f8ff4796f48ad87ff8003f4f6e6adca9a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7ae1294b6dea5c8b93c2b814fa7460c4047105b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7b2eb64b66d46359fab44333c2c484f4c9dd5de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7c0a99e949baa5f3a7ee2d6e84427982f82f76d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7d37e7ee96c392fa24c02a9143438a3a7d05741.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7de729aa50c10d8101ef504138c3769e3286753.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e83c604d1b8260958becd1c7c209745ff9151715.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e89bcea4393593313d18a4aa6dcb44cd75bc828d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e8a9427f34bbf5ddb28a39161acc36806e68f2d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e8d8fe5f4f8641998b8b805a20b2ca92d019ee59.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e8d9b65558398c0c10127b560807578ef117d7ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e907e8d1089557dfcc95a05160be5092e9119a53.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e95e3908479965856843317c8b0c42a6961dfd23.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e986d5f8d5591f3e0f1cdfad19c38c420fd93023.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e9b04e6d5527ba0b8089ba8bdd264e2d5759338b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e9b53fa68641f45baabf40b7cfb8b35a9a1b9c7f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea077e68dbc1bed2dd20a5f4dd35e0cad6330ee4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea591185b1c5f521023e250a26f742984255b241.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea62567e9ea16771d8445464c38f5a2931cb355a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea6a6d4cc262ea838dbb83ee747112f95fa297bc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eab6cdc59bf216f7045f0cf5f221bb91ec415cd2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eac353f963c52624cf79e82cc2b2c02eed94b677.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eac5952f46f4f2bf06257b00661774eeed48a323.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eb278488b2cca114adca5e4614d86f92447f937a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ebb241b947a0adfc8e50c5d71765c14af24593ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ebb9abf5b09e63cbe76390bb46ff7cbefb3141f0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec171210efd217c07d357fcf42e5372ad7e9abab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec3deb1382003ac010d9bc1c59d1878d3ec7a727.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec51d24ab5f24e003ed6751ae8ae5b327892b15a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec7ec8d547ee9713aa3b5b667f22cdcaa8f62b2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec7fc24902b1ebd8f2bf8088b0ecf6de8be8362d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec9f63a538940e5ace02ae5b5ddc01f730adac4d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eca613eaa8471ad7da66d2f8f2b8e07f6e02b467.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ecd7dec90b3c62bf3a30bd75d3c6869529a06b01.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ece60111633db08f765b3c7cd5cd768cbd030255.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ed37ba962e0288e2840eb0925d016b5a7e3b3164.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ed6bdf67720e938d538a867548ac3579b8238169.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ede81dbc4cb208ef6e684c76ba1eb451d37fe10c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee1a43f2210a8d1e5623411c95c33424cee5e747.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee239db5a67c23a383590a651f0d8a0be43a13c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee8e709eec7aef1fa681053c6d2969a5ff18c45c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee974931e65d6b16b7c868d462b95dcae20b7513.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eeb0e96b759e18cf703cfab0cda1385726f6e0a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eee408cf9456ff977aa7d12345e9b2f1e60639f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef2ebb4a86e7ed0001de9c5e607b66fe8877409f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef40f0acf1885096efb840ec5600ec421c4db331.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef5421703cbfa63a58ec02701e245d479a1fbfc1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef7cc2aa1ffd38298b52764a93cd1271b4d92f8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_efaa0cb33c71cb8ca7b83dd0e7a6c7b01f6b50a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_efb9e7d9af47cdf79f15f674f8976c05f08b0ce8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_efc6a7b25710f0626c3af534111b161e1459d2e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f01468c62c878295443981662e037ec5213cf7a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f020134822739be6fa0bb3d98e9dec79f025324a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f0209426a8e6bfeef7d8ae7b16db791888142298.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f028af9e5e3c25800dde938e991aaab4fc1d64aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f053c9c32518b895daaa3521827f37af78836fb8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f069b38b26c30bc770f74c856e47eb498f5818e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f0cad48d9bc80d58705ea60eb2dda4baad68cedb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f1246d1013d954a9316f4432c986d3be9459c548.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f12f1f1b679cabab04218037ef370d2c7e1fe332.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f15c41ddb04ec7f80235bb3db19198dd6b699713.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f18c74becc24a93427d9c0838784e9b6caad6e81.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f1ecc90ad7b86791a9e6f73a582aeff30f393804.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f21596e8c608a795ff971aea8e199db9e72b65d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f24bd5b92ce6bba640b8ec6b4e53fe35902c5572.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f24d42e820adc1a26a428d59df7ffdd7f8580176.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f24f26e45d5cf567d29fbe375fbf8abdec39186f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f25b87c435bc5d7d85d738f3fdf68947d79f5a77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f280e1639680ac1e5830a21f921bfe2cf364ef42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f2da112b1e07c44fc8a7f19368da203f6935049c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f30316cfe49323638f71ba688dd8ff9b2266b335.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3193ea266f3718398bc5622f8bc7042c3527a42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f34fdb8294257d951dcc9c4fa7ecf1192568b91b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f36aaa63ed42a578b953ebd614318d44cf44e8a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f395bec57c3b2e6e169134dd8d20b287d7405134.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3bf7ef503bb026258b3ec3d82d3ef1443046964.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3d0166931e4406873d8f552a5d5b61fde2391a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3fd08d56f8a9be1a8dd104cdb1ac58e283b5064.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3ff73f82aee3184849d04c2364eaa45c6d0de9c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f42cf0e5fe479690883507028748b0cd3dc83cbb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4658c32d562f9d60c5ca1262a2e0df2375063bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f48f8b681a405bfeba5aadaef40f32367ec5cd2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4900c0a5c0d03dc17d7a907ab40652d9920e756.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4a6438394dd3427f29aa0bbe58ad1f797c3c38d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4b87f983a5e84582efa1663f84da76cf60b5f6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4c803838f5644ccc6f04f7c8a6233fed0b6639e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4df1cbfbaf67705820f125b474469ad7ebab0c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f50fa4ea674a590d0a817367ad9915a5fce20c51.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f51f1a11f778d99a00aa5959a3e58a41fcbfb1e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f525b59df454ccf53da6cb201e0aa8d09f52a2ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f57f84892e2a8496169b7406e63b0d4f5aa63aaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f5803aadd93e33567aa6b23100ce4fbb6c040dd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f5f1797f6b672a55476348571ce17645c8a62869.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f6566441ac3074578cfe45758ba0583c0da0a5ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f672bf80a78885428b2c02e522426470653a7351.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f682399cd6412fed6a1141296a7e4d42078f7b29.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f6856ca950bcf173571766c3f04de4163be0402e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f69548d6cced86c21c09c6475237a0cb926df0ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f69878f4ca8cfe6b8d8748766f66a1ef8eab20ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f6f102a388ffb05c690a20a29cfe0b35a35eed61.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f7035f4bfd8f2f427720a07e3c311bccc1dba683.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f71f96ce4dcc7f789a8ace73c230c203b05ff6dc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f727911254904ce4341e4ff5f8bafc430b8cfbbf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f731289837f915e2aec1bd01eef1b3c1b099864d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f79def2b4edf6d18f6ef1d6b141f9e0435441f6a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f7aa9c39b06e55bf4bc9f9a2a0fb075c9d4e69ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f7cf08242b3fb1c643d4149bec985b667b9d28fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f851da732f397624717160f89271514bc334b59b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f861d8693f82d22e2c5b1abbcbae5f30f4433e5e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f87790f260630f312b84888dcbdf849ce130ae59.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f87991cb7787a29d3ce4711b4ce04c5fb6a14ca9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f90410c26d7649e21e2ae5e32e7af89d84d2ea70.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f92e9a82c879051d6fe3c42108f8a574187704af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f93bc23b8a4f1e0fc5c5756c4e1c835bf59dea09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f93bf815b520a9d9e17b43bf9d7fb870751b6225.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f974b12e83e214c30995a25631d37df1478927af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f9824fb32933b27501ae8a7f43f460a2dda6a814.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f98a6b193fec3203eaa75819f6b51aa45a48f212.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f9c58761c927b222112cb5cb6c9acb5d3c915785.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fa16fa84278b489af253b52839786f94aeeac36f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fa62a97675719c2e8e9bb97361b92ff1c7b9d2ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fa85f869a92f0482605e52019828244b12e12b44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fabdc143c29d5ca50ab1e96a814bda6d05b0d5d2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fac5a0f98b94530befd634891e42c424bb86f0e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fac99c3c82b77946f6844699d2333cd532a78a26.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_faf56e45b2240515e97fc1bfd552eb03b6de5094.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_faf686067fa433cea5e95dd523846dc881eff635.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb2fbb135d59028afcf867c2cf08edc323565528.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb4c15452f9155c5966990f09432e5eb7e28e785.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb4c5f8fecfbbe16e6648becb3b5ca89fa3d8a94.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb5bb49928ce5515d7b297d5eadd4ec70a22d60b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb79e1f9231692d736dbada062ed6821f34927bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb9477a613665cebcad781389ba7c5a36f51efe2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fba36678d5047ded97ee7a7ba9feb9569afdb6ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fba47fa8d9b5375bc408af68b67345ab9dba2eb8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fbea85b766bf0c918ee0baf24dffc6a5563d5105.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fbeec221cd63adaedceec39db41ea942f99f5133.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc030b61ae20c4b7d9b2d10930a17e01e9e93328.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc1790325b59bd44b0a5f6cf9723a25fd845cba7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc1eb85a00017efdc610e4259d2abe935b85304f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc5841a729099340d608e31023acbeaeade3e886.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc5ebf0f2200f37ccc0849e0c3745f6e2f00111d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc7b0916744b593435d8e1e7b6d874d760cd5e3b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc86c13e933cba40553ffba31d53aad27415ce4b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fcb0b08e29b2e1bf181fceceb9dc416e54f52b00.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fcb6ef39c3db49f26f736d6c9221dd825409ec4e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fcbe827108d252b2f5847fa8e132c9c3e56a90a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fccabea88b8e290688c1b360875d228e6fdf1624.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd10a3b937e9659716925e39a01d794914b08e26.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd19d7614f2ed5da21a52ed172ef62cc07c9c01a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd26e43ca652e6f58ff48c356165aa4349833b55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd345632e0cae0d549ba79626a08b1885711deb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd3558b4c7a667dbc365c4c2ceda646975408f51.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd614df484b263deae3b3c20adb0ce7b62eaa651.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd9cd1305633b62b68fb8474ce021f639f8492e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fde12cd366d6850ce26afce98e5076b695b4875b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe245e9ea974adce2b9807d33b9ba12d916eaffb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe72cdd69944d2d765478d4aed13066a02b76f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe8b8c3525fe86a20a2d6c69585f3e36c16caabd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe97b7adcd67ed9bda8831d1f3f1ca7590c6d251.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe9d98dbec5096a89b116f85675af772f023014a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_feb5e77111fe1e20bafdb83a925b5faeeb6214af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fecd7501265b4c4dcf015485e63e2324304f70d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fecffa403b3631b1957e1a9a06f18fdb3b4eee5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ff453e3bdc9752cb7b81f7cc3056325a8b9a8ad4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ff6862dbdbb20bc63a650e1f93e9ac169bb702b2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffb5b7349a671b182d73c8016590f26fe06a4cba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffb8adef0cef91a86f36872407fea35df90e8f2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffc6056d9fe125a4dbe08c1d86354e51f7daadd5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffd868d49abdb769ab82c21508d655daf54b8a99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fff7aa57cca501f221077124359a589b3a6f9d0a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fffbfcac254e33926131a71905e93f9cc0aef89e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/mask.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/rename_ck_autogen_files.output.txt aten/src/ATen/native/transformers/hip/flash_attn/ck/rename_ck_autogen_files.sh aten/src/ATen/native/transformers/hip/flash_attn/ck/rotary.hpp aten/src/ATen/native/transformers/hip/flash_attn/flash_api.h aten/src/ATen/native/transformers/hip/flash_attn/flash_api.hip aten/src/ATen/native/transformers/hip/flash_attn/flash_common_hip.hpp caffe2/CMakeLists.txt cmake/Summary.cmake docs/source/backends.rst test/test_transformers.py tools/amd_build/build_amd.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/backends/cuda/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/143695,xw285cornell,malfet,,,
52742b07c51,skip,not user facing,remove allow-untyped-defs from nn/utils/_deprecation_utils.py (#144136),torch/nn/utils/_deprecation_utils.py,https://github.com/pytorch/pytorch/pull/144136,bobrenjc93,aorenste,,,
d41134f7e57,inductor,not user facing,[Inductor] Fix `torch.polygamma()` when n == 0 (#144058),torch/_inductor/codegen/common.py,https://github.com/pytorch/pytorch/pull/144058,shink,jansel,,,
f8e1eacf2fe,inductor,not user facing,[MPSInductor] Extend `constant` to bool type (#144167),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144167,malfet,jansel,,,
6d259385407,inductor,not user facing,[MPSInductor] Add `remainder` op (#144162),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144162,malfet,jansel,,,
464b50dbd7e,inductor,not user facing,[MPSInductor] Add `floor_div` and `index_expr` implementation (#144083),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144083,malfet,jansel,,,
22580f160e9,mps,Untopiced,Multinomial sampling fix on mps for non contiguous tensors (#141515),aten/src/ATen/native/mps/operations/Distributions.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/141515,Isalia20,malfet,,,
1d091e47d6e,skip,not user facing,[Inductor UT] Generalize device-bias code in test_torchinductor.py introduced by #143884. (#144057),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/144057,etaf,EikanWang,jansel,,
dbdda654af2,nn_frontend,not user facing,[64-bit][CUDA] Upsample2D 64-bit indexing fix attempt 2 (#141923),aten/src/ATen/native/cuda/UpSampleNearest2d.cu test/test_nn.py,https://github.com/pytorch/pytorch/pull/141923,eqy,ngimel,,,
7e3cd0e488c,nn_frontend,not user facing,[CUDA] Check `size` calculation in `ilpReduce` for `softmax` (#144009),aten/src/ATen/native/cuda/SoftMax.cu test/test_nn.py,https://github.com/pytorch/pytorch/pull/144009,eqy,Skylion007,,,
98949df7a44,distributed (dtensor),bug fixes,Fix torch.distributed._functional_collectives.AsyncCollectiveTensor for aten.to. (#134661),torch/distributed/_functional_collectives.py,https://github.com/pytorch/pytorch/pull/134661,PHLens,bdhirsh,,,
3292220c43a,dynamo,not user facing,[dynamo][easy] Move symnode helpers to utils (#144158),torch/_dynamo/utils.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/144158,anijain2305,jansel,williamwen42,,
087c625261d,dynamo,not user facing,[dynamo] Trace torch.typename (#144163),test/dynamo/test_repros.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/144163,anijain2305,jansel,williamwen42,yanboliang,
479d6f21996,inductor,not user facing,[mps/inductor] Add support for log(). (#144169),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144169,dcci,jansel,malfet,,
ec1f56fdcfc,dynamo,not user facing,[user triton] add support for prune_configs_by in @triton.autotune (#142207),test/inductor/test_triton_kernels.py torch/_dynamo/guards.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/functions.py torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/142207,SamGinzburg,aakhundov,zou3519,,
f9bf9057efc,caffe2,not user facing,Fix ruff warnings in caffe2 and functorch (#144182),caffe2/perfkernels/hp_emblookup_codegen.py caffe2/perfkernels/sve_emblookup_codegen.py functorch/dim/__init__.py functorch/einops/_parsing.py functorch/einops/rearrange.py,https://github.com/pytorch/pytorch/pull/144182,cyyever,malfet,,,
f15af077fb3,fx,Untopiced,Fix get_source_partitions when weights are tied (#142446),test/fx/test_source_matcher_utils.py torch/fx/passes/utils/source_matcher_utils.py,https://github.com/pytorch/pytorch/pull/142446,yushangdi,angelayi,,,
b5b1e9456ae,inductor,not user facing,[MPSInductor] Add `masked` implementation (#144084),test/inductor/test_mps_basic.py torch/_inductor/codegen/common.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144084,malfet,Skylion007,jansel,,
816328fa513,dynamo,not user facing,[dynamo][lazy] LazyVT utils to get original value/source and is_hashable (#144160),torch/_dynamo/utils.py torch/_dynamo/variables/lazy.py,https://github.com/pytorch/pytorch/pull/144160,anijain2305,jansel,williamwen42,,
417d9c3522d,inductor,not user facing,[Inductor/Triton] Upcast FP16/BF16 math reductions to FP32 (#141052),test/inductor/test_op_dtype_prop.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/141052,arui-meta,blaine-rister,,,
636a2c7e0f8,inductor,not user facing,[Inductor][lowering] support out_dtype for dequant lowering (#143845),test/inductor/test_cpu_repro.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/143845,Valentine233,jansel,jgong5,leslie-fang-intel,
aa7d01ea225,releng,not user facing,Use sccache 0.9.0 on ROCm build job (#144125),.ci/docker/common/install_cache.sh .ci/docker/ubuntu-rocm/Dockerfile .github/workflows/_rocm-test.yml,https://github.com/pytorch/pytorch/pull/144125,huydhn,jeffdaily,jithunnair-amd,wdvr,
a881954b0ce,distributed,Untopiced,[PTD] Dump rcclexp proxy trace in pytorch (#143678),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/143678,dmwu,c-p-i-o,,,
df458be4e5e,quantization,Untopiced,[4/N] Apply py39 ruff and pyupgrade fixes (#143257),functorch/dim/__init__.py functorch/einops/_parsing.py functorch/einops/rearrange.py test/ao/sparsity/test_activation_sparsifier.py test/ao/sparsity/test_data_scheduler.py test/ao/sparsity/test_data_sparsifier.py test/custom_operator/test_infer_schema_annotation.py test/distributed/_composable/fsdp/test_fully_shard_training.py test/distributed/_tensor/test_pointwise_ops.py test/dynamo/test_export.py test/dynamo/test_repros.py test/export/test_lift_unlift.py test/functorch/test_vmap.py test/fx/test_partitioner_order.py test/lazy/test_ts_opinfo.py test/nn/test_module_hooks.py test/nn/test_packed_sequence.py test/nn/test_pooling.py test/onnx/internal/test_registraion.py test/onnx/onnx_test_common.py test/onnx/test_models_onnxruntime.py test/onnx/test_pytorch_onnx_no_runtime.py test/package/test_glob_group.py test/profiler/test_execution_trace.py test/profiler/test_memory_profiler.py test/profiler/test_profiler.py test/profiler/test_record_function.py test/profiler/test_torch_tidy.py test/test_autograd.py test/test_bundled_inputs.py test/test_cuda.py test/test_cuda_sanitizer.py test/test_dataloader.py test/test_functional_optim.py test/test_fx_experimental.py test/test_masked.py test/test_mkldnn_fusion.py test/test_native_functions.py test/test_nestedtensor.py test/test_nnapi.py test/test_numpy_interop.py test/test_ops.py test/test_optim.py test/test_reductions.py test/test_sparse.py test/test_spectral_ops.py test/test_static_runtime.py test/test_sympy_utils.py test/test_tensor_creation_ops.py test/test_testing.py test/test_torch.py test/test_transformers.py test/test_typing.py test/test_utils.py torch/fx/passes/annotate_getitem_nodes.py,https://github.com/pytorch/pytorch/pull/143257,cyyever,albanD,justinchuby,,
99f2491af91,skip,Untopiced,"Revert ""Use absolute path `path.resolve()` -> `path.absolute()` (#129409)""",.github/scripts/build_triton_wheel.py .github/scripts/close_nonexistent_disable_issues.py .github/scripts/collect_ciflow_labels.py .github/scripts/delete_old_branches.py .github/scripts/ensure_actions_will_cancel.py .github/scripts/generate_ci_workflows.py .github/scripts/gitutils.py .github/scripts/lint_native_functions.py .github/scripts/test_gitutils.py .github/scripts/trymerge.py aten/src/ATen/native/transformers/cuda/flash_attn/kernels/generate_kernels.py aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/generate_kernels.py aten/src/ATen/nnapi/codegen.py benchmarks/distributed/rpc/parameter_server/launcher.py docs/source/scripts/build_activation_images.py mypy_plugins/check_mypy_version.py test/allowlist_for_publicAPI.json test/distributed/_tensor/test_dtensor.py test/distributed/elastic/rendezvous/out_of_tree_rendezvous_test.py test/distributed/flight_recorder/test_fr_analysis.py test/dynamo/test_cudagraphs_expandable_segments.py test/inductor/test_cudagraph_trees_expandable_segments.py test/jit/fixtures_srcs/generate_models.py test/jit/test_backend_nnapi.py test/mobile/test_bytecode.py test/mobile/test_upgrader_codegen.py test/mobile/test_upgraders.py test/onnx/error_reproduction.py test/package/generate_bc_packages.py test/package/test_directory_reader.py test/package/test_load_bc_packages.py test/package/test_misc.py test/package/test_save_load.py test/quantization/core/test_docs.py test/run_test.py test/test_cuda_expandable_segments.py test/test_serialization.py test/test_type_hints.py tools/build_with_debinfo.py tools/code_coverage/package/util/setting.py tools/generate_torch_version.py tools/jit/gen_unboxing.py tools/linter/adapters/clangtidy_linter.py tools/nvcc_fix_deps.py tools/onnx/update_default_opset_version.py tools/setup_helpers/generate_code.py tools/stats/export_test_times.py tools/stats/import_test_stats.py tools/stats/sccache_stats_to_benchmark_format.py tools/stats/upload_artifacts.py tools/test/heuristics/test_heuristics.py tools/test/heuristics/test_interface.py tools/test/heuristics/test_utils.py tools/test/test_test_run.py tools/test/test_test_selections.py tools/test/test_upload_stats_lib.py tools/testing/discover_tests.py tools/testing/do_target_determination_for_s3.py tools/testing/explicit_ci_jobs.py tools/testing/modulefinder_determinator.py tools/testing/target_determination/gen_artifact.py tools/testing/target_determination/heuristics/filepath.py tools/testing/target_determination/heuristics/llm.py tools/testing/target_determination/heuristics/previously_failed_in_pr.py tools/testing/target_determination/heuristics/utils.py tools/testing/test_selections.py tools/testing/update_slow_tests.py tools/testing/upload_artifacts.py torch/_inductor/codecache.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/pattern_matcher.py torch/_logging/_internal.py torch/onnx/_internal/exporter/_capture_strategies.py torch/onnx/_internal/exporter/_core.py torch/package/package_exporter.py torch/testing/_internal/common_utils.py torch/utils/cpp_extension.py torch/utils/data/datapipes/gen_pyi.py torchgen/decompositions/gen_jit_decompositions.py torchgen/gen.py torchgen/gen_executorch.py torchgen/operator_versions/gen_mobile_upgraders.py torchgen/shape_functions/gen_jit_shape_functions.py,,,,,,
301c4570322,mps,bug fixes,[MPS] Fix `nllnd_loss_backward` crash with different dtypes (#144170),aten/src/ATen/native/mps/operations/LossOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/144170,malfet,Skylion007,kit1980,,
6f2451c2e9c,mps,Untopiced,[MPS] Add `aten::angle` (#143449),aten/src/ATen/native/mps/operations/UnaryOps.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py,https://github.com/pytorch/pytorch/pull/143449,sezelt,malfet,,,
79cbda3ab09,skip,not user facing,[ROCm] Get rid of extra rpath-link that was needed for libtinfo. (#143348),cmake/Dependencies.cmake,https://github.com/pytorch/pytorch/pull/143348,naromero77amd,jeffdaily,jithunnair-amd,pruthvistony,
45ef3309e32,quantization,not user facing,[BE] typing for decorators (#144161),torch/_compile.py torch/_functorch/apis.py torch/_functorch/functional_call.py torch/_inductor/decomposition.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/jagged_lowerings.py torch/_inductor/kernel/conv.py torch/_inductor/lowering.py torch/_inductor/mkldnn_lowerings.py torch/_library/custom_ops.py torch/_library/infer_schema.py torch/_library/triton.py torch/_meta_registrations.py torch/_ops.py torch/_refs/nn/functional/__init__.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py torch/distributed/_composable/checkpoint_activation.py torch/distributed/_composable/contract.py torch/distributed/_composable/replicate.py torch/distributed/pipelining/stage.py torch/distributed/tensor/__init__.py torch/distributed/tensor/_dispatch.py torch/distributed/tensor/_ops/utils.py torch/distributed/tensor/experimental/_tp_transform.py torch/fx/experimental/graph_gradual_typechecker.py torch/fx/experimental/migrate_gradual_types/constraint_generator.py torch/fx/experimental/proxy_tensor.py torch/optim/adadelta.py torch/optim/adam.py torch/optim/adamax.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/nadam.py torch/optim/optimizer.py torch/optim/radam.py torch/optim/rmsprop.py torch/optim/rprop.py torch/signal/windows/windows.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_fsdp.py torch/testing/_internal/custom_op_db.py torch/utils/checkpoint.py torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/144161,aorenste,Skylion007,albanD,,
93633d0e803,skip,not user facing,[ROCm][Windows] Fix export macros (#144098),c10/macros/Export.h torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/144098,m-gallus,jeffdaily,,,
005a4b95373,skip,not user facing,[Submodule] Bump Cutlass to 3.5.1 OSS PR (#144000),aten/src/ATen/native/cuda/RowwiseScaledMM.cu third_party/cutlass,https://github.com/pytorch/pytorch/pull/144000,drisspg,Skylion007,eqy,malfet,
f5df082fabf,skip,not user facing,[dynamo][dicts] Guarding lazily on dict keys (#143997),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/dynamo/test_dicts.py test/dynamo/test_misc.py torch/_dynamo/guards.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/143997,anijain2305,jansel,,,
60de73c3c73,Uncategorized,Untopiced,Update nightly PyTorch version to 2.7.0,version.txt,,,,,,
811c7149118,inductor,Untopiced,Fix nan propagation for minimum() and maximum() in MPS (#144086),aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/operations/BinaryOps.mm test/test_mps.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144086,jhavukainen,malfet,,,
e458b39fc41,cpp_frontend,improvements,c10::string_view -> std::string_view in Device.cpp (#144178),torch/csrc/Device.cpp,https://github.com/pytorch/pytorch/pull/144178,r-barnes,Skylion007,malfet,,
0dc1e6be192,skip,not user facing,[mps/BE] Fix linter warning/advice. (#144199),test/test_mps.py,https://github.com/pytorch/pytorch/pull/144199,dcci,Skylion007,malfet,,
b1bc880f26b,skip,not user facing,[EZ][BE] Cleanup `test_mps_basic` (#144194),test/inductor/test_mps_basic.py,https://github.com/pytorch/pytorch/pull/144194,malfet,Skylion007,dcci,,
68d30c6a256,skip,not user facing,Add check for unsupported sprase layout to resolve false INTERNAL ASSERT FAILED  (#139198),aten/src/ATen/native/BatchLinearAlgebra.cpp,https://github.com/pytorch/pytorch/pull/139198,jackson-tsang578,amjames,cpuhrsch,pearu,
f2d6cfa6775,dynamo,Untopiced,"Introduce CompileEventLogger, replace usages of metrics_context and chromium_event with it (#143420)",torch/_dynamo/convert_frame.py torch/_dynamo/pgo.py torch/_dynamo/utils.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/aot_autograd.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/143420,jamesjwu,aorenste,,,
67f85ccdcf5,skip,not user facing,[ca] add test_dtensor_compile.py to compiled autograd tests (#144107),test/distributed/_tensor/test_dtensor_compile.py test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/144107,xmfan,bdhirsh,jansel,zou3519,
157c185afeb,inductor,not user facing,[inductor] Add types to compile_tasks.py and runtime_utils.py (#144004),torch/_inductor/codecache.py torch/_inductor/runtime/compile_tasks.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/144004,jansel,yanboliang,,,
1e881ceecfe,skip,not user facing,Update torch-xpu-ops commit pin (#143984),test/inductor/test_torchinductor.py third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/143984,xytintel,EikanWang,desertfire,jansel,
b01556bd8a9,skip,Untopiced,"Revert ""[dynamo][dicts] Guarding lazily on dict keys (#143997)""",benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/dynamo/test_dicts.py test/dynamo/test_misc.py torch/_dynamo/guards.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/nn_module.py,,,,,,
f6488d85a01,dynamo,not user facing,[dynamo][user-defined] Remove __getattribute__ checks and add getsetdescriptor (#144173),test/dynamo/test_misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/144173,anijain2305,jansel,,,
51a37a42e0e,inductor,not user facing,[inductor][cpu] Fix bmm b_index for dynamic expressions in inductor autotuner (#143141),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_bmm_template.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/143141,frost-intel,jansel,jgong5,leslie-fang-intel,
9f94710e48b,skip,not user facing,Update core.py to fix typo (#144201),torch/testing/_internal/opinfo/core.py,https://github.com/pytorch/pytorch/pull/144201,jxmorris12,Skylion007,,,
3d3a07963f2,skip,not user facing,[reland][attempt2][AMD] Turn on TF32 for aten::mm (#144145),aten/src/ATen/Context.cpp aten/src/ATen/cuda/CUDABlas.cpp test/dynamo/test_graph_region_tracker.py test/dynamo/test_misc.py test/test_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/144145,xw285cornell,jianyuh,,,
d71f1111096,inductor,not user facing,[Inductor][CPP] Fix Inductor integer avg pool (#144059),test/inductor/test_torchinductor_opinfo.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/144059,DDEle,jansel,jgong5,leslie-fang-intel,
23e2953cd38,inductor,not user facing,[mps/inductor] Add support for floor(). (#144195),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144195,dcci,jansel,,,
c9ef98478aa,skip,not user facing,[mps/BE] Enable a test that now passes. (#144198),test/inductor/test_mps_basic.py,https://github.com/pytorch/pytorch/pull/144198,dcci,Skylion007,malfet,,
cb5fa17e443,skip,Untopiced,"Revert ""[ca] add test_dtensor_compile.py to compiled autograd tests (#144107)""",test/distributed/_tensor/test_dtensor_compile.py test/inductor/test_compiled_autograd.py,,,,,,
e1622dca7a7,skip,not user facing,Fix duplicate pattern error (#139321),test/inductor/test_pattern_matcher.py torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/139321,eellison,shunting314,,,
a8e97d9d4d6,complex_frontend,Untopiced,fix torch.acos and torch.asin for torch.complex datatypes on CPU (#134838),aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134838,jiayisunx,Skylion007,mingfeima,,
d85ae4be734,skip,not user facing,Update slow tests (#144236),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/144236,pytorchupdatebot,pytorchbot,,,
bba672e1177,export,Untopiced,[docs/export] update dynamic_shapes docs (#142510),torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/142510,pianpwk,yushangdi,,,
9225f149ebe,skip,not user facing,Enable clang-analyzer checks of Clang-tidy (#144222),.clang-tidy,https://github.com/pytorch/pytorch/pull/144222,cyyever,Skylion007,,,
d4609af1caf,dynamo,not user facing,Propagate callable parameter types using ParamSpec (#142306) (#144047),torch/_dynamo/decorators.py torch/_dynamo/polyfills/itertools.py torch/_ops.py torch/compiler/__init__.py,https://github.com/pytorch/pytorch/pull/144047,yijun-lee,Skylion007,XuehaiPan,,
defbf0d3397,distributed,Untopiced,[DTensor] Add strategy for _scaled_mm (#143760),test/distributed/_tensor/test_matrix_ops.py torch/distributed/tensor/_ops/_matrix_ops.py,https://github.com/pytorch/pytorch/pull/143760,lw,tianyu-l,,,
4c8d661348c,skip,Untopiced,Set `enable_trace_contextlib_contextmanager` flag to True (#140604),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/140604,guilhermeleobas,zou3519,,,
e222dd5d257,skip,not user facing,Rewrite _reparametrize_module to use `contextmanager` (#138203),torch/nn/utils/stateless.py,https://github.com/pytorch/pytorch/pull/138203,guilhermeleobas,zou3519,,,
c8713e659a9,skip,not user facing,"fix memleak, detach instead of clone to not drag around graph (#144154)",test/test_optim.py,https://github.com/pytorch/pytorch/pull/144154,janeyx99,Skylion007,albanD,clee2000,
5c783bf4104,skip,not user facing,[BE][Ez]: Update CUDNN Frontend submodule to 1.9.0 (#144200),third_party/cudnn_frontend,https://github.com/pytorch/pytorch/pull/144200,Skylion007,albanD,cyyever,,
d65a50ef348,skip,not user facing,Fix subclass unwrapping bug (#143945),test/export/test_export.py test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/subclass_parametrization.py,https://github.com/pytorch/pytorch/pull/143945,tugsbayasgalan,IvanKobzarev,,,
11a0663eebd,skip,not user facing,[BE] Parametrize `test_min_max` (#144249),test/test_mps.py,https://github.com/pytorch/pytorch/pull/144249,malfet,Skylion007,,,
ebeb433e734,skip,not user facing,[BE] Fix + parametrize `test_min_max_nan_propagation` (#144250),test/test_mps.py,https://github.com/pytorch/pytorch/pull/144250,malfet,Skylion007,,,
aa14fcd96c3,skip,Untopiced,"Revert ""export AOTI_TORCH_EXPORT on Windows. (#140030)""",CMakeLists.txt caffe2/CMakeLists.txt torch/CMakeLists.txt torch/csrc/inductor/aoti_torch/c/shim.h,,,,,,
e56768f030b,mps,bug fixes,[MPS] Fix bitwise shifts for uint8 (#144251),aten/src/ATen/native/mps/operations/BitwiseOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/144251,malfet,Skylion007,,,
e3aac7f8a07,fx,not user facing,detect fake mode in proxy_tensor creation in make_fx (#144168),test/test_proxy_tensor.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/144168,yushangdi,BoyuanFeng,tugsbayasgalan,,
c62873a09a2,distributed,not user facing,Fix incorrect python expression (#143675),test/distributed/checkpoint/test_fsspec.py torch/distributed/checkpoint/_fsspec_filesystem.py,https://github.com/pytorch/pytorch/pull/143675,mhorowitz,huydhn,mayankgarg1990,,
6d445bef0c1,cuda,Untopiced,[ROCm][NFC] Fix condition for small tensor tuning (#144087),aten/src/ATen/native/cuda/Reduce.cuh,https://github.com/pytorch/pytorch/pull/144087,doru1004,jeffdaily,,,
1b8a9430114,quantization,not user facing,remove allow-untyped-defs from ao/nn/sparse/quantized/utils.py (#144232),torch/ao/nn/sparse/quantized/utils.py,https://github.com/pytorch/pytorch/pull/144232,bobrenjc93,Skylion007,,,
b5cf8e24604,distributed,not user facing,[BE]: Remove redundant copy in torch chunk shard (#144269),torch/distributed/_shard/sharding_spec/chunk_sharding_spec.py,https://github.com/pytorch/pytorch/pull/144269,Skylion007,awgu,,,
729b7c0a84f,fx,Untopiced,[TGIF][Easy] Slightly improve the logging for tgif split pass (#143771),torch/fx/passes/split_utils.py,https://github.com/pytorch/pytorch/pull/143771,faran928,jingsh,,,
5d88002af60,inductor,not user facing,[inductor] Avoid specializing over symbolic value during constant folding (#144176),test/inductor/test_torchinductor.py torch/_inductor/fx_passes/joint_graph.py,https://github.com/pytorch/pytorch/pull/144176,StrongerXi,eellison,jansel,,
7d5249dbc2e,inductor,not user facing,[EZ][BE] Fix E226 flake8 violation (#144282),torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144282,malfet,Skylion007,,,
16c1b1048bc,inductor,not user facing,[MPSInductor] Add `nan` constant generation (#144281),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144281,malfet,atalman,dcci,,
00c18c88820,distributed,Untopiced,Make all-reduce input contiguous in `distributed.nn.all_reduce` (#144267),test/distributed/test_c10d_spawn_nccl.py torch/distributed/nn/functional.py,https://github.com/pytorch/pytorch/pull/144267,awgu,Skylion007,fduwjj,wz337,
d75ffccd0a4,export,not user facing,Migrate from Tuple -> tuple in torch/_export (#144262),torch/_export/__init__.py torch/_export/converter.py torch/_export/db/case.py torch/_export/non_strict_utils.py torch/_export/pass_base.py torch/_export/passes/functionalize_side_effectful_ops_pass.py torch/_export/passes/replace_autocast_with_hop_pass.py torch/_export/passes/replace_quantized_ops_with_standard_ops_pass.py torch/_export/passes/replace_set_grad_with_hop_pass.py torch/_export/passes/replace_with_hop_pass_util.py torch/_export/serde/dynamic_shapes.py torch/_export/serde/schema_check.py torch/_export/serde/serialize.py torch/_export/tools.py torch/_export/utils.py torch/_export/verifier.py,https://github.com/pytorch/pytorch/pull/144262,bobrenjc93,avikchaudhuri,,,
edbda2fad8e,export,not user facing,remove allow-untyped-defs from torch/export/_remove_auto_functionalized_pass.py (#144230),torch/export/_remove_auto_functionalized_pass.py,https://github.com/pytorch/pytorch/pull/144230,bobrenjc93,Skylion007,,,
301b9c8a900,dynamo,Untopiced,Fix PythonMod printing (#144078),test/inductor/test_indexing.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/144078,isuruf,anijain2305,,,
f879a6982da,skip,not user facing,Enhance provenance tracing unit test to cover `torch.compile()` (#143684),test/inductor/test_provenance_tracing.py,https://github.com/pytorch/pytorch/pull/143684,YUNQIUGUO,yushangdi,,,
5ccbfffd118,skip,not user facing,update expected results (#144274),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv,https://github.com/pytorch/pytorch/pull/144274,laithsakka,anijain2305,oulgen,,
66059f80d29,skip,not user facing,Migrate from Tuple -> tuple in torch/profiler (#144257),torch/profiler/_memory_profiler.py torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/144257,bobrenjc93,sraikund16,,,
c68c38c673b,export,not user facing,Support getattr for tensor subclasses in pre-dispatch export via patching tensor.getattr (#143946),test/export/test_export.py torch/_export/verifier.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/subclass_utils.py torch/export/_trace.py torch/testing/_internal/custom_tensor.py torch/testing/_internal/two_tensor.py,https://github.com/pytorch/pytorch/pull/143946,tugsbayasgalan,bdhirsh,,,
f013cfee38c,skip,not user facing,[TreeSpec] Support enum in defaultdict (#144235),test/test_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/144235,henryhu6,henrylhtsang,houseroad,,
0742b2366e7,skip,not user facing,Update torch.masked.mean to upcast dtype for bool tensors (#139999),torch/masked/_ops.py torch/testing/_internal/opinfo/definitions/_masked.py,https://github.com/pytorch/pytorch/pull/139999,GeorgeWigley,cpuhrsch,,,
48153c72b27,releng,not user facing,[Intel XPU] enable kineto for XPU Windows. (#144034),.ci/pytorch/windows/internal/xpu_install.bat cmake/Dependencies.cmake cmake/public/xpu.cmake,https://github.com/pytorch/pytorch/pull/144034,xuhancn,EikanWang,chuanqi129,sraikund16,
61c0a3d1cba,skip,not user facing,Fix lint in `test_provenance_tracing.py` (#144296),test/inductor/test_provenance_tracing.py,https://github.com/pytorch/pytorch/pull/144296,malfet,huydhn,xw285cornell,,
2f6f13562f6,autograd_frontend,Untopiced,[BE] Actually suppress vmap warning from gradcheck (#144287),torch/autograd/gradcheck.py,https://github.com/pytorch/pytorch/pull/144287,janeyx99,cyyever,soulitzer,,
73a6a403461,inductor,not user facing,[Inductor][CPP] Fix outer loop fusion buffer removed (#144243),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py,https://github.com/pytorch/pytorch/pull/144243,leslie-fang-intel,jgong5,,,
24ac87392bc,distributed,not user facing,[AsyncMM] re-enable and prepare for cutlass 3.5.1 update (#144011),torch/csrc/distributed/c10d/cuda/AsyncMM.cu torch/csrc/distributed/c10d/cuda/cutlass/gemm/kernel/persistent_async_input_scheduler.cuh,https://github.com/pytorch/pytorch/pull/144011,yifuwang,Skylion007,drisspg,,
168c2cb3f32,skip,not user facing,remove allow-untyped-defs from torch/nn/utils/_deprecation_utils.py (#144231),torch/nn/utils/_deprecation_utils.py,https://github.com/pytorch/pytorch/pull/144231,bobrenjc93,albanD,,,
f4e9aebbccd,skip,Untopiced,"Revert ""Update torch.masked.mean to upcast dtype for bool tensors (#139999)""",torch/masked/_ops.py torch/testing/_internal/opinfo/definitions/_masked.py,,,,,,
778d953951f,skip,Untopiced,"Revert ""[AsyncMM] re-enable and prepare for cutlass 3.5.1 update (#144011)""",torch/csrc/distributed/c10d/cuda/AsyncMM.cu torch/csrc/distributed/c10d/cuda/cutlass/gemm/kernel/persistent_async_input_scheduler.cuh,,,,,,
a3ab27b8e02,inductor,not user facing,Migrate from Tuple -> tuple in torch/_inductor (#144264),torch/_inductor/__init__.py torch/_inductor/aoti_eager.py torch/_inductor/autoheuristic/artifacts/_MMRankingA100.py torch/_inductor/autoheuristic/artifacts/_MMRankingH100.py torch/_inductor/autoheuristic/artifacts/_MixedMMA100.py torch/_inductor/autoheuristic/artifacts/_MixedMMH100.py torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autoheuristic/learnedheuristic_interface.py torch/_inductor/codecache.py torch/_inductor/codegen/block_analysis.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/rocm/ck_conv_template.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/wrapper.py torch/_inductor/comm_lowering.py torch/_inductor/compile_fx.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/compiler_bisector.py torch/_inductor/cpp_builder.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/custom_graph_pass.py torch/_inductor/decomposition.py torch/_inductor/dependencies.py torch/_inductor/dtype_propagation.py torch/_inductor/freezing.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/fx_passes/ddp_fusion.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/misc_patterns.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/fx_passes/reinplace.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/fx_utils.py torch/_inductor/graph.py torch/_inductor/index_propagation.py torch/_inductor/ir.py torch/_inductor/jagged_lowerings.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/loop_body.py torch/_inductor/lowering.py torch/_inductor/memory.py torch/_inductor/metrics.py torch/_inductor/ops_handler.py torch/_inductor/output_code.py torch/_inductor/pattern_matcher.py torch/_inductor/runtime/autotune_cache.py torch/_inductor/runtime/benchmarking.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py torch/_inductor/subgraph_lowering.py torch/_inductor/triton_bundler.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/144264,bobrenjc93,eellison,,,
551f1041531,inductor,not user facing,[mps/inductor] Add support for sign(). (#144298),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144298,dcci,malfet,,,
2e42be05954,skip,Untopiced,Use random64 in Fischer-Yates algorithm for large N (#143682),aten/src/ATen/native/TensorFactories.cpp test/test_sparse_csr.py test/test_tensor_creation_ops.py test/torch_np/test_random.py,https://github.com/pytorch/pytorch/pull/143682,ngimel,albanD,eqy,malfet,
fcf9dc3b118,skip,not user facing,Migrate from Tuple -> tuple in benchmarks (#144259),benchmarks/dynamo/common.py benchmarks/dynamo/microbenchmarks/operator_inp_utils.py benchmarks/fastrnns/cells.py benchmarks/fastrnns/custom_lstms.py benchmarks/fastrnns/factory.py benchmarks/functional_autograd_benchmark/torchaudio_models.py benchmarks/functional_autograd_benchmark/utils.py benchmarks/gpt_fast/generate.py benchmarks/instruction_counts/execution/runner.py benchmarks/instruction_counts/worker/main.py benchmarks/transformer/score_mod.py benchmarks/transformer/sdpa.py,https://github.com/pytorch/pytorch/pull/144259,bobrenjc93,yanboliang,,,
d0f5df83a50,skip,not user facing,[ca] add test_dtensor_compile.py to compiled autograd tests (#144107),test/distributed/_tensor/test_dtensor_compile.py test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/144107,xmfan,bdhirsh,jansel,zou3519,
2e1ea8598f4,skip,Untopiced,[dtensor] move all tests to distribute/tensor folder (#144166),.ci/pytorch/multigpu-test.sh .ci/pytorch/test.sh .github/labeler.yml test/distributed/_tensor/README.md test/distributed/_tensor/__init__.py test/distributed/_tensor/debug/test_comm_mode.py test/distributed/_tensor/debug/test_comm_mode_features.py test/distributed/_tensor/debug/test_op_coverage.py test/distributed/_tensor/experimental/test_local_map.py test/distributed/_tensor/experimental/test_register_sharding.py test/distributed/_tensor/experimental/test_tp_transform.py test/distributed/_tensor/test_api.py test/distributed/_tensor/test_attention.py test/distributed/_tensor/test_common_rules.py test/distributed/_tensor/test_convolution_ops.py test/distributed/_tensor/test_dtensor.py test/distributed/_tensor/test_dtensor_compile.py test/distributed/_tensor/test_dtensor_ops.py test/distributed/_tensor/test_embedding_ops.py test/distributed/_tensor/test_experimental_ops.py test/distributed/_tensor/test_init.py test/distributed/_tensor/test_math_ops.py test/distributed/_tensor/test_matrix_ops.py test/distributed/_tensor/test_op_strategy.py test/distributed/_tensor/test_optimizers.py test/distributed/_tensor/test_pointwise_ops.py test/distributed/_tensor/test_random_ops.py test/distributed/_tensor/test_redistribute.py test/distributed/_tensor/test_tensor_ops.py test/distributed/_tensor/test_utils.py test/distributed/_tensor/test_view_ops.py test/distributed/_tensor/test_xla_integration.py test/distributed/tensor/README.md test/distributed/tensor/__init__.py test/distributed/tensor/debug/test_comm_mode.py test/distributed/tensor/debug/test_comm_mode_features.py test/distributed/tensor/debug/test_op_coverage.py test/distributed/tensor/experimental/test_local_map.py test/distributed/tensor/experimental/test_register_sharding.py test/distributed/tensor/experimental/test_tp_transform.py test/distributed/tensor/test_api.py test/distributed/tensor/test_attention.py test/distributed/tensor/test_common_rules.py test/distributed/tensor/test_convolution_ops.py test/distributed/tensor/test_dtensor.py test/distributed/tensor/test_dtensor_compile.py test/distributed/tensor/test_dtensor_ops.py test/distributed/tensor/test_embedding_ops.py test/distributed/tensor/test_experimental_ops.py test/distributed/tensor/test_init.py test/distributed/tensor/test_math_ops.py test/distributed/tensor/test_matrix_ops.py test/distributed/tensor/test_op_strategy.py test/distributed/tensor/test_optimizers.py test/distributed/tensor/test_pointwise_ops.py test/distributed/tensor/test_random_ops.py test/distributed/tensor/test_redistribute.py test/distributed/tensor/test_tensor_ops.py test/distributed/tensor/test_utils.py test/distributed/tensor/test_view_ops.py test/distributed/tensor/test_xla_integration.py test/run_test.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/144166,wanchaol,Skylion007,,,
bf7747e9353,skip,Untopiced,Tests Generelization for multiple accelerator devices (#139184),test/distributed/fsdp/test_checkpoint_wrapper.py test/distributed/fsdp/test_distributed_checkpoint.py test/distributed/fsdp/test_fsdp_apply.py test/distributed/fsdp/test_fsdp_backward_prefetch.py test/distributed/fsdp/test_fsdp_checkpoint.py test/distributed/fsdp/test_fsdp_clip_grad_norm.py test/distributed/fsdp/test_fsdp_comm.py test/distributed/fsdp/test_fsdp_core.py test/distributed/fsdp/test_fsdp_dtensor_state_dict.py test/distributed/fsdp/test_fsdp_exec_order.py test/distributed/fsdp/test_fsdp_fine_tune.py test/distributed/fsdp/test_fsdp_fx.py test/distributed/fsdp/test_fsdp_input.py test/distributed/fsdp/test_fsdp_memory.py test/distributed/fsdp/test_fsdp_multiple_forward.py test/distributed/fsdp/test_fsdp_multiple_wrapping.py test/distributed/fsdp/test_fsdp_overlap.py test/distributed/fsdp/test_fsdp_pure_fp16.py test/distributed/fsdp/test_fsdp_traversal.py test/distributed/fsdp/test_fsdp_uneven.py test/distributed/fsdp/test_fsdp_unshard_params.py test/distributed/fsdp/test_hsdp_dtensor_state_dict.py test/distributed/fsdp/test_utils.py torch/testing/_internal/common_fsdp.py,https://github.com/pytorch/pytorch/pull/139184,rahulsingh-intel,kwen2501,,,
c3b28491c80,caffe2,not user facing,[caffe2] Add AVX512 support for box_cox operator (#143627),caffe2/perfkernels/batch_box_cox_avx512.cc caffe2/perfkernels/batch_box_cox_vec.h,https://github.com/pytorch/pytorch/pull/143627,efiks,hl475,,,
12fdb93ebd2,export,Untopiced,fix non-strict placeholder naming with kwargs (#144278),test/export/test_export.py torch/_export/utils.py,https://github.com/pytorch/pytorch/pull/144278,avikchaudhuri,pianpwk,yushangdi,,
8d35333498e,releng,binaries,[CD] Aarch64 builds should not override `OVERRIDE_PACKAGE_VERSION` envvar (#144285),.ci/aarch64_linux/aarch64_ci_build.sh,https://github.com/pytorch/pytorch/pull/144285,atalman,malfet,tinglvv,,
e4a05dec0f7,composability,not user facing,[BE][Ez]: Fix docs recommending inefficient tensor op order (#144270),torch/_refs/__init__.py torch/_tensor_docs.py torch/_torch_docs.py torch/csrc/utils/tensor_new.cpp torch/masked/maskedtensor/core.py torch/testing/_internal/common_optimizers.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/144270,Skylion007,XuehaiPan,awgu,,
6c54963f75e,skip,Untopiced,"Revert ""[dtensor] move all tests to distribute/tensor folder (#144166)""",.ci/pytorch/multigpu-test.sh .ci/pytorch/test.sh .github/labeler.yml test/distributed/_tensor/README.md test/distributed/_tensor/__init__.py test/distributed/_tensor/debug/test_comm_mode.py test/distributed/_tensor/debug/test_comm_mode_features.py test/distributed/_tensor/debug/test_op_coverage.py test/distributed/_tensor/experimental/test_local_map.py test/distributed/_tensor/experimental/test_register_sharding.py test/distributed/_tensor/experimental/test_tp_transform.py test/distributed/_tensor/test_api.py test/distributed/_tensor/test_attention.py test/distributed/_tensor/test_common_rules.py test/distributed/_tensor/test_convolution_ops.py test/distributed/_tensor/test_dtensor.py test/distributed/_tensor/test_dtensor_compile.py test/distributed/_tensor/test_dtensor_ops.py test/distributed/_tensor/test_embedding_ops.py test/distributed/_tensor/test_experimental_ops.py test/distributed/_tensor/test_init.py test/distributed/_tensor/test_math_ops.py test/distributed/_tensor/test_matrix_ops.py test/distributed/_tensor/test_op_strategy.py test/distributed/_tensor/test_optimizers.py test/distributed/_tensor/test_pointwise_ops.py test/distributed/_tensor/test_random_ops.py test/distributed/_tensor/test_redistribute.py test/distributed/_tensor/test_tensor_ops.py test/distributed/_tensor/test_utils.py test/distributed/_tensor/test_view_ops.py test/distributed/_tensor/test_xla_integration.py test/distributed/tensor/README.md test/distributed/tensor/__init__.py test/distributed/tensor/debug/test_comm_mode.py test/distributed/tensor/debug/test_comm_mode_features.py test/distributed/tensor/debug/test_op_coverage.py test/distributed/tensor/experimental/test_local_map.py test/distributed/tensor/experimental/test_register_sharding.py test/distributed/tensor/experimental/test_tp_transform.py test/distributed/tensor/test_api.py test/distributed/tensor/test_attention.py test/distributed/tensor/test_common_rules.py test/distributed/tensor/test_convolution_ops.py test/distributed/tensor/test_dtensor.py test/distributed/tensor/test_dtensor_compile.py test/distributed/tensor/test_dtensor_ops.py test/distributed/tensor/test_embedding_ops.py test/distributed/tensor/test_experimental_ops.py test/distributed/tensor/test_init.py test/distributed/tensor/test_math_ops.py test/distributed/tensor/test_matrix_ops.py test/distributed/tensor/test_op_strategy.py test/distributed/tensor/test_optimizers.py test/distributed/tensor/test_pointwise_ops.py test/distributed/tensor/test_random_ops.py test/distributed/tensor/test_redistribute.py test/distributed/tensor/test_tensor_ops.py test/distributed/tensor/test_utils.py test/distributed/tensor/test_view_ops.py test/distributed/tensor/test_xla_integration.py test/run_test.py torch/fx/experimental/symbolic_shapes.py,,,,,,
aa69d73e6bf,nn_frontend,Untopiced,[ROCm] fix torch.layer_norm invalid configuration problem when input is large tensor (#144007),aten/src/ATen/native/cuda/layer_norm_kernel.cu test/test_nn.py,https://github.com/pytorch/pytorch/pull/144007,hongxiayang,eqy,,,
355b0bc7e34,python_frontend,Untopiced,[typing] Add type hints to `@property` and `@lazy_property` in `torch.distributions`. (#144110),test/typing/pass/distributions.py torch/distributions/bernoulli.py torch/distributions/beta.py torch/distributions/binomial.py torch/distributions/categorical.py torch/distributions/cauchy.py torch/distributions/chi2.py torch/distributions/constraints.py torch/distributions/continuous_bernoulli.py torch/distributions/dirichlet.py torch/distributions/distribution.py torch/distributions/exp_family.py torch/distributions/exponential.py torch/distributions/fishersnedecor.py torch/distributions/gamma.py torch/distributions/geometric.py torch/distributions/gumbel.py torch/distributions/half_cauchy.py torch/distributions/half_normal.py torch/distributions/independent.py torch/distributions/inverse_gamma.py torch/distributions/kl.py torch/distributions/kumaraswamy.py torch/distributions/laplace.py torch/distributions/log_normal.py torch/distributions/logistic_normal.py torch/distributions/lowrank_multivariate_normal.py torch/distributions/mixture_same_family.py torch/distributions/multinomial.py torch/distributions/multivariate_normal.py torch/distributions/negative_binomial.py torch/distributions/normal.py torch/distributions/one_hot_categorical.py torch/distributions/pareto.py torch/distributions/poisson.py torch/distributions/relaxed_bernoulli.py torch/distributions/relaxed_categorical.py torch/distributions/studentT.py torch/distributions/transformed_distribution.py torch/distributions/transforms.py torch/distributions/uniform.py torch/distributions/utils.py torch/distributions/von_mises.py torch/distributions/weibull.py torch/distributions/wishart.py,https://github.com/pytorch/pytorch/pull/144110,randolf-scholz,Skylion007,,,
0aa74d0ab90,inductor,Untopiced,Skip L1 cache for single-use buffers (#143115),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/codegen/simd_kernel_features.py torch/_inductor/codegen/triton.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/143115,AmdSampsa,jansel,,,
096cb874d3d,composability,not user facing,remove allow-untyped-defs from torch/_prims/executor.py (#144233),torch/_prims/executor.py,https://github.com/pytorch/pytorch/pull/144233,bobrenjc93,Skylion007,,,
72e8f34715d,dynamo,not user facing,[AoTI Minifier] UX Improvement (#143330),test/dynamo/test_debug_utils.py test/test_utils_config_module.py torch/_dynamo/config.py torch/_dynamo/debug_utils.py torch/_dynamo/repro/after_aot.py torch/_dynamo/repro/after_dynamo.py torch/_dynamo/repro/aoti.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/143330,yushangdi,eellison,jansel,,
d38af6e8bc8,dynamo,not user facing,[ca] dedup node names when AOT bwd graph is reused multiple times (#144202),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py,https://github.com/pytorch/pytorch/pull/144202,xmfan,bdhirsh,jansel,,
861b65fe74f,linalg_frontend,Untopiced,[Easy] Fix linalg.norm hint message typo (#144323),aten/src/ATen/native/LinearAlgebra.cpp,https://github.com/pytorch/pytorch/pull/144323,zeshengzong,Skylion007,soulitzer,,
f4969c82351,distributed,Untopiced,fix torch.compile + ddp + non-reentrant AC pack hook firing count (#144271),test/dynamo/test_repros.py torch/_dynamo/backends/distributed.py,https://github.com/pytorch/pytorch/pull/144271,xmfan,bdhirsh,soulitzer,,
3beb7006dd5,cpp_frontend,improvements,c10::optional -> std::optional in a few places (#144340),torch/csrc/api/include/torch/nn/parallel/data_parallel.h,https://github.com/pytorch/pytorch/pull/144340,r-barnes,malfet,,,
4417be65e5b,dynamo,not user facing,[Dynamo] Inline functions in torch._functorch.autograd_function (#144306),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/144306,yanboliang,williamwen42,,,
242a4a3f83c,dynamo,not user facing,[Dynamo] Inline functions in torch._functorch.pyfunctorch (#144307),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/144307,yanboliang,williamwen42,,,
d146763f6fb,dynamo,not user facing,[Dynamo] Inline functions in torch._ops (#144308),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/144308,yanboliang,williamwen42,,,
430d54ee200,dynamo,not user facing,[Dynamo] Add functorch C++ bindings as in graph functions (#144309),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/144309,yanboliang,williamwen42,,,
c2c50d5f004,skip,not user facing,Fixed doc where more than one device specified since only one device is used (#17553) (#144043),torch/_tensor_docs.py,https://github.com/pytorch/pytorch/pull/144043,Stacie-Herda,soulitzer,,,
9ee242213b7,dynamo,Untopiced,"[RFC] Introduce cache hot loading APIs (a.k.a. ""Mega-cache"") (#143341)",test/inductor/test_codecache.py torch/_dynamo/pgo.py torch/_dynamo/testing.py torch/_inductor/codecache.py torch/_inductor/runtime/autotune_cache.py torch/compiler/__init__.py torch/compiler/_cache.py,https://github.com/pytorch/pytorch/pull/143341,oulgen,jansel,,,
7c9cf287c23,onnx,bug fixes,[ONNX] Handle list values as 0d inputs (#144343),test/onnx/exporter/test_building.py torch/onnx/_internal/exporter/_building.py,https://github.com/pytorch/pytorch/pull/144343,justinchuby,gramalingam,titaiwangms,,
96f4abba172,skip,Untopiced,[dtensor] move all tests to distribute/tensor folder (#144166),.ci/pytorch/multigpu-test.sh .ci/pytorch/test.sh .github/labeler.yml test/distributed/_tensor/README.md test/distributed/_tensor/__init__.py test/distributed/_tensor/debug/test_comm_mode.py test/distributed/_tensor/debug/test_comm_mode_features.py test/distributed/_tensor/debug/test_op_coverage.py test/distributed/_tensor/experimental/test_local_map.py test/distributed/_tensor/experimental/test_register_sharding.py test/distributed/_tensor/experimental/test_tp_transform.py test/distributed/_tensor/test_api.py test/distributed/_tensor/test_attention.py test/distributed/_tensor/test_common_rules.py test/distributed/_tensor/test_convolution_ops.py test/distributed/_tensor/test_dtensor.py test/distributed/_tensor/test_dtensor_compile.py test/distributed/_tensor/test_dtensor_ops.py test/distributed/_tensor/test_embedding_ops.py test/distributed/_tensor/test_experimental_ops.py test/distributed/_tensor/test_init.py test/distributed/_tensor/test_math_ops.py test/distributed/_tensor/test_matrix_ops.py test/distributed/_tensor/test_op_strategy.py test/distributed/_tensor/test_optimizers.py test/distributed/_tensor/test_pointwise_ops.py test/distributed/_tensor/test_random_ops.py test/distributed/_tensor/test_redistribute.py test/distributed/_tensor/test_tensor_ops.py test/distributed/_tensor/test_utils.py test/distributed/_tensor/test_view_ops.py test/distributed/_tensor/test_xla_integration.py test/distributed/tensor/README.md test/distributed/tensor/__init__.py test/distributed/tensor/debug/test_comm_mode.py test/distributed/tensor/debug/test_comm_mode_features.py test/distributed/tensor/debug/test_op_coverage.py test/distributed/tensor/experimental/test_local_map.py test/distributed/tensor/experimental/test_register_sharding.py test/distributed/tensor/experimental/test_tp_transform.py test/distributed/tensor/test_api.py test/distributed/tensor/test_attention.py test/distributed/tensor/test_common_rules.py test/distributed/tensor/test_convolution_ops.py test/distributed/tensor/test_dtensor.py test/distributed/tensor/test_dtensor_compile.py test/distributed/tensor/test_dtensor_ops.py test/distributed/tensor/test_embedding_ops.py test/distributed/tensor/test_experimental_ops.py test/distributed/tensor/test_init.py test/distributed/tensor/test_math_ops.py test/distributed/tensor/test_matrix_ops.py test/distributed/tensor/test_op_strategy.py test/distributed/tensor/test_optimizers.py test/distributed/tensor/test_pointwise_ops.py test/distributed/tensor/test_random_ops.py test/distributed/tensor/test_redistribute.py test/distributed/tensor/test_tensor_ops.py test/distributed/tensor/test_utils.py test/distributed/tensor/test_view_ops.py test/distributed/tensor/test_xla_integration.py test/inductor/test_compiled_autograd.py test/run_test.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/144166,wanchaol,Skylion007,,,
06ea81336f5,skip,not user facing,[Inductor UT] Remove excepted failure for aoti test_fft_c2c  (#144238),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/144238,etaf,jansel,,,
f002825e1ef,distributed,Untopiced,added `__add__` and `__mul__` hints to torch.Size (#144322),test/typing/fail/torch_size.py test/typing/pass/torch_size.py torch/_C/__init__.pyi.in torch/distributed/fsdp/_fully_shard/_fsdp_common.py,https://github.com/pytorch/pytorch/pull/144322,randolf-scholz,Skylion007,,,
013c796b1ee,skip,not user facing,Eliminate c10::optional usage in PyTorch (#144346),aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_fwd_ck.hip test/benchmark_utils/callgrind_artifacts.json,https://github.com/pytorch/pytorch/pull/144346,houseroad,hl475,,,
9e6a6389cee,functorch,not user facing,[functorch] clean up asserts in `test_dims.py` (#144276),test/functorch/test_dims.py,https://github.com/pytorch/pytorch/pull/144276,eqy,Skylion007,,,
094ca3154dc,composability,not user facing,Fix torch._refs.tensor error with empty list (#143461),test/test_tensor_creation_ops.py torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/143461,zeshengzong,ezyang,soulitzer,,
f8fcb9e7d38,inductor,not user facing,[Quant][Inductor][X86] Separate unary post op fusion and lowering for qlinear (#143903),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/freezing_patterns.py torch/_inductor/fx_passes/quantization.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/143903,Xia-Weiwen,jerryzh168,jgong5,leslie-fang-intel,
28b4992e7a6,skip,not user facing,Set prop_kind to forward_inference when grad is not needed for mkldnn_convolution_pointwise (#142855),aten/src/ATen/native/mkldnn/Conv.cpp,https://github.com/pytorch/pytorch/pull/142855,CaoE,jgong5,,,
e05d67790ee,skip,not user facing,Unskipped multiple inductor tests for ROCm (#143581),test/inductor/test_flex_decoding.py test/inductor/test_inductor_freezing.py test/inductor/test_max_autotune.py test/inductor/test_memory_planning.py test/inductor/test_pattern_matcher.py test/inductor/test_select_algorithm.py test/inductor/test_triton_kernels.py,https://github.com/pytorch/pytorch/pull/143581,iupaikov-amd,jeffdaily,,,
2ac41404a87,skip,not user facing,[dynamo][dicts] Guarding lazily on dict keys (#143997),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/dynamo/test_dicts.py test/dynamo/test_misc.py torch/_dynamo/guards.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/143997,anijain2305,jansel,,,
8ccf3f6f3f9,dynamo,not user facing,[dynamo][easy] Move dict tests to test_dicts.py (#144165),test/dynamo/test_dicts.py test/dynamo/test_misc.py test/dynamo/test_repros.py,https://github.com/pytorch/pytorch/pull/144165,anijain2305,jansel,,,
90e81a157ae,dataloader_frontend,not user facing,Migrate from Tuple -> tuple in torch/utils/data (#144255),torch/utils/data/_utils/collate.py torch/utils/data/datapipes/gen_pyi.py torch/utils/data/datapipes/iter/combining.py torch/utils/data/datapipes/iter/fileopener.py torch/utils/data/datapipes/iter/routeddecoder.py torch/utils/data/datapipes/iter/selecting.py torch/utils/data/datapipes/iter/sharding.py torch/utils/data/datapipes/iter/streamreader.py torch/utils/data/datapipes/map/combining.py torch/utils/data/datapipes/utils/common.py torch/utils/data/dataset.py torch/utils/data/graph.py,https://github.com/pytorch/pytorch/pull/144255,bobrenjc93,andrewkho,,,
78eded8e004,onnx,Untopiced,[ONNX] Use torch.export.Dim.AUTO in dynamo_export (#144356),torch/onnx/__init__.py,https://github.com/pytorch/pytorch/pull/144356,titaiwangms,justinchuby,,,
aaf56152ea1,linalg_frontend,Untopiced,[cpu/sorting] Throw an error when trying to sort complex numbers. (#144113),aten/src/ATen/native/Sorting.cpp test/test_sort_and_select.py,https://github.com/pytorch/pytorch/pull/144113,dcci,malfet,,,
7d9f26de051,skip,Untopiced,"Revert ""Unskipped multiple inductor tests for ROCm (#143581)""",test/inductor/test_flex_decoding.py test/inductor/test_inductor_freezing.py test/inductor/test_max_autotune.py test/inductor/test_memory_planning.py test/inductor/test_pattern_matcher.py test/inductor/test_select_algorithm.py test/inductor/test_triton_kernels.py,,,,,,
60a505022f3,skip,not user facing,[AMD] SDPA internal changes (#144320),aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/flash_api.h aten/src/ATen/native/transformers/hip/flash_attn/flash_common_hip.hpp,https://github.com/pytorch/pytorch/pull/144320,xw285cornell,eqy,jianyuh,leitian,
a5051a95217,skip,not user facing,Update torch.masked.mean to upcast dtype for bool tensors (#139999),test/inductor/test_cpu_repro.py torch/masked/_ops.py torch/testing/_internal/opinfo/definitions/_masked.py,https://github.com/pytorch/pytorch/pull/139999,GeorgeWigley,cpuhrsch,,,
f7000350905,dynamo,not user facing,[3.13t] use sysconfig to check for Python nogil builds (#144361),torch/__init__.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/144361,williamwen42,atalman,,,
8fc0ffe54b3,inductor,not user facing,[mps/inductor] Add support for rsqrt(). (#144374),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144374,dcci,malfet,,,
708ce3c0082,dynamo,not user facing,Add `is_dtype_supported` predicate to DeviceInterface (#144355),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py torch/_dynamo/device_interface.py,https://github.com/pytorch/pytorch/pull/144355,malfet,dcci,jansel,,
3068ce0337d,skip,not user facing,ROCm SDPA: Ensure attn_mask has the same dtype with q (#143242),aten/src/ATen/native/transformers/cuda/sdp_utils.cpp,https://github.com/pytorch/pytorch/pull/143242,xinyazhang,jeffdaily,,,
e14c36d3f49,skip,not user facing,Set maximum supported version of Python as 3.13 (#144396),setup.py,https://github.com/pytorch/pytorch/pull/144396,atalman,Skylion007,albanD,malfet,
373541fbf41,dynamo,not user facing,[BE]: Remove unnecessary copy of gradients in util (#144329),torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/144329,Skylion007,awgu,cyyever,,
d0070ca07e7,cpp_frontend,Untopiced,[18/N] Fix extra warnings brought by clang-tidy-17 (#144014),aten/src/ATen/CachedTensorUtils.cpp aten/src/ATen/native/Activation.cpp aten/src/ATen/native/mkldnn/xpu/Blas.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.cpp c10/core/RefcountedDeleter.cpp c10/cuda/CUDAFunctions.cpp c10/util/Flags.h torch/csrc/DynamicTypes.cpp torch/csrc/Generator.cpp torch/csrc/MemoryFormat.cpp torch/csrc/QScheme.cpp torch/csrc/api/include/torch/nn/modules/container/sequential.h torch/csrc/autograd/function.h torch/csrc/lazy/core/hash.h torch/csrc/mtia/Module.cpp torch/csrc/python_dimname.cpp torch/csrc/serialization.cpp torch/csrc/utils.cpp torch/csrc/utils/python_arg_parser.h,https://github.com/pytorch/pytorch/pull/144014,cyyever,Skylion007,albanD,,
768d73f6929,skip,not user facing,use `torch.special.xlogy` to implement `x_log_x` (#144220),test/distributions/test_distributions.py torch/distributions/kl.py,https://github.com/pytorch/pytorch/pull/144220,randolf-scholz,Skylion007,,,
0e1675a89bc,releng,not user facing,Relax aten.to restriction (#142420),.ci/docker/ci_commit_pins/executorch.txt test/export/test_export.py torch/_subclasses/functional_tensor.py,https://github.com/pytorch/pytorch/pull/142420,yushangdi,bdhirsh,,,
ab1f627aa4e,skip,Untopiced,fix randint distribution for large max (#143787),aten/src/ATen/core/DistributionsHelper.h aten/src/ATen/native/cuda/DistributionTemplates.h aten/src/ATen/test/rng_test.h test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py test/test_tensor_creation_ops.py,https://github.com/pytorch/pytorch/pull/143787,ngimel,eqy,,,
628acc4aced,skip,not user facing,`Dirichlet.mode`: use `dim=` instead of `axis=` (#144402),torch/distributions/dirichlet.py,https://github.com/pytorch/pytorch/pull/144402,randolf-scholz,Skylion007,,,
c194e5c9860,composability,not user facing,Remove extra copy torch/_prims (#144407),torch/_prims/__init__.py,https://github.com/pytorch/pytorch/pull/144407,LlamaFarm,awgu,,,
8c5d9927722,distributed,not user facing,[Pipelining] Refactor pp composability test to use faster MPCT (#144345),test/distributed/_composable/test_composability/test_pp_composability.py test/distributed/test_composability.py,https://github.com/pytorch/pytorch/pull/144345,wconstab,H-Huang,mori360,,
a5164a2b184,export,Untopiced,[BE] Clean up ExecuTorch Export Docstring (#141490),torch/export/__init__.py,https://github.com/pytorch/pytorch/pull/141490,musebc,JacobSzwejbka,,,
a742859fc27,onnx,docs,[ONNX] Update images and APIs to onnx_dynamo.rst (#144358),docs/Makefile docs/source/_static/img/onnx/onnx_dynamo_mlp_model.png docs/source/_static/img/onnx/onnx_dynamo_mlp_model_function_body.png docs/source/_static/img/onnx/onnx_dynamo_mlp_model_function_highlight.png docs/source/onnx_dynamo.rst docs/source/onnx_torchscript.rst docs/source/scripts/onnx/build_onnx_dynamo_diagnostics_rules_md.py,https://github.com/pytorch/pytorch/pull/144358,titaiwangms,justinchuby,malfet,,
dcc3cf7066b,releng,not user facing,[BE] fix ruff rule E226: add missing whitespace around operator in f-strings (#144415),.github/scripts/trymerge.py benchmarks/dynamo/common.py benchmarks/dynamo/microbenchmarks/cache_debug_microbenchmarks.py benchmarks/dynamo/microbenchmarks/fx_microbenchmarks.py benchmarks/dynamo/microbenchmarks/overheads.py benchmarks/dynamo/pr_time_benchmarks/check_results.py benchmarks/dynamo/runner.py benchmarks/dynamo/training_loss.py functorch/examples/compilation/fuse_module.py functorch/examples/maml_omniglot/maml-omniglot-higher.py functorch/examples/maml_omniglot/maml-omniglot-ptonly.py functorch/examples/maml_omniglot/maml-omniglot-transforms.py test/export/random_dag.py test/test_testing.py third_party/generate-xnnpack-wrappers.py torch/_dynamo/backends/debugging.py torch/_dynamo/debug_utils.py torch/_inductor/wrapper_benchmark.py torch/_logging/_internal.py torch/_numpy/testing/utils.py torch/distributed/_tools/ilp_utils.py torch/distributed/tensor/examples/convnext_example.py torch/distributions/utils.py torch/distributions/wishart.py torch/export/unflatten.py torch/profiler/_memory_profiler.py torch/profiler/_pattern_matcher.py torch/profiler/_utils.py torch/testing/_internal/distributed/rpc/examples/parameter_server_test.py,https://github.com/pytorch/pytorch/pull/144415,XuehaiPan,Skylion007,huydhn,,
8ac005ddb8b,distributed,Untopiced,[DTensor] Add `aten.view.dtype` op support (#144404),torch/distributed/tensor/_ops/_tensor_ops.py,https://github.com/pytorch/pytorch/pull/144404,awgu,wanchaol,,,
bc576355a26,releng,Untopiced,"Let aotriton.cmake detect the best binary package to use, and deprecate aotriton_version.txt (#137443)",.ci/docker/aotriton_version.txt .ci/docker/centos-rocm/Dockerfile .ci/docker/common/install_aotriton.sh .ci/docker/libtorch/Dockerfile .ci/docker/manywheel/Dockerfile .ci/docker/ubuntu-rocm/Dockerfile .ci/manywheel/build_rocm.sh cmake/External/aotriton.cmake,https://github.com/pytorch/pytorch/pull/137443,xinyazhang,jeffdaily,jithunnair-amd,,
fabf2ea12e1,skip,not user facing,[Quant][Inductor][X86] Separate binary post op fusion and lowering for qlinear (#144224),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/144224,Xia-Weiwen,jerryzh168,leslie-fang-intel,,
0d08084f1a0,inductor,not user facing,[Inductor] Add convolution output size checking to the meta function (#144225),test/inductor/test_torchinductor.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/144225,DDEle,jansel,leslie-fang-intel,,
d4871750d9e,releng,not user facing,[ROCm] Enable post-merge trunk workflow on MI300 runners; skip and fix MI300 related failed tests (#143673),.ci/docker/build.sh .github/actions/diskspace-cleanup/action.yml .github/actions/setup-rocm/action.yml .github/workflows/rocm.yml .github/workflows/trunk.yml test/distributed/_tools/test_sac_ilp.py test/distributed/tensor/parallel/test_micro_pipeline_tp.py test/distributed/test_c10d_gloo.py test/inductor/test_cpu_repro.py test/inductor/test_fp8.py test/inductor/test_loop_ordering.py test/inductor/test_mkldnn_pattern_matcher.py test/test_cuda.py test/test_matmul_cuda.py test/test_torch.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/143673,dnikolaev-amd,jeffdaily,malfet,pruthvistony,
7f1946aa9ba,fx,Untopiced,[aot] don't dce aten rng nodes (#144319),test/dynamo/test_repros.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/144319,xmfan,jansel,,,
6f28e466f3d,inductor,not user facing,[mps/inductor] Add support for tanh(). (#144443),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144443,dcci,malfet,,,
3797143e065,skip,Untopiced,"Revert ""[Quant][Inductor][X86] Separate binary post op fusion and lowering for qlinear (#144224)""",test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py torch/_meta_registrations.py,,,,,,
9631d1a021c,distributed,Untopiced,[pipelining] throw error with ZB and compile (#143599),test/distributed/pipelining/test_schedule.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/143599,H-Huang,wconstab,,,
1353f3beb47,inductor,not user facing,[mps/inductor] Add support for fmod(). (#144449),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144449,dcci,malfet,,,
b0be30dd791,cpp_frontend,Untopiced,[19/N] Fix extra warnings brought by clang-tidy-17 (#144448),aten/src/ATen/ParallelNative.cpp aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDASparseDescriptors.cpp aten/src/ATen/detail/MTIAHooksInterface.h aten/src/ATen/functorch/BatchRulesModules.cpp aten/src/ATen/native/Activation.cpp aten/src/ATen/native/mkldnn/xpu/Blas.cpp torch/csrc/Generator.cpp torch/csrc/api/include/torch/detail/TensorDataContainer.h torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/cuda/CUDAPluggableAllocator.cpp torch/csrc/cuda/CUDAPluggableAllocator.h torch/csrc/distributed/c10d/DMAConnectivity.cpp torch/csrc/distributed/c10d/FileStore.cpp torch/csrc/distributed/c10d/TCPStoreBackend.cpp torch/csrc/distributed/c10d/control_plane/WorkerServer.cpp torch/csrc/distributed/rpc/message.h torch/csrc/distributed/rpc/types.h torch/csrc/dynamo/python_compiled_autograd.cpp torch/csrc/lazy/core/cache.h torch/csrc/lazy/core/util.h torch/csrc/mtia/Module.cpp torch/csrc/profiler/python/pybind.h torch/csrc/profiler/stubs/base.cpp torch/csrc/profiler/unwind/line_number_program.h torch/csrc/utils/throughput_benchmark.cpp,https://github.com/pytorch/pytorch/pull/144448,cyyever,albanD,,,
1365ae859c7,releng,not user facing,[ROCm][CI] upgrade CI to ROCm 6.3 (#142152),.ci/docker/build.sh .ci/docker/common/install_rocm.sh .github/workflows/inductor-rocm.yml .github/workflows/periodic.yml .github/workflows/pull.yml .github/workflows/rocm.yml .github/workflows/slow.yml .github/workflows/trunk.yml test/cpp/api/rnn.cpp,https://github.com/pytorch/pytorch/pull/142152,jithunnair-amd,jeffdaily,pruthvistony,,
f2c10331781,skip,Untopiced,[Submodule] Upgrade to Cutlass 3.6 (#144180),aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h third_party/cutlass,https://github.com/pytorch/pytorch/pull/144180,drisspg,Skylion007,eqy,,
b8f383107ee,nn_frontend,docs,Link to transformer tutorial in transformer docs (#144425),torch/nn/functional.py torch/nn/modules/activation.py torch/nn/modules/transformer.py,https://github.com/pytorch/pytorch/pull/144425,mikaylagawarecki,albanD,,,
84443bd61ad,inductor,not user facing,feature_use: Remove JK from naming for feature use. (#143529),torch/_inductor/async_compile.py torch/_inductor/compile_fx.py torch/_inductor/triton_bundler.py,https://github.com/pytorch/pytorch/pull/143529,c00w,ezyang,,,
73278e6a5d8,dynamo,not user facing,easy: sort dictionary keys for inductor config when publishing (#143307),test/dynamo/test_utils.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/143307,c00w,xmfan,,,
bbec35f028a,vulkan,Untopiced,[BE]: Replace clone detach with detach clone to be more efficient (#144469),aten/src/ATen/native/vulkan/ops/Random.cpp test/cpp/lazy/test_lazy_ops_util.cpp torch/csrc/utils/tensor_new.cpp,https://github.com/pytorch/pytorch/pull/144469,Skylion007,awgu,,,
307ca094c92,skip,not user facing,[BE]: Remove redundant contiguous copy in flex attention (#144467),torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/144467,Skylion007,awgu,,,
0e02e6f95fe,composability,not user facing,[BE]: Remove redundant contiguous copy in torch/_decomp/decompositions (#144472),torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/144472,Skylion007,awgu,cyyever,malfet,
6bc17b0725f,skip,not user facing,Update #graph breaks for moco benchmark (#144266),benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv,https://github.com/pytorch/pytorch/pull/144266,guilhermeleobas,zou3519,,,
2b241a82068,skip,not user facing,Amazon Linux 2023: Preload cusparseLt.so (#144477),torch/__init__.py,https://github.com/pytorch/pytorch/pull/144477,atalman,Skylion007,nWEIdia,,
40305dd37ec,onnx,improvements,[onnx] Fix bug for exporting torch.cdist into onnx and support 'compute_mode' (#144213),test/onnx/test_pytorch_onnx_onnxruntime.py torch/onnx/symbolic_opset9.py,https://github.com/pytorch/pytorch/pull/144213,ygq65536,justinchuby,,,
127f836881e,releng,not user facing,S390x cancelled jobs cleanup (#144149),.github/scripts/s390x-ci/self-hosted-builder/actions-runner.Dockerfile .github/workflows/_linux-build.yml,https://github.com/pytorch/pytorch/pull/144149,AlekseiNikiforovIBM,seemethere,,,
f71688f30da,skip,Untopiced,"Revert ""[Submodule] Upgrade to Cutlass 3.6 (#144180)""",aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h third_party/cutlass,,,,,,
3e7e435bb1f,quantization,not user facing,[codemod] Remove unused-variable in caffe2/aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp +2 (#144371),aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp torch/csrc/distributed/c10d/intra_node_comm.cpp,https://github.com/pytorch/pytorch/pull/144371,r-barnes,Skylion007,,,
206a932f238,skip,Untopiced,[Submodule] Upgrade to Cutlass 3.6 (#144180),aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h third_party/cutlass,https://github.com/pytorch/pytorch/pull/144180,drisspg,Skylion007,eqy,,
28b1960d496,skip,not user facing,[CUDA] parse arch-conditional compute-capability when building extensions (#144446),torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/144446,eqy,Skylion007,ezyang,,
379b54603a4,inductor,Untopiced,[Inductor] [bc-breaking] Node Level provenance tracking (#144277),test/fx/test_fx_traceback.py test/fx/test_fx_xform_observer.py torch/_inductor/compile_fx.py torch/_inductor/pattern_matcher.py torch/fx/graph_module.py torch/fx/passes/graph_transform_observer.py torch/fx/traceback.py,https://github.com/pytorch/pytorch/pull/144277,yushangdi,desertfire,,,
b1c2c3967ad,distributed,Untopiced,[dtensor] deprecate _shard_tensor to use src_data_rank=None (#144171),torch/distributed/tensor/_api.py,https://github.com/pytorch/pytorch/pull/144171,wanchaol,awgu,,,
91cbeb7db95,mps,bug fixes,[MPSInductor] Fix `masked`/`where` for inf values (#144500),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144500,malfet,dcci,,,
66ce13b4974,dynamo,not user facing,"Revert D67299312: Multisect successfully blamed ""D67299312: [AoTI Minifier] UX Improvement"" for one test failure (#144475)",torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/144475,yushangdi,angelayi,zhxchen17,,
08eaaa61ea8,releng,not user facing,Inductor dashboard benchmarks: swap unused freeze_autotune_cudagraphs workflow for cppwrapper workflow (#144427),.ci/pytorch/test.sh .github/workflows/inductor-perf-test-nightly.yml,https://github.com/pytorch/pytorch/pull/144427,benjaminglass1,desertfire,huydhn,,
a55977f763a,quantization,not user facing,Migrate from Tuple -> tuple in torch/ao (#144265),torch/ao/nn/qat/modules/conv.py torch/ao/nn/quantizable/modules/activation.py torch/ao/nn/quantizable/modules/rnn.py torch/ao/nn/quantized/dynamic/modules/rnn.py torch/ao/nn/quantized/reference/modules/rnn.py torch/ao/ns/_numeric_suite_fx.py torch/ao/ns/fx/graph_matcher.py torch/ao/ns/fx/graph_passes.py torch/ao/ns/fx/mappings.py torch/ao/ns/fx/n_shadows_utils.py torch/ao/ns/fx/pattern_utils.py torch/ao/ns/fx/utils.py torch/ao/pruning/_experimental/data_sparsifier/base_data_sparsifier.py torch/ao/pruning/_experimental/data_sparsifier/data_norm_sparsifier.py torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py torch/ao/pruning/_experimental/pruner/match_utils.py torch/ao/pruning/_experimental/pruner/prune_functions.py torch/ao/pruning/sparsifier/base_sparsifier.py torch/ao/pruning/sparsifier/weight_norm_sparsifier.py torch/ao/quantization/__init__.py torch/ao/quantization/backend_config/utils.py torch/ao/quantization/experimental/adaround_fake_quantize.py torch/ao/quantization/experimental/adaround_loss.py torch/ao/quantization/experimental/adaround_optimization.py torch/ao/quantization/fake_quantize.py torch/ao/quantization/fx/_decomposed.py torch/ao/quantization/fx/_equalize.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/ao/quantization/fx/_model_report/detector.py torch/ao/quantization/fx/_model_report/model_report.py torch/ao/quantization/fx/_model_report/model_report_visualizer.py torch/ao/quantization/fx/convert.py torch/ao/quantization/fx/custom_config.py torch/ao/quantization/fx/fuse.py torch/ao/quantization/fx/lower_to_fbgemm.py torch/ao/quantization/fx/lower_to_qnnpack.py torch/ao/quantization/fx/lstm_utils.py torch/ao/quantization/fx/match_utils.py torch/ao/quantization/fx/prepare.py torch/ao/quantization/fx/qconfig_mapping_utils.py torch/ao/quantization/fx/utils.py torch/ao/quantization/observer.py torch/ao/quantization/pt2e/_affine_quantization.py torch/ao/quantization/pt2e/_numeric_debugger.py torch/ao/quantization/pt2e/graph_utils.py torch/ao/quantization/pt2e/prepare.py torch/ao/quantization/pt2e/qat_utils.py torch/ao/quantization/pt2e/representation/rewrite.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/qconfig_mapping.py torch/ao/quantization/quantize_fx.py torch/ao/quantization/quantizer/quantizer.py torch/ao/quantization/quantizer/x86_inductor_quantizer.py torch/ao/quantization/utils.py,https://github.com/pytorch/pytorch/pull/144265,bobrenjc93,aorenste,,,
3607ff2c1d0,skip,not user facing,Migrate from Tuple -> tuple in benchmarks/instruction_counts/core (#144253),benchmarks/instruction_counts/core/api.py benchmarks/instruction_counts/core/expand.py benchmarks/instruction_counts/core/types.py benchmarks/instruction_counts/core/utils.py,https://github.com/pytorch/pytorch/pull/144253,bobrenjc93,aorenste,,,
8db67e03193,skip,not user facing,Migrate from Tuple -> tuple in torch/_decomp (#144260),torch/_decomp/decompositions.py torch/_decomp/decompositions_for_jvp.py,https://github.com/pytorch/pytorch/pull/144260,bobrenjc93,aorenste,,,
bd1f5d1c32f,skip,not user facing,update xnnpack for disable libm on Windows  [submodule XNNPACK] (#141943),cmake/Dependencies.cmake third_party/XNNPACK,https://github.com/pytorch/pytorch/pull/141943,xuhancn,atalman,,,
1dd1d532ba3,skip,not user facing,[BE] Fix extra-semi warnings in int4mm_kernel.cpp (#144510),aten/src/ATen/native/cpu/int4mm_kernel.cpp,https://github.com/pytorch/pytorch/pull/144510,malfet,kit1980,,,
bf6dd955cd4,dynamo,Untopiced,Fix max(map(...)) (#142443),test/dynamo/test_functions.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/142443,guilhermeleobas,zou3519,,,
87c1f76e630,skip,Untopiced,"Revert ""Migrate from Tuple -> tuple in torch/_decomp (#144260)""",torch/_decomp/decompositions.py torch/_decomp/decompositions_for_jvp.py,,,,,,
04cb19d225b,inductor,not user facing,Add instantiation level to CutlassArgs (#144506),torch/_inductor/codegen/cuda/cutlass_utils.py,https://github.com/pytorch/pytorch/pull/144506,drisspg,huydhn,,,
6de110b862d,python_frontend,Untopiced,Support with statement on torch.Stream (#140138),test/test_accelerator.py test/test_cuda.py test/test_xpu.py torch/_torch_docs.py torch/csrc/Stream.cpp torch/csrc/Stream.h torch/cuda/streams.py torch/xpu/streams.py,https://github.com/pytorch/pytorch/pull/140138,guangyey,albanD,,,
d1b64ec3266,export,Untopiced,[export] Fix sym_bool serialization (#144295),test/export/test_serialize.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/144295,yiming0416,angelayi,avikchaudhuri,,
898fcb45907,performance_as_product,Untopiced,Simplify vec128 bfloat16/half fmadds (#144486),aten/src/ATen/cpu/vec/vec128/vec128_bfloat16_neon.h aten/src/ATen/cpu/vec/vec128/vec128_half_neon.h aten/src/ATen/test/vec_test_all_types.cpp aten/src/ATen/test/vec_test_all_types.h,https://github.com/pytorch/pytorch/pull/144486,swolchok,malfet,,,
9f09b719d33,skip,not user facing,Stop ignoring mypy errors in torch/testing/_internal/common_utils.py (#144483),torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/144483,aorenste,Skylion007,,,
9a841f9321a,cpp_frontend,Untopiced,Enable bugprone-unchecked-optional-access (#144226),.clang-tidy aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/core/jit_type.h aten/src/ATen/core/type.cpp aten/src/ATen/functorch/BatchRulesRandomness.cpp aten/src/ATen/functorch/BatchRulesReduceOps.cpp aten/src/ATen/functorch/BatchRulesScatterOps.cpp aten/src/ATen/functorch/BatchRulesViews.cpp aten/src/ATen/functorch/DynamicLayer.cpp aten/src/ATen/functorch/LegacyVmapTransforms.cpp c10/cuda/CUDACachingAllocator.cpp c10/util/OptionalArrayRef.h torch/csrc/api/include/torch/nn/functional/loss.h torch/csrc/api/src/nn/modules/embedding.cpp torch/csrc/distributed/rpc/rref_impl.cpp,https://github.com/pytorch/pytorch/pull/144226,cyyever,albanD,,,
0529908f130,inductor,Untopiced,Remove is_reduced_floating_point from namespace std (#144502),aten/src/ATen/native/cpu/Gelu.h aten/src/ATen/test/vec_test_all_types.h c10/util/BFloat16-math.h torch/_inductor/codegen/cpp_flex_attention_template.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/144502,swolchok,malfet,,,
d100a92d338,skip,not user facing,[CPU][Brgemm] add support for int8 brgemm (#143384),aten/src/ATen/native/CPUBlas.cpp aten/src/ATen/native/CPUBlas.h,https://github.com/pytorch/pytorch/pull/143384,Valentine233,ezyang,jgong5,leslie-fang-intel,
c8595ba7d02,skip,not user facing,[dynamo] Avoid graph break on updates to `obj.__dict__` (#144419),test/dynamo/test_dicts.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/144419,StrongerXi,anijain2305,jansel,,
4375c2c534b,skip,not user facing,Cleanup gpt_fast benchmark (#144517),benchmarks/gpt_fast/benchmark.py,https://github.com/pytorch/pytorch/pull/144517,nmacchioni,davidberard98,huydhn,,
493a52cb729,xpu,not user facing,Refine torch.xpu.get_device_properties API error message (#144379),torch/xpu/__init__.py,https://github.com/pytorch/pytorch/pull/144379,guangyey,EikanWang,,,
3b6b306b71e,distributed,not user facing,Migrate from Tuple -> tuple in torch/testing (#144256),torch/testing/_comparison.py torch/testing/_creation.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_dist_composable.py torch/testing/_internal/common_distributed.py torch/testing/_internal/common_fsdp.py torch/testing/_internal/common_methods_invocations.py torch/testing/_internal/common_modules.py torch/testing/_internal/common_nn.py torch/testing/_internal/common_optimizers.py torch/testing/_internal/common_pruning.py torch/testing/_internal/common_quantization.py torch/testing/_internal/common_utils.py torch/testing/_internal/custom_op_db.py torch/testing/_internal/dist_utils.py torch/testing/_internal/distributed/_tensor/common_dtensor.py torch/testing/_internal/distributed/checkpoint_utils.py torch/testing/_internal/distributed/multi_threaded_pg.py torch/testing/_internal/distributed/nn/api/remote_module_test.py torch/testing/_internal/distributed/rpc/jit/dist_autograd_test.py torch/testing/_internal/distributed/rpc/jit/rpc_test.py torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py torch/testing/_internal/jit_utils.py torch/testing/_internal/opinfo/definitions/linalg.py torch/testing/_internal/opinfo/definitions/nested.py torch/testing/_internal/opinfo/definitions/signal.py torch/testing/_internal/optests/generate_tests.py,https://github.com/pytorch/pytorch/pull/144256,bobrenjc93,aorenste,,,
fbad8335381,distributed,not user facing,Migrate from Tuple -> tuple in test/distributed/_composable (#144254),torch/distributed/_composable/checkpoint_activation.py torch/distributed/_composable/replicate.py,https://github.com/pytorch/pytorch/pull/144254,bobrenjc93,aorenste,,,
eddf83559e9,inductor,not user facing,[Intel GPU][Inductor] Convert Conv1D to 2D in inductor (#144140),torch/_inductor/kernel/conv.py,https://github.com/pytorch/pytorch/pull/144140,jianyizh,EikanWang,desertfire,jansel,
e5111d04309,skip,not user facing,"[Inductor UT] Add expected failure for newly added case on XPU, align CUDA. (#144457)",test/inductor/test_torchinductor_codegen_dynamic_shapes.py test/inductor/test_torchinductor_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/144457,etaf,EikanWang,jansel,liangan1,
8cc8989b26c,skip,not user facing,[Inductor UT] Generalize newly introduced device-bias hard code in (#144456),test/inductor/test_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/144456,etaf,EikanWang,jansel,malfet,
f295eff5121,profiler,bug fixes,[Profiler] Hide Kineto Step Tracker Behind Env Var (#144494),test/profiler/test_profiler.py torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/144494,sraikund16,ngimel,,,
1fe3af2c680,dynamo,not user facing,Migrate from Tuple -> tuple in torch/_dynamo (#144261),torch/_dynamo/_trace_wrapped_higher_order_op.py torch/_dynamo/backends/registry.py torch/_dynamo/bytecode_transformation.py torch/_dynamo/cache_size.py torch/_dynamo/compiled_autograd.py torch/_dynamo/convert_frame.py torch/_dynamo/create_parameter_op.py torch/_dynamo/device_interface.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/graph_deduplication.py torch/_dynamo/graph_region_tracker.py torch/_dynamo/guards.py torch/_dynamo/output_graph.py torch/_dynamo/pgo.py torch/_dynamo/polyfills/loader.py torch/_dynamo/replay_record.py torch/_dynamo/repro/aoti.py torch/_dynamo/resume_execution.py torch/_dynamo/symbolic_convert.py torch/_dynamo/test_case.py torch/_dynamo/testing.py torch/_dynamo/utils.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/lazy.py,https://github.com/pytorch/pytorch/pull/144261,bobrenjc93,aorenste,zou3519,,
184549b2d7e,skip,not user facing,Fix torch.normal ignores default_device (#144070),test/test_tensor_creation_ops.py torch/__init__.py torch/utils/_device.py,https://github.com/pytorch/pytorch/pull/144070,zeshengzong,ezyang,,,
08be9ec3120,distributed,not user facing,Migrate from Tuple -> tuple in torch/distributed (#144258),torch/distributed/_functional_collectives.py torch/distributed/_shard/sharded_optim/__init__.py torch/distributed/_shard/sharded_tensor/api.py torch/distributed/_shard/sharded_tensor/logger.py torch/distributed/_shard/sharded_tensor/reshard.py torch/distributed/_shard/sharding_spec/_internals.py torch/distributed/_state_dict_utils.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/distributed/_tools/ilp_utils.py torch/distributed/_tools/mem_tracker.py torch/distributed/_tools/runtime_estimator.py torch/distributed/_tools/sac_estimator.py torch/distributed/_tools/sac_ilp.py torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py torch/distributed/c10d_logger.py torch/distributed/checkpoint/_nested_dict.py torch/distributed/checkpoint/_traverse.py torch/distributed/checkpoint/api.py torch/distributed/checkpoint/default_planner.py torch/distributed/checkpoint/filesystem.py torch/distributed/checkpoint/optimizer.py torch/distributed/checkpoint/planner.py torch/distributed/checkpoint/resharding.py torch/distributed/checkpoint/state_dict.py torch/distributed/collective_utils.py torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py torch/distributed/elastic/agent/server/api.py torch/distributed/elastic/multiprocessing/errors/__init__.py torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py torch/distributed/elastic/rendezvous/dynamic_rendezvous.py torch/distributed/elastic/rendezvous/etcd_rendezvous_backend.py torch/distributed/elastic/rendezvous/utils.py torch/distributed/elastic/timer/file_based_local_timer.py torch/distributed/elastic/timer/local_timer.py torch/distributed/fsdp/_common_utils.py torch/distributed/fsdp/_debug_utils.py torch/distributed/fsdp/_exec_order_utils.py torch/distributed/fsdp/_flat_param.py torch/distributed/fsdp/_fsdp_extensions.py torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py torch/distributed/fsdp/_fully_shard/_fsdp_init.py torch/distributed/fsdp/_fully_shard/_fsdp_param.py torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py torch/distributed/fsdp/_fully_shard/_fsdp_state.py torch/distributed/fsdp/_init_utils.py torch/distributed/fsdp/_optim_utils.py torch/distributed/fsdp/_runtime_utils.py torch/distributed/fsdp/_state_dict_utils.py torch/distributed/fsdp/_trace_utils.py torch/distributed/fsdp/_traversal_utils.py torch/distributed/fsdp/_wrap_utils.py torch/distributed/fsdp/fully_sharded_data_parallel.py torch/distributed/fsdp/sharded_grad_scaler.py torch/distributed/fsdp/wrap.py torch/distributed/launcher/api.py torch/distributed/nn/api/remote_module.py torch/distributed/optim/functional_adam.py torch/distributed/optim/functional_adamax.py torch/distributed/optim/functional_adamw.py torch/distributed/optim/functional_rprop.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/_backward.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/microbatch.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py torch/distributed/rendezvous.py torch/distributed/rpc/__init__.py torch/distributed/rpc/backend_registry.py torch/distributed/run.py torch/distributed/tensor/_api.py torch/distributed/tensor/_dispatch.py torch/distributed/tensor/_dtensor_spec.py torch/distributed/tensor/_op_schema.py torch/distributed/tensor/_ops/_common_rules.py torch/distributed/tensor/_ops/_einsum_strategy.py torch/distributed/tensor/_ops/_math_ops.py torch/distributed/tensor/_ops/_pointwise_ops.py torch/distributed/tensor/_ops/_tensor_ops.py torch/distributed/tensor/_ops/_view_ops.py torch/distributed/tensor/_ops/utils.py torch/distributed/tensor/_redistribute.py torch/distributed/tensor/_sharding_prop.py torch/distributed/tensor/_shards_wrapper.py torch/distributed/tensor/_tp_conv.py torch/distributed/tensor/_utils.py torch/distributed/tensor/debug/_visualize_sharding.py torch/distributed/tensor/experimental/_attention.py torch/distributed/tensor/experimental/_func_map.py torch/distributed/tensor/experimental/_register_sharding.py torch/distributed/tensor/experimental/_tp_transform.py torch/distributed/tensor/parallel/_data_parallel_utils.py torch/distributed/tensor/parallel/_utils.py torch/distributed/tensor/parallel/ddp.py torch/distributed/tensor/parallel/fsdp.py torch/distributed/tensor/parallel/input_reshard.py torch/distributed/tensor/parallel/loss.py torch/distributed/tensor/parallel/style.py torch/distributed/tensor/placement_types.py torch/distributed/utils.py,https://github.com/pytorch/pytorch/pull/144258,bobrenjc93,aorenste,,,
2583d831d40,skip,not user facing,Fix poision child process issue when call getAccelerator() (#144368),aten/src/ATen/Context.h aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h aten/src/ATen/cuda/detail/CUDAHooks.cpp test/cpp/jit/test_misc.cpp test/test_cuda.py test/test_xpu.py,https://github.com/pytorch/pytorch/pull/144368,guangyey,albanD,atalman,gujinghui,
eeb57394f93,skip,not user facing,Generalize at::manual_seed for all accelerators (#144370),aten/src/ATen/Context.h,https://github.com/pytorch/pytorch/pull/144370,guangyey,EikanWang,albanD,gujinghui,
b2fde282839,profiler,Untopiced,[Profiler] Fix device setting error of other backends in torch.profiler (#144237),torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/144237,fmo-mt,sraikund16,,,
a222029f4e0,skip,not user facing,retracing in strict doesn't like dataclass registration (#144487),test/export/test_export.py,https://github.com/pytorch/pytorch/pull/144487,avikchaudhuri,angelayi,,,
e6b9e674659,skip,not user facing,[BE][Opinfo] Delete redundant `dtypesIfCUDA` (#144512),torch/testing/_internal/opinfo/definitions/signal.py torch/testing/_internal/opinfo/definitions/special.py,https://github.com/pytorch/pytorch/pull/144512,malfet,Skylion007,,,
473b745cb97,skip,Untopiced,"Revert ""[dynamo] Avoid graph break on updates to `obj.__dict__` (#144419)""",test/dynamo/test_dicts.py torch/_dynamo/variables/misc.py,,,,,,
f604338e31c,mps,bug fixes,[MPS] Make sure that MPSStream is usable from C++ (#144559),aten/src/ATen/mps/MPSStream.h aten/src/ATen/mps/MPSStream.mm aten/src/ATen/test/mps_test_metal_library.cpp,https://github.com/pytorch/pytorch/pull/144559,malfet,Skylion007,,,
8dba1ce73ba,mps,bug fixes,[MPS] Make MPSProfiler usable from C++ (#144560),aten/src/ATen/mps/MPSProfiler.h aten/src/ATen/mps/MPSProfiler.mm aten/src/ATen/test/mps_test_metal_library.cpp,https://github.com/pytorch/pytorch/pull/144560,malfet,Skylion007,,,
fdc4f9dde2d,distributed,Untopiced,Avoid running helper functions as test (#144544),test/distributed/_composable/fsdp/test_fully_shard_training.py torch/testing/_internal/common_fsdp.py,https://github.com/pytorch/pytorch/pull/144544,Flamefire,Skylion007,,,
7a93a58b3c9,skip,not user facing,"fix typo: ""assumbed""  (#144543)",aten/src/ATen/native/cuda/Blas.cpp,https://github.com/pytorch/pytorch/pull/144543,crcrpar,Skylion007,,,
603e1c0b02a,build_frontend,Untopiced,torchgen: move dispatch_helpers out of RegisterDispatchDefinitions.ini (#144363),aten/src/ATen/templates/RegisterDispatchDefinitions.ini aten/src/ATen/templates/RegisterDispatchKey.cpp torchgen/gen.py torchgen/gen_backend_stubs.py,https://github.com/pytorch/pytorch/pull/144363,swolchok,bdhirsh,,,
4143312e675,releng,not user facing,S390x ci periodic tests (#125401),.ci/docker/requirements-ci.txt .ci/pytorch/build.sh .ci/pytorch/test.sh .github/workflows/_linux-test.yml .github/workflows/s390x-periodic.yml test/inductor/test_compiled_autograd.py test/run_test.py test/test_autograd.py test/test_tensor_creation_ops.py,https://github.com/pytorch/pytorch/pull/125401,AlekseiNikiforovIBM,malfet,,,
b46d00c1b77,build_frontend,Untopiced,Shard RegisterDispatchKey (#144364),BUILD.bazel buckbuild.bzl build.bzl torchgen/gen.py torchgen/utils.py,https://github.com/pytorch/pytorch/pull/144364,swolchok,Skylion007,bdhirsh,,
868984c3e32,skip,Untopiced,[AOTI] Add a boxed_run API (#142213),benchmarks/dynamo/expected_ci_speedup_inductor_torchbench_cpu.csv test/inductor/test_aot_inductor_custom_ops.py torch/_inductor/package/package.py torch/csrc/inductor/aoti_package/model_package_loader.cpp torch/csrc/inductor/aoti_package/model_package_loader.h torch/csrc/inductor/aoti_package/pybind.cpp torch/csrc/inductor/aoti_runner/model_container_runner.cpp torch/csrc/inductor/aoti_runner/model_container_runner.h torch/csrc/inductor/aoti_runner/model_container_runner_cpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cpu.h torch/csrc/inductor/aoti_runner/model_container_runner_cuda.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cuda.h torch/csrc/inductor/aoti_runner/model_container_runner_xpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_xpu.h,https://github.com/pytorch/pytorch/pull/142213,desertfire,ezyang,,,
1ce35242776,skip,not user facing,use collective_comm activity for hccl traces (#144490),torch/csrc/profiler/kineto_shim.cpp,https://github.com/pytorch/pytorch/pull/144490,fenypatel99,sraikund16,,,
4abf5548826,mps,Untopiced,Use structure binding (#144524),aten/src/ATen/native/miopen/RNN_miopen.cpp aten/src/ATen/native/mkldnn/xpu/detail/QConv.cpp aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.cpp,https://github.com/pytorch/pytorch/pull/144524,cyyever,Skylion007,,,
4f04078aecf,releng,not user facing,[CI] Ensure ACL is obtained from GitHub (#141804),.ci/docker/common/install_acl.sh,https://github.com/pytorch/pytorch/pull/141804,theComputeKid,digantdesai,nikhil-arm,snadampal,
a37db5ae397,benchmark,Untopiced,operator benchmark change parsing from regex based to manual (#144297),benchmarks/operator_benchmark/benchmark_core.py,https://github.com/pytorch/pytorch/pull/144297,apakbin,XuehaiPan,jeffdaily,,
99600789c3e,inductor,not user facing,[ROCm][Inductor][CK] hackfix for segfault in addmm op (#144519),torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/144519,tenpercent,chenyang78,,,
396630ed783,skip,not user facing,Update the accuracy results for moco and llama (#144523),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/update_expected.py,https://github.com/pytorch/pytorch/pull/144523,huydhn,Skylion007,kit1980,,
10ff6b88943,export,Untopiced,[export] Add pickle protocol (#142253),torch/_export/serde/serialize.py torch/export/__init__.py,https://github.com/pytorch/pytorch/pull/142253,angelayi,avikchaudhuri,,,
be5afe16a67,fx,not user facing,Fix deepcopy hooks (#144531),torch/fx/graph_module.py,https://github.com/pytorch/pytorch/pull/144531,yushangdi,BoyuanFeng,,,
18c1dcb8f30,skip,not user facing,docs: get rid of copyright year (#144562),docs/cpp/source/conf.py docs/source/conf.py,https://github.com/pytorch/pytorch/pull/144562,kuraga,albanD,,,
7a81ba18b9c,export,Untopiced,[export] Add support for serializing symint inputs (#142284),test/export/test_serialize.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/142284,angelayi,avikchaudhuri,,,
68dad26b950,nn_frontend,docs,torch/nn/modules/linear.py: docs: improvements (#138484),torch/nn/modules/linear.py,https://github.com/pytorch/pytorch/pull/138484,kuraga,mikaylagawarecki,,,
4daf007b647,releng,not user facing,Request English for Issues (#144574),.github/ISSUE_TEMPLATE/bug-report.yml .github/ISSUE_TEMPLATE/feature-request.yml .github/ISSUE_TEMPLATE/pt2-bug-report.yml,https://github.com/pytorch/pytorch/pull/144574,PaliC,albanD,,,
6b902e6e1ab,releng,Untopiced,Update bug-report.yml to make it not look weird,.github/ISSUE_TEMPLATE/bug-report.yml,,,,,,
a94ec0a9a5a,skip,not user facing,[aoti] Remove example inputs from aoti_compile_and_package (#144520),test/export/test_draft_export.py test/inductor/test_minifier.py,https://github.com/pytorch/pytorch/pull/144520,angelayi,yushangdi,,,
c7f12a4a7b8,inductor,not user facing,[MPSInductor] Speedup maximum/minumum ops (#144581),torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144581,malfet,dcci,jhavukainen,,
1ff8a1c4ebe,releng,Untopiced,Update documentation.yml to request english,.github/ISSUE_TEMPLATE/documentation.yml,,,,,,
9ec8ecea714,releng,Untopiced,Update documentation.yml,.github/ISSUE_TEMPLATE/documentation.yml,,,,,,
db2a30932a1,skip,Untopiced,"Revert ""Generalize at::manual_seed for all accelerators (#144370)""",aten/src/ATen/Context.h,,,,,,
b80ecc4457d,skip,Untopiced,"Revert ""Fix poision child process issue when call getAccelerator() (#144368)""",aten/src/ATen/Context.h aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/DeviceAccelerator.h aten/src/ATen/cuda/detail/CUDAHooks.cpp test/cpp/jit/test_misc.cpp test/test_cuda.py test/test_xpu.py,,,,,,
6cfc0816759,skip,not user facing,Increase C10_COMPILE_TIME_MAX_GPUS to 128 (#144138),c10/core/Device.cpp c10/core/Device.h c10/cuda/CUDAMacros.h c10/cuda/CUDAStream.cpp,https://github.com/pytorch/pytorch/pull/144138,cyyever,albanD,,,
c9afa00a85f,skip,not user facing,update sleef for disable libm on Windows [submodule Sleef] (#142245),aten/src/ATen/CMakeLists.txt third_party/sleef,https://github.com/pytorch/pytorch/pull/142245,xuhancn,EikanWang,atalman,,
95d333f52ea,distributed,not user facing,[distributed] Fix _ReaderView.read() and readinto() to stop reading at the end of the slice (#143357),test/distributed/checkpoint/test_utils.py torch/distributed/checkpoint/utils.py,https://github.com/pytorch/pytorch/pull/143357,mhorowitz,saumishr,,,
c7dbee51066,export,Untopiced,[reland][export] don't decompose custom triton op when exporting (#144284),test/export/test_export.py torch/_functorch/config.py torch/_library/triton.py torch/export/_trace.py torch/export/experimental/__init__.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/144284,ydwu4,zou3519,,,
92ddb3d3d31,mps,improvements,[MPS] Expose `MPSProfiler::start/stopCapture`  to Python (#144561),.github/workflows/_mac-test-mps.yml docs/source/mps.rst test/test_mps.py torch/_C/__init__.pyi.in torch/csrc/mps/Module.cpp torch/mps/profiler.py,https://github.com/pytorch/pytorch/pull/144561,malfet,manuelcandales,,,
49c1f81be84,skip,not user facing,[ez] add lint commits to .git-blame-ignore-revs (#144576),.git-blame-ignore-revs,https://github.com/pytorch/pytorch/pull/144576,PaliC,janeyx99,,,
3753d302732,skip,Untopiced,"Revert ""Stop ignoring mypy errors in torch/testing/_internal/common_utils.py (#144483)""",torch/testing/_internal/common_utils.py,,,,,,
074aca3ed2b,dynamo,not user facing,[user triton] add support for @triton.heuristics after @triton.autotune (#142208),test/inductor/test_triton_kernels.py torch/_dynamo/variables/functions.py torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/142208,SamGinzburg,zou3519,,,
0cd9320c7f1,dynamo,not user facing,easy: dynamo_config: sort keys and set values (#143317),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/143317,c00w,masnesral,,,
8fa47c94551,dynamo,not user facing,[dynamo] log compiler collective duration to tlparse chromium trace (#144372),torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/144372,xmfan,ezyang,,,
1d3cd7bd098,distributed,not user facing,[Pipelining] Improve test_pp_dp (#144534),test/distributed/test_composability.py,https://github.com/pytorch/pytorch/pull/144534,wconstab,H-Huang,,,
11082aead37,distributed,Untopiced,[Pipelining] Fix FSDP+PP stream sync bug (#144535),test/distributed/test_composability.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/144535,wconstab,awgu,,,
e4b2e90e54c,releng,not user facing,Fix broken YAML template after #144574 (#144604),.github/ISSUE_TEMPLATE/pt2-bug-report.yml,https://github.com/pytorch/pytorch/pull/144604,huydhn,wdvr,,,
e1d0a2ff30f,inductor,not user facing,[Inductor] Restrict ND tiling analysis to MemoryDeps (#144497),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/codegen/simd.py,https://github.com/pytorch/pytorch/pull/144497,blaine-rister,eellison,,,
b7bef1ca84c,releng,not user facing,[aarch64] fix TORCH_CUDA_ARCH_LIST for cuda arm build (#144436),.ci/aarch64_linux/aarch64_ci_build.sh .ci/manywheel/build_cuda.sh,https://github.com/pytorch/pytorch/pull/144436,tinglvv,atalman,,,
2e3b0511544,skip,not user facing,[XPU] Fix TRITON_XPU_BUILD_FROM_SOURCE (#142850),scripts/install_triton_wheel.sh,https://github.com/pytorch/pytorch/pull/142850,DDEle,EikanWang,atalman,benjaminglass1,
388b75edec0,skip,not user facing,[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/144441,eqy,Chillee,malfet,,
63569d9745b,nn_frontend,not user facing,[CUDA][TF32] Add some missing TF32 decorators to `test_nn.py` (#144592),test/test_nn.py,https://github.com/pytorch/pytorch/pull/144592,eqy,Skylion007,,,
2ccbacfa24c,skip,not user facing,[mps/inductor] Add support for exp(). (#144606),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144606,dcci,malfet,,,
eaa24821f27,skip,Untopiced,"Revert ""[ez] add lint commits to .git-blame-ignore-revs (#144576)""",.git-blame-ignore-revs,,,,,,
4f406d22a2c,skip,Untopiced,"Revert ""[mps/inductor] Add support for exp(). (#144606)""",test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,,,,,,
80b756ed912,jit,not user facing,remove allow-untyped-defs from torch/jit/_pickle.py (#144625),torch/jit/_pickle.py,https://github.com/pytorch/pytorch/pull/144625,bobrenjc93,Skylion007,,,
ad221269b01,skip,not user facing,remove allow-untyped-defs from torch/distributions/pareto.py (#144624),torch/distributions/constraints.py torch/distributions/pareto.py,https://github.com/pytorch/pytorch/pull/144624,bobrenjc93,Skylion007,,,
b5485c9f419,skip,not user facing,remove allow-untyped-defs from torch/_functorch/utils.py (#144626),torch/_functorch/utils.py,https://github.com/pytorch/pytorch/pull/144626,bobrenjc93,Skylion007,,,
b8aae2773ff,distributed,not user facing,remove allow-untyped-defs from torch/distributed/_checkpointable.py (#144627),torch/distributed/_checkpointable.py,https://github.com/pytorch/pytorch/pull/144627,bobrenjc93,Skylion007,,,
f6688ac81d8,distributed,not user facing,remove allow-untyped-defs from torch/distributed/_shard/sharded_tensor/shard.py (#144623),torch/distributed/_shard/sharded_tensor/shard.py,https://github.com/pytorch/pytorch/pull/144623,bobrenjc93,Skylion007,,,
5e858254d29,inductor,not user facing,[mps/inductor] Add support for trunc(). (#144629),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144629,dcci,jansel,malfet,,
10887fc139e,skip,not user facing,[BE] Enable test_public_bindings on MacOS (#144591),test/test_public_bindings.py,https://github.com/pytorch/pytorch/pull/144591,malfet,Skylion007,,,
e0f67405a15,skip,not user facing,[mps/inductor] Add support for exp(). (#144606),test/dynamo/test_backends.py test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144606,dcci,malfet,,,
33551032334,dynamo,not user facing,[Dynamo] Supports autograd.Function forward returns constant (#144597),test/dynamo/test_autograd_function.py torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/144597,yanboliang,jansel,,,
32a91dedc5d,inductor,not user facing,[MPSInductor] Properly generate index expressions (#144632),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144632,malfet,dcci,,,
cec245806e1,inductor,not user facing,[MPSInductor] Implement bitcasts (#144638),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144638,malfet,dcci,,,
1664033e13c,functorch,not user facing,[Functorch] Refactor vmapify autograd function: remove cell mutation (#143811),torch/_functorch/autograd_function.py torch/_functorch/vmap.py,https://github.com/pytorch/pytorch/pull/143811,yanboliang,zou3519,,,
de04acaca9a,inductor,not user facing,Disable scuba logging for autotuning (#144568),test/dynamo/test_structured_trace.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/144568,masnesral,jamesjwu,,,
fd382f12699,fx,Untopiced,Micro-optimization in Graph.nodes.__iter__ (#144631),torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/144631,jansel,Skylion007,oulgen,,
91a65cbd31e,inductor,not user facing,[MPSInductor] Implement `check_bounds` (#144635),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144635,malfet,jansel,,,
cb66146f2b5,fx,Untopiced,[BE]: Update literal typing for torch/fx/graph nodelist (#144650),torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/144650,Skylion007,jansel,,,
334ee8ba40f,skip,not user facing,Fix a bug for conj_physical (#144391),aten/src/ATen/native/UnaryOps.cpp,https://github.com/pytorch/pytorch/pull/144391,ywq880611,jansel,,,
1376116ab1c,inductor,Untopiced,Config fuzzer (#139736),test/inductor/test_fuzzer.py torch/_inductor/config.py torch/_inductor/fuzzer.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/139736,exclamaforte,eellison,,,
3541d2a2aaa,python_frontend,improvements,Collect packages with importlib in collect_env (#144616),torch/utils/collect_env.py,https://github.com/pytorch/pytorch/pull/144616,AngryLoki,malfet,,,
9ae35b8bb13,python_frontend,improvements,[BE] Introduce `c10::SyntaxError` (#144647),c10/util/Exception.h torch/csrc/Exceptions.h,https://github.com/pytorch/pytorch/pull/144647,malfet,Skylion007,,,
46eeef9130f,mps,Untopiced,[MPS][BE] Surface syntax errors shader compilation (#144648),aten/src/ATen/native/mps/OperationUtils.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/144648,malfet,Skylion007,,,
0aa34e95919,skip,Untopiced,"Revert ""Collect packages with importlib in collect_env (#144616)""",torch/utils/collect_env.py,,,,,,
87843ee9ab5,export,Untopiced,[export] Unify single and multiple return for hops (#143227),test/export/test_export.py torch/_export/serde/export_schema.thrift torch/_export/serde/schema.py torch/_export/serde/schema.yaml torch/_export/serde/serialize.py torch/_higher_order_ops/run_const_graph.py torch/csrc/utils/generated_serialization_types.h,https://github.com/pytorch/pytorch/pull/143227,yiming0416,zhxchen17,,,
a08bd8154ef,inductor,not user facing,[MPSInductor] Add support for sizevars (#144662),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144662,malfet,jansel,,,
6e77d7cac5c,skip,not user facing,Add AOTAutogradCache support for cache hot loading APIs (#144499),test/dynamo/test_aot_autograd_cache.py test/inductor/test_codecache.py torch/_functorch/_aot_autograd/autograd_cache.py torch/compiler/_cache.py,https://github.com/pytorch/pytorch/pull/144499,jamesjwu,oulgen,,,
a85d1ee1061,skip,not user facing,Update slow tests (#144670),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/144670,pytorchupdatebot,pytorchbot,,,
7e2239f1f00,mps,improvements,[MPSInductor] Better error when kernel fails to compile (#144649),torch/_inductor/codegen/mps.py torch/_inductor/runtime/runtime_utils.py,https://github.com/pytorch/pytorch/pull/144649,malfet,Skylion007,dcci,jansel,
417354d9531,inductor,not user facing,[mps/inductor] Add support for truncdiv(). (#144666),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144666,dcci,malfet,,,
8633845090b,inductor,Untopiced,Support nanj in inductor (#144064),test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/144064,isuruf,amjames,eellison,,
c40d9171825,inductor,not user facing,[MPSInductor] Fix maximum/minimum for int types (#144665),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144665,malfet,dcci,,,
bee84e88f8c,quantization,not user facing,[BE][Easy] improve submodule discovery for `torch.ao` type annotations (#144680),torch/ao/__init__.py torch/ao/nn/__init__.py torch/ao/nn/quantized/modules/rnn.py,https://github.com/pytorch/pytorch/pull/144680,XuehaiPan,Skylion007,,,
fe8c5c7a2db,skip,not user facing,Update the Triton DeviceInterface in test/inductor/extension_backends/triton/device_interface.py (#144399),test/inductor/extension_backends/triton/device_interface.py,https://github.com/pytorch/pytorch/pull/144399,GeorgeWigley,jansel,,,
983bf604e50,python_frontend,Untopiced,ReshapeTransform: added missing argument in docstring (#144401),torch/distributions/transforms.py,https://github.com/pytorch/pytorch/pull/144401,randolf-scholz,janeyx99,malfet,,
f93d786f732,skip,not user facing,remove allow-untyped-defs from torch/nn/parameter.pyi (#144654),torch/nn/parameter.pyi,https://github.com/pytorch/pytorch/pull/144654,bobrenjc93,Skylion007,,,
cd477cdd1d0,quantization,not user facing,remove allow-untyped-defs from torch/ao/nn/quantized/reference/modules/linear.py (#144656),torch/ao/nn/quantized/reference/modules/linear.py,https://github.com/pytorch/pytorch/pull/144656,bobrenjc93,Skylion007,,,
d2f905760dd,releng,not user facing,[EZ] [CD] Eliminate stale TODO (#144696),.github/scripts/generate_binary_build_matrix.py,https://github.com/pytorch/pytorch/pull/144696,malfet,atalman,izaitsevfb,,
d44c3906b8b,releng,not user facing,[EZ] [CD] Add 3.13 to FULL_PYTHON_VERSIONS (#144697),.github/scripts/generate_binary_build_matrix.py,https://github.com/pytorch/pytorch/pull/144697,malfet,atalman,izaitsevfb,,
18deff02622,quantization,not user facing,remove allow-untyped-defs from torch/ao/nn/intrinsic/__init__.py (#144652),torch/ao/nn/intrinsic/__init__.py,https://github.com/pytorch/pytorch/pull/144652,bobrenjc93,Skylion007,,,
80c286cbecb,skip,not user facing,remove allow-untyped-defs from torch/_C/_dynamo/eval_frame.pyi (#144655),torch/_C/_dynamo/eval_frame.pyi,https://github.com/pytorch/pytorch/pull/144655,bobrenjc93,StrongerXi,,,
5a2e8fce9d3,releng,not user facing,Fix block pointer test module for triton CPU and add to CI (#144474),.ci/pytorch/test.sh test/inductor/test_torchinductor.py test/inductor/test_torchinductor_strided_blocks.py,https://github.com/pytorch/pytorch/pull/144474,kundaMwiza,jansel,,,
b7f95df65b5,linalg_frontend,Untopiced,[Feat]: Add Multithreading support for kleidiai groupwise GEMM kernels (#144074),aten/src/ATen/native/kleidiai/kai_kernels.cpp,https://github.com/pytorch/pytorch/pull/144074,nikhil-arm,digantdesai,,,
684d015c2f0,inductor,improvements,[AOTI] Support _int_mm (#144571),test/inductor/test_aot_inductor.py torch/_inductor/kernel/mm.py torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h torchgen/aoti/fallback_ops.py,https://github.com/pytorch/pytorch/pull/144571,desertfire,yushangdi,,,
4ceca4d60f8,skip,not user facing,[dynamo] Avoid graph break on updates to `obj.__dict__` (#144419),test/dynamo/test_dicts.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/144419,StrongerXi,anijain2305,jansel,,
91dbd7b75cd,export,Untopiced,[BE]: Improve typing inference with TypeIs (#144682),torch/nn/parallel/replicate.py,https://github.com/pytorch/pytorch/pull/144682,Skylion007,albanD,,,
3c55669b881,skip,not user facing,Enable grep_linter to use -a (#144589),tools/linter/adapters/grep_linter.py,https://github.com/pytorch/pytorch/pull/144589,clee2000,huydhn,,,
e15f91337bb,inductor,Untopiced,[inductor] Add unbacked symints binding in ShapeProp (#144605),test/test_fx.py torch/fx/passes/shape_prop.py,https://github.com/pytorch/pytorch/pull/144605,yushangdi,guowentian,pianpwk,,
5129d6ef51a,releng,not user facing,Fix inductor periodic smoke test wrong artifact (#144694),.github/workflows/inductor-perf-test-nightly-aarch64.yml .github/workflows/inductor-perf-test-nightly-x86.yml .github/workflows/inductor-perf-test-nightly.yml .github/workflows/inductor-periodic.yml,https://github.com/pytorch/pytorch/pull/144694,huydhn,clee2000,,,
1dab79470db,skip,not user facing,c10::string_view -> std::string_view in pytorch (#143591),c10/test/util/string_view_test.cpp c10/test/util/typeid_test.cpp c10/util/typeid.h,https://github.com/pytorch/pytorch/pull/143591,r-barnes,malfet,,,
0373cd99500,distributed,not user facing,remove allow-untyped-defs from torch/distributed/checkpoint/api.py (#144653),torch/distributed/checkpoint/api.py,https://github.com/pytorch/pytorch/pull/144653,bobrenjc93,Skylion007,,,
a54a784b820,skip,not user facing,[dynamo][dicts] Consolidate dict(..) construction (#144342),test/dynamo/test_higher_order_ops.py test/functorch/test_control_flow.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/144342,anijain2305,StrongerXi,,,
4f74864c94e,skip,Untopiced,"Revert ""[AOTI] Add a boxed_run API (#142213)""",benchmarks/dynamo/expected_ci_speedup_inductor_torchbench_cpu.csv test/inductor/test_aot_inductor_custom_ops.py torch/_inductor/package/package.py torch/csrc/inductor/aoti_package/model_package_loader.cpp torch/csrc/inductor/aoti_package/model_package_loader.h torch/csrc/inductor/aoti_package/pybind.cpp torch/csrc/inductor/aoti_runner/model_container_runner.cpp torch/csrc/inductor/aoti_runner/model_container_runner.h torch/csrc/inductor/aoti_runner/model_container_runner_cpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cpu.h torch/csrc/inductor/aoti_runner/model_container_runner_cuda.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cuda.h torch/csrc/inductor/aoti_runner/model_container_runner_xpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_xpu.h,,,,,,
c15d6508bdb,releng,not user facing,Binary builds Docker images - remove cuda 12.1 (#144575),.github/workflows/build-almalinux-images.yml .github/workflows/build-libtorch-images.yml .github/workflows/build-manywheel-images.yml,https://github.com/pytorch/pytorch/pull/144575,atalman,Skylion007,kit1980,malfet,
dcc04e92372,skip,not user facing,Stop ignoring mypy errors in torch/testing/_internal/common_utils.py (#144483),torch/_functorch/_activation_checkpointing/graph_info_provider.py torch/backends/cuda/__init__.py torch/mps/profiler.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/144483,aorenste,Skylion007,,,
58302c4eaa6,releng,improvements,[BE] [CD] Remove pygit2 dep for aarch64_wheel build (#144716),.ci/aarch64_linux/aarch64_ci_setup.sh .ci/aarch64_linux/aarch64_wheel_ci_build.py,https://github.com/pytorch/pytorch/pull/144716,malfet,atalman,,,
dfe06e555d4,skip,Untopiced,"Revert ""Stop ignoring mypy errors in torch/testing/_internal/common_utils.py (#144483)""",torch/_functorch/_activation_checkpointing/graph_info_provider.py torch/backends/cuda/__init__.py torch/mps/profiler.py torch/testing/_internal/common_utils.py,,,,,,
64bcf39180c,skip,Untopiced,"Revert ""[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441)""",aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,,,,,,
de9d6a25d71,inductor,not user facing,[mps/inductor] Add support for `ceil` (#144715),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144715,dcci,malfet,,,
eaa8a97b39f,releng,not user facing,[RelEng] Add `--ami` option to build_aarch64 (#144685),.ci/aarch64_linux/build_aarch64_wheel.py,https://github.com/pytorch/pytorch/pull/144685,malfet,atalman,,,
6d562776827,inductor,Untopiced,[export] Fix torchbind constant folding (#144684),torch/_export/serde/serialize.py torch/_inductor/constant_folding.py,https://github.com/pytorch/pytorch/pull/144684,yiming0416,zhxchen17,,,
b221f88fc1d,releng,not user facing,Leave SCCACHE_S3_KEY_PREFIX empty to share the cache among all build jobs (#144704),.github/workflows/_linux-build.yml .github/workflows/_linux-test.yml,https://github.com/pytorch/pytorch/pull/144704,huydhn,clee2000,,,
6053242890e,releng,improvements,[CD] Enable python3.13t builds for aarch64 (#144698),.ci/aarch64_linux/aarch64_ci_setup.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/144698,malfet,atalman,,,
e58c823ab80,dynamo,not user facing,Implement increment and add_to_set for CompileEventLogger (#143427),torch/_dynamo/utils.py torch/_inductor/codecache.py torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/143427,jamesjwu,masnesral,,,
17e05cde0c4,distributed,not user facing,ROCm: Skip tests in elastic/utils/distributed_test (#144692),test/distributed/elastic/utils/distributed_test.py,https://github.com/pytorch/pytorch/pull/144692,jagadish-amd,jeffdaily,,,
35b46a75f1b,inductor,not user facing,[mps/inductor] Add support for `round()` (#144731),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144731,dcci,malfet,,,
25de671ea84,inductor,not user facing,[Inductor][CPP] Enable Grouped GEMM Template (#143796),test/inductor/test_cpu_cpp_wrapper.py test/inductor/test_cpu_select_algorithm.py torch/_inductor/autotune_process.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_grouped_gemm_template.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/ir.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/143796,leslie-fang-intel,jansel,jgong5,,
9d98b66e7bb,inductor,not user facing,[Inductor][CPP] Enable Epilogue Fusion for Grouped GEMM Template (#143897),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_grouped_gemm_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/scheduler.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143897,leslie-fang-intel,jansel,jgong5,,
c031defe0b4,skip,not user facing,[RELAND] Generalize at::manual_seed for all accelerators (#144370),aten/src/ATen/Context.h,https://github.com/pytorch/pytorch/pull/144370,guangyey,EikanWang,albanD,gujinghui,
8436a5c2cb2,skip,not user facing,[Quant][Inductor][X86] Separate binary post op fusion and lowering for qlinear (#144224),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/144224,Xia-Weiwen,jerryzh168,leslie-fang-intel,,
80eff6e7205,mps,Untopiced,[MPS] fix triangular for >3D tensors (#144545),aten/src/ATen/native/mps/operations/LinearAlgebra.mm torch/testing/_internal/opinfo/definitions/linalg.py,https://github.com/pytorch/pytorch/pull/144545,Isalia20,malfet,,,
21cbee5d9bf,distributed,Untopiced,Drop unused num_elements variable (#144723),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/144723,c-p-i-o,Skylion007,fduwjj,,
1800f5f4613,distributed,Untopiced,Enable coalescing path on XPU and dispatch to XPU tensor barrier if XCCL backend is specified. (#143735),torch/csrc/distributed/c10d/ProcessGroup.hpp torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/143735,zhangxiaoli73,kwen2501,,,
95b41d2aa43,distributed,not user facing,Tests Generelization for multiple accelerator devices (#139749),test/distributed/tensor/test_dtensor_compile.py test/distributed/tensor/test_random_ops.py test/distributed/tensor/test_redistribute.py torch/testing/_internal/distributed/_tensor/common_dtensor.py,https://github.com/pytorch/pytorch/pull/139749,rahulsingh-intel,kwen2501,,,
ffb3f32693e,fx,Untopiced,Add max kwarg to torch._check with alternate size oblivious semantics (#144471),test/export/test_export.py torch/__init__.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/144471,ezyang,avikchaudhuri,,,
60d2e32fa4f,fx,Untopiced,[BE] Remove lambda from str (#144743),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/144743,ezyang,Skylion007,avikchaudhuri,,
347a74b8f52,skip,not user facing,Mark CUDA-12.6 as experimental for 2.6 release (#144769),RELEASE.md,https://github.com/pytorch/pytorch/pull/144769,malfet,atalman,,,
cbb1ed2966e,skip,not user facing,[1/N] OpenReg: Replace `open_registration_extension.cpp` with openreg (#141815),.gitignore test/cpp_extensions/open_registration_extension.cpp test/cpp_extensions/open_registration_extension/pytorch_openreg/_device_daemon.py test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/Module.cpp test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/OpenRegMem.cpp test/cpp_extensions/open_registration_extension/setup.py test/run_test.py test/test_cpp_extensions_open_device_registration.py test/test_transformers.py torch/csrc/Stream.h,https://github.com/pytorch/pytorch/pull/141815,Zhenbin-8,albanD,,,
5c727d5679e,skip,not user facing,[minifier] Fix config generator for callables (#144518),test/test_utils_config_module.py torch/testing/_internal/fake_config_module3.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/144518,yushangdi,desertfire,,,
f2975717f3c,build_frontend,bug fixes,[CD] Fix slim-wheel nvjit-link import problem (#141063),torch/__init__.py,https://github.com/pytorch/pytorch/pull/141063,malfet,kit1980,,,
7977a3638e6,releng,not user facing,[executorch hash update] update the pinned executorch hash (#140769),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/140769,pytorchupdatebot,pytorchbot,,,
d21738f24a6,skip,Untopiced,"Revert ""Fix torch.normal ignores default_device (#144070)""",test/test_tensor_creation_ops.py torch/__init__.py torch/utils/_device.py,,,,,,
eee7a47e94d,skip,not user facing,Support FunctionalTensor subclass in is_fake and maybe_get_fake_mode (#144719),torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/144719,soulitzer,bdhirsh,,,
e6668076535,inductor,Untopiced,[Fix]: Enable support for Arm Neon & SVE support for FP32 Gemm Wrapper (#144327),torch/_inductor/codegen/cpp_micro_gemm.py,https://github.com/pytorch/pytorch/pull/144327,nikhil-arm,cfRod,jgong5,leslie-fang-intel,
ec1c3ab3b28,inductor,not user facing,[inductor][triton] skip test_data_type_propagation if triton (#142054),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/142054,kundaMwiza,eellison,,,
e2891d43a8b,cpp_frontend,not user facing,[codemod] Remove unused-variable in caffe2/aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp +1 (#144783),aten/src/ATen/native/quantized/cpu/fbgemm_utils.cpp,https://github.com/pytorch/pytorch/pull/144783,r-barnes,albanD,malfet,,
26836912375,skip,Untopiced,[AOTI] Add a boxed_run API (#142213),benchmarks/dynamo/expected_ci_speedup_inductor_torchbench_cpu.csv test/inductor/test_aot_inductor_custom_ops.py torch/_inductor/package/package.py torch/csrc/inductor/aoti_package/model_package_loader.cpp torch/csrc/inductor/aoti_package/model_package_loader.h torch/csrc/inductor/aoti_package/pybind.cpp torch/csrc/inductor/aoti_runner/model_container_runner.cpp torch/csrc/inductor/aoti_runner/model_container_runner.h torch/csrc/inductor/aoti_runner/model_container_runner_cpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cpu.h torch/csrc/inductor/aoti_runner/model_container_runner_cuda.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cuda.h torch/csrc/inductor/aoti_runner/model_container_runner_xpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_xpu.h,https://github.com/pytorch/pytorch/pull/142213,desertfire,ezyang,,,
b4b4e574691,releng,not user facing,[CD] Enable profiling for XPU Windows nightly wheels (#144316),.ci/pytorch/windows/internal/xpu_install.bat .circleci/scripts/binary_windows_build.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/144316,chuanqi129,atalman,xuhancn,,
bdd942efd76,skip,Untopiced,"Revert ""Increase C10_COMPILE_TIME_MAX_GPUS to 128 (#144138)""",c10/core/Device.cpp c10/core/Device.h c10/cuda/CUDAMacros.h c10/cuda/CUDAStream.cpp,,,,,,
9157a748a6c,dynamo,not user facing,[MPSInductor] Add dummy properties (#144509),torch/_dynamo/device_interface.py torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/144509,malfet,jansel,,,
6f5dce30350,distributed,Untopiced,[Pipelining] Fix PP grad scaling (#144352),test/distributed/pipelining/schedule_registry.py test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py test/distributed/test_composability.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/144352,wconstab,H-Huang,,,
aa57f0c6637,distributed,not user facing,[Pipelining] Refactor common utils from test_pp_dp (#144596),test/distributed/test_composability.py,https://github.com/pytorch/pytorch/pull/144596,wconstab,H-Huang,,,
130452dad6f,distributed,not user facing,[Pipelining] fix test_schedule.py (missing destroy_process_group (#144734),test/distributed/pipelining/test_schedule.py,https://github.com/pytorch/pytorch/pull/144734,wconstab,H-Huang,,,
64829b356a2,skip,not user facing,[PrivateUse1] Support parseDispatchKey with modified PrivateUse1 (#144325),c10/core/DispatchKey.cpp,https://github.com/pytorch/pytorch/pull/144325,fmo-mt,albanD,,,
c000214826a,autograd_frontend,improvements,Allow GradientEdge as torch.autograd.backward outputs (#144744),test/test_autograd.py torch/autograd/__init__.py,https://github.com/pytorch/pytorch/pull/144744,soulitzer,albanD,,,
ea3395e4f23,skip,not user facing,[ROCm] Improvements for vectorized elementwise kernels (#143269),aten/src/ATen/cuda/jiterator.cu aten/src/ATen/native/cuda/CUDAJitLoops.cuh aten/src/ATen/native/cuda/CUDALoops.cuh aten/src/ATen/native/cuda/Dropout.cu aten/src/ATen/native/cuda/MemoryAccess.cuh aten/src/ATen/native/cuda/jit_utils.cpp aten/src/ATen/native/cuda/jit_utils.h,https://github.com/pytorch/pytorch/pull/143269,jerrymannil,jeffdaily,pruthvistony,,
8ad37ed7101,skip,not user facing,Stop ignoring mypy errors in torch/testing/_internal/common_utils.py (#144483),torch/_functorch/_activation_checkpointing/graph_info_provider.py torch/mps/profiler.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/144483,aorenste,Skylion007,,,
8c2aa0c5331,inductor,not user facing,[cutlass backend] cexpr the arg before writing to cpp file (#144714),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cpp_wrapper_gpu.py,https://github.com/pytorch/pytorch/pull/144714,henrylhtsang,ColinPeppler,chenyang78,desertfire,
825fe150248,skip,not user facing,EZ fix to make sure local pytest run succeeds in export (#144764),test/export/test_export_legacy.py,https://github.com/pytorch/pytorch/pull/144764,tugsbayasgalan,avikchaudhuri,,,
9199c79a9c8,inductor,not user facing,[Quant][Inductor][X86] Separate unary post op fusion and lowering for qconv (#144312),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/144312,Xia-Weiwen,jerryzh168,leslie-fang-intel,,
e2251fffbb0,inductor,not user facing,[MPSInductor] Add `min`/`max` to MetalExprPrinter (#144798),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144798,malfet,dcci,,,
db787181b53,sparse_frontend,Untopiced,"Back out ""[Submodule] Upgrade to Cutlass 3.6"" (#144738)",aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h third_party/cutlass,https://github.com/pytorch/pytorch/pull/144738,drisspg,huydhn,,,
d87aad68775,nn_frontend,Untopiced,[5/N] Apply Ruff fixes and pyupgrade to Python 3.9 (#144205),torch/autograd/__init__.py torch/autograd/function.py torch/autograd/functional.py torch/autograd/gradcheck.py torch/autograd/graph.py torch/autograd/profiler.py torch/autograd/profiler_util.py torch/nn/attention/__init__.py torch/nn/attention/_utils.py torch/nn/attention/flex_attention.py torch/nn/common_types.py torch/nn/functional.py torch/nn/modules/activation.py torch/nn/modules/adaptive.py torch/nn/modules/container.py torch/nn/modules/conv.py torch/nn/modules/flatten.py torch/nn/modules/lazy.py torch/nn/modules/module.py torch/nn/modules/normalization.py torch/nn/modules/padding.py torch/nn/modules/pooling.py torch/nn/modules/rnn.py torch/nn/modules/utils.py torch/nn/parallel/_functions.py torch/nn/parallel/comm.py torch/nn/parallel/data_parallel.py torch/nn/parallel/distributed.py torch/nn/parallel/parallel_apply.py torch/nn/parallel/replicate.py torch/nn/parallel/scatter_gather.py torch/nn/utils/_deprecation_utils.py torch/nn/utils/_expanded_weights/conv_utils.py torch/nn/utils/_expanded_weights/embedding_expanded_weights.py torch/nn/utils/_expanded_weights/expanded_weights_impl.py torch/nn/utils/_expanded_weights/group_norm_expanded_weights.py torch/nn/utils/_expanded_weights/instance_norm_expanded_weights.py torch/nn/utils/_expanded_weights/layer_norm_expanded_weights.py torch/nn/utils/_expanded_weights/linear_expanded_weights.py torch/nn/utils/_named_member_accessor.py torch/nn/utils/convert_parameters.py torch/nn/utils/fusion.py torch/nn/utils/parametrize.py torch/nn/utils/prune.py torch/nn/utils/rnn.py torch/nn/utils/stateless.py,https://github.com/pytorch/pytorch/pull/144205,cyyever,albanD,,,
6a5f895e549,optim,bc breaking,Removed unused _RequiredParameter (#144771),torch/optim/optimizer.py,https://github.com/pytorch/pytorch/pull/144771,dmpiergiacomo,,,,
48f7e7c3781,quantization,Untopiced,[torch][ao][EASY] Change print to log in numeric debugger to avoid large output (#144790),torch/ao/quantization/pt2e/_numeric_debugger.py,https://github.com/pytorch/pytorch/pull/144790,dulinriley,tarun292,,,
18786c65e5c,dynamo,not user facing,[BE] Extend `test_remove_no_ops` (#144795),test/inductor/test_torchinductor.py torch/_dynamo/device_interface.py,https://github.com/pytorch/pytorch/pull/144795,malfet,Skylion007,,,
9610a22e94d,skip,not user facing,Fix FakeTensor device creation for MPS (#144796),test/inductor/test_mps_basic.py torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/144796,malfet,ezyang,,,
d2ca8163c0a,inductor,not user facing,[MPSInductor] Support `abs` in MetalPrintExpr (#144826),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144826,malfet,dcci,,,
e0bbff6019b,distributed,Untopiced,[c10d][ez] Add comments to the end of Macro for better readability (#144789),torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/144789,fduwjj,c-p-i-o,,,
9cd6f46130c,dynamo,not user facing,[ca] raise error message on AOT Autograd caching (#144595),torch/_dynamo/compiled_autograd.py,https://github.com/pytorch/pytorch/pull/144595,xmfan,bdhirsh,,,
ae7df51232d,distributed,bug fixes,[c10d] Fix CudaEventCache for dangling references (#144496),test/cpp/c10d/ProcessGroupNCCLTest.cpp test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/144496,fduwjj,kwen2501,,,
79312ddb736,distributed,Untopiced,[PP] Don't allow for num_microbatches > num_stages for single stage schedules (#144702),torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/144702,H-Huang,kwen2501,,,
7d71ddbe5d7,dynamo,not user facing,"Add non_c_binding torch functions to allowlist for AOTAutogradCache, confirm no special handlers for them (#144802)",test/dynamo/test_trace_rules.py torch/_dynamo/trace_rules.py torch/_functorch/_aot_autograd/autograd_cache.py,https://github.com/pytorch/pytorch/pull/144802,jamesjwu,bdhirsh,,,
326c7cae287,skip,not user facing,Fix global namespace pollution in ATen/Dispatch.h (#138626),aten/src/ATen/Dispatch.h,https://github.com/pytorch/pytorch/pull/138626,slyfox3,malfet,,,
b801210035c,inductor,not user facing,"Restore support for other types of async_compile pools (spawn, fork) (#144491)",test/inductor/test_async_compile.py torch/_inductor/async_compile.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/144491,masnesral,haifeng-jin,jansel,,
69b883d7ace,cpp_frontend,Untopiced,Remove C10_EMBEDDED (#144808),c10/util/BFloat16.h c10/util/Half.h,https://github.com/pytorch/pytorch/pull/144808,swolchok,janeyx99,malfet,,
6ba53a5f1c5,skip,not user facing,[AMD] De-noise tf32 warnings (#144797),aten/src/ATen/Context.cpp,https://github.com/pytorch/pytorch/pull/144797,xw285cornell,jianyuh,leitian,yoyoyocmu,
d9d7cca009b,dynamo,bug fixes,make eval_frame safe (#141357),torch/csrc/dynamo/eval_frame.c,https://github.com/pytorch/pytorch/pull/141357,nullhook,jansel,,,
e263f0af237,fx,not user facing,[BE] Make a SymbolInfo NamedTuple (#144745),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/144745,ezyang,Skylion007,avikchaudhuri,,
62ce3e6e84d,skip,not user facing,refresh benchmarks results after recent recent regressions  (#143075),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv,https://github.com/pytorch/pytorch/pull/143075,laithsakka,bobrenjc93,huydhn,,
7e1c1e65ebf,dynamo,Untopiced,Graph freezing preparation for non-Inductor backends (#139902),test/dynamo/test_backends.py torch/_dynamo/config.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/139902,sujoysaraswati,eellison,gujinghui,masnesral,
dc8692b0eb0,skip,Untopiced,Enable s8s8s8 for qlinear with mkl-dnn (#139887),aten/src/ATen/native/quantized/cpu/qlinear.cpp test/quantization/core/test_quantized_op.py,https://github.com/pytorch/pytorch/pull/139887,annop-w,huydhn,,,
7c52c97a65f,skip,not user facing,Expose several APIs to public (torch python APIs) (#144525),torch/csrc/DynamicTypes.h torch/csrc/autograd/python_cpp_function.h torch/csrc/profiler/python/combined_traceback.h torch/csrc/utils/object_ptr.cpp torch/csrc/utils/tensor_new.h,https://github.com/pytorch/pytorch/pull/144525,dilililiwhy,albanD,cyyever,,
154185dcd0e,skip,Untopiced,"Revert ""Removed unused _RequiredParameter (#144771)""",torch/optim/optimizer.py,,,,,,
2bc18a90554,skip,not user facing,restore rng generation for fbcode (#144819),aten/src/ATen/core/DistributionsHelper.h aten/src/ATen/native/cuda/DistributionTemplates.h aten/src/ATen/test/rng_test.h test/test_tensor_creation_ops.py,https://github.com/pytorch/pytorch/pull/144819,ngimel,kit1980,malfet,,
c7a95991006,quantization,Untopiced,Handle meta tensors in FX quantization (#144726),torch/ao/quantization/quantize.py,https://github.com/pytorch/pytorch/pull/144726,kausv,iamzainhuda,janeyx99,,
d782e46a361,quantization,Untopiced,[BE] typing for decorators - library (#138969),torch/_export/passes/replace_quantized_ops_with_standard_ops_pass.py torch/_higher_order_ops/auto_functionalize.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/invoke_subgraph.py torch/_higher_order_ops/out_dtype.py torch/_higher_order_ops/prim_hop_base.py torch/ao/quantization/fx/_decomposed.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py torch/distributed/fsdp/_fully_shard/_fully_shard.py torch/fx/experimental/migrate_gradual_types/constraint_generator.py torch/library.py torch/nn/attention/experimental/_paged_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/138969,aorenste,zou3519,,,
0dca7568320,releng,not user facing,"Revert ""Upload METADATA file with whl binaries (#143677)"" (#144706)",.circleci/scripts/binary_upload.sh .lintrunner.toml scripts/release/upload_metadata_file.py,https://github.com/pytorch/pytorch/pull/144706,clee2000,atalman,janeyx99,malfet,
c07dc64017d,dataloader_frontend,Untopiced,Update pin memory related APIs to not pass 'device' argument (#131858),test/test_cpp_extensions_open_device_registration.py test/test_dataloader.py test/test_nestedtensor.py torch/_tensor_docs.py torch/distributed/fsdp/_flat_param.py torch/distributed/fsdp/_runtime_utils.py torch/storage.py torch/utils/data/_utils/pin_memory.py torch/utils/data/dataloader.py,https://github.com/pytorch/pytorch/pull/131858,wizzniu,albanD,,,
d065e8a9de7,skip,not user facing,[ez] add lint commits to .git-blame-ignore-revs (#144576),.git-blame-ignore-revs,https://github.com/pytorch/pytorch/pull/144576,PaliC,janeyx99,,,
443de667b1e,skip,Untopiced,"Revert ""Enable s8s8s8 for qlinear with mkl-dnn (#139887)""",aten/src/ATen/native/quantized/cpu/qlinear.cpp test/quantization/core/test_quantized_op.py,,,,,,
ee8f833d133,dynamo,not user facing,Undo leading underscore on ctx for breakpoint (#144864),torch/_dynamo/comptime.py,https://github.com/pytorch/pytorch/pull/144864,ezyang,Skylion007,,,
7e80758efc0,skip,not user facing,[CUDAGraph][Docs] add `cuda` to `torch.randn` (#144793),docs/source/torch.compiler_cudagraph_trees.rst,https://github.com/pytorch/pytorch/pull/144793,BoyuanFeng,eellison,,,
069419569d0,skip,not user facing,[PagedAttention] Support different input position for each batch index (#144693),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/nn/attention/experimental/_paged_attention.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/144693,BoyuanFeng,drisspg,,,
6ac0616504d,nn_frontend,Untopiced,[ROCm] hipblaslt rowwise f8 gemm (#144432),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDABlas.h aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp cmake/Dependencies.cmake cmake/public/LoadHIP.cmake test/test_matmul_cuda.py,https://github.com/pytorch/pytorch/pull/144432,jeffdaily,drisspg,,,
a6763b7b81c,skip,not user facing,[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/144441,eqy,Chillee,malfet,,
898a90c6bb9,dynamo,Untopiced,[dynamo][hop] Introduce FlexAttentionBackwardHighOrderVariable (#144533),test/inductor/test_compiled_autograd.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/144533,xmfan,zou3519,,,
834086c0230,export,Untopiced,[export] Load side info about pos/kw argument kind for serialization. (#144686),test/export/test_export.py test/export/test_serialize.py torch/_export/serde/export_schema.thrift torch/_export/serde/schema.py torch/_export/serde/schema.yaml torch/_export/serde/serialize.py torch/csrc/utils/generated_serialization_types.h,https://github.com/pytorch/pytorch/pull/144686,zhxchen17,angelayi,,,
b410378d93b,composability,Untopiced,Register nonzero for meta device for FBLSim (#144727),test/test_meta.py torch/_meta_registrations.py torch/fx/experimental/_config.py,https://github.com/pytorch/pytorch/pull/144727,lurunming,ezyang,,,
904641769e7,inductor,not user facing,[MPSInductor] Implement `pow()` (#144827),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144827,malfet,dcci,jansel,,
d812fdd4901,export,Untopiced,fix as_bool serde (#144791),test/export/test_export.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/144791,avikchaudhuri,pianpwk,,,
b88dcb4835c,dynamo,not user facing,dynamo: Don't crash when tracing a missing attr on a constant. (#144593),test/dynamo/test_compile.py test/dynamo_expected_failures/TestProfiler.test_profiler_correlation_id torch/_dynamo/variables/constant.py,https://github.com/pytorch/pytorch/pull/144593,c00w,jansel,,,
05095a45f2e,releng,not user facing,Fix the wrong artifact in remaining workflows (#144812),.github/workflows/inductor-micro-benchmark-x86.yml .github/workflows/inductor-micro-benchmark.yml .github/workflows/inductor-perf-compare.yml .github/workflows/torchbench.yml,https://github.com/pytorch/pytorch/pull/144812,huydhn,clee2000,,,
d62b3979dad,skip,not user facing,cpp_wrapper: Move #includes to per-device header files (#143909),.lintrunner.toml setup.py torch/_inductor/codecache.py torch/_inductor/codegen/aoti_runtime/implementation.cpp torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/device_op_overrides.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/wrapper.py torch/_inductor/codegen/xpu/device_op_overrides.py torch/csrc/inductor/aoti_include/array_ref.h torch/csrc/inductor/aoti_include/common.h torch/csrc/inductor/aoti_include/cpu.h torch/csrc/inductor/aoti_include/cuda.h torch/csrc/inductor/aoti_include/xpu.h torch/csrc/inductor/array_ref_impl.h torch/csrc/inductor/cpp_wrapper/array_ref.h torch/csrc/inductor/cpp_wrapper/common.h torch/csrc/inductor/cpp_wrapper/cpu.h torch/csrc/inductor/cpp_wrapper/cuda.h torch/csrc/inductor/cpp_wrapper/device_internal/cpu.h torch/csrc/inductor/cpp_wrapper/device_internal/cuda.h torch/csrc/inductor/cpp_wrapper/device_internal/xpu.h torch/csrc/inductor/cpp_wrapper/xpu.h,https://github.com/pytorch/pytorch/pull/143909,benjaminglass1,desertfire,,,
c7b2f7dd142,skip,Untopiced,Add generator parameter to rand*_like functions (#136780),aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/native_functions.yaml test/expect/HasDecompTest.test_has_decomposition.expect test/test_tensor_creation_ops.py torch/_subclasses/fake_impls.py torch/_torch_docs.py torch/overrides.py,https://github.com/pytorch/pytorch/pull/136780,moltinginstar,ezyang,,,
0f051eaf66b,skip,Untopiced,"Revert ""Fix global namespace pollution in ATen/Dispatch.h (#138626)""",aten/src/ATen/Dispatch.h,,,,,,
4e1834f5f3b,cuda,Untopiced,use cooperative schedule in scaled_mm for fast_accum=false (#144809),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/144809,ngimel,drisspg,,,
7265dc0622d,skip,Untopiced,Enable s8s8s8 for qlinear with mkl-dnn (#139887),aten/src/ATen/native/quantized/cpu/qlinear.cpp test/quantization/core/test_quantized_op.py,https://github.com/pytorch/pytorch/pull/139887,annop-w,huydhn,,,
fb4b5a92995,onnx,improvements,[ONNX] Use python_dispatcher in type promotion (#144801),torch/onnx/_internal/fx/passes/type_promotion.py,https://github.com/pytorch/pytorch/pull/144801,justinchuby,titaiwangms,,,
2645fc45b13,skip,not user facing,export AOTI_TORCH_EXPORT on Windows. (#140030),CMakeLists.txt torch/csrc/inductor/aoti_torch/c/shim.h,https://github.com/pytorch/pytorch/pull/140030,xuhancn,desertfire,jgong5,malfet,
4831f89790d,composability,Untopiced,"support numbers as tensors for aten.copy(Tensor, Tensor) (#141161)",test/dynamo/test_repros.py torch/csrc/utils/python_arg_parser.cpp,https://github.com/pytorch/pytorch/pull/141161,bdhirsh,ezyang,,,
d7f45fc575f,composability,Untopiced,dynamic shape support for interpolate(antialias=True) backward (#141198),test/functorch/test_aotdispatch.py test/test_fake_tensor.py torch/_meta_registrations.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/141198,bdhirsh,Chillee,ezyang,,
774f21a3707,export,Untopiced,[export] handle buffer/input mutations for joint-graph (#144806),test/export/test_experimental.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/144806,pianpwk,avikchaudhuri,zhxchen17,,
ee97d80be2b,jit,Untopiced,Apply Ruff fixes and pyupgrade to torch/jit (#144208),torch/jit/__init__.py torch/jit/_async.py torch/jit/_builtins.py torch/jit/_dataclass_impls.py torch/jit/_decompositions.py torch/jit/_freeze.py torch/jit/_fuser.py torch/jit/_ir_utils.py torch/jit/_monkeytype_config.py torch/jit/_passes/_property_propagation.py torch/jit/_recursive.py torch/jit/_script.py torch/jit/_serialization.py torch/jit/_shape_functions.py torch/jit/_state.py torch/jit/_trace.py torch/jit/annotations.py torch/jit/frontend.py torch/jit/generate_bytecode.py torch/jit/unsupported_tensor_ops.py,https://github.com/pytorch/pytorch/pull/144208,cyyever,davidberard98,,,
b90231a1896,inductor,not user facing,[inductor][BE] don't try/except ImportError for AttrsDescriptor versions (#144807),torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/144807,davidberard98,ezyang,,,
926f9056a9b,dynamo,not user facing,speculation_log: Raise a unique error for divergence issues (#144785),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/144785,c00w,ezyang,,,
c8bcb22e5fb,skip,not user facing,Default Copies are not vectorized in v3.6.0 of cutlass (#144837),aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_kernel.h aten/src/ATen/native/transformers/cuda/flash_attn/kernel_traits.h,https://github.com/pytorch/pytorch/pull/144837,drisspg,Skylion007,eqy,,
64928511258,dynamo,not user facing,symbolic_convert: Don't fail when we hit a undefined name (#144784),test/dynamo/test_compile.py test/dynamo_expected_failures/TestAutogradFunctionVmapAPICPU.test_has_vmap_staticmethod_and_has_generate_vmap_rule_cpu test/dynamo_expected_failures/TestAutogradFunctionVmapAPICPU.test_no_vmap_staticmethod_and_no_generate_vmap_rule_cpu torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/144784,c00w,jansel,williamwen42,,
d595b960597,skip,Untopiced,"Revert ""restore rng generation for fbcode (#144819)""",aten/src/ATen/core/DistributionsHelper.h aten/src/ATen/native/cuda/DistributionTemplates.h aten/src/ATen/test/rng_test.h test/test_tensor_creation_ops.py,,,,,,
05505771a0f,inductor,not user facing,[MPSInductor] Properly convert index (#144917),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144917,malfet,dcci,,,
41ec2e8d3e7,inductor,not user facing,[MPSInductor] Fix codegen regression (#144924),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/144924,malfet,dcci,,,
843627b7b19,distributed,Untopiced,Remove unnecessary once flag usage (#143255),aten/src/ATen/core/CachingHostAllocator.h aten/src/ATen/detail/CUDAHooksInterface.cpp aten/src/ATen/detail/HIPHooksInterface.cpp aten/src/ATen/detail/HPUHooksInterface.cpp aten/src/ATen/detail/IPUHooksInterface.cpp aten/src/ATen/detail/MAIAHooksInterface.cpp aten/src/ATen/detail/MPSHooksInterface.cpp aten/src/ATen/detail/MTIAHooksInterface.cpp aten/src/ATen/detail/XPUHooksInterface.cpp torch/csrc/autograd/engine.cpp torch/csrc/autograd/engine.h torch/csrc/distributed/c10d/socket.cpp,https://github.com/pytorch/pytorch/pull/143255,cyyever,albanD,,,
1230de4c1b1,inductor,not user facing,[Quant][Inductor][X86] Separate binary post op fusion and lowering for qconv (#144318),test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/144318,Xia-Weiwen,jerryzh168,leslie-fang-intel,,
52a620845bf,skip,not user facing,OpenReg: Use device agnostic API (#144840),test/cpp_extensions/open_registration_extension/pytorch_openreg/__init__.py test/cpp_extensions/open_registration_extension/test/test_openreg.py,https://github.com/pytorch/pytorch/pull/144840,Zhenbin-8,albanD,,,
9e568cbaa22,skip,not user facing,Add tests for different dtypes with max autotune (#144721),test/inductor/test_max_autotune.py,https://github.com/pytorch/pytorch/pull/144721,exclamaforte,cpuhrsch,etaf,,
e3c4d1b7d6e,skip,not user facing,[c10d][fr] Fix the bug when we still mark mismatch when there are match case (#144916),tools/flight_recorder/components/builder.py,https://github.com/pytorch/pytorch/pull/144916,fduwjj,c-p-i-o,,,
49bdc418be4,nn_frontend,improvements,Add strict kwarg to `nn.Module.set_submodule` and fix bug for non dot delineated strings (#143455),test/test_nn.py torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/143455,mariovas3,mikaylagawarecki,,,
0b17c09893c,skip,not user facing,restore rng generation for fbcode (#144819),aten/src/ATen/core/DistributionsHelper.h aten/src/ATen/native/cuda/DistributionTemplates.h aten/src/ATen/test/rng_test.h test/test_tensor_creation_ops.py,https://github.com/pytorch/pytorch/pull/144819,ngimel,kit1980,malfet,,
7d8c087e24a,distributed,Untopiced,[Pipelining] Improve shape inference debug logging (#144929),torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/144929,wconstab,H-Huang,,,
57d5659c3ba,distributed,not user facing,XFAIL test_save_load_checkpoint (#144927),test/distributed/test_c10d_ucc.py,https://github.com/pytorch/pytorch/pull/144927,huydhn,kit1980,malfet,,
0c0583254ee,inductor,not user facing,[inductor] fix index.Tensor fallback (#144736),test/inductor/test_torchinductor.py test/test_meta.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/144736,shunting314,jansel,,,
1c290912e4a,skip,Untopiced,"Revert ""Add tests for different dtypes with max autotune (#144721)""",test/inductor/test_max_autotune.py,,,,,,
f31452268bf,skip,not user facing,Add flop formula for _scaled_mm (#144872),docs/source/conf.py test/allowlist_for_publicAPI.json test/test_flop_counter.py torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/144872,lw,vkuzo,,,
6470b0ea6f0,releng,not user facing,Update torch-xpu-ops commit pin (#144739),.github/workflows/xpu.yml third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/144739,xytintel,EikanWang,malfet,,
6559374494c,skip,Untopiced,"Revert ""Add flop formula for _scaled_mm (#144872)""",docs/source/conf.py test/allowlist_for_publicAPI.json test/test_flop_counter.py torch/utils/flop_counter.py,,,,,,
241a8a101b3,cpp_frontend,Untopiced,Fix erroneous at_vreinterpretq_u16_bf16 call (#144883),aten/src/ATen/cpu/vec/vec128/vec128_bfloat16_neon.h,https://github.com/pytorch/pytorch/pull/144883,swolchok,Skylion007,malfet,tinglvv,
3d29de3ac82,inductor,not user facing,"[aoti] Deduplicate ""V.aot_compilation"" and ""V.graph.aot_mode"" flags. [1/n] (#144709)",torch/_inductor/compile_fx.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/144709,zhxchen17,angelayi,desertfire,,
1b34665767f,skip,not user facing,[mps] Massage test_full_truncation to work only on the supported dtypes. (#144877),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/144877,dcci,jansel,malfet,,
727ae133182,skip,Untopiced,Cholesky mps implementation (#144193),aten/src/ATen/native/mps/kernels/LinearAlgebra.metal aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py tools/autograd/derivatives.yaml torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/144193,Isalia20,malfet,,,
519269a4158,releng,Untopiced,[BE] - Remove conda test and upload scripts and env variables from Workflows Part 1 (#144870),.ci/pytorch/check_binary.sh .ci/pytorch/run_tests.sh .ci/pytorch/windows/internal/smoke_test.bat .circleci/scripts/binary_populate_env.sh .circleci/scripts/binary_upload.sh .github/templates/linux_binary_build_workflow.yml.j2 .github/templates/macos_binary_build_workflow.yml.j2 .github/templates/upload.yml.j2 .github/templates/windows_binary_build_workflow.yml.j2 .github/workflows/_binary-build-linux.yml .github/workflows/_binary-test-linux.yml .github/workflows/_binary-upload.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-main.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/144870,atalman,malfet,,,
53256edff92,fx,Untopiced,[export] Support module inputs for non strict mode. (#143925),test/export/test_export.py torch/_export/non_strict_utils.py torch/_export/passes/lift_constants_pass.py torch/_export/serde/serialize.py torch/_export/utils.py torch/export/_trace.py torch/export/_unlift.py torch/export/exported_program.py torch/fx/_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/143925,zhxchen17,tugsbayasgalan,,,
13d35ea67a0,skip,not user facing,[BE] Add missing throw of `std::runtime_error` in scrc/cuda/utils.cpp (#144962),torch/csrc/cuda/utils.cpp,https://github.com/pytorch/pytorch/pull/144962,rec,Skylion007,amjames,malfet,
829c4570ca9,skip,Untopiced,"Revert ""[mps] Massage test_full_truncation to work only on the supported dtypes. (#144877)""",test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,,,,,,
ad15436db69,releng,not user facing,Fix `pt2-bug-report.yml` formatting (#144987),.github/ISSUE_TEMPLATE/pt2-bug-report.yml,https://github.com/pytorch/pytorch/pull/144987,malfet,Skylion007,zou3519,,
e32d2bf8531,optim,docs,Document decoupled_weight_decay for Adam for consistency with N/RAdam (#144984),torch/optim/adam.py,https://github.com/pytorch/pytorch/pull/144984,janeyx99,albanD,,,
3004b657f05,inductor,not user facing,[Inductor][FlexAttention] Supports dynamic shapes with custom kernel options (#144938),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/144938,yanboliang,drisspg,,,
31a73eb7124,cuda,Untopiced,fix acquire pattern in topk (#144945),aten/src/ATen/native/cuda/TensorTopK.cu,https://github.com/pytorch/pytorch/pull/144945,ngimel,Skylion007,,,
cf28d613f1e,releng,not user facing,Allow ROCm runner to upload benchmark results if found (#144710),.github/workflows/_rocm-test.yml,https://github.com/pytorch/pytorch/pull/144710,huydhn,jeffdaily,kit1980,,
7c3aa1da1c9,python_frontend,improvements,Prevent _legacy_load with weights_only=True (#144914),test/quantization/bc/test_backward_compatibility.py test/test_serialization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/144914,mikaylagawarecki,albanD,malfet,,
b8abdaa286f,skip,not user facing,Make functionalization `ViewMeta` serializable with pickle. (#143712),.gitignore BUILD.bazel aten/src/ATen/FunctionalStorageImpl.cpp aten/src/ATen/FunctionalStorageImpl.h aten/src/ATen/FunctionalTensorWrapper.cpp aten/src/ATen/FunctionalTensorWrapper.h aten/src/ATen/FunctionalizeFallbackKernel.cpp aten/src/ATen/FunctionalizeFallbackKernel.h aten/src/ATen/templates/FunctionalInverses.h aten/src/ATen/templates/RegisterFunctionalization.cpp aten/src/ATen/templates/ViewMetaClasses.cpp aten/src/ATen/templates/ViewMetaClasses.h aten/src/ATen/templates/ViewMetaClassesPythonBinding.cpp build.bzl build_variables.bzl caffe2/CMakeLists.txt test/dynamo/test_aot_autograd_cache.py test/functorch/test_aotdispatch.py tools/setup_helpers/generate_code.py torch/_C/__init__.pyi.in torch/_C/_functionalization.pyi torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/functional_utils.py torch/_functorch/_aot_autograd/input_output_analysis.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/csrc/Module.cpp torch/csrc/autograd/python_torch_functions_manual.cpp torch/csrc/functionalization/Module.cpp torch/csrc/functionalization/Module.h torchgen/api/functionalization.py torchgen/api/types/signatures.py torchgen/gen.py torchgen/gen_functionalization_type.py,https://github.com/pytorch/pytorch/pull/143712,ysiraichi,bdhirsh,,,
3908be676c7,optim,bug fixes,Fix loading older state_dict into AdamW after refactor (#144972),test/test_optim.py torch/optim/adamw.py,https://github.com/pytorch/pytorch/pull/144972,janeyx99,albanD,,,
aad5f600ff0,skip,not user facing,[mps] Massage test_full_truncation to work only on the supported dtypes. (#144877),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/144877,dcci,jansel,malfet,,
a9bfc5f70ca,nn_frontend,not user facing,Fix boundary conditions for hardswish backward (#143899),aten/src/ATen/native/cpu/Activation.cpp test/test_nn.py,https://github.com/pytorch/pytorch/pull/143899,CaoE,ezyang,jgong5,,
577708e6dea,skip,not user facing,Unskipped multiple inductor tests for ROCm (#143581),test/inductor/test_flex_decoding.py test/inductor/test_inductor_freezing.py test/inductor/test_max_autotune.py test/inductor/test_memory_planning.py test/inductor/test_pattern_matcher.py test/inductor/test_select_algorithm.py test/inductor/test_triton_kernels.py,https://github.com/pytorch/pytorch/pull/143581,iupaikov-amd,jeffdaily,,,
3a5bf0bc361,skip,not user facing,expose extra torch_python apis (#144746),torch/csrc/Storage.h torch/csrc/tensor/python_tensor.h torch/csrc/utils/device_lazy_init.h,https://github.com/pytorch/pytorch/pull/144746,garfield1997,albanD,,,
4ea189422de,skip,Untopiced,"Revert ""[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441)""",aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,,,,,,
aa4a1ff0273,skip,Untopiced,"Revert ""Prevent _legacy_load with weights_only=True (#144914)""",test/quantization/bc/test_backward_compatibility.py test/test_serialization.py torch/serialization.py,,,,,,
46b92c025d7,skip,Untopiced,"Revert ""Cholesky mps implementation (#144193)""",aten/src/ATen/native/mps/kernels/LinearAlgebra.metal aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py tools/autograd/derivatives.yaml torch/_inductor/lowering.py,,,,,,
0e6d44df3f1,inductor,not user facing,Add heuristic to fail block pointer match early (#144681),torch/_inductor/codegen/block_analysis.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/144681,kundaMwiza,jansel,,,
95c363cc9b3,dynamo,not user facing,dynamo: Don't crash with internal error if getattr on a tensor fails (#144817),test/dynamo/test_compile.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/144817,c00w,jansel,williamwen42,,
7c7bcb1e334,skip,not user facing,update IS_JETSON check (#144725),torch/testing/_internal/common_cuda.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/144725,Fuzzkatt,eqy,,,
a33e02cb265,releng,not user facing,[executorch hash update] update the pinned executorch hash (#144813),.ci/docker/ci_commit_pins/executorch.txt .ci/docker/common/install_executorch.sh,https://github.com/pytorch/pytorch/pull/144813,pytorchupdatebot,huydhn,pytorchbot,,
181d93b4f26,skip,not user facing,[BE] Move `is_device_supported` to helper function (#144971),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/144971,malfet,Skylion007,dcci,jansel,
171fb7f358b,skip,not user facing,easy: Fix missing tab in test/dynamo/test_compile.py (#145013),test/dynamo/test_compile.py,https://github.com/pytorch/pytorch/pull/145013,c00w,jansel,masnesral,,
d2a77f48c9d,skip,not user facing,Add tests for different dtypes with max autotune (#144721),test/inductor/test_max_autotune.py,https://github.com/pytorch/pytorch/pull/144721,exclamaforte,cpuhrsch,etaf,,
288d67d6c2e,inductor,not user facing,[inductor] [bug fix] align `avg_pool` with eager when handling `uint` (#144313),test/inductor/test_torchinductor.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/144313,shaoyuyoung,jansel,jgong5,,
fea9d18d5a0,skip,not user facing,[Utilization Log] Concurrently collect aggregate data during the output interval (#143235),tools/stats/monitor.py,https://github.com/pytorch/pytorch/pull/143235,yangw-dev,huydhn,,,
573fc42f255,distributed,not user facing,[BE][CP] Use run_subtests instead of parametrize (#143240),test/distributed/tensor/test_attention.py,https://github.com/pytorch/pytorch/pull/143240,fegin,XilunWu,,,
c3fcb3606dc,dynamo,not user facing,Profile compile_inner instead of _compile_inner (#144930),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/144930,laithsakka,jamesjwu,,,
2ea394ba299,mobile,Untopiced,Modernize C++ code (#144603),aten/src/ATen/core/boxing/impl/boxing.h aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h aten/src/ATen/core/op_registration/infer_schema.h aten/src/ATen/core/op_registration/op_registration.cpp aten/src/ATen/core/op_registration/op_registration.h aten/src/ATen/cpu/vec/sve/vec_common_sve.h aten/src/ATen/cpu/vec/sve/vec_double.h aten/src/ATen/cpu/vec/sve/vec_float.h aten/src/ATen/cpu/vec/sve/vec_int.h aten/src/ATen/cpu/vec/sve/vec_qint.h aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h aten/src/ATen/cpu/vec/vec512/vec512_double.h aten/src/ATen/cpu/vec/vec512/vec512_float.h aten/src/ATen/cpu/vec/vec512/vec512_int.h aten/src/ATen/cuda/detail/LazyNVRTC.cpp aten/src/ATen/native/TensorShape.cpp aten/src/ATen/native/UpSample.h aten/src/ATen/native/ao_sparse/quantized/cpu/fbgemm_utils.h aten/src/ATen/native/metal/MetalTensorImplStorage.h aten/src/ATen/native/mkl/LinearAlgebra.cpp aten/src/ATen/native/mkl/SparseCsrLinearAlgebra.cpp aten/src/ATen/native/mkl/SpectralOps.cpp aten/src/ATen/native/mkldnn/BinaryOps.cpp aten/src/ATen/native/mkldnn/Conv.cpp aten/src/ATen/native/mkldnn/Copy.cpp aten/src/ATen/native/mkldnn/Gelu.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Matmul.cpp aten/src/ATen/native/mkldnn/MkldnnTensorMath.cpp aten/src/ATen/native/mkldnn/Normalization.cpp aten/src/ATen/native/mkldnn/Pooling.cpp aten/src/ATen/native/mkldnn/Prelu.cpp aten/src/ATen/native/mkldnn/Relu.cpp aten/src/ATen/native/mkldnn/SoftMax.cpp aten/src/ATen/native/mkldnn/TensorShape.cpp aten/src/ATen/native/mkldnn/UnaryOps.cpp aten/src/ATen/native/mps/MetalShaderLibrary.h aten/src/ATen/native/quantized/cpu/qnnpack/test/test_utils.h aten/src/ATen/native/ufunc/add.h aten/src/ATen/native/vulkan/ops/Clone.cpp torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/144603,cyyever,malfet,,,
5e6e6200bfa,skip,Untopiced,"Revert ""[dynamo][dicts] Consolidate dict(..) construction (#144342)""",test/dynamo/test_higher_order_ops.py test/functorch/test_control_flow.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/user_defined.py,,,,,,
94c0f153025,skip,Untopiced,"Revert ""cpp_wrapper: Move #includes to per-device header files (#143909)""",.lintrunner.toml setup.py torch/_inductor/codecache.py torch/_inductor/codegen/aoti_runtime/implementation.cpp torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/device_op_overrides.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/wrapper.py torch/_inductor/codegen/xpu/device_op_overrides.py torch/csrc/inductor/aoti_include/array_ref.h torch/csrc/inductor/aoti_include/common.h torch/csrc/inductor/aoti_include/cpu.h torch/csrc/inductor/aoti_include/cuda.h torch/csrc/inductor/aoti_include/xpu.h torch/csrc/inductor/array_ref_impl.h torch/csrc/inductor/cpp_wrapper/array_ref.h torch/csrc/inductor/cpp_wrapper/common.h torch/csrc/inductor/cpp_wrapper/cpu.h torch/csrc/inductor/cpp_wrapper/cuda.h torch/csrc/inductor/cpp_wrapper/device_internal/cpu.h torch/csrc/inductor/cpp_wrapper/device_internal/cuda.h torch/csrc/inductor/cpp_wrapper/device_internal/xpu.h torch/csrc/inductor/cpp_wrapper/xpu.h,,,,,,
42c64bd35c2,skip,not user facing,[MPSInductor] More is_dtype_supported gating (#144981),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/144981,malfet,dcci,,,
6c713ccb5e0,skip,Untopiced,"Revert ""Make functionalization `ViewMeta` serializable with pickle. (#143712)""",.gitignore BUILD.bazel aten/src/ATen/FunctionalStorageImpl.cpp aten/src/ATen/FunctionalStorageImpl.h aten/src/ATen/FunctionalTensorWrapper.cpp aten/src/ATen/FunctionalTensorWrapper.h aten/src/ATen/FunctionalizeFallbackKernel.cpp aten/src/ATen/FunctionalizeFallbackKernel.h aten/src/ATen/templates/FunctionalInverses.h aten/src/ATen/templates/RegisterFunctionalization.cpp aten/src/ATen/templates/ViewMetaClasses.cpp aten/src/ATen/templates/ViewMetaClasses.h aten/src/ATen/templates/ViewMetaClassesPythonBinding.cpp build.bzl build_variables.bzl caffe2/CMakeLists.txt test/dynamo/test_aot_autograd_cache.py test/functorch/test_aotdispatch.py tools/setup_helpers/generate_code.py torch/_C/__init__.pyi.in torch/_C/_functionalization.pyi torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/functional_utils.py torch/_functorch/_aot_autograd/input_output_analysis.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/csrc/Module.cpp torch/csrc/autograd/python_torch_functions_manual.cpp torch/csrc/functionalization/Module.cpp torch/csrc/functionalization/Module.h torchgen/api/functionalization.py torchgen/api/types/signatures.py torchgen/gen.py torchgen/gen_functionalization_type.py,,,,,,
3afc5170d44,sparse_frontend,Untopiced,[Submodule] Upgrade to Cutlass 3.6 part deux (#144911),aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/sparse/cuda/ComputeSparseTile.h third_party/cutlass,https://github.com/pytorch/pytorch/pull/144911,drisspg,Skylion007,eqy,,
5d54e7b812a,distributed,not user facing,"[Pipelining] move scale_grads to base class, add docs (#144833)",torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/144833,wconstab,H-Huang,,,
60771024155,distributed,not user facing,[DSD][BE] Rewrite some tests to remove `with_comms` (#143241),test/distributed/checkpoint/test_state_dict.py,https://github.com/pytorch/pytorch/pull/143241,fegin,XilunWu,mori360,,
45e6647268e,distributed,Untopiced,[FSDP2] Make post-backward condition more robust (#144781),test/distributed/_composable/fsdp/test_fully_shard_comm.py torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py torch/distributed/fsdp/_fully_shard/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/144781,awgu,fegin,,,
55b0819beec,skip,Untopiced,"Revert ""Add tests for different dtypes with max autotune (#144721)""",test/inductor/test_max_autotune.py,,,,,,
a61a65ff822,dynamo,not user facing,[MPSInductor] Add `Worker.current_device` method (#145023),test/inductor/test_mps_basic.py torch/_dynamo/device_interface.py,https://github.com/pytorch/pytorch/pull/145023,malfet,dcci,,,
176cde62404,distributed,not user facing,Use torch with statement in torch distributed module (#144951),torch/_C/__init__.pyi.in torch/distributed/_symmetric_memory/__init__.py torch/distributed/algorithms/ddp_comm_hooks/mixed_precision_hooks.py torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py,https://github.com/pytorch/pytorch/pull/144951,guangyey,albanD,kwen2501,,
ba3f1c29eef,distributed,Untopiced,[dcp] Add extension mechanism (#143358),torch/distributed/checkpoint/__init__.py torch/distributed/checkpoint/_extension.py torch/testing/_internal/distributed/checkpoint_utils.py,https://github.com/pytorch/pytorch/pull/143358,mhorowitz,saumishr,,,
9c909bf3bb1,distributed,Untopiced,[dcp] Integrate stream extensions into DCP impl (#143359),test/distributed/checkpoint/test_dtensor_resharding.py test/distributed/checkpoint/test_file_system_checkpoint.py test/distributed/checkpoint/test_file_system_checkpoint_cpu.py torch/distributed/checkpoint/_fsspec_filesystem.py torch/distributed/checkpoint/filesystem.py,https://github.com/pytorch/pytorch/pull/143359,mhorowitz,saumishr,,,
7b56b039afe,skip,Untopiced,[dcp] Add ZStandard transformer (#143360),.ci/docker/requirements-ci.txt requirements.txt test/distributed/checkpoint/test_dtensor_resharding.py test/distributed/checkpoint/test_file_system_checkpoint.py test/distributed/checkpoint/test_file_system_checkpoint_cpu.py torch/distributed/checkpoint/_extension.py,https://github.com/pytorch/pytorch/pull/143360,mhorowitz,albanD,saumishr,,
3a50aba7d3f,dynamo,not user facing,[dynamo] add option to not skip on empty graph (#144885),test/dynamo/test_repros.py torch/_dynamo/config.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/144885,williamwen42,jansel,,,
5d02575aa15,dynamo,not user facing,[Trace Python dispatcher] Support torch.DispatchKey & torch.DispatchKeySet (#144439),test/dynamo/test_python_dispatcher.py torch/_dynamo/guards.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/torch.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/144439,yanboliang,zou3519,,,
43a00d73b36,dynamo,not user facing,[Trace Python Dispatcher] Support FuncTorchInterpreter (#144444),test/dynamo/test_python_dispatcher.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/144444,yanboliang,williamwen42,zou3519,,
adbbcd87d9f,skip,not user facing,OpenReg: Split Allocator (#144843),test/cpp_extensions/open_registration_extension/pytorch_openreg/_device_daemon.py,https://github.com/pytorch/pytorch/pull/144843,Zhenbin-8,albanD,,,
1d43b815085,skip,Untopiced,patch for block-wise quantization + pt2e (#144492),test/quantization/pt2e/test_quantize_pt2e.py torch/_export/passes/constant_folding.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantize_pt2e.py,https://github.com/pytorch/pytorch/pull/144492,cccclai,angelayi,billmguo,,
f13c864eda0,inductor,not user facing,Fuzzer Improvements (#144952),test/inductor/test_fuzzer.py torch/_inductor/fuzzer.py,https://github.com/pytorch/pytorch/pull/144952,exclamaforte,masnesral,,,
cfd9cc19a37,releng,not user facing,[executorch hash update] update the pinned executorch hash (#145022),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/145022,pytorchupdatebot,pytorchbot,,,
fd8e0e3e107,skip,not user facing,[mps/inductor] Introduce is_mps_backend/skip_if_mps decorators. (#145035),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/145035,dcci,jansel,,,
465a1cfe2e8,skip,not user facing,update get start xpu (#143183),docs/source/notes/get_start_xpu.rst,https://github.com/pytorch/pytorch/pull/143183,ZhaoqiongZ,EikanWang,atalman,leslie-fang-intel,
96c0dbbe97b,skip,not user facing,Enhance running pr time benchmarks locally experience. (#144838),benchmarks/dynamo/pr_time_benchmarks/benchmark_base.py benchmarks/dynamo/pr_time_benchmarks/benchmark_runner.sh benchmarks/dynamo/pr_time_benchmarks/benchmarks/benchmark_base.py benchmarks/dynamo/pr_time_benchmarks/check_results.py,https://github.com/pytorch/pytorch/pull/144838,laithsakka,huydhn,,,
a0d2c09115b,skip,not user facing,Add flop formula for _scaled_mm (#144973),test/test_flop_counter.py torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/144973,lw,jeffdaily,,,
dbed747aae2,releng,not user facing,Add Intel GPU specific CMake files to merge rules (#135110),.github/merge_rules.yaml CODEOWNERS,https://github.com/pytorch/pytorch/pull/135110,EikanWang,atalman,,,
f522502b972,skip,Untopiced,"Revert ""patch for block-wise quantization + pt2e (#144492)""",test/quantization/pt2e/test_quantize_pt2e.py torch/_export/passes/constant_folding.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantize_pt2e.py,,,,,,
b7af210d8d8,skip,not user facing,Add SM89 support for f8f8bf16_rowwise() (#144348),aten/src/ATen/native/cuda/RowwiseScaledMM.cu test/inductor/test_aot_inductor.py test/inductor/test_fp8.py test/inductor/test_max_autotune.py,https://github.com/pytorch/pytorch/pull/144348,alexsamardzic,drisspg,,,
a215e174a11,releng,not user facing,[BE] Remove conda from scripts and build files Part 2 (#145015),.github/scripts/build_triton_wheel.py scripts/read_conda_versions.sh scripts/release/anaconda-prune/prune.sh scripts/release/anaconda-prune/run.sh scripts/release/promote/conda_to_conda.sh scripts/release/restore-backup.sh setup.py,https://github.com/pytorch/pytorch/pull/145015,atalman,Skylion007,malfet,,
18eba9575f5,quantization,Untopiced,[Accelerator] Use uniform `GetAllocator` for devices in `new_qtensor` function (#144849),aten/src/ATen/quantized/Quantizer.cpp,https://github.com/pytorch/pytorch/pull/144849,Stonepia,albanD,guangyey,,
567552b98bd,skip,not user facing,fix typo in doc and import for torch._library.triton (#144882),torch/_library/__init__.py torch/_library/triton.py,https://github.com/pytorch/pytorch/pull/144882,ydwu4,zou3519,,,
18638b91fe3,inductor,not user facing,Adding more compile time logging in pad_mm (#144884),torch/_inductor/fx_passes/pad_mm.py,https://github.com/pytorch/pytorch/pull/144884,Mingming-Ding,oulgen,,,
46fbd63405c,skip,not user facing,Fix unbind_copy and add its decomposition (#134319),aten/src/ATen/native/TensorShape.cpp test/distributed/tensor/test_dtensor_ops.py test/expect/HasDecompTest.test_aten_core_operators.expect test/expect/HasDecompTest.test_has_decomposition.expect test/functorch/test_ops.py test/functorch/test_vmap.py test/test_mps.py tools/autograd/gen_variable_type.py torch/_inductor/decomposition.py torch/_prims/context.py torch/_prims_common/wrappers.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/134319,rec,amjames,eellison,,
d996d7ec135,releng,not user facing,upgrade to sccache 0.9.1 - dealing with nvcc -E correctly (#145012),.ci/docker/common/install_cache.sh,https://github.com/pytorch/pytorch/pull/145012,wdvr,malfet,,,
2ef7b686666,inductor,not user facing,"[inductor] fix TORCH_LOGS=""benchmarking"" (#144997)",torch/_inductor/runtime/benchmarking.py,https://github.com/pytorch/pytorch/pull/144997,ColinPeppler,eellison,nmacchioni,,
0eda02a94c7,python_frontend,improvements,Prevent legacy_load when weights_only=True (correctly) (#145020),test/test_serialization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/145020,mikaylagawarecki,albanD,kit1980,,
eaef6136885,skip,not user facing,Fix issue with test/nn/test_convolution:TestConvolutionNNDeviceTypeCUDA.test_conv_large_batch_1_cuda (#145067),test/nn/test_convolution.py,https://github.com/pytorch/pytorch/pull/145067,rec,Skylion007,nWEIdia,,
cac10b81902,skip,not user facing,Fix NJT OpInfo entry for nn.functional.prelu (#144582),test/test_nestedtensor.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/144582,jbschlosser,soulitzer,,,
a8ef423fed4,nested tensor_frontend,bug fixes,Fix NJT min / max backward() for non-ragged reductions (#144583),aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorUtils.cpp test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/144583,jbschlosser,soulitzer,,,
07e23653cd9,skip,not user facing,Fix RMSNorm epsilon value type for BF16 or FP16 (#142848),aten/src/ATen/native/layer_norm.cpp,https://github.com/pytorch/pytorch/pull/142848,fmo-mt,albanD,,,
64e54d5af63,distributed,not user facing,[Pipelining] Relax scale_grads assert (#145010),torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/145010,wconstab,mori360,tianyu-l,,
4e4b8592a32,skip,not user facing,parametrized test name handles class arguments (#133546),torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/133546,nmacchioni,eellison,,,
d02c396fbbf,quantization,Untopiced,add fp8 support to index_cuda (#144747),aten/src/ATen/native/cuda/IndexKernel.cu test/test_fake_tensor.py,https://github.com/pytorch/pytorch/pull/144747,danielvegamyhre,vkuzo,,,
c338dda6be9,inductor,not user facing,fix test_rng bisector test (#143662),torch/_inductor/inductor_prims.py,https://github.com/pytorch/pytorch/pull/143662,eellison,zou3519,,,
712d9882d22,skip,not user facing,Skip test responsible for causing flakiness (#145109),test/test_python_dispatch.py,https://github.com/pytorch/pytorch/pull/145109,zou3519,williamwen42,,,
c434a64f31e,skip,not user facing,Delete torch._library.register_functional_op (#145110),test/test_python_dispatch.py torch/_custom_op/functional.py,https://github.com/pytorch/pytorch/pull/145110,zou3519,williamwen42,,,
02385ed625d,composability,not user facing,[Break XPU][Inductor UT] Fix broken XPU CI introduced by community changes (#145058),test/inductor/test_aot_inductor.py test/inductor/test_async_compile.py test/inductor/test_fuzzer.py test/inductor/test_torchinductor.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/145058,etaf,jansel,,,
7c1fb9b1ae4,inductor,not user facing,[inductor] Refactor CachingAutotuner so that it can pickle (#144044),torch/_inductor/codecache.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/144044,jansel,eellison,,,
0b151f260f2,inductor,improvements,[AOTI] Add an option to skip optimizing generated wrapper code (#144866),test/inductor/test_aot_inductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/144866,desertfire,chenyang78,hl475,,
8e4539245e6,skip,not user facing,Update ci_expected_accuracy for TIMM levit_128 for further investigation (#145112),benchmarks/dynamo/ci_expected_accuracy/aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv,https://github.com/pytorch/pytorch/pull/145112,huydhn,izaitsevfb,malfet,,
17c3a10cbd2,inductor,not user facing,PEP585 update - torch/_inductor/fx_passes (#145107),torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/fx_passes/ddp_fusion.py torch/_inductor/fx_passes/decompose_mem_bound_mm.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/_inductor/fx_passes/misc_patterns.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/pre_grad.py torch/_inductor/fx_passes/reinplace.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/145107,aorenste,bobrenjc93,oulgen,,
ee3e89190a7,inductor,not user facing,refactor benchmarking to use dynamo_timed (#144315),test/inductor/test_benchmarking.py torch/_inductor/runtime/benchmarking.py,https://github.com/pytorch/pytorch/pull/144315,nmacchioni,eellison,,,
2f51d062106,inductor,not user facing,basic InductorBenchmarker (#133058),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_inductor/config.py torch/_inductor/runtime/benchmarking.py,https://github.com/pytorch/pytorch/pull/133058,nmacchioni,eellison,,,
55084443cab,skip,not user facing,"Added swizzle searching, disabled fp16 accum, and enabled ping-pong for cutlass (#144829)",torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/144829,masnesral,Chillee,,,
668fb7dfba4,skip,not user facing,[ca] Use aot_eager on flex attention test (#145097),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/145097,xmfan,drisspg,,,
4eea2f74964,inductor,Untopiced,[inductor] Fix ignored options for torch.compile (#145131),test/inductor/test_halide.py torch/__init__.py,https://github.com/pytorch/pytorch/pull/145131,jansel,exclamaforte,,,
bdfeda5c9a0,distributed,not user facing,composability test cleanup (#145011),test/distributed/test_composability.py,https://github.com/pytorch/pytorch/pull/145011,wconstab,H-Huang,fduwjj,,
2c4281d7da1,distributed,not user facing,Make MultiProcContinuousTest timeout configurable (#145099),torch/testing/_internal/common_distributed.py,https://github.com/pytorch/pytorch/pull/145099,wconstab,d4l3k,fduwjj,,
07669ed9605,releng,not user facing,PEP585 update - benchmarks tools torchgen (#145101),benchmarks/dynamo/benchmarks.py benchmarks/dynamo/common.py benchmarks/dynamo/microbenchmarks/operator_inp_utils.py benchmarks/fastrnns/custom_lstms.py benchmarks/fastrnns/factory.py benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py benchmarks/functional_autograd_benchmark/utils.py benchmarks/gpt_fast/common.py benchmarks/instruction_counts/applications/ci.py benchmarks/instruction_counts/core/api.py benchmarks/instruction_counts/core/expand.py benchmarks/instruction_counts/core/types.py benchmarks/instruction_counts/core/utils.py benchmarks/instruction_counts/execution/runner.py benchmarks/instruction_counts/execution/work.py benchmarks/instruction_counts/main.py benchmarks/operator_benchmark/pt/as_strided_test.py benchmarks/operator_benchmark/pt/cat_test.py benchmarks/operator_benchmark/pt/qcat_test.py benchmarks/operator_benchmark/pt/stack_test.py benchmarks/transformer/attention_bias_benchmarks.py benchmarks/transformer/score_mod.py benchmarks/transformer/sdp.py benchmarks/transformer/sdpa.py tools/flight_recorder/components/builder.py tools/flight_recorder/components/config_manager.py tools/flight_recorder/components/loader.py tools/flight_recorder/components/types.py tools/flight_recorder/components/utils.py tools/flight_recorder/fr_trace.py tools/github/github_utils.py tools/linter/adapters/_linter.py tools/linter/adapters/docstring_linter.py tools/linter/adapters/import_linter.py tools/linter/adapters/set_linter.py tools/packaging/build_wheel.py tools/stats/check_disabled_tests.py tools/stats/import_test_stats.py tools/stats/sccache_stats_to_benchmark_format.py tools/stats/upload_dynamo_perf_stats.py tools/stats/upload_external_contrib_stats.py tools/stats/upload_stats_lib.py tools/stats/upload_test_stats_running_jobs.py torchgen/gen.py,https://github.com/pytorch/pytorch/pull/145101,aorenste,bobrenjc93,,,
2859b11bdbd,distributed,Untopiced,[pytorch/ncclx] Remove Alltoallv specialization for PTD all_to_all (#145045),torch/csrc/cuda/nccl.cpp,https://github.com/pytorch/pytorch/pull/145045,wconstab,d4l3k,ezyang,fduwjj,
dc9b77cc552,mps,devs,[MPS] Support includes in metal objects (#145087),.lintrunner.toml aten/src/ATen/native/mps/kernels/SpecialOps.metal c10/metal/special_math.h cmake/Metal.cmake setup.py test/test_mps.py torch/mps/__init__.py torch/utils/_cpp_embed_headers.py,https://github.com/pytorch/pytorch/pull/145087,malfet,dcci,,,
4bf29f44b73,inductor,not user facing,[aoti] Remove torch.ops.aten._assert_tensor_metadata.default in post_grad_pass (#145028),test/inductor/test_aot_inductor.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/145028,yushangdi,angelayi,,,
2bf772d1baf,inductor,not user facing,PEP585 update - torch/_inductor/codegen (#145106),torch/_inductor/codegen/block_analysis.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_bmm_template.py torch/_inductor/codegen/cpp_flex_attention_template.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_grouped_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/memory_planning.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/compile_command.py torch/_inductor/codegen/rocm/rocm_benchmark_request.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py torch/_inductor/codegen/rocm/rocm_template_buffer.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/simd_kernel_features.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/codegen/triton_utils.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/145106,aorenste,bobrenjc93,,,
5e4cf3e6ad6,skip,not user facing,Moved .all() checks for distributions to _is_all_true (#145029),torch/distributions/distribution.py,https://github.com/pytorch/pytorch/pull/145029,Chillee,Skylion007,zou3519,,
1d9fc9df38e,fx,not user facing,Downgrade ignored guard to info level (#145075),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/145075,ezyang,Skylion007,,,
8a572340336,inductor,not user facing,[MPSInductor] Implement `i0` and `i1` ops (#145092),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/145092,malfet,dcci,jansel,,
3ee531f8b94,nested tensor_frontend,improvements,Support NJT chunk() backward on batch dim (#144584),test/test_nestedtensor.py torch/csrc/autograd/FunctionsManual.cpp torch/nested/_internal/ops.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/144584,jbschlosser,soulitzer,,,
b63b81410c0,nested tensor_frontend,bug fixes,Fix NJT frexp() to handle both outputs (#144585),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/144585,jbschlosser,soulitzer,,,
5802be698ef,skip,Untopiced,"Revert ""parametrized test name handles class arguments (#133546)""",torch/testing/_internal/common_utils.py,,,,,,
4f8237dbadb,skip,not user facing,"[mps/inductor] Skip ""double"" tests as 64-bits FP is not supported. (#145123)",test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/145123,dcci,jansel,malfet,,
c95efc37baa,distributed,not user facing,PEP585 update - torch/distributed/tensor (#145141),torch/distributed/tensor/_api.py torch/distributed/tensor/_collective_utils.py torch/distributed/tensor/_dispatch.py torch/distributed/tensor/_dtensor_spec.py torch/distributed/tensor/_op_schema.py torch/distributed/tensor/_ops/_common_rules.py torch/distributed/tensor/_ops/_conv_ops.py torch/distributed/tensor/_ops/_einsum_strategy.py torch/distributed/tensor/_ops/_math_ops.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/_ops/_pointwise_ops.py torch/distributed/tensor/_ops/_tensor_ops.py torch/distributed/tensor/_ops/_view_ops.py torch/distributed/tensor/_ops/utils.py torch/distributed/tensor/_random.py torch/distributed/tensor/_redistribute.py torch/distributed/tensor/_sharding_prop.py torch/distributed/tensor/_shards_wrapper.py torch/distributed/tensor/_tp_conv.py torch/distributed/tensor/_utils.py torch/distributed/tensor/debug/_comm_mode.py torch/distributed/tensor/debug/_op_coverage.py torch/distributed/tensor/debug/_visualize_sharding.py torch/distributed/tensor/examples/comm_mode_features_example.py torch/distributed/tensor/examples/torchrec_sharding_example.py torch/distributed/tensor/experimental/__init__.py torch/distributed/tensor/experimental/_attention.py torch/distributed/tensor/experimental/_func_map.py torch/distributed/tensor/experimental/_register_sharding.py torch/distributed/tensor/experimental/_tp_transform.py torch/distributed/tensor/parallel/api.py torch/distributed/tensor/parallel/ddp.py torch/distributed/tensor/parallel/fsdp.py torch/distributed/tensor/parallel/loss.py torch/distributed/tensor/parallel/style.py torch/distributed/tensor/placement_types.py,https://github.com/pytorch/pytorch/pull/145141,aorenste,bobrenjc93,,,
a79100ab11e,dynamo,not user facing,PEP585 update - torch/_dynamo (#145105),test/test_custom_ops.py torch/_dynamo/_trace_wrapped_higher_order_op.py torch/_dynamo/backends/cudagraphs.py torch/_dynamo/backends/debugging.py torch/_dynamo/backends/distributed.py torch/_dynamo/backends/registry.py torch/_dynamo/bytecode_analysis.py torch/_dynamo/bytecode_transformation.py torch/_dynamo/callback.py torch/_dynamo/code_context.py torch/_dynamo/codegen.py torch/_dynamo/compiled_autograd.py torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/create_parameter_op.py torch/_dynamo/current_scope_id.py torch/_dynamo/debug_utils.py torch/_dynamo/decorators.py torch/_dynamo/device_interface.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/external_utils.py torch/_dynamo/funcname_cache.py torch/_dynamo/graph_deduplication.py torch/_dynamo/graph_region_tracker.py torch/_dynamo/guards.py torch/_dynamo/logging.py torch/_dynamo/metrics_context.py torch/_dynamo/mutation_guard.py torch/_dynamo/output_graph.py torch/_dynamo/pgo.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/polyfills/builtins.py torch/_dynamo/polyfills/functools.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/pytree.py torch/_dynamo/profiler.py torch/_dynamo/replay_record.py torch/_dynamo/repro/after_aot.py torch/_dynamo/repro/aoti.py torch/_dynamo/resume_execution.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/testing.py torch/_dynamo/trace_rules.py torch/_dynamo/types.py torch/_dynamo/utils.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lazy.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/script_object.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py torch/_library/infer_schema.py torch/compiler/__init__.py,https://github.com/pytorch/pytorch/pull/145105,aorenste,bobrenjc93,,,
5b5766665d7,composability,not user facing,PEP585 update - torch/_C torch/_decomp torch/_lazy torch/_library torch/_numpy torch/_prims torch/_refs torch/_strobelight (#145102),torch/_C/_dynamo/compiled_autograd.pyi torch/_C/_dynamo/eval_frame.pyi torch/_C/_dynamo/guards.pyi torch/_C/_functions.pyi torch/_decomp/__init__.py torch/_decomp/decompositions.py torch/_decomp/decompositions_for_jvp.py torch/_decomp/decompositions_for_rng.py torch/_lazy/device_context.py torch/_lazy/extract_compiled_graph.py torch/_library/autograd.py torch/_library/custom_ops.py torch/_library/fake_class_registry.py torch/_library/triton.py torch/_library/utils.py torch/_numpy/_funcs_impl.py torch/_numpy/_ndarray.py torch/_numpy/linalg.py torch/_prims/__init__.py torch/_prims/context.py torch/_prims/rng_prims.py torch/_prims_common/wrappers.py torch/_refs/__init__.py torch/_refs/fft.py torch/_refs/linalg/__init__.py torch/_refs/nn/__init__.py torch/_strobelight/cli_function_profiler.py,https://github.com/pytorch/pytorch/pull/145102,aorenste,bobrenjc93,,,
cede43e06b2,inductor,not user facing,[MPSInductor][BE] NaN-propagating min/max to header (#145157),c10/metal/utils.h torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/145157,malfet,dcci,,,
893ca1dfe1a,inductor,not user facing,PEP585 update - torch/_inductor/[_-i]* (#145137),torch/_inductor/__init__.py torch/_inductor/aoti_eager.py torch/_inductor/async_compile.py torch/_inductor/autoheuristic/autoheuristic.py torch/_inductor/autoheuristic/autoheuristic_utils.py torch/_inductor/autoheuristic/learned_heuristic_controller.py torch/_inductor/autoheuristic/learnedheuristic_interface.py torch/_inductor/autotune_process.py torch/_inductor/bounds.py torch/_inductor/choices.py torch/_inductor/codecache.py torch/_inductor/comms.py torch/_inductor/compile_fx.py torch/_inductor/compile_worker/__main__.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/compiler_bisector.py torch/_inductor/config.py torch/_inductor/constant_folding.py torch/_inductor/cpp_builder.py torch/_inductor/cpu_vec_isa.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/debug.py torch/_inductor/decomposition.py torch/_inductor/dependencies.py torch/_inductor/dtype_propagation.py torch/_inductor/exc.py torch/_inductor/extern_node_serializer.py torch/_inductor/freezing.py torch/_inductor/fuzzer.py torch/_inductor/fx_utils.py torch/_inductor/graph.py torch/_inductor/hooks.py torch/_inductor/index_propagation.py torch/_inductor/inductor_prims.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/145137,aorenste,bobrenjc93,,,
8cc415774f4,inductor,not user facing,[mps/inductor] Introduce a metal approx for erf() and use it. (#145161),c10/metal/special_math.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/145161,dcci,malfet,,,
10e4d3aebb3,distributed,not user facing,[DCP] Fix fsspec fsync bug on .finish() (#144753),torch/distributed/checkpoint/filesystem.py,https://github.com/pytorch/pytorch/pull/144753,cassanof,Skylion007,saumishr,,
78bff1e8c1b,quantization,not user facing,PEP585 update - torch/_functorch (#145139),torch/_functorch/_activation_checkpointing/graph_info_provider.py torch/_functorch/_activation_checkpointing/knapsack.py torch/_functorch/_activation_checkpointing/knapsack_evaluator.py torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/collect_metadata_analysis.py torch/_functorch/_aot_autograd/dispatch_and_compile_graph.py torch/_functorch/_aot_autograd/functional_utils.py torch/_functorch/_aot_autograd/input_output_analysis.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/logging_utils.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/subclass_parametrization.py torch/_functorch/_aot_autograd/subclass_utils.py torch/_functorch/_aot_autograd/traced_function_transforms.py torch/_functorch/_aot_autograd/utils.py torch/_functorch/aot_autograd.py torch/_functorch/autograd_function.py torch/_functorch/deprecated.py torch/_functorch/eager_transforms.py torch/_functorch/functional_call.py torch/_functorch/fx_minifier.py torch/_functorch/make_functional.py torch/_functorch/partitioners.py torch/_functorch/pyfunctorch.py torch/_functorch/utils.py torch/_functorch/vmap.py,https://github.com/pytorch/pytorch/pull/145139,aorenste,bobrenjc93,,,
9e0437a04a2,quantization,not user facing,PEP585 update - torch/ao/quantization (#145140),torch/ao/quantization/__init__.py torch/ao/quantization/_equalize.py torch/ao/quantization/_learnable_fake_quantize.py torch/ao/quantization/backend_config/_common_operator_config_utils.py torch/ao/quantization/backend_config/_qnnpack_pt2e.py torch/ao/quantization/backend_config/backend_config.py torch/ao/quantization/backend_config/executorch.py torch/ao/quantization/backend_config/utils.py torch/ao/quantization/experimental/adaround_optimization.py torch/ao/quantization/fuse_modules.py torch/ao/quantization/fuser_method_mappings.py torch/ao/quantization/fx/_equalize.py torch/ao/quantization/fx/_lower_to_native_backend.py torch/ao/quantization/fx/_model_report/detector.py torch/ao/quantization/fx/_model_report/model_report.py torch/ao/quantization/fx/_model_report/model_report_visualizer.py torch/ao/quantization/fx/convert.py torch/ao/quantization/fx/custom_config.py torch/ao/quantization/fx/fuse.py torch/ao/quantization/fx/fuse_handler.py torch/ao/quantization/fx/graph_module.py torch/ao/quantization/fx/lower_to_fbgemm.py torch/ao/quantization/fx/lower_to_qnnpack.py torch/ao/quantization/fx/match_utils.py torch/ao/quantization/fx/pattern_utils.py torch/ao/quantization/fx/prepare.py torch/ao/quantization/fx/qconfig_mapping_utils.py torch/ao/quantization/fx/quantize_handler.py torch/ao/quantization/fx/tracer.py torch/ao/quantization/fx/utils.py torch/ao/quantization/observer.py torch/ao/quantization/pt2e/_affine_quantization.py torch/ao/quantization/pt2e/_numeric_debugger.py torch/ao/quantization/pt2e/graph_utils.py torch/ao/quantization/pt2e/prepare.py torch/ao/quantization/pt2e/qat_utils.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/qconfig.py torch/ao/quantization/qconfig_mapping.py torch/ao/quantization/quantization_mappings.py torch/ao/quantization/quantize_fx.py torch/ao/quantization/quantizer/composable_quantizer.py torch/ao/quantization/quantizer/embedding_quantizer.py torch/ao/quantization/quantizer/quantizer.py torch/ao/quantization/quantizer/utils.py torch/ao/quantization/quantizer/x86_inductor_quantizer.py torch/ao/quantization/quantizer/xnnpack_quantizer.py torch/ao/quantization/quantizer/xnnpack_quantizer_utils.py torch/ao/quantization/quantizer/xpu_inductor_quantizer.py torch/ao/quantization/utils.py,https://github.com/pytorch/pytorch/pull/145140,aorenste,bobrenjc93,,,
cd8d0fa20c4,export,Untopiced,Tweak schema_check to handle annotated builtin types (#145154),scripts/export/update_schema.py test/export/test_schema.py torch/_export/serde/schema_check.py,https://github.com/pytorch/pytorch/pull/145154,aorenste,bobrenjc93,,,
97d4d3c40ae,export,not user facing,PEP585 update - torch/_export (#145138),torch/_export/__init__.py torch/_export/converter.py torch/_export/db/case.py torch/_export/db/examples/list_unpack.py torch/_export/non_strict_utils.py torch/_export/pass_base.py torch/_export/pass_infra/node_metadata.py torch/_export/pass_infra/proxy_value.py torch/_export/passes/add_runtime_assertions_for_constraints_pass.py torch/_export/passes/collect_tracepoints_pass.py torch/_export/passes/constant_folding.py torch/_export/passes/functionalize_side_effectful_ops_pass.py torch/_export/passes/insert_custom_op_guards.py torch/_export/passes/lift_constants_pass.py torch/_export/passes/replace_autocast_with_hop_pass.py torch/_export/passes/replace_quantized_ops_with_standard_ops_pass.py torch/_export/passes/replace_view_ops_with_view_copy_ops_pass.py torch/_export/serde/aoti_schema.py torch/_export/serde/dynamic_shapes.py torch/_export/serde/schema.py torch/_export/serde/schema_check.py torch/_export/serde/serialize.py torch/_export/serde/union.py torch/_export/tools.py torch/_export/utils.py torch/_export/verifier.py,https://github.com/pytorch/pytorch/pull/145138,aorenste,bobrenjc93,,,
371a361db9e,skip,not user facing,Enable bfloat16 testing on MacOS14+ (#145159),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/145159,malfet,Skylion007,jansel,,
c64e6576327,distributed,not user facing,PEP585 update - torch/distributed/fsdp (#145162),torch/distributed/fsdp/_common_utils.py torch/distributed/fsdp/_debug_utils.py torch/distributed/fsdp/_dynamo_utils.py torch/distributed/fsdp/_exec_order_utils.py torch/distributed/fsdp/_flat_param.py torch/distributed/fsdp/_fsdp_extensions.py torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py torch/distributed/fsdp/_fully_shard/_fsdp_common.py torch/distributed/fsdp/_fully_shard/_fsdp_init.py torch/distributed/fsdp/_fully_shard/_fsdp_param.py torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py torch/distributed/fsdp/_fully_shard/_fsdp_state.py torch/distributed/fsdp/_fully_shard/_fully_shard.py torch/distributed/fsdp/_init_utils.py torch/distributed/fsdp/_limiter_utils.py torch/distributed/fsdp/_optim_utils.py torch/distributed/fsdp/_runtime_utils.py torch/distributed/fsdp/_state_dict_utils.py torch/distributed/fsdp/_trace_utils.py torch/distributed/fsdp/_traversal_utils.py torch/distributed/fsdp/_unshard_param_utils.py torch/distributed/fsdp/_wrap_utils.py torch/distributed/fsdp/api.py torch/distributed/fsdp/fully_sharded_data_parallel.py torch/distributed/fsdp/sharded_grad_scaler.py torch/distributed/fsdp/wrap.py,https://github.com/pytorch/pytorch/pull/145162,aorenste,bobrenjc93,,,
316808e4e92,distributed (torchelastic),not user facing,PEP585 update - torch/distributed/elastic torch/distributed/checkpoint (#145163),torch/distributed/checkpoint/_checkpointer.py torch/distributed/checkpoint/_dedup_save_plans.py torch/distributed/checkpoint/_dedup_tensors.py torch/distributed/checkpoint/_fsspec_filesystem.py torch/distributed/checkpoint/_nested_dict.py torch/distributed/checkpoint/_storage_utils.py torch/distributed/checkpoint/_traverse.py torch/distributed/checkpoint/api.py torch/distributed/checkpoint/default_planner.py torch/distributed/checkpoint/filesystem.py torch/distributed/checkpoint/format_utils.py torch/distributed/checkpoint/logger.py torch/distributed/checkpoint/logging_handlers.py torch/distributed/checkpoint/metadata.py torch/distributed/checkpoint/optimizer.py torch/distributed/checkpoint/planner.py torch/distributed/checkpoint/planner_helpers.py torch/distributed/checkpoint/resharding.py torch/distributed/checkpoint/state_dict.py torch/distributed/checkpoint/state_dict_loader.py torch/distributed/checkpoint/stateful.py torch/distributed/checkpoint/storage.py torch/distributed/checkpoint/utils.py torch/distributed/elastic/agent/server/api.py torch/distributed/elastic/agent/server/local_elastic_agent.py torch/distributed/elastic/control_plane.py torch/distributed/elastic/events/__init__.py torch/distributed/elastic/events/api.py torch/distributed/elastic/events/handlers.py torch/distributed/elastic/metrics/api.py torch/distributed/elastic/multiprocessing/__init__.py torch/distributed/elastic/multiprocessing/api.py torch/distributed/elastic/multiprocessing/errors/__init__.py torch/distributed/elastic/multiprocessing/errors/error_handler.py torch/distributed/elastic/multiprocessing/subprocess_handler/handlers.py torch/distributed/elastic/multiprocessing/subprocess_handler/subprocess_handler.py torch/distributed/elastic/multiprocessing/tail_log.py torch/distributed/elastic/rendezvous/api.py torch/distributed/elastic/rendezvous/dynamic_rendezvous.py torch/distributed/elastic/rendezvous/utils.py torch/distributed/elastic/timer/api.py torch/distributed/elastic/timer/debug_info_logging.py torch/distributed/elastic/timer/file_based_local_timer.py torch/distributed/elastic/timer/local_timer.py torch/distributed/elastic/utils/api.py torch/distributed/elastic/utils/data/cycling_iterator.py torch/distributed/elastic/utils/store.py,https://github.com/pytorch/pytorch/pull/145163,aorenste,Skylion007,,,
b6c5562c1fa,export,not user facing,PEP585 update - torch/export (#145165),torch/export/__init__.py torch/export/_draft_export.py torch/export/_remove_effect_tokens_pass.py torch/export/_swap.py torch/export/_trace.py torch/export/_tree_utils.py torch/export/_unlift.py torch/export/decomp_utils.py torch/export/dynamic_shapes.py torch/export/experimental/__init__.py torch/export/exported_program.py torch/export/graph_signature.py torch/export/passes/__init__.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/145165,aorenste,bobrenjc93,,,
6cb186e279b,distributed,not user facing,PEP585 update - torch/distributed (#145164),torch/distributed/__init__.py torch/distributed/_checkpointable.py torch/distributed/_composable/checkpoint_activation.py torch/distributed/_composable/contract.py torch/distributed/_composable/replicate.py torch/distributed/_functional_collectives.py torch/distributed/_functional_collectives_impl.py torch/distributed/_shard/_utils.py torch/distributed/_shard/metadata.py torch/distributed/_shard/sharded_optim/__init__.py torch/distributed/_shard/sharded_optim/api.py torch/distributed/_shard/sharded_tensor/__init__.py torch/distributed/_shard/sharded_tensor/api.py torch/distributed/_shard/sharded_tensor/logger.py torch/distributed/_shard/sharded_tensor/logging_handlers.py torch/distributed/_shard/sharded_tensor/metadata.py torch/distributed/_shard/sharded_tensor/reshard.py torch/distributed/_shard/sharded_tensor/shard.py torch/distributed/_shard/sharded_tensor/utils.py torch/distributed/_shard/sharding_plan/api.py torch/distributed/_shard/sharding_spec/_internals.py torch/distributed/_shard/sharding_spec/api.py torch/distributed/_shard/sharding_spec/chunk_sharding_spec.py torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding_bag.py torch/distributed/_state_dict_utils.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/distributed/_tools/ilp_utils.py torch/distributed/_tools/mem_tracker.py torch/distributed/_tools/memory_tracker.py torch/distributed/_tools/mod_tracker.py torch/distributed/_tools/runtime_estimator.py torch/distributed/_tools/sac_estimator.py torch/distributed/_tools/sac_ilp.py torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py torch/distributed/algorithms/join.py torch/distributed/algorithms/model_averaging/averagers.py torch/distributed/algorithms/model_averaging/hierarchical_model_averager.py torch/distributed/algorithms/model_averaging/utils.py torch/distributed/c10d_logger.py torch/distributed/collective_utils.py torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py torch/distributed/launcher/api.py torch/distributed/logging_handlers.py torch/distributed/nn/api/remote_module.py torch/distributed/optim/apply_optimizer_in_backward.py torch/distributed/optim/functional_adadelta.py torch/distributed/optim/functional_adagrad.py torch/distributed/optim/functional_adam.py torch/distributed/optim/functional_adamax.py torch/distributed/optim/functional_adamw.py torch/distributed/optim/functional_rmsprop.py torch/distributed/optim/functional_rprop.py torch/distributed/optim/functional_sgd.py torch/distributed/optim/named_optimizer.py torch/distributed/optim/optimizer.py torch/distributed/optim/utils.py torch/distributed/optim/zero_redundancy_optimizer.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/_backward.py torch/distributed/pipelining/_unflatten.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/microbatch.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py torch/distributed/rendezvous.py torch/distributed/rpc/__init__.py torch/distributed/rpc/api.py torch/distributed/rpc/backend_registry.py torch/distributed/rpc/constants.py torch/distributed/rpc/options.py torch/distributed/rpc/server_process_global_profiler.py torch/distributed/run.py torch/distributed/utils.py,https://github.com/pytorch/pytorch/pull/145164,aorenste,bobrenjc93,,,
980c75fe6e9,inductor,not user facing,[MPSInductor] Add `TrueDiv` and `Round[Int|Decimal]` (#145160),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/145160,malfet,dcci,jansel,,
19584b28fd0,skip,not user facing,[dynamo][dicts] Consolidate dict(..) construction (#144342),test/dynamo/test_higher_order_ops.py test/functorch/test_control_flow.py torch/_dynamo/mutation_guard.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/144342,anijain2305,StrongerXi,,,
2b809e58ad3,onnx,not user facing,PEP585 update - torch/onnx (#145174),torch/onnx/__init__.py torch/onnx/_experimental.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/diagnostics/infra/_infra.py torch/onnx/_internal/diagnostics/infra/context.py torch/onnx/_internal/diagnostics/infra/decorator.py torch/onnx/_internal/diagnostics/infra/utils.py torch/onnx/_internal/exporter/_building.py torch/onnx/_internal/exporter/_compat.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_dispatching.py torch/onnx/_internal/exporter/_ir_passes.py torch/onnx/_internal/exporter/_onnx_program.py torch/onnx/_internal/exporter/_schemas.py torch/onnx/_internal/exporter/_torchlib/ops/hop.py torch/onnx/_internal/fx/decomposition_skip.py torch/onnx/_internal/fx/dynamo_graph_extractor.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/_internal/fx/passes/modularization.py torch/onnx/_internal/fx/passes/readability.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/patcher.py torch/onnx/_internal/fx/type_utils.py torch/onnx/_internal/io_adapter.py torch/onnx/_internal/jit_utils.py torch/onnx/_internal/onnx_proto_utils.py torch/onnx/_internal/onnxruntime.py torch/onnx/_internal/registration.py torch/onnx/_onnx_supported_ops.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset10.py torch/onnx/symbolic_opset11.py torch/onnx/symbolic_opset17.py torch/onnx/symbolic_opset18.py torch/onnx/symbolic_opset19.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/145174,aorenste,justinchuby,,,
972d4a154d8,skip,not user facing,Add facility to run dynamo UTs for non-cuda devices (#140929),test/dynamo/test_activation_checkpointing.py test/dynamo/test_backends.py test/dynamo/test_export.py test/dynamo/test_modules.py,https://github.com/pytorch/pytorch/pull/140929,ankurneog,EikanWang,guangyey,kwen2501,
507bf65c6a9,complex_frontend,Untopiced,fix torch.exp for torch.complex datatypes on CPU (#140358),aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/140358,jiayisunx,Skylion007,mingfeima,,
c922ccb7c43,complex_frontend,Untopiced,fix sigmoid for torch.complex datatypes on CPU (#140391),aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h aten/src/ATen/native/cpu/UnaryOpsKernel.cpp torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/140391,jiayisunx,Skylion007,mingfeima,,
ed669a9db7b,complex_frontend,Untopiced,fix torch.div for torch.complex datatypes on CPU (#140375),aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h,https://github.com/pytorch/pytorch/pull/140375,jiayisunx,mingfeima,,,
92b9da1fc2b,complex_frontend,Untopiced,fix torch.atan for torch.complex datatypes on CPU (#144749),aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/144749,jiayisunx,Skylion007,mingfeima,,
53e2408015c,releng,not user facing,Improve cleanup of cancelled jobs on s390x for tests too (#144968),.github/workflows/_linux-test.yml,https://github.com/pytorch/pytorch/pull/144968,AlekseiNikiforovIBM,huydhn,,,
57b2b64acf5,skip,not user facing,Fix always true scaled_mm test (#143912),test/test_matmul_cuda.py,https://github.com/pytorch/pytorch/pull/143912,dnikolaev-amd,drisspg,malfet,pruthvistony,
6374332d339,skip,Untopiced,"Revert ""PEP585 update - torch/distributed (#145164)""",torch/distributed/__init__.py torch/distributed/_checkpointable.py torch/distributed/_composable/checkpoint_activation.py torch/distributed/_composable/contract.py torch/distributed/_composable/replicate.py torch/distributed/_functional_collectives.py torch/distributed/_functional_collectives_impl.py torch/distributed/_shard/_utils.py torch/distributed/_shard/metadata.py torch/distributed/_shard/sharded_optim/__init__.py torch/distributed/_shard/sharded_optim/api.py torch/distributed/_shard/sharded_tensor/__init__.py torch/distributed/_shard/sharded_tensor/api.py torch/distributed/_shard/sharded_tensor/logger.py torch/distributed/_shard/sharded_tensor/logging_handlers.py torch/distributed/_shard/sharded_tensor/metadata.py torch/distributed/_shard/sharded_tensor/reshard.py torch/distributed/_shard/sharded_tensor/shard.py torch/distributed/_shard/sharded_tensor/utils.py torch/distributed/_shard/sharding_plan/api.py torch/distributed/_shard/sharding_spec/_internals.py torch/distributed/_shard/sharding_spec/api.py torch/distributed/_shard/sharding_spec/chunk_sharding_spec.py torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding_bag.py torch/distributed/_state_dict_utils.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/distributed/_tools/ilp_utils.py torch/distributed/_tools/mem_tracker.py torch/distributed/_tools/memory_tracker.py torch/distributed/_tools/mod_tracker.py torch/distributed/_tools/runtime_estimator.py torch/distributed/_tools/sac_estimator.py torch/distributed/_tools/sac_ilp.py torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py torch/distributed/algorithms/join.py torch/distributed/algorithms/model_averaging/averagers.py torch/distributed/algorithms/model_averaging/hierarchical_model_averager.py torch/distributed/algorithms/model_averaging/utils.py torch/distributed/c10d_logger.py torch/distributed/collective_utils.py torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py torch/distributed/launcher/api.py torch/distributed/logging_handlers.py torch/distributed/nn/api/remote_module.py torch/distributed/optim/apply_optimizer_in_backward.py torch/distributed/optim/functional_adadelta.py torch/distributed/optim/functional_adagrad.py torch/distributed/optim/functional_adam.py torch/distributed/optim/functional_adamax.py torch/distributed/optim/functional_adamw.py torch/distributed/optim/functional_rmsprop.py torch/distributed/optim/functional_rprop.py torch/distributed/optim/functional_sgd.py torch/distributed/optim/named_optimizer.py torch/distributed/optim/optimizer.py torch/distributed/optim/utils.py torch/distributed/optim/zero_redundancy_optimizer.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/_backward.py torch/distributed/pipelining/_unflatten.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/microbatch.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py torch/distributed/rendezvous.py torch/distributed/rpc/__init__.py torch/distributed/rpc/api.py torch/distributed/rpc/backend_registry.py torch/distributed/rpc/constants.py torch/distributed/rpc/options.py torch/distributed/rpc/server_process_global_profiler.py torch/distributed/run.py torch/distributed/utils.py,,,,,,
0b2a3687b9f,fx,not user facing,PEP585 update - torch/fx (#145166),test/test_fx.py test/test_fx_experimental.py torch/fx/_compatibility.py torch/fx/_pytree.py torch/fx/_symbolic_trace.py torch/fx/_utils.py torch/fx/experimental/accelerator_partitioner.py torch/fx/experimental/const_fold.py torch/fx/experimental/debug.py torch/fx/experimental/graph_gradual_typechecker.py torch/fx/experimental/merge_matmul.py torch/fx/experimental/meta_tracer.py torch/fx/experimental/migrate_gradual_types/constraint_generator.py torch/fx/experimental/migrate_gradual_types/constraint_transformation.py torch/fx/experimental/normalize.py torch/fx/experimental/optimization.py torch/fx/experimental/partitioner_utils.py torch/fx/experimental/proxy_tensor.py torch/fx/experimental/recording.py torch/fx/experimental/rewriter.py torch/fx/experimental/schema_type_annotation.py torch/fx/experimental/shape_inference/infer_symbol_values.py torch/fx/experimental/sym_node.py torch/fx/experimental/symbolic_shapes.py torch/fx/experimental/validator.py torch/fx/graph.py torch/fx/graph_module.py torch/fx/immutable_collections.py torch/fx/interpreter.py torch/fx/node.py torch/fx/operator_schemas.py torch/fx/passes/_tensorify_python_scalars.py torch/fx/passes/dialect/common/cse_pass.py torch/fx/passes/graph_drawer.py torch/fx/passes/graph_manipulation.py torch/fx/passes/graph_transform_observer.py torch/fx/passes/infra/partitioner.py torch/fx/passes/infra/pass_manager.py torch/fx/passes/net_min_base.py torch/fx/passes/operator_support.py torch/fx/passes/param_fetch.py torch/fx/passes/pass_manager.py torch/fx/passes/reinplace.py torch/fx/passes/runtime_assert.py torch/fx/passes/shape_prop.py torch/fx/passes/split_module.py torch/fx/passes/split_utils.py torch/fx/passes/splitter_base.py torch/fx/passes/tools_common.py torch/fx/passes/utils/common.py torch/fx/passes/utils/fuser_utils.py torch/fx/passes/utils/matcher_utils.py torch/fx/passes/utils/matcher_with_name_node_map_utils.py torch/fx/passes/utils/source_matcher_utils.py torch/fx/proxy.py torch/fx/subgraph_rewriter.py torch/fx/traceback.py,https://github.com/pytorch/pytorch/pull/145166,aorenste,bobrenjc93,,,
d0100050dd2,inductor,not user facing,"[aoti] Deduplicate ""V.aot_compilation"" and ""V.graph.aot_mode"" flags. [2/n] (#145091)",torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/145091,zhxchen17,ydwu4,,,
225a10febeb,releng,not user facing,[CI] Add xpu linux build into pull workflow (#145084),.github/workflows/pull.yml .github/workflows/xpu.yml,https://github.com/pytorch/pytorch/pull/145084,chuanqi129,atalman,huydhn,,
cf05f6a1345,fx,Untopiced,[BE]: Improve typing for torch/fx/_pytree.py and torch/utils/_pytree.py (#145173),torch/fx/_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/145173,Skylion007,bobrenjc93,,,
bd97ce0b454,quantization,not user facing,PEP585 update - torch/ao (#145199),torch/ao/nn/intrinsic/qat/modules/conv_fused.py torch/ao/nn/qat/modules/conv.py torch/ao/nn/quantized/dynamic/modules/conv.py torch/ao/nn/quantized/functional.py torch/ao/nn/quantized/modules/conv.py torch/ao/nn/quantized/modules/functional_modules.py torch/ao/nn/quantized/reference/modules/conv.py torch/ao/nn/quantized/reference/modules/linear.py torch/ao/nn/quantized/reference/modules/rnn.py torch/ao/nn/quantized/reference/modules/sparse.py torch/ao/nn/quantized/reference/modules/utils.py torch/ao/nn/sparse/quantized/utils.py torch/ao/ns/_numeric_suite.py torch/ao/ns/_numeric_suite_fx.py torch/ao/ns/fx/graph_matcher.py torch/ao/ns/fx/graph_passes.py torch/ao/ns/fx/mappings.py torch/ao/ns/fx/n_shadows_utils.py torch/ao/ns/fx/ns_types.py torch/ao/ns/fx/pattern_utils.py torch/ao/ns/fx/qconfig_multi_mapping.py torch/ao/ns/fx/utils.py torch/ao/ns/fx/weight_utils.py torch/ao/pruning/_experimental/activation_sparsifier/activation_sparsifier.py torch/ao/pruning/_experimental/data_sparsifier/base_data_sparsifier.py torch/ao/pruning/_experimental/data_sparsifier/benchmarks/evaluate_disk_savings.py torch/ao/pruning/_experimental/data_sparsifier/benchmarks/evaluate_forward_time.py torch/ao/pruning/_experimental/data_sparsifier/benchmarks/evaluate_model_metrics.py torch/ao/pruning/_experimental/data_sparsifier/data_norm_sparsifier.py torch/ao/pruning/_experimental/data_sparsifier/lightning/callbacks/data_sparsity.py torch/ao/pruning/_experimental/data_sparsifier/lightning/tests/test_callbacks.py torch/ao/pruning/_experimental/data_sparsifier/quantization_utils.py torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py torch/ao/pruning/_experimental/pruner/match_utils.py torch/ao/pruning/_experimental/pruner/prune_functions.py torch/ao/pruning/sparsifier/base_sparsifier.py torch/ao/pruning/sparsifier/utils.py,https://github.com/pytorch/pytorch/pull/145199,aorenste,bobrenjc93,,,
54a00af2c60,optim,not user facing,PEP585 update -  torch/nn torch/optim torch/package torch/profiler torch/serialization torch/sparse torch/xpu (#145175),torch/nn/functional.py torch/nn/modules/conv.py torch/optim/_adafactor.py torch/optim/_functional.py torch/optim/adadelta.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/lr_scheduler.py torch/optim/optimizer.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py torch/optim/sparse_adam.py torch/optim/swa_utils.py torch/package/_digraph.py torch/package/analyze/find_first_use_of_broken_modules.py torch/package/analyze/trace_dependencies.py torch/package/file_structure_representation.py torch/package/find_file_dependencies.py torch/package/glob_group.py torch/package/importer.py torch/package/package_exporter.py torch/package/package_importer.py torch/profiler/_memory_profiler.py torch/profiler/_pattern_matcher.py torch/profiler/_utils.py torch/profiler/profiler.py torch/profiler/python_tracer.py torch/serialization.py torch/sparse/__init__.py torch/sparse/_triton_ops.py torch/sparse/_triton_ops_meta.py torch/sparse/semi_structured.py torch/xpu/__init__.py torch/xpu/memory.py torch/xpu/random.py,https://github.com/pytorch/pytorch/pull/145175,aorenste,bobrenjc93,,,
805c4b597a1,foreach_frontend,not user facing,PEP585 update - torch/_higher_order_ops torch/_subclasses torch/backends torch/compiler torch/cuda torch/masked torch/mtia torch/nested (#145202),torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/auto_functionalize.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/effects.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/foreach_map.py torch/_higher_order_ops/invoke_subgraph.py torch/_higher_order_ops/scan.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/utils.py torch/_higher_order_ops/while_loop.py torch/_subclasses/_fake_tensor_utils.py torch/_subclasses/fake_tensor.py torch/_subclasses/fake_utils.py torch/_subclasses/functional_tensor.py torch/_subclasses/meta_utils.py torch/backends/_coreml/preprocess.py torch/backends/_nnapi/prepare.py torch/backends/_nnapi/serializer.py torch/backends/quantized/__init__.py torch/backends/xeon/run_cpu.py torch/compiler/__init__.py torch/compiler/_cache.py torch/cuda/__init__.py torch/cuda/_sanitizer.py torch/cuda/gds.py torch/cuda/jiterator.py torch/cuda/memory.py torch/cuda/nccl.py torch/cuda/random.py torch/cuda/tunable.py torch/masked/_ops.py torch/masked/maskedtensor/_ops_refs.py torch/mtia/__init__.py torch/mtia/memory.py torch/nested/__init__.py torch/nested/_internal/nested_tensor.py torch/nested/_internal/ops.py torch/nested/_internal/sdpa.py,https://github.com/pytorch/pytorch/pull/145202,aorenste,bobrenjc93,,,
dea7ad3371b,distributed,not user facing,PEP585 update - torch/testing (#145200),torch/testing/_comparison.py torch/testing/_creation.py torch/testing/_internal/check_kernel_launches.py torch/testing/_internal/common_device_type.py torch/testing/_internal/common_distributed.py torch/testing/_internal/common_dtype.py torch/testing/_internal/common_fsdp.py torch/testing/_internal/common_jit.py torch/testing/_internal/common_methods_invocations.py torch/testing/_internal/common_modules.py torch/testing/_internal/common_nn.py torch/testing/_internal/common_optimizers.py torch/testing/_internal/common_pruning.py torch/testing/_internal/common_quantization.py torch/testing/_internal/common_utils.py torch/testing/_internal/distributed/_tensor/common_dtensor.py torch/testing/_internal/distributed/checkpoint_utils.py torch/testing/_internal/distributed/common_state_dict.py torch/testing/_internal/distributed/multi_threaded_pg.py torch/testing/_internal/distributed/rpc/jit/dist_autograd_test.py torch/testing/_internal/distributed/rpc/jit/rpc_test.py torch/testing/_internal/distributed/rpc/jit/rpc_test_faulty.py torch/testing/_internal/distributed/rpc_utils.py torch/testing/_internal/jit_metaprogramming_utils.py torch/testing/_internal/jit_utils.py torch/testing/_internal/logging_tensor.py torch/testing/_internal/opinfo/core.py torch/testing/_internal/opinfo/definitions/__init__.py torch/testing/_internal/opinfo/definitions/_masked.py torch/testing/_internal/opinfo/definitions/fft.py torch/testing/_internal/opinfo/definitions/linalg.py torch/testing/_internal/opinfo/definitions/nested.py torch/testing/_internal/opinfo/definitions/signal.py torch/testing/_internal/opinfo/definitions/special.py torch/testing/_internal/opinfo/utils.py torch/testing/_internal/optests/generate_tests.py torch/testing/_internal/subclasses.py,https://github.com/pytorch/pytorch/pull/145200,aorenste,bobrenjc93,,,
5fd881a5b67,skip,Untopiced,"Revert ""PEP585 update -  torch/nn torch/optim torch/package torch/profiler torch/serialization torch/sparse torch/xpu (#145175)""",torch/nn/functional.py torch/nn/modules/conv.py torch/optim/_adafactor.py torch/optim/_functional.py torch/optim/adadelta.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/lr_scheduler.py torch/optim/optimizer.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py torch/optim/sparse_adam.py torch/optim/swa_utils.py torch/package/_digraph.py torch/package/analyze/find_first_use_of_broken_modules.py torch/package/analyze/trace_dependencies.py torch/package/file_structure_representation.py torch/package/find_file_dependencies.py torch/package/glob_group.py torch/package/importer.py torch/package/package_exporter.py torch/package/package_importer.py torch/profiler/_memory_profiler.py torch/profiler/_pattern_matcher.py torch/profiler/_utils.py torch/profiler/profiler.py torch/profiler/python_tracer.py torch/serialization.py torch/sparse/__init__.py torch/sparse/_triton_ops.py torch/sparse/_triton_ops_meta.py torch/sparse/semi_structured.py torch/xpu/__init__.py torch/xpu/memory.py torch/xpu/random.py,,,,,,
c6986ca2e12,skip,Untopiced,"Revert ""[dcp] Add ZStandard transformer (#143360)""",.ci/docker/requirements-ci.txt requirements.txt test/distributed/checkpoint/test_dtensor_resharding.py test/distributed/checkpoint/test_file_system_checkpoint.py test/distributed/checkpoint/test_file_system_checkpoint_cpu.py torch/distributed/checkpoint/_extension.py,,,,,,
00ffeca1b1f,distributed,not user facing,PEP585 update - torch/distributed (#145164),torch/distributed/__init__.py torch/distributed/_checkpointable.py torch/distributed/_composable/checkpoint_activation.py torch/distributed/_composable/contract.py torch/distributed/_composable/replicate.py torch/distributed/_functional_collectives.py torch/distributed/_functional_collectives_impl.py torch/distributed/_shard/_utils.py torch/distributed/_shard/metadata.py torch/distributed/_shard/sharded_optim/__init__.py torch/distributed/_shard/sharded_optim/api.py torch/distributed/_shard/sharded_tensor/__init__.py torch/distributed/_shard/sharded_tensor/api.py torch/distributed/_shard/sharded_tensor/logger.py torch/distributed/_shard/sharded_tensor/logging_handlers.py torch/distributed/_shard/sharded_tensor/metadata.py torch/distributed/_shard/sharded_tensor/reshard.py torch/distributed/_shard/sharded_tensor/shard.py torch/distributed/_shard/sharded_tensor/utils.py torch/distributed/_shard/sharding_plan/api.py torch/distributed/_shard/sharding_spec/_internals.py torch/distributed/_shard/sharding_spec/api.py torch/distributed/_shard/sharding_spec/chunk_sharding_spec.py torch/distributed/_shard/sharding_spec/chunk_sharding_spec_ops/embedding_bag.py torch/distributed/_state_dict_utils.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/distributed/_tools/ilp_utils.py torch/distributed/_tools/mem_tracker.py torch/distributed/_tools/memory_tracker.py torch/distributed/_tools/mod_tracker.py torch/distributed/_tools/runtime_estimator.py torch/distributed/_tools/sac_estimator.py torch/distributed/_tools/sac_ilp.py torch/distributed/algorithms/_checkpoint/checkpoint_wrapper.py torch/distributed/algorithms/_optimizer_overlap/optimizer_overlap.py torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py torch/distributed/algorithms/ddp_comm_hooks/optimizer_overlap_hooks.py torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py torch/distributed/algorithms/join.py torch/distributed/algorithms/model_averaging/averagers.py torch/distributed/algorithms/model_averaging/hierarchical_model_averager.py torch/distributed/algorithms/model_averaging/utils.py torch/distributed/c10d_logger.py torch/distributed/collective_utils.py torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py torch/distributed/launcher/api.py torch/distributed/logging_handlers.py torch/distributed/nn/api/remote_module.py torch/distributed/optim/apply_optimizer_in_backward.py torch/distributed/optim/functional_adadelta.py torch/distributed/optim/functional_adagrad.py torch/distributed/optim/functional_adam.py torch/distributed/optim/functional_adamax.py torch/distributed/optim/functional_adamw.py torch/distributed/optim/functional_rmsprop.py torch/distributed/optim/functional_rprop.py torch/distributed/optim/functional_sgd.py torch/distributed/optim/named_optimizer.py torch/distributed/optim/optimizer.py torch/distributed/optim/utils.py torch/distributed/optim/zero_redundancy_optimizer.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/_backward.py torch/distributed/pipelining/_unflatten.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/microbatch.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py torch/distributed/rendezvous.py torch/distributed/rpc/__init__.py torch/distributed/rpc/api.py torch/distributed/rpc/backend_registry.py torch/distributed/rpc/constants.py torch/distributed/rpc/options.py torch/distributed/rpc/server_process_global_profiler.py torch/distributed/run.py torch/distributed/utils.py,https://github.com/pytorch/pytorch/pull/145164,aorenste,bobrenjc93,,,
b5655d9821b,releng,not user facing,PEP585 update - .ci android aten (#145177),.ci/aarch64_linux/aarch64_wheel_ci_build.py .ci/aarch64_linux/build_aarch64_wheel.py .ci/pytorch/smoke_test/check_binary_symbols.py android/pytorch_android/generate_test_torchscripts.py aten/src/ATen/native/transformers/cuda/flash_attn/kernels/generate_kernels.py aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/generate_kernels.py,https://github.com/pytorch/pytorch/pull/145177,aorenste,Skylion007,,,
803017f3cb7,inductor,not user facing,[inductor] fix MA on poor gpu (#145133),test/inductor/test_max_autotune.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/145133,shunting314,jansel,,,
0afd335174a,optim,not user facing,PEP585 update -  torch/nn torch/optim torch/package torch/profiler torch/serialization torch/sparse torch/xpu (#145175),torch/nn/functional.py torch/optim/_adafactor.py torch/optim/_functional.py torch/optim/adadelta.py torch/optim/adagrad.py torch/optim/adam.py torch/optim/adamw.py torch/optim/asgd.py torch/optim/lr_scheduler.py torch/optim/optimizer.py torch/optim/rmsprop.py torch/optim/rprop.py torch/optim/sgd.py torch/optim/sparse_adam.py torch/optim/swa_utils.py torch/package/_digraph.py torch/package/analyze/find_first_use_of_broken_modules.py torch/package/analyze/trace_dependencies.py torch/package/file_structure_representation.py torch/package/find_file_dependencies.py torch/package/glob_group.py torch/package/importer.py torch/package/package_exporter.py torch/package/package_importer.py torch/profiler/_memory_profiler.py torch/profiler/_pattern_matcher.py torch/profiler/_utils.py torch/profiler/profiler.py torch/profiler/python_tracer.py torch/serialization.py torch/sparse/__init__.py torch/sparse/_triton_ops.py torch/sparse/_triton_ops_meta.py torch/sparse/semi_structured.py torch/xpu/__init__.py torch/xpu/memory.py torch/xpu/random.py,https://github.com/pytorch/pytorch/pull/145175,aorenste,bobrenjc93,,,
2cffbff7daf,releng,not user facing,Add 3.13t Windows and MacOS binary builds (#141806),.ci/pytorch/windows/condaenv.bat .ci/pytorch/windows/internal/smoke_test.bat .ci/wheel/build_wheel.sh .github/scripts/generate_binary_build_matrix.py .github/templates/macos_binary_build_workflow.yml.j2 .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/141806,atalman,albanD,,,
eb553ae3cf1,releng,Untopiced,Fix broken gpt_fast micro benchmark after #144315 (#145235),.github/workflows/inductor-micro-benchmark-x86.yml .github/workflows/inductor-micro-benchmark.yml benchmarks/gpt_fast/benchmark.py,https://github.com/pytorch/pytorch/pull/145235,huydhn,nmacchioni,,,
b3e90c8c332,python_frontend,new features,Add support for torch function on dtype arguments (#145085),test/test_overrides.py torch/csrc/utils/python_arg_parser.cpp,https://github.com/pytorch/pytorch/pull/145085,ezyang,Skylion007,vmoens,,
efa88e04e15,fx,Untopiced,Don't overspecialize float when propagating cache guards to ShapeEnv (#145078),test/inductor/test_codecache.py torch/fx/experimental/symbolic_shapes.py torch/utils/_sympy/printers.py,https://github.com/pytorch/pytorch/pull/145078,ezyang,Skylion007,,,
af204135d8b,nested tensor_frontend,bug fixes,Fix NJT fill.Scalar for contiguous inputs (#144586),test/test_nestedtensor.py torch/nested/_internal/ops.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/144586,jbschlosser,soulitzer,,,
128f3627b12,nested tensor_frontend,bug fixes,Implement backward for NJT matmul (#144587),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/144587,jbschlosser,soulitzer,,,
85811631d78,skip,not user facing,[Intel CPU] Fix issue #143489. (#145062),aten/src/ATen/native/ConvolutionMM2d.cpp,https://github.com/pytorch/pytorch/pull/145062,RanTao123,soulitzer,,,
505ade74716,inductor,not user facing,"[inductor] Simplify mode options, only apply CompilerBisector changes once (#145232)",torch/__init__.py torch/_inductor/__init__.py,https://github.com/pytorch/pytorch/pull/145232,jansel,yanboliang,,,
df67ac4c861,distributed,not user facing,[CI][CUDA][Distributed][FSDP] Remove hardcoded world size of 2  (#145195),test/distributed/fsdp/test_distributed_checkpoint.py test/distributed/fsdp/test_fsdp_apply.py test/distributed/fsdp/test_fsdp_traversal.py,https://github.com/pytorch/pytorch/pull/145195,nWEIdia,Skylion007,atalman,,
323fb4dad0e,fx,Untopiced,Unconditionally exclude upper bound in all size oblivious tests (#144867),torch/__init__.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/144867,ezyang,bobrenjc93,,,
693d8c7e945,dynamo,bug fixes,"Output of nonzero is transposed, fix fake tensor (#144695)",test/inductor/test_auto_functionalize.py test/test_fake_tensor.py torch/_dynamo/debug_utils.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_subclasses/fake_impls.py,https://github.com/pytorch/pytorch/pull/144695,ezyang,albanD,bobrenjc93,,
2f9d378f7bd,foreach_frontend,not user facing,PEP585 update - torch/utils (#145201),torch/utils/_backport_slots.py torch/utils/_config_module.py torch/utils/_content_store.py torch/utils/_cxx_pytree.py torch/utils/_foreach_utils.py torch/utils/_freeze.py torch/utils/_get_clean_triton.py torch/utils/_ordered_set.py torch/utils/_python_dispatch.py torch/utils/_pytree.py torch/utils/_stats.py torch/utils/_strobelight/cli_function_profiler.py torch/utils/_sympy/functions.py torch/utils/_sympy/interp.py torch/utils/_sympy/solve.py torch/utils/_sympy/symbol.py torch/utils/_sympy/value_ranges.py torch/utils/_traceback.py torch/utils/backend_registration.py torch/utils/benchmark/utils/_stubs.py torch/utils/benchmark/utils/common.py torch/utils/benchmark/utils/compare.py torch/utils/benchmark/utils/compile.py torch/utils/benchmark/utils/cpp_jit.py torch/utils/benchmark/utils/fuzzer.py torch/utils/benchmark/utils/sparse_fuzzer.py torch/utils/benchmark/utils/timer.py torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py torch/utils/bottleneck/__main__.py torch/utils/bundled_inputs.py torch/utils/cpp_extension.py torch/utils/data/_utils/collate.py torch/utils/data/dataloader.py torch/utils/data/datapipes/_decorator.py torch/utils/data/datapipes/_typing.py torch/utils/data/datapipes/dataframe/dataframes.py torch/utils/data/datapipes/dataframe/datapipes.py torch/utils/data/datapipes/dataframe/structures.py torch/utils/data/datapipes/datapipe.py torch/utils/data/datapipes/gen_pyi.py torch/utils/data/datapipes/iter/callable.py torch/utils/data/datapipes/iter/combinatorics.py torch/utils/data/datapipes/iter/combining.py torch/utils/data/datapipes/iter/filelister.py torch/utils/data/datapipes/iter/fileopener.py torch/utils/data/datapipes/iter/grouping.py torch/utils/data/datapipes/iter/routeddecoder.py torch/utils/data/datapipes/iter/selecting.py torch/utils/data/datapipes/iter/sharding.py torch/utils/data/datapipes/iter/streamreader.py torch/utils/data/datapipes/map/combinatorics.py torch/utils/data/datapipes/map/combining.py torch/utils/data/datapipes/map/grouping.py torch/utils/data/datapipes/utils/common.py torch/utils/data/dataset.py torch/utils/data/distributed.py torch/utils/data/graph.py torch/utils/data/graph_settings.py torch/utils/data/sampler.py torch/utils/flop_counter.py torch/utils/hipify/hipify_python.py torch/utils/hooks.py torch/utils/jit/log_extract.py torch/utils/mobile_optimizer.py torch/utils/model_dump/__init__.py torch/utils/module_tracker.py torch/utils/tensorboard/_pytorch_graph.py torch/utils/tensorboard/summary.py torch/utils/tensorboard/writer.py torch/utils/viz/_cycles.py,https://github.com/pytorch/pytorch/pull/145201,aorenste,bobrenjc93,,,
bac62341ebf,inductor,not user facing,PEP585 update - torch/_inductor (#145198),torch/_inductor/jagged_lowerings.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/kernel/unpack_mixed_mm.py torch/_inductor/loop_body.py torch/_inductor/lowering.py torch/_inductor/memory.py torch/_inductor/metrics.py torch/_inductor/mkldnn_ir.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/mock_cache.py torch/_inductor/ops_handler.py torch/_inductor/optimize_indexing.py torch/_inductor/output_code.py torch/_inductor/package/package.py torch/_inductor/pattern_matcher.py torch/_inductor/remote_cache.py torch/_inductor/runtime/autotune_cache.py torch/_inductor/runtime/benchmarking.py torch/_inductor/runtime/compile_tasks.py torch/_inductor/runtime/hints.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py torch/_inductor/subgraph_lowering.py torch/_inductor/triton_bundler.py torch/_inductor/utils.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/145198,aorenste,bobrenjc93,,,
895659cb41c,skip,Untopiced,"Revert ""Fix RMSNorm epsilon value type for BF16 or FP16 (#142848)""",aten/src/ATen/native/layer_norm.cpp,,,,,,
35f5668f7e6,cuda,new features,[NVIDIA] RTX50 Blackwell Support codegen (#145270),cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/145270,johnnynunez,ezyang,,,
7dd9d1f2433,releng,not user facing,Update clickhouse-connect to 0.8.14 (#144915),.github/workflows/close-nonexistent-disable-issues.yml .github/workflows/weekly.yml,https://github.com/pytorch/pytorch/pull/144915,clee2000,huydhn,,,
19c3ba44a24,skip,not user facing,Use TORCH_CHECK instead of std::runtime_error in stack.h and ivalue.h (#145280),aten/src/ATen/core/ivalue.h aten/src/ATen/core/stack.h,https://github.com/pytorch/pytorch/pull/145280,janeyx99,Skylion007,malfet,,
e261629dc85,skip,not user facing,[triton] Update triton pin to include warp specialization support (#145120),.ci/docker/ci_commit_pins/triton.txt,https://github.com/pytorch/pytorch/pull/145120,htyu,bertmaher,,,
1e8d6d6f0ee,dynamo,not user facing,[SkipFiles] New modules added to torch.* are inlined by default (#145279),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/145279,zou3519,yanboliang,,,
266fd35c584,releng,not user facing,"Fix ExecuTorch, XLA, Triton hash updates (#145314)",.github/merge_rules.yaml .github/workflows/weekly.yml,https://github.com/pytorch/pytorch/pull/145314,huydhn,izaitsevfb,,,
1908116ace4,mps,not user facing,[MPS][BE] Move vectypes from Quantized to utils (#145312),aten/src/ATen/native/mps/kernels/Quantized.metal c10/metal/utils.h,https://github.com/pytorch/pytorch/pull/145312,malfet,dcci,,,
c106e9b4c61,mps,not user facing,[BE][MPS] Move Gamma kernels to its own file (#145289),aten/src/ATen/native/mps/kernels/Gamma.metal aten/src/ATen/native/mps/operations/Gamma.mm,https://github.com/pytorch/pytorch/pull/145289,malfet,dcci,manuelcandales,,
f2d7fe12d81,mps,not user facing,[BE][MPS] Mark gamma inputs as const (#145295),aten/src/ATen/native/mps/kernels/Gamma.metal,https://github.com/pytorch/pytorch/pull/145295,malfet,manuelcandales,,,
2d1649bc2a1,skip,Untopiced,"Revert ""[triton] Update triton pin to include warp specialization support (#145120)""",.ci/docker/ci_commit_pins/triton.txt,,,,,,
1ce533867f3,dynamo,not user facing,Teach dynamo to handle GenericAlias without a graph break (#145240),test/dynamo/test_misc.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/145240,aorenste,anijain2305,,,
f2cfe8b59f5,jit,not user facing,PEP585 update - mostly toplevels (#145178),torch/_C/_cudnn.pyi torch/__init__.py torch/_custom_op/impl.py torch/_dispatch/python.py torch/_guards.py torch/_jit_internal.py torch/_linalg_utils.py torch/_lobpcg.py torch/_logging/_internal.py torch/_logging/scribe.py torch/_logging/structured.py torch/_lowrank.py torch/_meta_registrations.py torch/_ops.py torch/_prims_common/__init__.py torch/_prims_common/wrappers.py torch/_sources.py torch/_tensor.py torch/_tensor_str.py torch/_torch_docs.py torch/_utils.py torch/_utils_internal.py torch/_vmap_internals.py torch/_weights_only_unpickler.py torch/amp/grad_scaler.py torch/autograd/__init__.py torch/autograd/graph.py torch/contrib/_tensorboard_vis.py torch/functional.py torch/futures/__init__.py torch/hub.py torch/jit/_pickle.py torch/library.py torch/overrides.py torch/random.py torch/signal/windows/windows.py torch/storage.py torch/torch_version.py torch/types.py,https://github.com/pytorch/pytorch/pull/145178,aorenste,bobrenjc93,,,
27598cd154d,fx,Untopiced,[fx] move DCE rand check to import time (#145118),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_ops.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/145118,xmfan,ezyang,zou3519,,
46851022ffc,inductor,not user facing,[Inductor][CPU] Add auto-tuning support for da8w8 sym act sym wgt GEMM (#143187),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/fx_passes/quantization.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/143187,sanchitintel,jansel,jgong5,leslie-fang-intel,
3a585126131,skip,not user facing,[Inductor] inplace padding (#140249),test/inductor/test_inplace_padding.py test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_fx.py torch/_inductor/config.py torch/_inductor/graph.py torch/_inductor/lowering.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/140249,shunting314,eellison,jansel,,
dcd9de79e7b,dynamo,not user facing,[dynamo] Re-enable a AOT-Dispatch test with Dynamo (#145299),test/dynamo_expected_failures/TestAOTModuleSimplified.test_aot_module_simplified_fake_tensor_gm_raises,https://github.com/pytorch/pytorch/pull/145299,StrongerXi,zou3519,,,
40e27fbcf2b,skip,not user facing,Refactor CPUReproTests to be more vector-length agnostic (#141245),test/inductor/test_cpu_repro.py,https://github.com/pytorch/pytorch/pull/141245,Mousius,desertfire,malfet,,
99dbc5b0e25,quantization,not user facing,PEP585 update - test (#145176),test/benchmark_utils/test_benchmark_utils.py test/conftest.py test/cpp_extensions/python_agnostic_extension/python_agnostic/ops.py test/custom_operator/test_infer_schema_annotation.py test/distributed/_composable/fsdp/test_fully_shard_autograd.py test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_extensions.py test/distributed/_composable/fsdp/test_fully_shard_frozen.py test/distributed/_composable/fsdp/test_fully_shard_init.py test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py test/distributed/_composable/fsdp/test_fully_shard_state_dict.py test/distributed/_composable/fsdp/test_fully_shard_training.py test/distributed/_composable/test_checkpoint.py test/distributed/_composable/test_composability/test_2d_composability.py test/distributed/_composable/test_contract.py test/distributed/_shard/sharded_tensor/test_sharded_tensor.py test/distributed/_shard/sharding_spec/test_sharding_spec.py test/distributed/_tools/test_mem_tracker.py test/distributed/_tools/test_runtime_estimator.py test/distributed/_tools/test_sac_ilp.py test/distributed/checkpoint/e2e/test_e2e_save_and_load.py test/distributed/checkpoint/test_checkpoint.py test/distributed/checkpoint/test_dtensor_checkpoint.py test/distributed/checkpoint/test_file_system_checkpoint.py test/distributed/checkpoint/test_file_system_checkpoint_cpu.py test/distributed/checkpoint/test_fsspec.py test/distributed/checkpoint/test_state_dict.py test/distributed/elastic/agent/server/test/api_test.py test/distributed/elastic/agent/server/test/local_elastic_agent_test.py test/distributed/elastic/multiprocessing/api_test.py test/distributed/elastic/multiprocessing/tail_log_test.py test/distributed/elastic/rendezvous/api_test.py test/distributed/elastic/rendezvous/dynamic_rendezvous_test.py test/distributed/elastic/rendezvous/rendezvous_backend_test.py test/distributed/elastic/rendezvous/utils_test.py test/distributed/elastic/test_control_plane.py test/distributed/elastic/utils/util_test.py test/distributed/fsdp/test_fsdp_backward_prefetch.py test/distributed/fsdp/test_fsdp_comm.py test/distributed/fsdp/test_fsdp_core.py test/distributed/fsdp/test_fsdp_grad_acc.py test/distributed/fsdp/test_fsdp_hybrid_shard.py test/distributed/fsdp/test_fsdp_misc.py test/distributed/fsdp/test_fsdp_mixed_precision.py test/distributed/fsdp/test_fsdp_optim_state.py test/distributed/fsdp/test_fsdp_sharded_grad_scaler.py test/distributed/fsdp/test_fsdp_state_dict.py test/distributed/fsdp/test_fsdp_tp_integration.py test/distributed/fsdp/test_fsdp_unshard_params.py test/distributed/fsdp/test_fsdp_use_orig_params.py test/distributed/fsdp/test_utils.py test/distributed/launcher/api_test.py test/distributed/nn/jit/test_instantiator.py test/distributed/optim/test_zero_redundancy_optimizer.py test/distributed/pipelining/schedule_registry.py test/distributed/pipelining/test_schedule.py test/distributed/tensor/debug/test_comm_mode_features.py test/distributed/tensor/experimental/test_tp_transform.py test/distributed/tensor/parallel/test_tp_examples.py test/distributed/tensor/test_matrix_ops.py test/distributed/tensor/test_pointwise_ops.py test/distributed/tensor/test_view_ops.py test/distributed/tensor/test_xla_integration.py test/distributed/test_c10d_common.py test/distributed/test_c10d_functional_native.py test/distributed/test_c10d_nccl.py test/distributed/test_c10d_spawn_gloo.py test/distributed/test_c10d_spawn_nccl.py test/distributed/test_c10d_spawn_ucc.py test/distributed/test_dynamo_distributed.py test/dynamo/test_activation_checkpointing.py test/dynamo/test_export.py test/dynamo/test_functions.py test/dynamo/test_modules.py test/dynamo/test_python_autograd.py test/dynamo/test_reconstruct.py test/dynamo/test_repros.py test/dynamo/test_subclasses.py test/dynamo/test_torchrec.py test/dynamo/test_trace_rules.py test/export/test_converter.py test/export/test_draft_export.py test/export/test_lift_unlift.py test/export/test_passes.py test/functorch/test_aotdispatch.py test/functorch/test_parsing.py test/functorch/test_rearrange.py test/fx/test_dce_pass.py test/fx/test_future.py test/fx/test_fx_split.py test/higher_order_ops/test_with_effects.py test/inductor/test_aot_inductor.py test/inductor/test_codecache.py test/inductor/test_cooperative_reductions.py test/inductor/test_cutlass_backend.py test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py test/inductor/test_fp8.py test/inductor/test_fuzzer.py test/inductor/test_group_batch_fusion.py test/inductor/test_inplacing_pass.py test/inductor/test_max_autotune.py test/inductor/test_mmdecomp.py test/inductor/test_padding.py test/inductor/test_pattern_matcher.py test/inductor/test_perf.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_config_overrides.py test/inductor/test_torchinductor_dynamic_shapes.py test/inductor/test_torchinductor_strided_blocks.py test/mobile/model_test/android_api_module.py test/mobile/test_lite_script_module.py test/mobile/test_lite_script_type.py test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/internal/test_diagnostics.py test/onnx/model_defs/word_language_model.py test/onnx/onnx_test_common.py test/onnx/test_models_onnxruntime.py test/onnx/test_onnxscript_no_runtime.py test/onnx/test_op_consistency.py test/onnx/test_pytorch_onnx_no_runtime.py test/onnx/test_pytorch_onnx_onnxruntime.py test/quantization/ao_migration/common.py test/quantization/bc/test_backward_compatibility.py test/quantization/core/test_quantized_op.py test/quantization/core/test_workflow_ops.py test/quantization/eager/test_quantize_eager_ptq.py test/quantization/fx/test_model_report_fx.py test/quantization/fx/test_quantize_fx.py test/quantization/jit/test_ondevice_quantization.py test/quantization/jit/test_quantize_jit.py test/quantization/pt2e/test_duplicate_dq.py test/quantization/pt2e/test_metadata_porting.py test/quantization/pt2e/test_numeric_debugger.py test/quantization/pt2e/test_quantize_pt2e.py test/quantization/pt2e/test_quantize_pt2e_qat.py test/quantization/pt2e/test_representation.py test/run_test.py test/test_datapipe.py test/test_fx.py test/test_fx_experimental.py test/test_jit_autocast.py test/test_jit_fuser_te.py test/test_jit_string.py test/torch_np/numpy_tests/core/test_scalar_methods.py test/typing/pass/disabled_jit.py,https://github.com/pytorch/pytorch/pull/145176,aorenste,bobrenjc93,,,
3cbc8c54fd3,skip,not user facing,[BE][export] Remove disabled floordiv test in export (#145292),test/export/test_export.py,https://github.com/pytorch/pytorch/pull/145292,yiming0416,pianpwk,,,
31c2f36989e,skip,Untopiced,Fix triton masked loading for non-block tl.loads (#144782),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/144782,isuruf,eellison,,,
079a3e0f759,inductor,not user facing,[BE] Add type annotations to cudagraph_utils.py and test_cases.py (#145291),torch/_inductor/cudagraph_utils.py torch/_inductor/output_code.py torch/_inductor/test_case.py,https://github.com/pytorch/pytorch/pull/145291,BoyuanFeng,Skylion007,,,
4b77ff97840,dynamo,Untopiced,Fix PythonMod printing for C++ (#143385),c10/util/generic_math.h test/inductor/test_indexing.py torch/utils/_sympy/printers.py,https://github.com/pytorch/pytorch/pull/143385,isuruf,anijain2305,leslie-fang-intel,,
fbaef0ac038,dynamo,Untopiced,Add a language option for symbolic shape guards (#143164),test/dynamo/test_logging.py torch/_dynamo/guards.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/143164,isuruf,ezyang,,,
0efa8433923,dynamo,Untopiced,Dynamic shape guards in C++ (#139899),benchmarks/dynamo/pr_time_benchmarks/benchmarks/basic_modules_benchmarks.py benchmarks/dynamo/pr_time_benchmarks/benchmarks/sum_floordiv.py test/dynamo/test_logging.py torch/_C/_dynamo/guards.pyi torch/_dynamo/config.py torch/_dynamo/guards.py torch/_dynamo/source.py torch/csrc/dynamo/guards.cpp torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/139899,isuruf,albanD,anijain2305,jansel,
0dbff7e4be4,skip,not user facing,Add MKLDNN support for Half GELU (#145339),aten/src/ATen/native/Activation.cpp,https://github.com/pytorch/pytorch/pull/145339,CaoE,Skylion007,leslie-fang-intel,yanbing-j,
c2b401933f7,skip,not user facing,[torchbench] Fix mobilenetv2 inductor freezing fail_accuracy (#145296),benchmarks/dynamo/torchbench.py benchmarks/dynamo/torchbench.yaml,https://github.com/pytorch/pytorch/pull/145296,IvanKobzarev,eellison,zou3519,,
288f21cc110,mps,not user facing,[MPS][BE] Prepare Gamma funcs to be moved ot headers (#145309),aten/src/ATen/native/mps/kernels/Gamma.metal,https://github.com/pytorch/pytorch/pull/145309,malfet,dcci,,,
c27dd9cf722,skip,not user facing,Fix deprecated pytorch_sphinx_theme editable installation (#145347),.ci/docker/requirements-docs.txt docs/requirements.txt,https://github.com/pytorch/pytorch/pull/145347,huydhn,ZainRizvi,,,
057d9aff39c,skip,not user facing,[S481486] [MTIA] Correct mtia.device_count() API (#145338),aten/src/ATen/detail/MTIAHooksInterface.h torch/_C/__init__.pyi.in torch/csrc/mtia/Module.cpp torch/mtia/__init__.py,https://github.com/pytorch/pytorch/pull/145338,chaos5958,BoyueZheng,egienvalue,,
698106951ec,dynamo,not user facing,[dynamo] Re-enable `test_fs` family for dynamo (#145302),test/test_multiprocessing.py,https://github.com/pytorch/pytorch/pull/145302,StrongerXi,zou3519,,,
0bff3778804,skip,not user facing,Align CPU behavior with CUDA for `ConvTranspose` when `out_channels=0` (#142859),aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp test/nn/test_convolution.py,https://github.com/pytorch/pytorch/pull/142859,chunyuan-w,malfet,mingfeima,,
30717d25fe4,skip,not user facing,Move Dynamo test to skip from expected_failures (#145390),test/dynamo_expected_failures/TestCppExtensionJIT.test_cpp_frontend_module_has_same_output_as_python test/dynamo_skips/TestCppExtensionJIT.test_cpp_frontend_module_has_same_output_as_python,https://github.com/pytorch/pytorch/pull/145390,zou3519,williamwen42,,,
ac8ddf11506,export,Untopiced,[export][be] Clean up local imports from export [1/n] (#145287),torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/145287,zhxchen17,pianpwk,,,
e8e3c03f96f,inductor,not user facing,[Test][Inductor] Fix test_tma_graph_breaks (#145271),test/inductor/test_triton_kernels.py,https://github.com/pytorch/pytorch/pull/145271,Aidyn-A,jansel,,,
b81209557bb,skip,not user facing,Fix tests broken by #145176 (#145393),test/dynamo/test_repros.py,https://github.com/pytorch/pytorch/pull/145393,aorenste,ZainRizvi,,,
70ccbade83a,mps,not user facing,[MPSInductor] Add `gamma` op (#145341),aten/src/ATen/native/mps/kernels/Gamma.metal c10/metal/special_math.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/145341,malfet,Skylion007,dcci,,
3917053f63b,releng,not user facing,[audio hash update] update the pinned audio hash (#145328),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/145328,pytorchupdatebot,pytorchbot,,,
95ff9f03408,skip,not user facing,[Doc] Add period at the end of the sentence (#145384),torch/compiler/__init__.py,https://github.com/pytorch/pytorch/pull/145384,malfet,huydhn,kit1980,svekars,
0940eb6d44f,linalg_frontend,Untopiced,Reverting the PR adding Kleidiai-based int4 kernels (#145392),.gitmodules BUILD.bazel CMakeLists.txt WORKSPACE aten/src/ATen/CMakeLists.txt aten/src/ATen/Config.h.in aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cpu/int_mm_kernel.h aten/src/ATen/native/kleidiai/kai_kernels.cpp aten/src/ATen/native/kleidiai/kai_kernels.h aten/src/ATen/native/kleidiai/kai_pack.h aten/src/ATen/native/kleidiai/kai_ukernel_interface.cpp aten/src/ATen/native/kleidiai/kai_ukernel_interface.h aten/src/ATen/native/native_functions.yaml buckbuild.bzl cmake/Dependencies.cmake cmake/Summary.cmake cmake/TorchConfig.cmake.in docs/source/backends.rst setup.py test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_torchinductor.py test/test_linalg.py third_party/kleidiai torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/quantized_lowerings.py torch/_meta_registrations.py torch/backends/__init__.py torch/backends/kleidiai/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_quantization.py torchgen/aoti/fallback_ops.py,https://github.com/pytorch/pytorch/pull/145392,albanD,ZainRizvi,atalman,digantdesai,
a57133e3c77,cuda,build,[NVIDIA] Jetson Thor Blackwell Support codegen (#145395),cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/145395,johnnynunez,eqy,malfet,,
2efa98d69d3,skip,not user facing,Binary upload checksum (#144887),.circleci/scripts/binary_upload.sh,https://github.com/pytorch/pytorch/pull/144887,clee2000,atalman,,,
9f150786bb8,dynamo,not user facing,[dynamo] Fix numpy test accuracy error induced by randomness divergence (#145293),test/dynamo_expected_failures/TestGradient.test_second_order_accurate test/torch_np/numpy_tests/lib/test_function_base.py,https://github.com/pytorch/pytorch/pull/145293,StrongerXi,lezcano,,,
0cb9b2284a3,skip,Untopiced,[BE]: Simplify set add with set update (#145152),torch/_functorch/partitioners.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/145152,Skylion007,XuehaiPan,albanD,,
5531fafffef,skip,not user facing,[compiled autograd] Proxy opaque nodes for built-in autograd nodes (#143296),aten/src/ATen/TensorGeometry.h build_variables.bzl test/dynamo/test_backward_higher_order_ops.py test/inductor/test_compiled_autograd.py test/inductor/test_distributed_patterns.py tools/autograd/gen_autograd_functions.py torch/_dynamo/compiled_autograd.py torch/csrc/autograd/engine.cpp torch/csrc/autograd/engine.h torch/csrc/autograd/function.h torch/csrc/autograd/function_hook.h torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/dynamo/compiled_autograd.cpp torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/143296,zou3519,jansel,,,
8c7c5f7bfcb,skip,not user facing,[compiled autograd] Proxy a node for CopyBackwards into the graph (#143304),torch/csrc/autograd/functions/tensor.cpp,https://github.com/pytorch/pytorch/pull/143304,zou3519,jansel,xmfan,,
784bb2127ca,skip,not user facing,[compiled autograd] Proxy nodes for user-defined C++ torch::autograd::Function (#143387),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/autograd/custom_function.cpp torch/csrc/autograd/custom_function.h torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/143387,zou3519,jansel,xmfan,,
ec820fe57c2,skip,not user facing,[compiled autograd] Always proxy autograd.Function nodes; handle AOT backwards (#143405),test/dynamo/test_backward_higher_order_ops.py test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/_dynamo/external_utils.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/autograd/function.py torch/csrc/autograd/python_function.cpp,https://github.com/pytorch/pytorch/pull/143405,zou3519,jansel,xmfan,,
99dd1bf1b93,skip,Untopiced,[compiled autograd] stop specializing on metadata during initial trace (#143417),torch/_dynamo/compiled_autograd.py torch/_dynamo/trace_rules.py torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/autograd/init.cpp torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/143417,zou3519,jansel,xmfan,,
082c28c3c65,skip,Untopiced,[compiled autograd] support Tensor Subclasses in AOTBackward (#144115),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/subclass_utils.py,https://github.com/pytorch/pytorch/pull/144115,zou3519,bdhirsh,jansel,xmfan,
dddf52b1b91,skip,Untopiced,"Revert ""Enable grep_linter to use -a (#144589)""",tools/linter/adapters/grep_linter.py,,,,,,
6e53588789c,skip,Untopiced,"Revert ""[BE]: Simplify set add with set update (#145152)""",torch/_functorch/partitioners.py torch/export/_trace.py,,,,,,
de945d78da9,skip,not user facing,[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/144441,eqy,Chillee,malfet,,
f0a210bf5d2,skip,Untopiced,"Revert ""Output of nonzero is transposed, fix fake tensor (#144695)""",test/inductor/test_auto_functionalize.py test/test_fake_tensor.py torch/_dynamo/debug_utils.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_subclasses/fake_impls.py,,,,,,
e6a84be3d30,dynamo,Untopiced,[PyTorch] Add backend aot_eager_decomp_partition_with_mode (#143250),torch/_dynamo/backends/debugging.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/143250,silverlakeli,bdhirsh,,,
35c8c31f11e,fx,Untopiced,Fix for failure in D68425364 (#145304),test/test_fx.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/145304,aorenste,izaitsevfb,,,
4803e20bc78,skip,not user facing,[S481486] Move MTIA dynamic library loading from __init__.py to a separate module (#145322),torch/mtia/__init__.py,https://github.com/pytorch/pytorch/pull/145322,chaos5958,albanD,yuhc,,
d0a2e11284e,skip,not user facing,[BE][export] Change custom_op registeration style (#145315),test/export/test_export.py,https://github.com/pytorch/pytorch/pull/145315,yiming0416,zou3519,,,
1e32842324c,cuda,Untopiced,Improve softmax's perf in cuda (#144679),aten/src/ATen/native/cuda/PersistentSoftmax.cuh aten/src/ATen/native/cuda/SoftMax.cu,https://github.com/pytorch/pytorch/pull/144679,ywq880611,eqy,,,
c9e12d6a3b6,releng,not user facing,[ROCm] Update rocm.yml and add rocm-mi300.yml (#145398),.github/workflows/rocm-mi300.yml .github/workflows/rocm.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/145398,amdfaa,jeffdaily,,,
0d28188cc8f,skip,not user facing,Move privateuse1 test out of test_utils and make them serial (#145380),test/run_test.py test/test_extension_utils.py test/test_utils.py,https://github.com/pytorch/pytorch/pull/145380,albanD,Skylion007,janeyx99,,
d95a6babcc5,skip,Untopiced,"Revert ""Align CPU behavior with CUDA for `ConvTranspose` when `out_channels=0` (#142859)""",aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp test/nn/test_convolution.py,,,,,,
5a18f1e1eb5,dynamo,not user facing,[dynamo] Support fx map_aggregate (#145351),test/dynamo/test_functions.py torch/_dynamo/exc.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/145351,anijain2305,zou3519,,,
28b6430823a,cpp_frontend,Untopiced,Introduce a new API isAcceleratorExcluded (#144959),aten/src/ATen/DeviceAccelerator.h,https://github.com/pytorch/pytorch/pull/144959,guangyey,albanD,,,
719938c77f4,skip,not user facing,Generalize pin memory logic for accelerator when non blocking copy happened (#143783),aten/src/ATen/native/TensorConversions.cpp test/test_accelerator.py,https://github.com/pytorch/pytorch/pull/143783,guangyey,EikanWang,albanD,,
5e6451ea788,distributed,Untopiced,[c10] catch c10 error and log message (#145413),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/145413,c-p-i-o,fduwjj,,,
faa10faa2ca,distributed,Untopiced,[ROCm] CK SDPA - Move arch check to CK patch (#144777),aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_00042c36bc588e60a7c8a9ba297a8a25d8ac0660.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0029076f83a3dc695a167beda6fe19230a2b114b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_006c417a52a1bd7c55e45d111483d26f4480caeb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_008f2429c678d13386a06e8d8b15c4b480940ff3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_00a2adbe938d458d51ca5fc4020667a215b672a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_012c0f480917c329f4c3c6c666cf32af2d82b294.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_014c209d5cfc6b965bfd78c64bf132c0154e32be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0153ec18d3ded0f8bdc6459ea5757ebd94d9faf2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01ac1a2ecf9a487809e46faa92e267df2d47de91.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01ca79005067e20e4eed5a72ff9187cde702cd1c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01cb354dddef6e99e4ac843f2adafcddfc58d520.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01d12033d59ce2799a2a024e5d9232325ccf1320.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01d3b034a2d8d0b83c0aefa4faac6c3f28ce737f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01e2428c5447aa9a78f79f73f31cf685c586872d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01e8aedb7b7d77f44a46b2e9b7a826f245aaf4a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01e8f0df0c54ce619e5b66441b3c96a5e18b05d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01ee0083f6df962c4a754cd3295b1a436c590a0e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01f74764c3c3284fdd1b67d0ea781c2261ed0de6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0225857454eaab2eb664aef7a0849ce12c32fdf9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0237c76137df14fb808ade8bd6837045f2aaa5c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0271bd8b7c270e1593871b638288a4923342c446.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_02d88a03cd3966dd0cff550065f58c3ffecfff6c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_02ff94e3c787a7b06ffc90c25777fa74f225e32c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_030a759dcc92028b4c6f317fc230b98cb929e806.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_031b12f9fd94e01aaff2c0da4f35f346822087e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_036887daf6cc092e7422a17882488e59cecfb643.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_037c6c80fcec3eb8b0bef50ad6af6d27bf5447f5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0392491c5a6dfc742c2be483419a40f6a7a7ea56.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_03a71615a088e972c998f9c7cb44566c268c5124.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_03ff035717140f7385282419598cb4fb2881ce8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_041a0718891596ddac1fb0088637029233ccbe60.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_042a156e9eb935555ab14a84461959b466c2fb5b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04641230fe9a50a221047f7a1df8a370f72805b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04c363e11d202c6d2f4bb753661c5a2043edc0ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04caeecbc01667ec6f5599358a0a20423aa9a00b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04f39b453505f68a5091f68b1c3de48369d1e7ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04ffca078cfab8bc6c4ccd1cc8994a1bb4a88ea7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0502e718337eab7d47aa65cea7d3c5f641484520.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0513b2f3bd8ad51315aadb7f63737201898adca8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_053981d9e7af2ebc0f91e61ac5e25cbe68c95bd8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_054fda16133a0d25077967b05425f9128e1fe1a5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05538339c21c92c53d237865d72debaaf2ee5075.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0595316f0dfffda03e5296b959a49ec3f3c48d67.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05dfe927fd64a564c5fad537fb7c41ee9c94c2c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05e60b3ab7477f9edc8576a8bf43e3a62b8d5ef8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05f794c7023cbb7e35f1fd1ae45bd2377bfbc520.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0628931bf5cc1daa6e106cf60bb21fa1aac6b1df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_062c8c3c1cf6c33af4574099e9b6ac54a55ad776.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0682150e93f547e00f13cd8984779bf49b91e50c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_069c663be0267c009be4814e9e4e7c13ec999411.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_06ae52ef937cc27c544e32025ea0dadb7fad982d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_06b74acd9abfbd1c4ec2f4c718eeb92a0bca7bab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_06ba94794a14f0f0022af6f5f3c16e1e16959d4c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_071751b1012b90f7b57f8591cd06ae1fd27d9cd3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0766e7aa4b263a811408b285213e47176ee2bdaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_076b3beb57b30afb30636f948e3989b346b38d20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0789852b0cd3cc030c78b28f2fd5b6b0546382a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_078b96ad691a85eebd18586db0b62b8911016d9c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_07c3fc96d2bebe546dce6ebf46e5c7a519959599.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_07ff04fcc273e469737512893ea3fb5876ac131d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0801c56831b4c6428200db6318638a2129bb197a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0836d5dfc0f939ab9a4064b403339373caf35b56.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0842c4e3aabdf55405b3ce09ce1899245ddf11ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_085722b43cde5f37242edb071f639da7c4a0bd48.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0878b9aa31429d23a93cd953cc6a2fc5f43d0d3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_089a347aef8a920e3b59d5ffe71fc5bfe002609c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_089de13222caec1483207d4a54249f8da4f9c151.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_091cb49c1958fb4342d79f367ea93cf2b472f785.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_093834d4d3fe76e1745e4482c6b51b550c6f3dfc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09513bff5c1da6aadf11d2e8272a422eabff21bc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_096863cd93d1b105a617d0daa1d4f37d7fb6b893.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0968cebd81ade762c2f92fffc0153fa7a2b91eb5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_096e888c52d0f4a5847d7515fcc66208b1ff40d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_097b3e1dae9bfb2e89398706508f8e01966fd4ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09d76cca48b71dbcc9bd96734787209fee4c9a74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09e50367b62bb09071e28b44235a7c112645a706.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09ecb6347009f6a5d5530a6acf90f9f40288cbcf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a2b116fd5065109aae46ee547e4f49ad0e9d6e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a4e76d89b175e1d9fd2e9fb908d5fce1ebb945d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a55ed15ef58c941e06dda890aeb530e28eb7bba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a672fca51de618e3441cf8764e8e83eb782f2c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a68c2f9a3acdd787b81be455cbc7836c8bfd90c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a89417a043556970f72eebd48b4f3e7ac15377a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a92671b6ea99891c0d69b1c793f4d131b9a82ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0aafb881e34a3794970a1282af740b3f19c138b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ace6e29e1d3060c3086c08fe27b471e375f9c75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ad9d68fcee021437e13ffdf94d78252205f5a31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b2647b5982405a48e8c8888552a4b89386ccdd9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b2efefea81036641561bed80c75d77651176f74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b3153af7bcdba33115a0d31f121fd76be2ffbcc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b532fcf26f90c82a792cde7943634f667c1d033.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b90a0186d8b8004e3f19886c7992c8e04d0e066.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b9585ba1c10acf67115c5899b3546608541820d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0bb81407c8a2b3cdc5fecf655b3ad64d5d729cc9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0bc7910aac798f0555e9e505ad7f177c9fbbd92c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0be8cf70c6be969ecfca675782c860b5b75ac089.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0befed50a89d80c22b2c8c3d5ba67d73c3d0190e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c32a2d9701e23dd930119c4ee8089042b5b0ac5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c3b2ec99fa7b09c7f78dcc3142a661d686044ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c8a0bb89a6f05289c0405df5126fa0cc16252e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c93c65e5942a2f43f2e491547add02777dd2eee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c9bd38b8f9009d932ec49204fdea39a52885246.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0caeedaa7d50f1741d618fb6c573529eebb075b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0cdef49859c80c6b3ba18eb2fb4c35c72abc1cf2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0cee6b9427c164d78994150305a47f73954a67c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0d0e0147a92061d32608a34e7b47bd534eb787fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0d13a4c8d169877da6408584dc1f20a6f7c5e3aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0dde401aa76cb5425563cbbdb0362748148da3ca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e007c36231ccdae12f102eacca1f74b0711b9c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e0a2370f2a320484d8f9f21e3197425c2dbe9ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e1dbc9c433ce8ec33ace9e62550261d613db582.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e3f4cd28a4c06cc109f6a0798a77844bcc750b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e661b5f30566d1f159f060c264849c7ae4772f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ebacd06455ab20eba78b389462946716b5819f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ef309b923172f4c0fb38d9b9f5325b33b4877c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ef9b9413697d6f4573c6605bff6f58d027c5016.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0efdaa9266a5a464009297dc59db92504f8bf1a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0f0c699d9c3b0ed62097e38ba05e40e815cf474e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0f588dcb2ef86677ebf84e406eb802e9921d1f1e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fbb0bef3b388867e75d7a8a187b8b4b650a42ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fbddf533661642d84bf5a16149692d5a892182a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fcb7492feb79e27e0bda73e57ef7dab410e2bb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fd4068ea93fcf4df463e3bf3a6898d23b65da7f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_103186dbad604763008e0204a1ea90baecef8877.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1037f1bc50c4a65dac09ba56b701256b701c4322.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_10a055e5c3d6a953d470db5dc21449766248058a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_10c24f1f9009e46afa3a59193784cc2575f79056.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_10ceed95b0a0a01f844678717c88e0426fb503fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1132b11429034d96d82c82dbfdb69e460ad8a564.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_11e7df31541c3aa919e9825ad7dc4432f9a03c0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_11ff174ff2175e9ec22ac3a0fa59dd7713b79643.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1211733062ed30b876f1d63bffa642d77e258dd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_12207f4b6e7fac27d6c16493a5373f448a2aaae8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1241814f76107d74ed069ecec99a248676487eee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_12d5c8a4988efe60ef7943ecd73e18a28a736583.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_12d60c8abecb3bc9b84b0ea7851628ab17d8b0b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_131691f01cc7f29affb88152dd48c7a484315dcd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_131c1fdc4206bb952b2fea675f24e3b09f605eef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_133c51948cf8584900807998da14d788039f53b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_135ea67de101135ed5fe04f5cab1ec1d7b3714bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_137fa6780d9e6bde10aec10a875c039fdbbc652e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1386cd75411e61a8dbbaf2b916e62f4f5f99104f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_13d5f2ec83b3331654e37ea0b44d88cd98abaa37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_13f747525ad31e76c88774fb2208e470da9c2310.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14221590b90c48d3cf259fb4e834ccfaf7f3209b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_144f19363ef26efd36f0436cfa9f84f181a8824c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_146eb8c40e3146e06936f3141b2c4d92a578ddec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14baaaf1e90a075ab802c6e7d97c4b1605c8bd72.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14c4ebd1792c781d219bd21b691b575f64635730.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14d11aad7b666f500f68b264a2fcca6dfc5f1a05.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14d4630876785655bd4950566e81ae0b645c0d3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14f77aeeafe4b28f314fde5ebccfd2a554872781.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14fea611f3c253aebf726af3e5fdb7e63e18e13a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_151a4425b411596c46c7032f6b83d3152a0e0cd4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_153e897098539c3466da9d7a37234daf16476277.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1552dc38d26f6badb7a9bcb5ce9124d54cc45ed3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_155bafb551768855c8c01faa63e44764ebe6c110.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_155c3549d067464d186a99b8205317cc000d4898.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1573e3d855d28c54af612ab950b081302891d56d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_157768cd725813f8111d265cfdfea7f42034e5e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_157b89d8d625b8244b5cceaa4d3e5fc5a09c8989.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_158d5ce564c3ae1eefb54e3d41dde2604560ef4a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_159ee1f1b44d1a8fbaead65d8449413bb616d15e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15b255dde1a9d915e582ee2a83de7d83190c6a24.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15cf7068183421b141ed5d6e7fe902d06b6492a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15dc02ea7e0908cf0bd48034f5a49debfaa36219.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15e8e1ab8c63db96843054bb7a98d708ae6a9c44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15fe3e8f4add16a088fe44458353fa7c0c4f9658.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_16047b5544acef40e39932672cac6f562e200948.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1621507cf219fe608715d4e5bb6e5764022e2d61.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_162b0dfbe3f615b1d164290799b2457437a0044b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_164a947a6c2ba83a5b1cb7074aee0bdac6c9c64e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_165dfb45658df8f1ae8dc0738ac9614740f2576c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_167f5328b035ed59a6f05dfee31edd704c4b07ee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1687ddf65ce4ed2997583e20fee9f201e86633b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_16f94f5c65c37624f5458c165daf83517d9e3c81.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_173c44dd85077e6b12dd06fdcf6b11ba349e1866.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_17b9b96edda151072215502cc2b606bf1f6f0b03.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1847fef2c06ea581b0ab31af1cb0556c572696ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_187963e1969301abfa61d06afc97faea2bb4efb1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1886d4bf54b3a4a9e093360998b2059b3c03d072.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_188a70d526394e254274df95de0727850820326c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1899e28aff2fb168cdc3af7132dd7fd09c2e1ced.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_18a4d71b31c451a50df7996e3db864bc3c3882ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_18b92b4e249195ac3e0c74d246585a4c9e0992fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_18ed7195a9443c84956c3f32839cb3ab9056bdfc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1914250fce818584291c69a5f058a58cfbd83df9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_193699a5daa14ca2def07489e0b563149bc403f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_19af6a7f9e5020e8d0f0ca0f6258001f6ce592c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_19cd9f7b08cec83736605af63d9fcaf463a1aea4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_19df4e13108e043361e9528b71df56f04f696a0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a11dd5ebb989503a1c182684e7f247e2f8cd9c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a236be9da05a07d11cd28034d90cdf89941a172.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a5e18f6333ed2cce509f07cb8bd5868951d66a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a6785392af35e27d6697b584cb6f17a766d3fee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a6bc2762b95d550485aa720edaf71138d94cd07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a8da3e6ab050262b659c801ccf9a14787d7f176.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a96f0ac76f117e66eba97cb990c2350561ec2ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a98bcbe900f8c141136d18c114b02fffbe8bca1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a99b2625adffa8215276bb88fc65bae944b846b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1acf2f892742b1d236d2b31a8185c6869126adad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1b3e7c8969027d3316875f33dc50fe022e05ce37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1be43f8b629e7039f57b95866d5777273377470d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1be746990a2032f0363ad9f9112cc994983f4706.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1bf767e7104cfc8322f26df35907fbf04b8948f3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1c1b0f85e085dd0769c566fb16aafe5ab5952714.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1c2a2d78176e3f0a78e3ad78217e75a4430c0de5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1c65ba6dba01da9caa84ba89453b61d81376763f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1cbf88db44aa5f884438288a325270d29c7a04b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1cc459e57bfed5ec7f40ea4a4dd9f72f3ad7a709.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d02609fb803ea2697e2c2cef35e6f923d2578cf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d0b822743e0205f60521d38d7c64f589fdf0f58.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d21263e16dafe79b9fe2f998847296e575c14e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d3ef3d5ded0dfe2a0bafb52ea8f841658db35fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d498e418ebbf33bed58b4074d1edf3d9bdd07c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1da23de9604b5d98fe02529075bad995954c12ca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1db03461737f1e359f389a8d297476f9b60faabd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1dc6e599144a093203fd7f92ac6d3c2cd7180d49.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1de2f97d49f015b9af0b186801e939c6f357a0c4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1df893ee660d37fba7eaca452ae65b3e45a73087.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e22f2d99804198c61251b4629a3f18ed3dcd42e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e33ce1fa113b221e5303b4093c2c4e748ce8298.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e42736d4f677a59a172bd6f162616a437696351.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e7d7888480b83c78833214b32e10f37a6e20301.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e9130607a2d24cb0662a47e9cf12c6602143838.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e943fcc2e64c618fc1415b3f1a0db4d70aa8494.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1edaf9d4270d2ac61c299320e06ba73f44730364.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f0cad6ad5b172e51c569e84cd54a19b4eb0ed05.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f13a6d0f8c798c0c4ba4ad202d081899fe081ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f6bc5faf18be193212217788d476ce6fd384bfb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f7faa0b33a9aada86f032174afd40d18efa7715.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f81f8cce0d77dec9f977b9eeb0778b70a13fa75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1fcdcb750f382fc7828a9886585f50efbe5be735.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1fd9fa7c2e13d0bad5fddb2b5a316bbc09d397ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1fda1c96568eab89a8f6498f8bb23c1223cdc7b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2005aca3520b171bb82d10ad70fef44f28c19776.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_204a573ce6b7d2f90aede543939315561cc43177.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_20588bcac681a5d69f252d7523a3681a0c6b6181.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2081430c92864c29bb9f409e7c27caee1de00749.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_20d5c3c86398f6ce55abc90db3e362dbf9f457f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_20f7ea0aabd069362ba4bbd66623cea5b6e1a6bd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_210ef512b7862837f54acbc3b21e135a192647a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2122c973581930ab7a4ebc90b3bf1cdaa229a87f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21411df58165946bf02942b597d94de7dd856987.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_216806a4598c885e517e664fc8280c59ec3cbf11.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2173b7c710d418f44dc2b41bec5905024334eae5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2177d95cdf45f6fec95d1812f2ef183a75259e38.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21828c7d3f5574690f12f841c27f025206e6165b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2184fba2eec5899bb40d49d4508196e6be1ec1b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21e235e31d6955393ac8e825bd69ead70687b7c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21f860d42fdc2cc6bd743d53ba546e332c22fedf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22105635385fbfb5d2f330df83ba6747bcb27f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_224f9af5e5ca519b21b71a54acb49f50b4999c47.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22511de2592b6e350737e44865e1fed6496e3f32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22632f996eb63fbe4bc5748c5897b775087446a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_226662cf1c9900a4334d2cadcc5f5ac3ad355f05.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2273457ac3be01cc1595a015a5f598f8290c77e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22a07ecf1a59f72ec6bef3e970d7f33cf54c5f44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22c142d869ef940ca876c93033ad53b576ed34f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_23047ea90076e3b0a3eb0586d49b9ee74ca6d279.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_230861e81e5acc523fa680534eed757b7b4a4e1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_232f61bf31dbb5de5d7039d5ff2338068a759b68.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_233132e712eba8972ba444c604f89e01c5b84cc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_235bf652702c2976551778b9159e09188575c63c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_236b3eef02b904304348b9d35f715b639d63218f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_238e4c1ca112afec494fbe47a85b553302c43395.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_23914c00690ac5c4f89cdbbaf00732ba66c5c0ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_23c9b46da8774462de8c24e14b12df3ed596eb57.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_242013527a0266ad479715ee3e6ae01c45de29d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_24410fd9a4150c33186a2a365d06d8f6ea621c20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_245d90000b55ab8b6055b1934880fc6c4870b34b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_24643917fc970c043d1c80d8d4b17ec92deeb8a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_249668a3212cd00edaae871758be30a5a1fea589.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_249e6b93baae25dff97a0bc9145a8d328ed3f317.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2543da478310245e19e6c6a0d9ed7ad99540b3bc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_256ef175029a43e64164176d4eb212baf9d27bb9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_258d747083272ea657604ac84867ecea17bd65da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_25938733446b6c0dcd159719f08d04a9aa467967.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_25b3225da1e1842f83592971a1f62a0fe30aa9d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2660282ad39ef034fecbdb74acedfb48620b7dfd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26835ba70606c769e56d19dbfe74061361aa855e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2695783ae8f0034692efd6563f789ef03fd0f4f3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26d77b228420a3ead919474ec9c6fb2800f86890.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26ea90eb5a527434c1740933a1d2dd863eccf14c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26f90358e522d7bb7c76c3a2c6010f0f38788bb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2703018e71d57d3266fc35e2e18a78faa3dd52ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_278639d44a4a8372a627a7c31e9527c8faa26f97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_27c2000d32c230a57a6712f27bc0fba02722f5fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_280bfced8745fbd9266207463fb41476dc23afff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_281d897ad17d7f6db2741b396e6b85a9b8f35286.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_285e61dad8f63fb973cb2eb899c959e400622652.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_288458c5a0720ef152848713119ebce6d76db6d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_289071756e7d0582eb61ce6483fa3c988d2e10b5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28e4d2c757e4b8c366a2c320360e21ff0ef671a8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28f1ef32c4384ec26f3dc5e3af6a74fc8cebae92.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28f2e2b108a53308a0cb6c123c8d318cbc2eadb4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28f7634d29bef11fd466b452a46b0612f38c949b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_290c484c2a366258941ee0051e139ea716a9de2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_291a8bdf9d63b112e7fe5fa7e8835a6789cb8ecf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_292454f2d82184ab0491ea0675750c6ec55d659c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_292b4f995d622826af5d1f2bffa7ba68467c841a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_295a523f815eb822d66162d4feb75fe0bc50b648.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_296c5836ba118969c4ba89ed62a98dffe3105738.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2995d39cd62f20622a31f11a292ed175abb5fdf9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_29bffc159b0bb826ba489ae763dae141bfe8e802.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_29c9e5384809b21f39e78bb2e43af345a9a21d19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_29fe68ba10b3480dddc9866c51ca8b5efe962cc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a3a980a26682d879c3a3425f3ba5be3f5761adf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a45129fc4995abcb8f880692f11c6186fc01641.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a833fc01e88bd8e256ef64ae8251dd0ed10720b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a97c457144cb63a9c6c3d6be613b47bd0df9928.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ad492377add5c8f6d0d2dbf9ee9e4338bbd9f1f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ae344010d49f7f9a6caab2cb84be7f87d2d96bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2af6c5be53732eb1939a2f93232af7dc011dec1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b0bcb241e5a1be1d35366461408d06e095a26ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b3326e055da32cc979892a2fbd0f7b003cb9f98.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b3af90387f1d227119c5dcd4b71362940bbce52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b4050988e5790a28dbe10b4c20e14f10f6cf85c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b49a9b0801a06dd89c7f7182d7590b515df1592.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b50073f6dfeb7ea77d5dce288a1d2f08f8f6362.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b5317b6cde327a842170ebff20c2b03d81379ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b8169ce4b4b9a17ac96fbb232e6a93f22071ab4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b823c3b99e7c8d1cdc39a5dbc7365a383bf9ccb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ba934408c75da5479cc41f96b98ea7d333635ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2bb6da1095bd8669c0e48b5cd808cf0dcefa2674.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c0bda0feaade2b554d648d72f219ac9c389bf09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c2e75e6f659a500dd3cf2cfd65118f111342119.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c77bd7e89ed832cc31b2995566a49bec6e4cb52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c7aede7762a524a7a424cc4dc46e43fdedf73a2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c808da5c2514806c2953bb77d5692e5d7c97aa3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c82e3c4e445e1e02f14435e4ca01a90850139a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c9756060ac0e73dbcfc58a9222a78f0283cd029.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2caba3ab83239e474412fcf89fe0fbef97e51bf1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2cf351fc2c2da4a8e1760a3affc9a5947c6b3bda.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d06f77a4054ca615d96636c0e2eba2a89850142.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d1f2d1e57095f756ddd11e8e9d4f6f253e3ffa3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d23a26e0a59a8323dd97632e610d24624143fbe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d43460c011b8d5e01ea98c9b8ddce962de59a96.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d446754d7000673779d15d3e73039fd3c10a720.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d7b637e0313cb423b22cd8844cc2997b3ff73e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d9a04b7f41dd6f0db017157a44790f35c626e2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d9c659ba43bb907fd4e3e36a50958288bafd1a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2da2b905c4ce32234c2af62328adae6b1f9217a8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2db33b5442d2e0948762b1f2147a321a9d6907be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2dfac5a83def98340c8786d55a30a98ad68b9eed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2e30f50071113dc4ab59468d568ac9deb06b0342.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2e43e401abbfb1b6737e4dc822f68421abbc648a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2e8b4260626beeac76c26dbcee3cba1457b30e99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ea394a09c8691a534ad2219bedf73724b6dd5ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2eba937ff6d0302ab013db7349d4feb914107f1f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2f0247e301a7b076b6ec8a778c3b47e330638963.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2f32f2d658f1f69840fbad511ce8a3851c859d52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2f55a23a0f24ff7062a4c286944f25d2db3e20a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_30024440e780fdf9ec94deccc85216d8bbb5788a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_303b7b04496e4db7c1ba2436485dc7c8a4c88448.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3076a6de0e2612279e0ed64612f7393856bcc9ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_30c8e4d5c761fda50e010da779e8e4730051d403.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_30f0200092b0e18d57a9f5e512d565f1c0229436.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3108502fd29d3a24b32177bcea968121ee809115.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3110540b50e95e99a5cccebe47d9d3a83093c2fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_311104394c8bef8d4ecff35c1409221e723a5a8a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_311731442b756308c0a869f21b7b8b103aa613e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31222e158484773d2257f4a31e3dfbdb68336a8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3163272d25bc2db2ffaa1fea87648b45ee68d408.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_319df310195191895005b30151da8c1afab6c82f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31a968898f0bc6366313e41eddb5e3a3ed12dc98.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31b807c48c472e9b1311a6037cd98e21d6706889.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31c3760f5978baf9780ce4587ae4c768af0e49d1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31c4b866692ba5c3d115482bef4790733863c1fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3206cc121ce8955ed59ea3b12b858ee2e0cf82f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_320a6196b662a1d3dc7441a9536d825dc356b95d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_321500dd4c41e4d68834814a48a639f5ca36a2fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_322a86568f89a5a5a165cfffbae9ca6949f2477e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_32438250078ba2a47345ec4955dafb4e4de78a25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_32527660fa7aeb9a951a9f2fc3c53989bd141c48.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_325fbcb9e503e68fafea08abf86a4951f440850f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_32652a27e8605cef59c8341813b68e7513be23c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_327e27892bc57f3dec0da24f94f2a483d6c9321b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_328a311bafd1c153525393b252e4170f8aafb370.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_33099fcfc218ffdf69edb4f2f0e46121bea9fafc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_33746071156e9ad46f403a539dc237e0a44122a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_33e7c1e5f41a451c7baff54f7238b220f1bdf8a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3400f0af03743dce328486f8fc805dd30bd6da31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3408103188e27b3bc55dce0c1716c0b4d32d6494.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_342d29c85070f488a14b1915f948e5fd69019c99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_344932e2655d7b32704be8de9a63bbd8c3369f02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_345a939a2491166dc520e9a2b9de7e43671e0c2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_345ea796c8d97bfe3b7c9663bf15e2e5e7696235.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_34807a8e90bf1cd839f32fd718afa6469c35a4fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_349241529745bf138552f49d9a93db418663ad65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_34c2db98d8e2e690f499f41cfd5afb831b756f54.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3511c54e6a6f9eec378d8b661121066536195d3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_351425a006aeeff4d69c8570cb6bf1e1427d2c21.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_354121d3bad1d448bd413718fa096f54faa12e95.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_356f83cb96d0313abcdb24955edd4264df72aed7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_357f7e626135cc9176a295f3d1f336a7c3852688.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_358399e756ed5026baf3ab78af17489dc07b9532.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_358d28c958c0a831a615a4811d13279b18db09c4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3642b78913a853a62dbff8b99d9ae3fa458f461d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_366662dccf2f650bcd8123c49006c759cd4c0ef6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_367e58867c46d96c9bbaa96eaaa9f93595c9e099.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_36a0a960541bd8a2dc6741579de685b7c0a5f6d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_377b70f54cb2778b5ce3df936b477f775eea8b3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_378759ae25465c32960487375828e23c5f1ac869.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_378bf438642e5d863e31145ada2a0688059aa5d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_37ad61bf8427a26775969f8a9166fd0bfb7446b4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_37fe04467e87ec2110f60c7aea0cc9bf2ca07481.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38010c9bf7341588f071f889b7a0b4dcc4e7a14c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_381b29d9888365bff0f109d897b508eebfd8a61f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3824e97d5ecba46e06d5ec1a9456c810d80227a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38273a2f8e6bbb42ba0b0871b6c95abb34531f33.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38a5ff72f22e0ad040a281e66b1aca0bf3a2aadb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38abcbeaa4d33d3150f2b0238bb62ebbfe960980.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38b94d76503e13c911781169fbc378517332c42e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38bb367362fe2c4849ded728ec5dd00969ce188f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38e12dad9e3bafe177ed3c27c833825813e18fc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38f8a89468cf9c8606cf12a930db062a83cd0ea0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3937d9dfb68351de2942e32f35e2ca1ce71edfa8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_39422621a00ff79b2f5ec0dafb957c77693537b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3967a8807c9451b09227c0f685c18aafeb062fd2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3992d5df4ba2e999caf6889a852db4e1ba078e65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_39d3071347a0c98f3221104036f477aa13bffa4d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a1dca5feb864e8981387c2d07e62acef1730aa8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a2280997eb6f1d091094fc54cecf42b7c9c3a2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a2643099365d0903c799585f41dc1a525ac9f9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a6b9566559ed2b1c85f2bea1c55e72c41dc47bd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3af86f458fb4dfcceb7db3357fbae0dc15142a15.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3afbb5ac9048a962a60f48886728220ae6c2aeaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b26eafe76cca8e74e819220b6de1f4279d48e43.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b4ecb47f9ebe8c2784976c3e9bbe4834b475cf1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b508b92f7e123b21658f6e17d624ffa87831fee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b5b3c218e4a7b459e54080e24c5b730221eac02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3bb129e6dee6848043dd0e8fa812ae80fec4d014.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3bb3b682eab96e4e173affad75b9d8e73f1dd690.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3be7cea6df8e6dd56194e1172f28943667f1c4ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3bed3aaf24c73073c604a3b23bb4b0358b8e3490.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c1454ffc1418dac641f63671e947d9f550b1f0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c38bb80e9880335faaea81985ed5d0e713ecb08.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c3b7e4b8c1efe59f79a15512716fce2282a79a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c64c33870ebc329921cfa3867d58b1857421f65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cb0cee09d633b6f70febbba63a1e090522cfb4a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cce3baac1e3ca03af0c3f4ee4d0158ad1031e9f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3ccf0a9d5a5451da5dbf6075ccea45e4a140550a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cd7a9ca49c1149d46f6b05b0fefc41ecaeb6ea1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cf45927b6d931e31e2209685d787efa28eed8ba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d1cea88a2277b87d405025ba256272a1720f88d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d289100991d4c8c362f64c8f6c4ba395c2f3495.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d3f3eb2f5eb1f3287879604892b1c230df85f1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d45624dc6e33c477c73a155500b015b6c010de8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d55cb42b0096a8ae338ce100f86e378aa1a04c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3da8c31f6d5bcaacfa4a21aed4d1d3caecb48922.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3dba3cd44f78c950fe7ceaa5f0629dfc607b30f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3dff884e176ec7cff86d17c6afe1ddaa4dd6007d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e143d88eaa0d9cfea856b2f3a57d1275a656627.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e2557f206fd81d82a3b9d59113105040beb891f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e562e6c3af28b8478020ce3c3bf73c036001c93.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e61b019e1398a6a3c36143fb84b5ff22c9f4508.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e839660557dee9d5bcda9b56940ce23236c5f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3eb2ea922daabbba131b90713e06d8caf5f30662.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3ecf565a5a1c4a09887c67ac3b9a019dca427ac0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3f34433b784d1e405ade3378918641372a30bf6b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3f5e01b4f2ca8ea10898c39d6570bd74e85f46ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3f7315955f555768f24585a50d75e216c40f062d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3fad30ff0739ab5dede67a96e859f8c474c245f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3fcc6893456a559c7d22714116022fc69b372266.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4018b1fcee808b6cccd131418b6ae9e8bf900d8f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4018f690b6322588041bb467beabd8a7bc79a2e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_40357c5e9739eae136a7abf92bc38d3ac94753f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4052ca6a3ec02f6559e4bbf1edde42ad2d127c26.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_405e7efa263223148318ae96bd1929b382e994e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_40aa64439b80ff8dd12498b3e5f6b625da16e285.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_40db688a9189e1c47c300d474df946a248a63303.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4118e3ab290263ed2576feaf22a1944bf2ddcb7a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_415b183c50dd2663dabe3eb8b780913b778c54ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4160f6b6d0869740a5a411abd80108f729f810eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_417b1cb14b67dc82f614831550f7deb0895bd7e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_419461cdb5687ebbb7bf0be136071d70420c1619.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_41b68458076e6cb129d3ec793e95b91430a0c8a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_41db3f29d1940e59dadc357c040ea37a6ff208d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4217a48a1677bd26cd48e512f1fc8830a8a551b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_428ce4e14cf94b284ffa735fe03d923cc74c9fe0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_429b82a27571ac91e3631cbdb7e0a58155abf962.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_42e2326066c91452335eac05f25a6311376bd9e5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4306c6c37cf472ad262f53941611b5e60072bdf6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4347e039c003489dd528faf5d710e687321a3fd7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4356b3a2ff49f72b91a6b9c215df285f2798ad47.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4377ac04be3a6cbdbfbe57612a469412812fb5b5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_438e3565f4c720e6c9691b0d33c1392936e2e7ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4395d3c96b3f4556b9765fd0a3b5701b2fb10948.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_43e7c78e8f65be35e2753a0ad5123118555c56b2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_43f2156a04b18bab55af60e9357f28d8a4604e8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4409f2a7deb027e864afdfc9975d3ab93c5dcc9a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4432c5214c4d40c54ca2d02f0d4785c6d6902370.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44462715ed5f192532760d6f4c66ff9d4e20e254.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44564dddf8b492d80be54854abb8d1d831e42679.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_445cd8fa559588f4264ce6192f2de3e3065365ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_445e28a8a51cd435130ded2abc9fc606e522c713.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4462b192a64efb60d5484798526278ac7a0fb9fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4466b6c6b2ec3acb40ac1cda432efa1e4e62d9d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44690e48f30657b0fcfa26fb3b9af3ef76e792e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44c181996532676f2140fd026707135144e9d37b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44cc95831c347212021c0bab7b43acd7daabce42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44d82b58fdc3e5b7a7c20490ce7f5acce4e6ec79.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_451fbbdc2dcf2ec81efce34673ee6c425cc16ca2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4568af1b2f104664fd05d21ad789aed39ecfa42b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_457eaffbff3c58183a656687010daa2c16cfc26e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_458d708d13577f2b92e6d5adfe952a87e0cf7be5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_459c8fb6028991321b09a990c2188d854d940268.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_459ea3713aef9b916e1b38a882a45012930924d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_45b9871c220c0065d74bffeed4021d0304a9625c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_45f4363f50af1e7ccd24751d5f5b181bf32c604f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4601680af41c8738089ff377147e0547dcad114d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_461737a13e24009bf1a5a4b780175043a9f2e33e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4666db0ff7b035e54f2c0e59acedc2131b722a55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_468a5f057fd5cef2df5f919f5102f47e86901e3b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_474fe2d739eca8c93fdcb2c105d4154cee6ca1c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_47548aa042c69bb9c59a8bf706b44028aaa41830.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_47f3ced9b5ddb0dfee8ed5e7df8eca0bbe273047.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_47fe73f04cef91cd2a0682e905483968ff80eadb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_481415463f0316ebe25ff2fda47c68cc54db3359.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4824e1f8cda50f80988857611da766685da94494.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48280c91d7cd8712fd533e246a6b0f758834abc9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_482e34930d11ff493007b1613993e01acc1af78d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48300e0aeabe337785d4c7b41796ce65df6cc42a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_483eaea4096c8f5bee16a64860432f0634a253d8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48435e5dd23e49e19dd313f9891ffec800ce74c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_486f6c7c7655c34b7b9973ff357b0813f0a3fd7c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_487724686efd35731e5335efa949486c93ae26e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_489e7be0f85656d012a6451b65f6c1d2613b187d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48ae3af78583258c4b13c11a442022e0e058bb85.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48d7d145f96aa8958a9208d0c8887742a8c834fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48e9e858abf6f77489f3fadc4ee81edacd26705a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4904c5910a2d0595b39a3f87652a9d1ef4fcbe80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_490a68220a7b621ae9817d7b77f55de239b0a4f3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4911bdd71351610d55916d452495e599960d0a41.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_492fbc418e829f89bcb8d93f8afd2869dd8dfccc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_49d4c005d723cdab9fbc307933c1257d114b539e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_49f5017cc0f5c8c8dc71492e7765cf729c1f225c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a06b5b153ea6e8b1e20d9aad9d4633333fd98f5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a2e6b05e7e4de2cb23d815f8b2c8adf22131c0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a4a00bd6ea27ff20a2903d619e1361b5e27672a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a5dbf601de5754c03a03a1a42395dc0766fb8ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a9f3da698a6103caf25d785928dd9f814ac27b4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ab5d6e8fbfd92e9f7e47bda5cfbb0d4162a6319.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4afd02981f92fbef6277c1985cc479c12bae9239.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b1eaca3c37a82d19f8dc91f06764170069ca3af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b2e7f96b095ebfb66ecc7a75752fba2a63e4f37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b30f472f00bec9da0564ddc40e07112b5f9a117.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b45948f2795293e72530b02669c4f549608ea7f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b4c03c916393d6be7c5181369ebcef949eaa763.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b68e4d00295b294320b94bc777d7d34609127e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b7393d55600c9892558248f4131fc06a6cf3309.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b74439f42140cdda9bb0f78d995d741212a35f4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b76e5dce9af523422782dd25d8dcf6f25edc68f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4baf664bfdf070362bcc91af77d1bc406f744351.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4bc48576f285325345fa1205e5e7e01787b74f71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4bd4d46397a3749646b232b306688e52b8c6e584.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4be4a98f150f3f9ab6f03b5fd0968c5454565c9a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4beca56234ff6fb4f23b9b24822887fd9a3d0df9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4bef4d120e71bfcfe61d67aa44d24ceb907c2b9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4c0c50a1fac82d47dff2357ee3ddbfa0b2c8d487.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4c69d06e3f32e3b6d28d3e54ad764b472741c193.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4c8720923c3452e3aebd7b9c1b4b23f0c35d7e4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4cabdafad0bf803223ba5e8f474cd59233dc48cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4cb1861e31df98bdfd731efc3d335055090d83af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4cd3de43cc1f7588d62a10362f59d113ee818846.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ce03571f1d2779bdeaf0a6a2d617e236d191c11.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ce671f5defd76ca08614a7a1f184c36c0f1e2ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d3b1ae63e127b6e6afe39e354d4995afc5faeaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d5f3cf0f78f73df79665c26b20b0805615e1b04.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d65e58c9f147498ed04dd51fe1393770603a6d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d7dc0f356b630179916f8fc2041b7f1402b46df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4da9e9b7277bc90518ab92860bef2097ba96d982.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4db2e63cfebcf84043f79be0321708cd159c62b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4dbdd9c3f496a27bde68cf86374999ff2dd53505.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4dc87b7d385e7b092e4706c464217b004fd8a6a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4dde56efe17f4fd36a11cc959320a5e43f1dc232.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e0a88ccef04e81b8c684b695f7cb4310e448915.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e15e4f16de26068cba30ef12fc29332d45e460e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e47f8fa40332c6ed12d9971e0b539049a871c34.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e760de14b71a41882ec4a2c7362565af36d1a5d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e79dce18e49ffe024fe4cd0693ad3399f5edaee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e9a933b916285d9580a76df543cfafc88a536cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ec2075f394acfb14fae7b1ef4304fd9b654ba0d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ed6da5357b67cc28aee4afa9523adaf055c4e32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ef35d82ceb4af2e07719c16109c6d72eaedce67.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f0aded9d1baec3125ce8e176248cb146ca580fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f1e1c969b57659e7e1367ac9ba10ed5ef5b69a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f44435491aa68acb3217b0e693232c67641a2db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f4a5d56721bb1a1332a65882132a8c5763932ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f6243c6850c0a2d2b7bf1476e12f95f187257b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fa4d21931b9afcbd70b1567995d3eeb6f9308aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fa883a36a76edb276a66c5d779294f170d6d4b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fd34faa8b168e2ac7862641229e6146d3e28aee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fe530cbf6363a8f08a94728e45e88ecde299e7b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ff20bafbf156fe8fb80bdd84a5d2f3a4a944c1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_501dcf3213efd214cc2ce8c9ba0027f991d241b4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5052b2318dbb78b1a82ef03666a35a623f44481b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5093976cb7b32a8bd28ce92fc13af00a3e21f737.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50e59bd079f4d205b613056f975fd2b4e372ab10.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50e7b11019fc2299d70869253877319b03388244.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50f887556a3540609649744957651ca667b91774.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50f915b4d9bd18a3c25a85917392ea4a5e88b349.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_515128c6978449b33ce0c35b02a9e9aaad65ef7a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_522a2a9435103ed405dc1500d31652f1d431a49d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_523e5bf45ec5008aa3aba4773e68a78e122b2fe7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_52688999141a72e61322140db29043ef9f7fbc3d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_526c89b7a04758b4badbf9695b316f877b8bb053.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_528db08068589c6e4c096054d26a2e5be63285b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_52a89981a05963efcea7ba5c1e967638beeebbbb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_52a8a323414448c50571a334f29bc0a38919b61d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_532a6ffd8a21d3e98342fd401f0247f62ca4e038.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5344427df3ae9392c4fc4c25c232196828e70648.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5382a30dcf702daae19bd6705864bfe36e09502c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_53bd60bd2afee49b30a583c32a45ae9f2076db08.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5403eec1cdd216d5c4a7ba977e2ef92a0d7fcc8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_540bd57333c6839ccf5cf2e928edb996bc60c371.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_541874a7633e5713720b9d084b6d1c6715a51a17.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54208a6e8c5263e38f9ffcb062564ab61d2785ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5435b4651a90e331fcdcf224282457e3dc038a30.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54402a22ceee3b665a3f24edb98b8398c35c6f5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54548ad36fb92d0963893146c8db20f53cbf0c8f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5467aea26852aa9a9e3dae76b906005ddf6fbae1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_548b347672451e8391388a400d016803f4c4cf8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54940ce53998becf9bddf56df7d19894a7658168.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_549b6956eaf678f7eb901567d1a515eddbedae5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54b6e18b10d529eb6b32d7c19c59eaefc7184376.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54ff49018f1c12b9fa31e523ad40b9cc162ba34d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_555ba79201a585bc091ccfc326fd24e851d1eecc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_556cd05288e1666f5c67fb87ad02ce660e4c589c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55b14cf2998a61611d1de2594e926fcdc378999c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55bd9c4f1b7a0621c67f3e964d946ce22fb2fc80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55bf8444c1c26b91fd490c7216f4d0f8aa0a1f1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55cda610c235987e13232e828f8d86fa88030560.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55ea83a47c6299fefa4220ed88f7a8e1dd938215.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_566b4782793c6526bfce7362efbf6bf069928b2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_566e26d4969bc6bbe9b092bedab11cddb3360c0f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56964a17f902257aca9d08c736516a2c67d9a0e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56cc4399c5567a9495f17d54c712cc9e65e57521.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56de9a7dfb1201b56528740e9d8a07b62710fcaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56ffe9e21362afe9c3a407c09d5de186954931a6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5724d91c1fd6290a6cf8d52a3801ac6b921dc7d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_572e68bd619e118292768f0925ccf92cbfa68415.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5732094f5917e9164ee0f973ac6ec47245a69101.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5789f267d34c9961ced63ad07ffea2c6d2911415.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5854f09511778dd1779a839b0b194896070f69ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58679919fcd292a2a69543de0db94e2985c9d364.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58762476c7f2bb05dce92ec22c0acbeb03676746.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_587fc33d02b1932235b8d152e57559060211d591.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58a784fb478ff5b3f1e2da9765a3a777efda92e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58a7ab44bbd9fbc97c7805860d5f6ac81d6ae468.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58eb2edc7738d8d18ac359691da261ceaaf71788.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5919133d2ed892745013b2fc5d503414cf0a4d83.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5939e6610e41aff8d1ccdb66d9e84d3e48e8d379.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_594929c433b049a8cf949ff476309a8faf5c25fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_597a0276ec419f18f060a5186e6bb703ae434ac8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_59901147b7188212b8d8feea15831a11425fe4b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_59beb9cb4e161f9dcff79080149076488d436301.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_59d366421e0b51c90fa53c366d47ed8d51b3a329.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a05b4e7782bd0e29ca9f6d33fc59d4304136d41.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a216f777feec4752f5882677b18168225da4b53.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a29b93cee012c79d4364502f1d90f947c73641d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a85ae0a16e4b293b549bcb6a3ee52df7fccca32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5aba1183efe205af38e79a1b2dccea5fa515d02e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5ace1c9b00f160a17355d4583d49c47887ac33c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5af96b404feac271dac8f4190180754480d3ba80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5b413bdc825ae863d53dab548f2145dc0de8fd37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5b55946ff3c15a44b9c741e9f6bbbcb5bd4c8577.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5b7a4ea3bb8905a22ae97a94c354b1cbe38093bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5ba578c0e7abf1127dd0370f06d7278656c93ab9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5bc803342862aa30e23e5be7d84e611bc571c529.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5be9ed84ad9be1627db7a66af9370679816c0897.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5bead6be6e39ece0e5d44335083336f7f546d2f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5c36fc744dfb0d985c9113175e76c7ec1c935054.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5c742b9ac6749f189d597ac97d46d35189472c50.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5cd03e29403ad53d6d52e5e81182ea6ff5aff2be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5cd41b6f578f3c903eb9d58ebfab62eb296044e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5d707d065ae152450f9def619ddc3dddb9089e88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5d7ed4c885fb32a0b548186e56d64bab98071d30.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5daedab8931f2eefb649b91e80145cb71b63360c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5de27c4081377f59363c2bf2ea8624217566d2d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e0abf4e2b6be3e2c555c2134705b9dcaee617ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e62968de58d9df7d687d671f37d63393f189321.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e735b12d130ebf849ac5d6752e413ecf3e69fbf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e840be0741afa4d41fd4789c8300223fdc63ddc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5ea53f7c6370845fa94aa9b395c52fd1900b62de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5efe77ca5c394a60af0313072cdd132216a52bf3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f20263fd84776f155519b3481be5e2c5b035585.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f3c3bed2b584ea2031debf9f953f5f8f7012171.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f71e663978dbcba859c5114ec675a712e343fd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f8925f929a5b26f3544ca31938aa75b3c59d34d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f954a393b7b5a7131c13d0c4578443f468a738d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fa19223cf296d7fd10e15e2571e63c84a80fbb1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fa7fafd4227918e0c7f0c6ca3b2bd673cd07279.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fb062527121e627871b3f1b2a94b96c42e51205.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fc66c5b53f83bf1e023e81e9d51f0285b3ae731.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6018ab272d7306689c7dc5a6d5326efea1471235.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6049c01db99fce654e9351e711b113cf7424550a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_606f5e0b99814b0a82a731de36f28024bc317801.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_60801d21c14796c08377349ec86a6c800af497b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6082d55544b5280b49b071ea277fb1827193fa2a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_609616f72bf16a060fa50091ac139ddc06bf9d88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_609f68180582384ba81aae2b1d4a4c52dde2c68c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_60efa9c427dc278c0d1bc31189f683cd45e4d873.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61204f6805d5d830aa6fca2a9b5f238ed63c3a73.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61220f6dca850a5b5ccf1f619a267c40c37efeca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_614a9f10ebc51bde3f580ef527c17f89489c12c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_615430cb65d8d540836c7f12b3367abd3c8e63d2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_618031345ea71cc17e458eb97a559b7c94d3ae43.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61896aa9e4e4d7e494c1755b1e77a08e0e264f8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61a44ac409e914c12281f1d26e5b52d8bfd0df75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61a9e92183ba87924e73ff0b5e25bd12d6038e69.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62048a8ae1c0096f3372b0114c15edbe813425fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6214f820b39a8ba81e547a78ed19a909ac13221c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_621da34ee666903307d3a09b7a032f2a70054759.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_628b28f65f19e7d1b22fb3b85b7cf3d09cd54ebc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_629e0b97b3fece7c12504f4c8f1860d611b57269.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62ab710e4acc711430745e05e036dd6a4d6bcdca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62ba7a5a0f3a714eb5f9f2af20f7bfbc82a30350.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62eb2f81e73d65fddce7ff43c397da6529317607.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_634d530731c7ade2c7beecfd1bbbca8583032217.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6360621af3f7e1e81a8be48fea8d2750fdecbbf4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6376eb68c550b50b9aea42a7a2cc3bda186b0e40.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_63c411351ec59bdbed2590c599f9eddf7807b371.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_63f121a3c8928c10a2d86b487cd13fa995da670d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_643b3798f11997d33ccb58d90ed6c10d5411b735.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_649336d59a8b35919e593217b6fd4314a04ea359.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64a0ca185449a49fa485892fde6af745ba758167.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64b3488ddf3bb1a4870371882f0a5d267bdfdf73.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64c3c1e3dac623f07c2dc1b934ccb868cafcb38c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64cf03c0aa3f1b2a7b76b4e3418eb5063b982a29.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64fe2db75cb20428856b02cd1cc8d7b393a6ad9c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_65794d9c185b21f59274ac5d4db10a7abc0be968.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_658552954505a2092662071401e135e84956c4c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_65910c8b7a30acc731948ab58467fdbe4fe32f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_661b49505cfecbe4ec3e5c7371de3aaaa85ac9d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_661ffaf653085dd7f122d603bb3ba4b001e5f3c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_662767e588220d0dc6137b00cc1d8dcc91e97134.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6649f19deeaea20663bee781af7edced7f7a4fc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66968bbf7e210911fcb95ba90c79837230ab1ce3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66a020f728df204ff51e37d2ddc21afb0aad5e7b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66be70b088b20fc8de464167c35745461ddab640.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66f651d3415562206c1049b172261fddba01ea6c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_671828f15eec2a58be23063a1a8132d337cd26de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6767cce35ab784aa42ebcb75af7305bc38a8721a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6785dcec0197fdbb50124ab06efa627f1a2c0567.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_678a4a8210a972bb2ed89d6ac754fb79438ab2da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_67fb736c61088b8dd92fe0371f5c98e23bf9077f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_680e81c3700f130df142c9a37a368944ca548721.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_683e8a33fdb7053760c9c135002b0a94facbe015.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_687f4aaafd1a5b9ee85aadc6fab79ad0c27a2ea2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_688aaa193f332ed13e017e78ec07a7c80e45f6c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6905ba47078abd7a5b6a51eb93b26095517e7f70.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_69214eb450c3b249017480efb8d092b0edad6dc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6979ef43adffdb62100270a62706fb811963925a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_69cbe8eca7e3510f5caa7f13419cfbefbf031754.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a3f42d5c9ccdd3807e488b00f02bc6ab5d8d99a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a4b6226b355bf35d4d07aaef1828091f03ad2ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a66604bb15f97a56847a7c968dbe32d247cbc13.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a7b6781ffff9a42beebb4d73f0d15461ddd4479.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a7eb3d86aa385f9ecffbc5ba10489e56856f918.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a95543aeed81adfb6d847f78212585a36122ae3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6abeb7b50ae6a1fc62535b9a1dabbde6f177a9d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6af23d1460abfe875e71f7911697c42fef0f41c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6af4c15a119e805e4407b184625f57966f8833d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6b0ef67ce0f178aa2863c4909f5bdd7f766c9b2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6b638314efcc4f16aa4a6e58e6caf2fda1711519.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6bad2ed9f91bc1efd89ea66cd5c775fa140cf931.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6cfb7075345704340ff33dc0ef7c04ef127f26ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d07bf9c05e41dcf2416e05dab4bdde17158db76.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d17b92fab5bee7717bf9aff6a6bef7cee3816e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d307974bdeeef95cca0d130ebb7aeb77fb1b6eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d40d762ed576832b3a752453e9881b5fe6d2650.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d470f5c6fb81032fcd7974180297d4bb2a8427d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d5aad18f59e47a3fa3278c7ef1a6372830c33d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6db86621d626722434f2ae9b7b8ab435a8dd8827.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6dd707cf48a17d31abef94215c5720419faa0a39.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e240106c771ebea461fc2a87b6da68e510aba70.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e6a4475ea795935f4cbf2dc0ac156a33d754587.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e7e1d245baabe2f6293e3d85318f9936b333500.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e8cda718e10824956f0ee39bbb0891eafa45a7b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6eca9cd905ea8b0454cf9564643894682b08cb97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6eebd0c2fbfc85f938b10535855c388971129a28.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6ef5803b33d97db72eb8a8528aeb3fc956a938cc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f31b3345893eec8ed1ddf1d8de2512b46ff6187.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f3d098f8bb63133924aab70d26a6ed64018c13b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f8788c537cbf6833c58a6ca15c0a36de33c9fbd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f88527a2cdb5adf51407f4661a254bb32d7de23.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6fa6478cc27e52fd9511fbff38369c921155cfb9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6ff4605d82507fc4bd6e96095eaee5173ea41973.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6ff58a5186d69efd6062f3717bd315394ea6592b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_703246f1f53a988cf252eff88bdf814bd382d3ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_70586668a61ab88bc46b763df8f1c2ea52001ea0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_70c8e45f6ea7cf5dba9eeadd0b19481d9f5defb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_70cf755f1485c065222be4daab84283a9c3d0eb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_714c5369aa848021e020d874289e3ae4e0f74d77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7177f939ac3dae8749cbf4232dcf04d2cf63b48f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71a2d046629a4b65c90d0e18d061c4984062f844.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71b6100efe30d836dab557ea4ac54c4b9d35c6aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71dcbe9f481c92215f3b636bc0e86ce8f65e6472.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71e3980331dc4bcec6ab6f4c345c7b5f71356979.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71e5fb3544dafa9da03fd2de4bb9bd0718f6009f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7237ce5f3cf13ace3efc0b0227ae5a8c1fdfce1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_724d1d4408196d611b2e0535bf8833652acbd6ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7264e378e1ea1d4dd97f6949d66f3492883b663e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_72abb25dba0c48b380b2dabeb6ab7efaa706d180.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7309c38fc8a2d5ad6efd449107dc54a7509624fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7344f96bed2f56793b1c2583485aa161cdf30379.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7393267865f1c2b0aa1a09a586f54cec98eea4ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_73d4901b8ef034590314048de7223a572d61ee0f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_73ec21ed6e040260c4f04ef68ef9307aa86985a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_741401abfbbbdf0dd1d62df8bc3e85371ead71d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_743176ecb1f0bc800c870861585edf56f88d7739.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_744ec604c577a27e0aae5b39711a9e2eb82801b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_745705ae121a1a331527cedfe4d31218a428a0df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_748a3d76e8ab73af9a5d2302d33e3b1d1b866dd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7497eca4d1a18306b406b367653622a8d64095bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_74ba59d347ce8916a22b40e6f22a3c89e13db4d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_74d5f2aef029f2103bb419cc982cae99fd1a9253.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7524904ac5a2040c7ea72aef5942212f291a21bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_758b211174da0f398b2a093e7389905b4f9c4060.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7596c14b8fee751d03f42ca48ea4f66e87fc2e2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7597ce4d2e5264bdeda47487d5bdb55a014c6616.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75a310a6eb86e3e8baac7a930c3ffbef372942b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75c38912947881caa14b3fc7ab7bca317e296dc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75f2010bf6c478d2f0eba77e912697661306c1cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75f21e38ad01fade35b1db40adabd75eb602410c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7601e6aea44b96e94fb019501be6b102c6e6a654.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_761bde840c0c8149b24a8f6f264e963c4e9e8ceb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_765940baaaa2ae6ade43ef4c94a220eaa63702b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76674fc182dfa6329c73a354aa3adf458429444a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76704ca28a4877a1e84022e022614709adabb280.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_768c80fd3ea17813df1bf19a158186834fd00780.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76be322fc072ca19baa82707e260c6eba936ae19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76f884e9ca116ee47b446efe9fc770c178a858d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_770ad1eb1b30ad8f1e7c17df486093129b2d5630.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_77200e875e0ef160b311c7de450c137772312d0d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_772016803aa3ca6ebe785557118365f9be7c4339.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7726be8909f631c04d4395fa4ffd03a736f447f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7728d5bec7941c9b6d5632bee8d67ed92b9c03ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7764814a0de7702f0b7b5ce9dede6440603f4853.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_77a814291d8f01870274149b9d82fb75921d6e20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_77d0223697ed41c4c2fd8830f8df6e5620db547f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7831ce329f2a0812ebb1dd103ea4ba8cb7ba531d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7838849e57ee9cd292e588f587a8079b57becfc8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_783ec08544591a22f59dc12f169b7327b4185a1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_784c35fee4d372123631312f1051c43e1fa12378.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78663faeb0425f45e8a0da0f7b1a5ddbee5e07e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7872c45ba170f2782c4b5b75cfc78ac79a4cf157.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7878e2a4d3b96a552e03d1ffc33debfd50c9f7f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78e1edca5abe1bb3e7aa946eab6484b7bed806a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78e945db4afa1330fe3978bc1bc9ae99828ae287.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78f7e2a2c08cd87702793f91b6935cbe4c22be55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_797750ac0b18b48f56ceb4640256e9bd3a36621a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7993fc08ac5c6ce7a2eceb1227f4e3718dc4cf5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_79a7dce707954e765d97cb22e57d9bd6168860d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_79d0b8053ddf99a4d4447656d733c2da026b3a7c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_79f182ae021e23869d7bebf2a9b4575bdc910ed0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a0ab620e6d62259a559e329460e46e6e3f7c3f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a13d62a715fd717f0d4101f787349cb49cbe70f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a242e5953f44316b6a4f6587ec26283ed6cbcae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a2e032f6500fbc5468183415b6dd1d3e43f0bee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a890b126da2d8cfbf84f048b779cac2dd56b509.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a902ed4ae3cc6558c73b730ff3949778007a230.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7aa14aa94d625b33df1adfa30ef4d91769592608.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ab03a62e064864e1e9c1cd506c1b2e1786a777c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7adf69b51f0a8cc9ae7e250e60df38758230fe4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7afd1a756247b15b078d15a39e350a07c22982da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b2d3680c3578c7292349b58843aef7a82e0087d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b5680f97836be4a369802e8115617a83875703e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b67045d438a7e4b8f3a313a5df5a85f351c1be5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b7fa76609243a8709f349ffc0d9d88157f28dc9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b9a3bf1a9b37e0bd9bae6249609e5994dc0dba1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7bb7b63e8a4c1df4eac4d978e166867195bd6e53.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c19fc90e5a9c422dbf529d2def286f47dea0f50.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c23dde1a386436e9864c8fa5f1706c0d2fbfd0d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c3d8ef4da515960bf40eb1feb04d21950ad5ae5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c4710e8f4e27fae4ae079f1667c3a1879cb6da8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7cbe4562c51d6829ec5942e11035c452fe318b3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7cdc419d4248dfdeeab1f0980aec35fa134e52e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d08373ace7087bdaca4ce8b0bc329f553f88d77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d0f767c17385eb7d756cbe8ed444d7cef72dea5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d12e9cb599d24631c082e3cf65d2c58b6d4d44f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d2f87c021e0b6a27b2d7e30351fd50f06414b5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d5667b27f15a06d4040354fba3601d48bb9c045.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7dac5d4cf103d658e129673549549f1276f134e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7dd260849b86c46b685955cab54ba07d49b47954.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ddd621da88c57798db1e689b93b692b6519ff96.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7dfe21ee27f8a0ca0407ef0dea73cd73ae6940db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e1bdde812c332c9fc58613698568a04771b9fa8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e332a6aeecfb12dcf70c69157fd3137343fb9f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e6129eead18d13a4a6cb9550384fddabc7a2a16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e89f79217037e361bb0909d06534e40f5026b4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e9519dd0d0f940fd5efd61bd32df7528ba7e3fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e9c7feb747241c9c7de2adf3a19933a1c4c0995.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ea9c37d92e344f3cc58cd4d1d00f19167e3623e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ec038393ec329a894aee9bbac078a40f57a4684.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ec04763d635c5bc3e810737b5d948c59f117d5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ee953cb24e28bcdc8f05783894b23cbf83bdf35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f6ccdb3c2d595fffd05bc5e6417b157276547fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f80d44e82e601dc48d4c8b4e710ef7265894b6c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f9403cb91d6aabebf081afae94a8ba397d8d24f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f9bb3486fee7b7c9e24300b8a4e4ce88a11bfc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7fa76fc1b066a15b08dc6c24a7cf33a58b4cb6cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7fe409f4421193fb48a54aa5f26bd6229d23204c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ff65c7abd9b0d8a2df9302d6dc167637b3a72f0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8004763f674dfb3f14b66dfdeb2a046e413ce2cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8007bf7ae1b71bf8ac4a793aa519ad333aa7a7ba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8021fa266c77e6b5bd1af2a9c22c686e5a6eac78.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_802b21f9588d72c3c3e3b9a3b269f19c484d5aa4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8046f566fa7188c92568b277354e8b06ad382544.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_806f9ab9baf631df1d3a8d801e4cf93a102526cf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_807545400aa6e70ff49a5f38ed6a218a180bd87f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80987e2d765efc320eaee813607c94c80ee35aa4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80a72d70d80b66c19e85daa00497308381050048.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80bfb0e6032892cc58cef4dd403f305a5b76851b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80cf0997573f4bcfbaaf75e40f519580a7495a17.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80efc341089a50ed5669b3c86f6ddd9b124d1442.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80f51f0e178c33e6196df1d2e47bd38bf5391cc8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80fb694fce7b4c3c459fca43c89c6002fbfdaef5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_810dd4e870ceda3ba9b5f0084a4b025b2e609d57.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_811db756577b61cde9fe8279d956980db9ee21a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_813e60e8405aca3f7fbed19452ae37574ada9a77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_815918206483d2ae04a45aa67d69dfb986587214.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_816c48e129a0235cb3a19124ddb28cce286fb368.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81acf1d17650712b71a499bb66909bfcfcb6aecb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81bb8f13b6f20a72c9ce6d0b53f81eddbf05f1c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81dd3ea61bb61de02667b14f5a94198f48c7307b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81f6c575c3fa2ccc7e65022f1ba65c8cfc16541e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82048cf91270631f98ac37dc488a1fb2e00ce004.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8250f27341241086515d833aa53ae873d4ece3fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8278845045d68027dcf3bf867ecde2fb12ec51d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82ad0c0580516485ea432d98f53e73f6dfec548c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82c932e6eaaf44861c794539d9caf8b50192fc44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82d7f61e6313930f063758b61102e7a43b118beb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82f0f3d71108dcc49234a258f0f3b21ea2123cc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82f1d7e1a93bf2fa80c409e6827ea88af56c44f0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8301bfc0394936a68fa0098580f06e77c88ebed9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83080406598df6bd3102db70a554e496e29db96a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_830e3532f27b391585d5de90f3bdf97992b67651.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8352031044ef2e4a22e27ad04ab5d2c02121faee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_835a906031a258c6362313eec783678bd8125c91.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_836a308c2d2afd6e0dfbfda61984b631c4ccffc6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83d580a612af85533c87aecdd7b0345c71b75980.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83d920a76114c63156740ba5dd6f3846c4b21c28.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83ddca2c6ecbba4314c434e7471ffb8fa642f936.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83f6a1837a65df12b7c55d25ca28cc939c2a6328.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_843e7888cba5f463d19fcb71aaaab25dc3d2c09d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8441910c34830ad2459fb85c2c14af02da718fdc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8457ea5726149efb8778e6d90798b8e48288fc9a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_847feaf237911478173377a501ee19ee325b012b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84cca7528c7d1bf49ba79625733ff0ae7522c096.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84dc4af43de08130a04bfa06df9799b6e9e96900.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84e8ae99e184013739019c93d07caddce532382b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84fc5e94f89d6a9287cf64662a372784511468dd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8513d96a66a4d9fb8dfc84afba7e1d8c200248a6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85156f2c556c6ef6180608c361b7b35ede71ffea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_854c8003a508ed3f8cbe6967c4ae2635a491c721.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85908fe6dc9c629c82d6953081b10021e64583b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85960fe542635079de5eca3c7785890cd4740005.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85fdde4b25e2fc8cbdd46c2850c19eac8d9af8f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86309c036d96367939ccc3e8922595ac35a3e179.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86513d6e065a44bcb0c789eed1e7e5456e800ab6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_865eb90b1a2d64acc0f6fbe1d807c501fd4be3cd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8689126a7eb09d81baaf8f99dbff8932fbeab3cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86d73393d0d8b769f30222f7817563a955c36dfc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86fa51b8c7a2f3fac5cf4cd2951ed2ede5c35450.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_875b08ca602fe48840c72cd61798acb98540fcd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_876a418fbe6183d0392b7a7d9986d067e323e2b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_877e33463b3bf1853c6d2d2009af8d27bf88abbe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8793dc3217e154b65ebba065aa10ab4dc2374ae8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_87e3a06266deda093bdf28af82d8666066157fc6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8840e8899b4e632714632450bcef001c6070f955.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88ac7f6cbdfca2e397bcb86af4216e87166601c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88c04463f9c5ce565a9daa8c22e16de80fadd707.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88d52c5f70abb525b9c8aa8fc1cb3997c33ed67c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88ea5b5346c87cc4fc1e841c518080df4ab811a2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88ed7f650c958a644c8031aeb88688b1e42458e5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_890aa875ac13957f00b30210477924697abf0c9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_89617bdea526d12d6a33ed42b9b0018c0b173722.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_89a3327da9a3411ff1cddc67eb647083cd947a92.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a1fd28acfe85b3adac859c4bbffa4d28fe634fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a58d4bca33c4c0e79141a56688049237d170d1b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a824621a50cdc3cbadc4b1f9ef18e1325385082.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a980749c6b2a18c80426dd189e5506334343ca4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8adbdcd28cb2f078f89adf9aad2b3d4a0a477823.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8b17c082f249649eca733a8f0cdf9a1205c3e3d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8b9043572cabb65435627a3faf23b18d039bbcd8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8b92990df507e82f96eeb7aa3ec00c01437566fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8bd1a40b12ce927323594fcce61eb9c20cc5e3d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8bd7b8c63a51c8639b3cf27ad09d41ae47c480d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c074afcf33e3f3534ac3577484237fcfd2ca48e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c13c4f3f645a2bb475eb1c55ce1de452f0e2332.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c3bd4e029bba76ebfc79e6522dbc8ca0bba5dd2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c4688cbd23727dd0ea9a36fb977b31aeae98d65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c7970957024de050748d3e31cef434f582d968b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8cdcdeb845e7bcdb89ef70ab2a97157d4db3cb52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8cf1007430da272174d3476d042f398627e83512.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8d079c1eb36db8461fa8b861c56760afcd97cc34.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8d7549e66ef309e32779ddc2a1f14e79bae53754.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8d79fe8a600c3b4e0ec9aa510f8036ba2b608985.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8da8285bd6182355e3164cdc5a983375cdf0a61d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e1b48a28b71c7f4c78eb14321b39951a7c5e903.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e2c587db8bd9f1b551624e0cf8b67a90245d7da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e2d5f979fc4fbd0991581a020a414f9c8656ae2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e431313fe082958d31b68d2fd0d61df0fe56736.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e50ea8dd480012cbe10be392cd26d1870e6ef9b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e675919a6c7758cbbeecb83b7ac6c62f95cdb46.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e812705ae3e452810794fa7caceef2ef6066dfb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e816fcad5e9ecfca94a6491eb2274bcc41e558b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e938d0e3ad30db201880642e57758285b2ec4cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8efb5fc2ace6839eac741c5e6616665845f43566.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8f607ee20c0d92b6dbd0338f139517fdcce98d0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8f6e463eedd3e65b9c79feed3cd92ad8cbc9f036.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8f7166d4bb0c1c9b9999ba16a1adbf09ebfdb6f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fa4c40e244b412a07933d369704bcdaa6d5e74c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fb224b40a7be7db0a9c5c08cc5ab05b526c14e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fb33fc20f2e85e915f1b1529ae87981dfcaf86d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fc08b4f3959a2375ac03f40c4ce12d70cdc2d80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9009b7d39346537aa6c4a4e46b81139f603edb60.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_900d7f81c73b35ea64095d01c5d48d9190839e0a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9068ba8df8b0e977e9769f6acf6cfee6b00b9922.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_906fa8bf5e992ddc25815486ae9c24d8bfba7227.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_90b17d8cba28cceddb3ef907df878aeef0762d15.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_90da0d469cca5c8481504148468460c85a15c559.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_90e5c56e92712d00092ba102a5eb5176a3e5d471.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_910cb8bd09d287a1566265eb1e8894fe68d3cc81.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_915b75db795dbef037b14b003ee073665fe35d3e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9163ae070075f26926a86d39e15c27e6edb1f1cf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91695dea4171747fb3cc6d910459f800608d07c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_919ae177b7a793fa352c4f6bb8e4175f3064d814.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91a6200e36944b1f11106c02f7fcee053f01ee71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91b9e2616c2fe0480096b1ccf0f74d584b220146.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91c916e14198f6d18dc89915e379b01070434e91.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9207a63fc55c411c73e4f93306c5ffed800dd249.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92121fd448b4640a17e1a7fe73bb7b58714c0afb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_921f789d619db6f225e8e9d646e93bbc9dc1a669.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92739f4464512feee083b875e11e11eee4f5b448.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92992be6252f2afdc368bd4baec4b8a55ae0abf8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92b0770fe64e3c60b9e56170aa88bbf74802a813.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92b722cdabcfaa388ccc6ccceb7e42462f3bdcd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92ba64cdf615c1be2865f027a293cb530fc07dc6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92d841e6d783bb46d841aafd9027f92dd1b61b88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92e53359c69bbe4d7405d45261a8a62008eb7d06.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92f9ad0fb65638cfffb3e7786f2cbf01d9585b23.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_93054acb8a9508fd0f0f486367fb62454de47c39.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_931cf8d05cfa45319f4e5bb49334d35a530bffcf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_93728d999ae43ee1b5a16e60b90cf8533c7d303f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_937801fbb43fb6797f0425f08d13926b74d87c4a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_937c48d0b7096ad6c8bc445f13f2c8c1934695ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_93b885d6869400b0dc2ef1b2c2636ddfd21cde31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_942439e4f5644a3a4630481bc7d98834b29b6e1c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_94a94d145e575747c8956ac703810582c819e2e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_94aa519eb57e5797125728492d9330f5c0f0670a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_94f6f9dee9f0c3825d91f4d320a5280070e60ee7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_95061acc6650fc7b79fa1fe5b2b1e083555eec2c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_951343832a5bfd060c8d12da0d8a090f070a717d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9545f95c1093c60f0fb6c794636f79aaeb53b733.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_95530399ad7b43d8ce2c89da24c71056f2146b18.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9583148fd684a7e6a312127e023798278415bd27.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9594816877815bc0294610ca24f986fdccdc7c6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_960ecb3013071fb65f2d5ed4c947c4bf303e5308.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9638c9618dbf2af119e37596f7eb0fd3f8d72748.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_963986150adcd6e1d3886bacf2166de1252e14df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_964f916d3484295b5918e2e4c22c5529588a5662.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9689ecd7bf51bcffe9f5002959bdda41c50a3c8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_968fc75a7d102aca068e3ceb6111728c280fa837.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96c129dd4c798343d6f78ab78056f0faf2f1c9d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96c5e79f54b71677124f555b0ae4bfd27248d099.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96caa2056d99eb67ada498e287b4fae984397691.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96dee49ec6755006d67f0c30c65f50558bba69b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96f1bb85dff8c97846f6b2e8796a6289bcd0d9d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_970073c70133ff2ee4737f803a0ac43801c47242.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_971a08c2e48d805b295d979b24173a04cf58def0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_97246460c21bc66c0f13936d27477a9fca1c44d1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9745b04a8026a01828c5dd606d89d044d3ed1d99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_976cf509d9c2bf86ba6ee5ded544fa8e6717f590.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_977137b371df841993c8d0584be7d83aca6add78.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_97851d5ecbf02f8af623988b1a39c0b91e51533a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9801b25e0f132d647934deb395b62a3f70cc7c88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_987a617fae00fa90a1ba60937b0312c81087c19e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_987f00dd759d9714693e7517dfaa8bb427294d42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9893336a4b00b2a63f23ed7e13ec54c82d9e5063.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_98e484adeddf3394d8d7693b808d83b64c71ee69.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_98f5efcd500ce6b9ffc14bc9877e0ba457539925.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_98f9a4f4d85f292b78123599a2e1798f12aa545b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9990e6ad243a48b84304b5cad0c663c0802aedfd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_99ae680eed89ea93a3a94586bd5a68dbc5439f37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_99e2f290b962f1617b0a9d4fd6d55c43e4439d6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_99f8352674bd6bbe98944a1c0a769a4fc028a623.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a0a70932bd587759df1e5e150b25b0126d7b529.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a20fa19d8d30654602e363806f559113218d66d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a8e04fe9432a60f86ff0369e8c1851821074a04.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a9edbe35a8fac7796f00bde836bd547044770ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ab73ea77ec20ea3bfaf995dacf93a6960ecdca0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ad1f99284aafc8d7908d062f179a056eb314925.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ae866c7db36286876818bfb718ac35204fa3843.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9afe4b6f3b901ff4af81bd4f1cd8ff19f09d0b07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b062dd633645772e4f2caffd111af73184f7657.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b327f0fa1155f2235d76be45cd22e3db5a69429.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b4dcde1ae3446b825dea739d4295c1d1ec5c4be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b6d08e63b9a90f2524cbfa8c5fcf8b82a1d2d36.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b73c92a13757877f34bd8a13c6fb29b60999020.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b841b7cf5da31f0c30ec42c91cc8d5bd3fedd03.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9bcc791049e3ff9ebc1a9085d2d20efcc2f99b71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9bf235679af1ca03a6e601b4cf6cd0416d1c9091.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9c4fc7cda4b560040cec93f63021b529aa1ee3fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ca3b1d36d777213eb381b47871bf15dd163c994.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9cc3ef3d3b36f52089548e9dce522b0448e2c26a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9d3d274058bc0a3d4d35d90669587761fdfbdba1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9d6759d8855c4c6289f1f241a1628cf0406c1b64.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9d69d441f48f9ea346dd8e00376a9a708da3ad87.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9dc424f0e192155e3c4e786e5b87d5a1a3e6c4ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9e51083e13aa4dfa8c969f8f916835a8e5e9ca39.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9eef1b54d5d3841f3fa6b84cca6c7ad33efa2d9f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9f0517550c7a23882b95de451e8099ea2186b4ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9fb389d4b5ba590baa951f17da06f0e53d2bfa55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a017be7b8bcf303b30a147f41346898acc5fab7d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a02a71fdd587e47ee68e0cc76c3c4494ce06c359.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a02f152e9184af0b3d77082d8bdf519dbbfceb2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a046e888e3836b0bd3c49fec8e1872e880798f0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a0874fc5ac87a1ec487c7722bf3b1bdaa924ee09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a094599fb5caf5e7aba728cd4713a8d0c6368a46.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a0a556c9358ddd6db719458c81d2d6d822a895da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a103cd47156a98ad2cf2c325ea00df3f1d67fb72.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a189292c81a18d21a2921ce6740f81ebf4c046ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a1c71e7d33f0597fe090a3524e33e18b2e562680.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a1cba1509c413c870c5d784410855ee1bd737da2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a1d6ad9de7ac7993ae1923a2ef070b7dacb8c563.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a20c91b2f11bb7e5058ca7935b0bda4f5558a9dc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a21f3637624762547af1292e1b85e640b1d329dc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a225c4f1f3c7b271957768bb9235131c67afb48a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a2482a64659c838f3da55f56e3cbbee1dbfe6722.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a25e2aed617e1ff31f93ae7e054313ee0dceee97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a2a715b7e9c1a576f011dfe5769c5b392e984f82.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a2ef5d30a2318ae06430d17f84878800c4ca7364.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3339150d8bf9d073827738527f6cbe15b854607.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3709e4fc53d2254a03ea7660b8c72d2f47cf1ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a388a284f45f711d82a6ed87036d87cef1872eb1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3ac4f93722dc314086f1b7d7b8adc687cd75f82.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3d7aa46528ee74e2bef1e87c1feceacfa55e173.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3dc780b17152f696f9b957432c2eae8fb16e85e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3f9c236d24b30bc9c3fad90cfd6eb00da835de2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3ff8445ba691807caadd9f26e7eb90851875280.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a421c2ed6b295c458071f1988b9d6f7b46e8992c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a4700d87a19a173e84d64e43cffabbed52366e35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a487f617c4b84c6a0328fedac750d41dc3dafe27.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a48843d844f78690c7a45b730652f0f763c595c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a4980becb0d3149fee575bad1fc3b463d08aabf5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a4b7f10440331a8a88ff93ba253217c2832bcf9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a55b47aafc4340e69e300ac61a7601a5c14513b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a55c7dd576e5b1061c059e5e99aeedf4389e2d25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a59423c095db052603d77073d409534bceef425f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5a7833f4597bb03a3e845d5580d677e97421040.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5bdc110955c05c6c6ea236a6f60266a4a6dce5e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5c0109313de1f6245d2a80f8539485b849e9d55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5c4dc0d70c547dbbfb661e879ba7f9adfafc2ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5d4eb673bafd81e3a0ee213da4603d88b8460ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5e5cae764142683b70d3344cf07dd1edb7d69e2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5f2f0cef657ae5e333d65ae4ab20529a43cd7de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5f8b7b2a891aa9f2ab49762eb31d835efdf18b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5fa94bb32a80e81886b711ebfcf2df5f5405866.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a622fa57764ec746e02f6d4bd4846b48c722b807.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a62a2ab489839ea1a1bfd1b24e54a3c232ed934f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a6461d72fb6ba50e81de3f661528c96dcfdc3f3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a64b4cf3f6706e4b4e0af4402e2263b9a1585f9b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a65c43b870705c780d734f9ef063f55cf8b3b52d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a673f35edd69241c6b921d6712dfd064d78ecbad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a71305f191f06cd53b7563971c706e8b71b19e2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a74b0e7dd816ad08eec5a1bba6e227afee9813ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a7784b03ad757d51c234fa86ea9891f055ecd5c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a78fecb9725ceb4bcf2aa037d43bc43efeb1c3fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a7f7553a7d2f6d42fe695cdc64423c85223af440.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a821661d8280c6e9d27f2c9ce1b3c855387b5a76.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a85d35b2fd98742427930eb536e346ffb005edd8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a8a4af070ee46d802cb11086b93daf91538f8a04.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a8a744edfa3a19d1493611df5bd0d4d59b707d43.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a92b43d374642df991edef1f6036dc898bf77cf8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a93324ccf11b273ed20fd960c61df897c8890b1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a93a03b33305b33055273711ab31a5b8d8298d5d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a968df29f5ae1463706b7981b3bde55918e1aa65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a98925d99dc484da41dd55700e151cf545cf821d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9b50c6ebb27986ce5b378d8c39315eb9cb91dea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9d2be18e2d53a5144f97dfdebb225fcb6d611d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9df9ac4ee78e5f4d5bd0567e58a7090907c61e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9f00f270680de81df7737e848e0408cb070e68b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa1041530f794c7b8dc4a8321ea0fcdd338fff35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa522b43c5e5ea69bcabb4c0fe28def2bd081a12.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa6d13b09f85ee62bb5018608812181fb43afc86.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa82d20635e592edbf00439294835f6f39ad54a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa996b9c843200a2ec33ed4319b48106cd7c6384.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aafe891dad43815e635f81225705ff944f990d75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab09941bddfa9d61985b55f9b6bf0edec9bb89f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab0be5a2072b5e87f5ee58149688796b6513219f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab0c3fe9529e24327686070731d0ac3ada76245e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab1ca4ce061f7f69a250356f613cab00d1e2ac71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab1d7f93427095e39bfc1d986b3d7fe54073ec75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab43f4a56c166dad0113f51b337a083f4df7cdb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab56e886d53a1d88fada0f10f00b9f398dc54568.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab6cd5c9242f8278c8f3d9ce57b97d605c7e5a3e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab877ae2a1aab04498bf2b26b3fe99d6488ef151.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_abf6c6412f9853855b74a96e862935ddef66f763.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_abf92a5314fd33491b5eb6ebd2418b7e0d5db774.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ac1ccde31b47e0e56ee0daab6403fed7895208c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ac5e9aee85cd16903bf7b82a4ac10402b0b26e22.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ac9382cf8bb56ffd962c99329bf67da992f8810d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aceb0641213e9a45ba48bcf72bb23845720d8b79.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad091c69d19b27f7ad50ef6311532ad8b642a9c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad82071cc074fd30437f6158b5eb2c6df1f8c587.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad989d2ce769f20e175fa88f4082c1c25fe03062.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad9b99a194b59d3149842c15733394da275b12c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ada016be2bd0e377fbe01fa7adb9bbb8febce100.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adae2d4f8b2dac799e03ea6f279e6ecdf66f5381.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adaef10ff2c5d89530310bdf1d53a194f06a94ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_add29e3e9828911a117dccaa5650e77805730d14.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adda7ad787524e3e47dcc1b65c41b2faea38f55f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_addb6a14043c5a4df0f5042b3770b40c4e90795c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adf160741a4f751d2f15d6eb23d4121cdca62b55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae1ab1f4bbe86bb9bbc22e4774648076c321136f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae1afeb6cfdf860ff08e4c2f11c922fd5bfa621a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae239476d61f48379754b97f29d7a285cc3192de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae4e7253ad4873576052ec0a9400597bb7975753.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae4e80cb185759dd9b3eb3c67c239964b3694caa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae51b30c7e1cd30e550187458350c8db7c59a9ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae7899b1ef159ecbf01f27014601eb79b31b49b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae87b1d5c50606430b544ed650d87df24366e7d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae8d0bdde763e617beafc0365ec4a3cd11df6c55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aebb2441e6cc1ccba4a391566e547402bcf7ced2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aebd5fed34ebceb879ae3dffaf58c7c04ab5fe80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aebff7e6605b273bad844b8f70ef031625bff48e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aec87e65afa93e84d7a947c52f291c1c7360033c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aece14f7a220222eb4ce6783ec2b9fce6fde94b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_af06c0dae15684f83e15722a4c07342af9ea011c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_af6ccfa11add1ae49888337e84d9c446d2f67da4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afadc4f76e237514db0bc0203102297b79730bd0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afc4b47a6fa62a4ca5cff6a7e01c9f6b371d2215.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afcafd07c1f56e74373ccf37db35976023456d50.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afccf699f593c828e11efc053b144044e45b32d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afda8f46b5ded4c2aa9d722fec17b75004b59f7d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afdab954fd111ec48721f25710d61c0c8affd8db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b00e062055933388e37525df5766f3c14cd3538a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b01dc872c24db4db0c9179fc07e17f41060390de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b03ab68e33844f97aa58d463e00037bc11c50da0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b04f14f829eff73afaa57a875f74ebd1e6860979.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b0544a38dfdf4d81dc95894387845f48435e299a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b0dd965d5d9080ed5c6a04b7eea9890f3a264f20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b0f555b74ed36f1bef8f47880b3edc6760f27788.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b1766695dbb790bd614b83dc7569ad449404cc89.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b18a615e66d7cd739ce35412811359a03cb23a8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b192c55f002d8540d5f965cc4df0c2e33f4b9ff9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b19f05f6848403480ba41d37cdbf44ccca1b1f8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b1ad101ce91348266d3885afdf2996a0fdb72135.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b1c5d55d47d6038e9162d32ac968ff58c0942938.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b20c6252863a73341b0010191fad4c834860f884.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b20e314642cf565e4f32bceffdb5c0e653ab627b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b24f91dec2029b25d0d96962528410df55a468ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b285e2f1970b78e18002464eeda63798229bbc3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b298e213f927b518c693660110f08bdd94990ef0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b2af5f5b5ee3ae964824a3e9c7bbeb5bb39c557c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b2f91e937b427ecc932c0cb0c90b2c2378db0be6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3063d06723ac70c5f8802ab49c5c35e1debf56e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b31f56244076c501cb09b4b90975132cae4c4386.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3486244e0b7d6dbcaa1951e8b8883ce441c3f99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b34c1ce348c3d9cdf6bbec9758de9d5fe94c43fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b38a1d3cffae01332a3a9d9472ff1b2c443e82af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3a104733f678193068d8642d6560faa03897258.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3da22d3482738a8474ae15e8e5fca9020c4e195.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b41735d250b5a16967281a5f07873b9cde3df4d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b41a30092e8138877c1f6c25656e0f8ae2c2444e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b41ea5293bc1c56efa2c4b5681d965aa6f2ce6c3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4588379eaa268d79fe8f8e4457b009f204a5fb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b493c99888d82cd2852bfb101f99a2e6a27665b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4a5715b550f67b8870ba66e1e6282a26cc1dbf3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4b037a2e262d11d3ed7d9feeb41b9e05427a739.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4bd2d206ceb237ed2c51f58abb5cbf96e39d07b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4ec377c44ac18527ca6a01bc3b146706a6e1e09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4f12f10d7b968e0d8e7c23f36d3a360de74a905.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b50e6df20a2426abd3d2ff2262a37c009196024c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b513834918d5ea789e2db21abece7c2d3532a7e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5248f443a12d96815c04409a00102923c717023.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5371415448fffffd58bf014dac9f4876153657b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5ac596c636df55e81293228cbc53dcbb3024e5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5ba2e73df35f6e0f7317303823fde92a42b1a35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5bccc85f74f54a2ceb17fe3040b04fe306c53f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5c3131fb8e5a25bd4a14bc9075eb6fa01b61d02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5c7fca1f76a31b0390e92d90d569fab94d4f783.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5db3d5b1d8af89381fc4b8073f84c5fa25fdef5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b60a4e87a7aabfe3c1ce02b408522f3ec862e3d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b6b17ae67adee9e56a022cd2a5514fb9c4e99920.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b72a804bb3c99830653d41ac0bd49943c801b89a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b737410b404a51043fc3bd503c0b107c297e4c9f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b75843bb13058ffe29251e053800c509c7590544.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b774450ebadaacf23e944aaf8ca90eada01e8a5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b779cc0b0380e1e6a2b51fc6216fdd72215b882b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b7a03ab0b7887cc7ed0cb40e56360a8d36c0bb8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b80d0828ba6d24ea3c1a97bd9835ee937b4b32fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b872f9e6ebe330cc1818ea82b53acec79a2f672c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b8fbc6f6e9c515edce3c7a438b3bc308b30d3857.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9385db12001110c42eff6aabad935a69ad3afe2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9559dd36a0a4f5e068a722e285f485137bd5ef0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9627f9c8d0088df0364a64643f2b5dcd951f2bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9a742ceeb6736a2c8f9439d0b05e10d3e0c5c6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9baf70220079e6d4e87eb01a7259923d8a01e29.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9d00ab8373747a5c6b9d2f8dd50ceb14db4163c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9ed0a64deb55616646ea98b21a891c971cd98ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ba145535e53899fe127987aa854f81234a9c51c4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ba8b09f0aaa40a7c9ad5f0458b460d3e328f3c74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bafbef3f13d429ec3e9f4672218998d5669d79f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bb111b7acc269f8d5e70915d3efde4c425aa5f5c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bb28a4e95723e3df380f98b5ac107c4df353850b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bb35c86443cc9ea38c06ebc0656306483c95ef67.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bba10ecb79ede07324e1198a71a95ff26e9eb235.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bbe23201fbebed25781f249e5c77c31e0e7f9ddb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bbfd025488e52b97c04995c4c5faff371b77e4d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc1ae1dddb8cc5d78196da6b26ebe66c1ce7e567.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc238fd2095b26a167b41cdec8280182330b7b25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc4425e30a0b17e8b31726817e8d3177b5c51934.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc4e0f0496a34d2fb43c80ce0162ad4183f29064.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc6ce17223d8d83a64b8c96ac88223e4441a4692.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc744db85d4237ee9640f1658e0caab7648e3bb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc79e255d25744725e2a9db9f90d5cc2b8a0e0c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc897852a4ca992961843144f4ec4f8b86dd5e9d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bcb6f0730fd09b4c6c60913425927dfdb8f83d82.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bcd7ccdceb7baf3b986f2a0248827822a5f72e47.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bcf8836c8cf932cc2748e313885003f0e11a887f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd064e302ff5b983dbdb4ccf51383fb29ddff44f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd28203f47b6a48e9b66302cf8312f3796ca500c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd37f4f7914805a97d5073f1ebf8a8b8c2648d31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd3daa5f99b4522d932334924347353ce2854821.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd6aa39d0ae3c87d011610cdb5e2e317f337c454.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd80a1774d8b7d8bee4e8663392b97cda11dcbf5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd8bf7c572c1984ca3061062cf3c31d993f6762d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd9c47f3305e47db6ab6bc627fb3d80269633074.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bdab172627718278a71a93e3737ef08ad9259a4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bde24a8dbe6add6f2dd2beb48b1280f3a84a9b2a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_be1e1533fc37b41838bd37edc2b6d2f2e76ae1c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_be4dd90ccb2f258029d0156cf23f940b694cf08d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_be8ec1163a01b9cd9a802d8b44669e8770c20234.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_beae876d6da465687f162136231f15767cc7bb14.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_beb9afccc15de7dfcb2e7d898abc0d61201de73e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bec30e7107c5dce3fe6aa87d83ed96da75478da0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bec9e4c0317e8d351f60258ed6611fbf365c4024.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_becc2a4d7ac045365300bf8bd45fc6d3e1e1c8b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bed5a8c5cf683f6dfaefad72c2e2f5c2f2b2732f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bef3bd014a918feddadc98eed92a7734f9bcd890.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bf9cdf86a7944cd690b0fcbbaec235863acd10bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0338fbc05f86270ded7df2bd3e2758a03961b62.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0342686e4efd26413c6719782ed13603479c4e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c063318cb851ccaa923be12d34c84d839bc64bb8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c08095341ca7e3a1debeb780c1878e351692bee2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0a3c4ac0a50bb9b7ad764929dbee98c856b1210.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0f76aff077c28f8afd7b22f284cf2894e08a043.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c112c01d201c366bdd7acccf2e1b18b00f671153.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c11d68fe766fc753c657362673704005b538660b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c137c03bf161b2ec6a9a046fa49d7bbf80ae47b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c197d1f050f42d82e6851fa286db6f81ba197f40.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1b76bc7a17f573c0d52c07ae9ff4302662ae61f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1b94e19d762ddc33cc4e94c6675d93cbde21e3d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1f40c3421b9ad8cf43940530ec50bcf620058f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1f721a330b2d0fac13b22061616d7b10c0f91e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c250ea59ab6e1ee39cce15cbd3f181047cdee31a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2541b6b5cf27de3f45f60671d36602f07ce1783.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c27b3026f1dc3056dee3a3e64bf31c45683607c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c28de8f96c8315877031a2d56261e95fee6aef44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c29110dd501853e87ebc122dd1971b0bb1bcd92f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2940fd05efd52bdf8a3f9aa4b78bde9b5809b34.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2a2856bf9a81544a30d535a13554e3a8107c476.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2b719893a4d8a1e71857966d399f06c0a41749c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2f04447e6a94c94a2315454e71d7d607a9fd0f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2fcced07cc194a8050bc7b2f791453b3f5b2064.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c323a4d1f24d59bddd20ed2f2fb6446627b0ae8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c355189ade9b1a8269230232db754a3881b53168.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c35ea54eb6cd0f3756c462c66d9be956279b46ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c363ee1b087f6b504a3dd3972b96e77db02b0582.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c3cfaf0d53869c373f6d0ec821b008dbb819141a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c3d0eaf9399c863d672e8c08d123739bab837d4b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4015f0d0a7a5173810f6f17c00065e03fc61a89.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c402e84359b2037a29efd1d6ce7213ba7605ab25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c41b6eda4f250da059fe0c428428219ff5a250ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c42ab428503e8f8bfa78c8cb8d9afad9f5185118.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4376ac8d82db1bc25fa273a80dfbf8b71ee5e2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c45a5e40f6a66bc5292a56e0097c69fe37cedfb3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c487a1a9933239270f44b1e08e1cf5323521c089.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4997f79435cf64add10506acb97d0647cfbb3d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4b34d3cb673447773f6da23e9cf52b98e99f718.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4c3425fe683d35dc3335db77d183ad1620b7a92.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4c6c405cefe204824e8fad1b3dd34bba87e796a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4de1bc135191f3c2aff740f4c6bb7e98da42f84.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4dec99707511cebd9188d216ee0a148d729b470.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c538dc4f65d02776875627cbd20a9c794d70b043.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c53e295b68e807774ed31bb914e4bc59312a77d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c56aa150611b0d4800470c1493dc907082a5c23f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c581974c8b6f43f60d0af29c350d850b55c03121.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c59937be2b9a13d6520fdcc922e4e75c9fa085ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c59a22c6efd8bb8815887325aa0b739e260cc754.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c59ab718fa23f24f09a713ac28a339208a7a5802.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c5b440ca9a5196ee1e72c878c87d96934e9273c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c5fcdea177734366d3bf283317a65cc3fffda611.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c5fef330a975002ed15670e8e7b26a10376d3cb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c64f4cdce32189065362a502105c31bd2d9d99a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c6e2da8b791d31f4ba05ef5f833fd6dea9e35f1c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c7568e11e44ce70924d27e683190422cfae5c31d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c7af2bbfac25de2853be344b9f636226c1c0112d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c806d7803d06ef8aac1d5caac9f36aafd47653d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c80dce1a17d073259250ec0c87ade69e639ffa8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c8dbfaffc8a9b573f194f9c63f1175d9725f8950.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c8f6461673882d636772ae4d26e78eabcb568f31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c919b8ed877d4244d01a17ecb948b459e361ff24.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c921a4790f982d48bcaf950123c699647afb739b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9312d7159369d13f3148a6f0882dfad6921ceec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9530e20038eb40c49bc8b045be0cf4e7e6b4eac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c977735a36c325706bd19a12df66ed0839b032b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9ad71883a19b522486706d3705700c012a6fc19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9ba0a3369d4e4eaea1c902a90e6501f232dd57c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9f1e7e478a2208c4d32e2d7e6abebdc16bcc5fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9f28230817c9d9805c41dfcd4e834fe302e1df1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9fb8343e623e46f01893a2b61345d1ca5928671.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9fe51f982abd60e567d4238d3266fb60e45814b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca00cfdc5592b7440d72482a18781e9cf3afb05a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca1992a2634cd6674076611be54197c715ad8271.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca3975efd767ddf7c12e308d948bdcaf0968493a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca3d98ff43fbb80ceb82fc22ab039bee898969b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca4c6ad28aff1976c6dd36974ec3b339aa3090e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca5681d4e5871aacef74bdba9e368445875252d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca920c3239bb5796b1ab2fc75177eb3b820aa784.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cabb7b12cdd9b8b522af577e13232b2459dbd38d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cae6c7efbfc831e2bcfc8c1efa1a486c02627cbf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_caede7a18f3e3d5e24f6c70392413a2cda16ac15.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb10303a0b79f2710eb7c66896d3c1f8b12c04dd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb1a0ce432c27f4cfa51731c3ef181bf60c8a727.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb1b91c16e0255fe7a0a85638b98d94634e143a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb1deea4f4fab0db31d46a91228601f0c272d6e6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb20538073888bdb3174a8e9c32d7449072aa753.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb3d5273945c5d40cc05c2660af2df1fb7a15f3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb4576e8ea5d59d7663f3760009a00a19e1b0667.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cbd571f4fe576fdb17d5f75a558cb6747087c7f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cbe5a98163e878c7697e554758ebd0597c2c1760.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cbf3e4d4d4837a0cb33b78c4f2767b1d93da0850.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cc127a63d56099e08125b16939dac82f0173122b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cc4ac5a18f57f2ebb65f7e356e858ab0d59b2133.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cc54b107e1b557ea36b5cbaf7fe3dfce05415c86.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ccac6c0e61b65c9422c7f30fbd979031698370a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ccd0b777df1328bf24e070ed4cdf8615bb2199fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cd0453a5c3828c1358360f31f5d3b7258e17fdb9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cd4efcdd12184211c74e7b3f2f30fecf1041ca32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cd757a8bbeabd16a44d149ab188430f6d79ddcaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cde0582e1aef74f9209de638b553ec0671476258.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce4714e4f33340859c106a3129993e22652262e2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5064e27ba427cb951f7e1b01328b0beb6b2b7c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5ad502dd40353312d561e9f40aa478c16ef5b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5b5932f6df9a194ceb0d69220fba9596528eec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5c161b725becf059fb4439c668edd454ac77d1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce909cb5f96a4884caa0d2eb8c5e6bc7fa352797.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ceb9544e2a0caae2c9e3dd8bbd2c509e8dca1379.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cee81ab2e2678816c7b516d2d4c50e8cb5874c68.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cf5c6c0bfaf98f6e655fc443246b81fcc730fe97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cf73e1fc0015094861ca0c1c81bacdbe0c5b8f37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cfda56a4eb08b803332f25bda6209932d9624acc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cfec97bdfb6fa95e057eaf5a8138853e1c0884f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d00f65bc99ca08eba66564d34f72f2769bff9491.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d036096f49a89730f8af7e75457c88cb8ae64165.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d049a1b8f4c1c6d37973ce38593efda1de8ce0cd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d04dc4ed02eb42c3fe303342801ed3073a0dcb8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d06ba4c996570ddab77b6ff1e2a0101b638543eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0863830fc5d43dc6d6400280e892bb7de2892d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d090b771a4f9750132f549c82a88b4ab00dce5c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0b09e8513646fbb2a007544a63ec9e2b04dc4c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0daa59f5dce6fc3965193ae37d8c82a3d1834e6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0dd0165ee91c095a19ceddf08789e3576912590.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0de618ff3ea9f67b90f2227fb7fcc74ea34183d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0f63cafbeb445408c884727b473667fb479675e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d137b7b6e04e1caf43a62bd6788a75361cfa98f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1840494c4fa78ff399c0399b3ad7ca3d22d4587.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d18727988e47264b42b4153dc82fc1a750f08db0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1c0dfd19a08d61586758091370acbdc6f267017.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1c25cfc437d8bd803860e39a45b2f3b9fa48393.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1d3eacc320104100bce46235fe656e5a8223c66.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d20d45aa85c0daa299da98c277cee826fe67bd27.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d257148f457557ea80ca56690e525db3a4b0ff55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d25ce4b3e9cc392ceafebc7fe3bcbe05aaad4bbc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2d08c5470a385d0160b2c1441fd1c30fff1c17c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2daccc4b3a0f90bff39cb4597f8b7e484613d9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2dfdb42c1b380e860aa5609302f29698dd27923.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2f4b869ff23874b6bde0aab68c419108b7e69f4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d32c64ef01aa228277d031a74df51363f98aa2b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d34d6cdcd81a456125ab5e0875466c6334d8e5c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d34fcb56caa8f80404789fba0ffac447483a4d84.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3784fb4c0685d7b651f4113f3c71e050881f3a5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3a23ded424200d0c6f06b1dbd0a7b7b0e7b5d9b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3a2edf232786d458e2125f8dfeda8847f842afa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3af8763f289dace1054bdcb4dfeda28b0aefcae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3fce1e11aee2273620e75efe4aa0390fcde9ba5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d40569ae9dbd693c0ab3d6ba69704d31e451011b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d41b6a64dd181f2efa65aaed03a3d229b3566c1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d41cd6b60a97e7071518cbd1a63abb8b910df024.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d43715cce8935439f90172d141050d78c7e76fb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4605b2ad3e3753c5f255678abc1690b949c5abc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4645b713821371161a9925dec8a3d6c157ba1aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4aff499ad527be5fe33b8e92547df57af26d40d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4b99af9a573df50a27fccbec3fa8e350f1854eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4c9f975891087e6eed6393629b41155deafc509.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d50ac8e8a03f8e7ec2c6e993dd39f09f465dab57.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d54ac01458df3f240e0656d82330f9de23ba9651.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d54b3731883a5f8393d60d27487f8d017aedd3f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d5e82799f4452e148c3e02acd6526cf30757eb52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d5edfe3e3dc3008b928c8e6dbd50784b905f189e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d600779c17b7b21c18e1308e6d765fe02a7945d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d6149eea92f2c40c11de3b778102fcf9b6a006b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d623b36cc3f56d1001b2d3abadd8a5628fefd014.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d63c8c746055851217a514321cd735eaf6937263.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d64b8b52f4a98801e185e2f132b2f80c29dd0c37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d66b79c4ebdcfd239cecec58203606bc123bd6bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d66c30148a6fa816937f2f095802264d3dfa0273.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d703eea8075cacec4d41fee7dc4734f593ee79e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d712f23ef88ae5d7b161d36f42d22a5ba53b6354.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d713fe25dc90b3511fc259cebf463376dcb55d84.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7145383e39dec0e346b5094401acf85ef3c2075.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d723b191785c97d284675f700a7baeb52a2eb791.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7290cc4c3036c9205e689cbcc60e7d16b97a7d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d733f4c03e338ea7c6d8f759c1132499bdcea059.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d773df9ccfc1ace90fe3afb5c00976deabedf6f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7adde8780b39f1364c572a19c3bfb19417678e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7bda8157fb27d544e049fd7d2ec735725f1bf44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7fae2c18645d36a181a0bdd2d8ca7a4ac0f6d1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d82773721479613ad72e334510a248f1436b38d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d867098db97b3f26e71a151c63b74260bfab21f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d86e4dcbe9c4cac8f7c8c5d97ce384ae0cbdbfbc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d8901a63986cc28ef24cab012b32114851a8c1ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d9061c204d8a85c974676f4438994a0be9d69a60.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d924ee32b178b6bffa7a71603d6e2818f66177a5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d937609afa8e21a761dad6b01ff3f26346e450fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d95835bc6f000d3a3379bbc38d90e83dcaf867ee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d992eab7de49033f5480c5e86a69e675db0d2a19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d9c23b7f8fcc4e4f4c81f5f00cfd345b98df2e0f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d9c3e27b522320dcca5ee84fa534b03aae2bfea9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da07d8b5666423da30a95e3b2cabd3839d200981.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da29a515d14dac02066bcd4701285b9916b43cf5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da6afccdee4107507a64323e17bf12c46da2b92a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da74887afedbd67928fe4d596709f9ff92530611.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da822ea727fb3543e445e4000f7e6ebb946d6a3b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da9f6e1d59132fe96709490af25bd794f267851c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db0d0cf55d90b3f3c9eecada1db93c420f34b1ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db5016bff9e5dc37184d2b9417eb351c7ea1c322.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db85839ee8d464c5a81b8dad9839f5e0f4b467a8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db8f0bd93b352d28c5b6d78f4332026993f0bea4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbae1670fac6812b2d2cbad973e4b475509ea504.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbb06b43d5d65429e23cc717448cf1fffb0cfd74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbc4135fce01e8731fec7a78d0cc0fdeeae28b90.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbcea8f7b5930abf76eecefce92d0db785d2df5d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbde2ef18e2174ebe13a6e7c8c2a6b05a6612047.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc039d422a57c159ea4dbcc867d766ff1b356a07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc08afbff5def8bcb4e823657ce01f57c9dc77c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc184767d723f4995791848cdc68bd948408204f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc1a7f9b1afeba6690fdc0d0d1755ea89c805573.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc34b6ef496d4e0d8fbbe10731d4a7b1c136c036.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc3d625c5ad3e871f5a727ac946df642d988b9ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc4d27535b9570b8f4b790470a83c1d0a9a2b6ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc5ba6d73f331c76e696953606c5b347b6a46f3f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc62a8db637d32e7dfdb2521cbdae6e1fbbd5fd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc818f3ce244743cb1dbff9aca399df90742a6d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc91797c1474a368e9cb056b50b4629d7736c3cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc9e54273c0ea2358fb573a7d918aa7b09fe07f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dcf815ef540060cc7ed43e1c57a28e1d080c5621.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd10bbf37503bbc92af82bc3487989b41b20ca85.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd11806cd2d3ef1127f676b2d98bf8fff2a1e5ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd35634440edb25cb095800b882c70aaceca1dbb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd67d442001d2b167e70e8730abde4d4461b8569.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd9494d9ac35eba6794a4f9120d2db9932596ef8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dda8d021381083bc48b7fb1840729254dd8e5137.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ddcb1cfea1b0dbe50a02252cba99428fd977527e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dde93ffe7fca311e136e42fbcd12b05c9fc7174c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ddf5339054f47d9ed6cc7f9e66ab21ce3bccf3db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de1ff66d2aeb47d2fdccaa4bb6b9d066b380c99e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de26a187c4db06115072a5132e1166b5b03368b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de36bc309877917a18fd21acb30563c7e2f233c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de5359f0fba3da9dfed06ddbea8fe2a33a9cf40c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de6683d175affaa5ff261ab8503f64172d8eba8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de7eb562a7eff31d589e12945d80233aac202ae2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de85901d66dc04b1143bb6404445baf65693b781.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_deb9ec2cccab94920e40f62a1f0f094acd919d07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df0b2bcba57e77d975ec5304fc50cbd09cddf4bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df4bb75ca79f805a81fbad750ad22f6d22b0d8ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df4c9eb48da49a61957537270d94e56cb4e426be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df5b1c6758d4b8540158299dd0362297083084c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df645b3888dc8d1df50c47c0d75822eebd3eb019.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df66feebc9a0dcc508ce002c255154622875e524.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dfcd68acfca68d1acac94f493e25be0ef20f209f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e02a198f23c409b715761b702d7b0e6e5992701f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e035773419a9b3631698a3d375d829af55f7731e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e088f0f7363804cf5403adef70828ab32d09a02a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e0966fa1ff013e477b1706928de6cb7f8587c154.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e09d9baa269dfbb30b714389d1733be51cc419b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e0e48d7edfe9513f24ad9fae68cac3aa940b17dd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e10f47a44400de385ddbeb99475b717c5646fb41.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e11a3b7d4fdfed64e64f7a95dbc64eff541092d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e13b86fe4e153e0bfa8d1e75f3641fe32b0c5149.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e16075c3a5fcfe63ba12e854bb1fed6873f014ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e16edb824cecf459a8ec51b8dc74b1e06369aceb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e1c1a31a1d8556cbe0b6ea76faacc78855108539.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e1cc934ba7baab1a2eb062df1e4ee5066e9ffbc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e1d85ad2c9d197f501267fe0804e6985802fbd18.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2762543d3380185e304f84749a70db1b8d3dd8c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e28fd64c2f2b27577109a984e6ab82f5f0fcb296.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2b629c37cf94134693ce455b8c88b72a39df7fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2bf6805a489739abb77c13173d57723e9304afa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2c9f955f227430c6224ebc347649386be7f01eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2deafd2f36cee29109fb824e0135407453adcfe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e3015c5d50481547aa5754d042d9d7040cf1c7ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e307a1b0d5a8f94e0a0f4032f401d20b4b643523.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e334e691714f0b99773c2ac515ed82de0f387065.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e34b7e452a4db74189334697e3a240ad68085f0e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e389d0e4442cd8304081892ddc75043e68a6398c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e465193d97d43237c22c04478ca5833011d8dc8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e477abef05ff37ec27705eda51896e2aa3a04966.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e4d9a2396ceccdadab24602f30e9070901a76dc7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e502730dea6987e2c038446c448aa08bdcc23113.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e514c6b4bc75d95a150104a17972abae77cb47ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e52e3053f30f780f346fa6b7a836ad2554cb85df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e56757fb17f5e94a6ba1fb14540a68c36d571159.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e578ec9e09d3b78dca6b5bf0be1538657f02f319.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5935fbda313d3518f142f43d46f56c600f69286.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5b2bb9f8466de1ad5210e4c39ee7b8ecacdffa9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5b65fc519ea7cfcd19f7eddbc3acad6842ff558.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5c5079636a4a31a849ce8a5af89d50330a74628.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5ccd5f7ddc894b2717112cbfc766804e02b7bd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e618fb4e529104fc90069c8779ce5463460bd516.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e638053e01268a4c5883620fc6a9901951e2e01a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e639a1e84faa98477b05df71d363b9ff0f9b2760.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e68a9e05debd456a9975953f7b0d510e7a0f6978.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6973d75297bd2c3432a7c88e8a9ee1c9ae693bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6b53fb8d81148ff384d31a703bb4c2e7a5a33af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6e0ec1db1ea308e226f675e68e29b839e41b252.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6e6b10e73733716e71ebf5a53703fb935fc5e02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7153f9a9b0b7c54ddf2debbe297efcffbb4fcfa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e73a776ae4ba68c23acab1a5a6381684051738ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e75c757c67aa23cb88e1aced6fcf36b7b28391db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e75d492ac3a6ab75648056bcf26250a4aa929cfd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e76879f8ff4796f48ad87ff8003f4f6e6adca9a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7ae1294b6dea5c8b93c2b814fa7460c4047105b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7b2eb64b66d46359fab44333c2c484f4c9dd5de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7c0a99e949baa5f3a7ee2d6e84427982f82f76d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7d37e7ee96c392fa24c02a9143438a3a7d05741.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7de729aa50c10d8101ef504138c3769e3286753.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e83c604d1b8260958becd1c7c209745ff9151715.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e89bcea4393593313d18a4aa6dcb44cd75bc828d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e8a9427f34bbf5ddb28a39161acc36806e68f2d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e8d8fe5f4f8641998b8b805a20b2ca92d019ee59.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e8d9b65558398c0c10127b560807578ef117d7ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e907e8d1089557dfcc95a05160be5092e9119a53.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e95e3908479965856843317c8b0c42a6961dfd23.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e986d5f8d5591f3e0f1cdfad19c38c420fd93023.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e9b04e6d5527ba0b8089ba8bdd264e2d5759338b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e9b53fa68641f45baabf40b7cfb8b35a9a1b9c7f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea077e68dbc1bed2dd20a5f4dd35e0cad6330ee4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea591185b1c5f521023e250a26f742984255b241.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea62567e9ea16771d8445464c38f5a2931cb355a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea6a6d4cc262ea838dbb83ee747112f95fa297bc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eab6cdc59bf216f7045f0cf5f221bb91ec415cd2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eac353f963c52624cf79e82cc2b2c02eed94b677.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eac5952f46f4f2bf06257b00661774eeed48a323.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eb278488b2cca114adca5e4614d86f92447f937a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ebb241b947a0adfc8e50c5d71765c14af24593ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ebb9abf5b09e63cbe76390bb46ff7cbefb3141f0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec171210efd217c07d357fcf42e5372ad7e9abab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec3deb1382003ac010d9bc1c59d1878d3ec7a727.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec51d24ab5f24e003ed6751ae8ae5b327892b15a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec7ec8d547ee9713aa3b5b667f22cdcaa8f62b2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec7fc24902b1ebd8f2bf8088b0ecf6de8be8362d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec9f63a538940e5ace02ae5b5ddc01f730adac4d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eca613eaa8471ad7da66d2f8f2b8e07f6e02b467.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ecd7dec90b3c62bf3a30bd75d3c6869529a06b01.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ece60111633db08f765b3c7cd5cd768cbd030255.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ed37ba962e0288e2840eb0925d016b5a7e3b3164.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ed6bdf67720e938d538a867548ac3579b8238169.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ede81dbc4cb208ef6e684c76ba1eb451d37fe10c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee1a43f2210a8d1e5623411c95c33424cee5e747.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee239db5a67c23a383590a651f0d8a0be43a13c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee8e709eec7aef1fa681053c6d2969a5ff18c45c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee974931e65d6b16b7c868d462b95dcae20b7513.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eeb0e96b759e18cf703cfab0cda1385726f6e0a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eee408cf9456ff977aa7d12345e9b2f1e60639f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef2ebb4a86e7ed0001de9c5e607b66fe8877409f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef40f0acf1885096efb840ec5600ec421c4db331.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef5421703cbfa63a58ec02701e245d479a1fbfc1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef7cc2aa1ffd38298b52764a93cd1271b4d92f8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_efaa0cb33c71cb8ca7b83dd0e7a6c7b01f6b50a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_efb9e7d9af47cdf79f15f674f8976c05f08b0ce8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_efc6a7b25710f0626c3af534111b161e1459d2e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f01468c62c878295443981662e037ec5213cf7a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f020134822739be6fa0bb3d98e9dec79f025324a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f0209426a8e6bfeef7d8ae7b16db791888142298.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f028af9e5e3c25800dde938e991aaab4fc1d64aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f053c9c32518b895daaa3521827f37af78836fb8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f069b38b26c30bc770f74c856e47eb498f5818e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f0cad48d9bc80d58705ea60eb2dda4baad68cedb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f1246d1013d954a9316f4432c986d3be9459c548.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f12f1f1b679cabab04218037ef370d2c7e1fe332.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f15c41ddb04ec7f80235bb3db19198dd6b699713.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f18c74becc24a93427d9c0838784e9b6caad6e81.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f1ecc90ad7b86791a9e6f73a582aeff30f393804.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f21596e8c608a795ff971aea8e199db9e72b65d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f24bd5b92ce6bba640b8ec6b4e53fe35902c5572.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f24d42e820adc1a26a428d59df7ffdd7f8580176.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f24f26e45d5cf567d29fbe375fbf8abdec39186f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f25b87c435bc5d7d85d738f3fdf68947d79f5a77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f280e1639680ac1e5830a21f921bfe2cf364ef42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f2da112b1e07c44fc8a7f19368da203f6935049c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f30316cfe49323638f71ba688dd8ff9b2266b335.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3193ea266f3718398bc5622f8bc7042c3527a42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f34fdb8294257d951dcc9c4fa7ecf1192568b91b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f36aaa63ed42a578b953ebd614318d44cf44e8a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f395bec57c3b2e6e169134dd8d20b287d7405134.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3bf7ef503bb026258b3ec3d82d3ef1443046964.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3d0166931e4406873d8f552a5d5b61fde2391a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3fd08d56f8a9be1a8dd104cdb1ac58e283b5064.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3ff73f82aee3184849d04c2364eaa45c6d0de9c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f42cf0e5fe479690883507028748b0cd3dc83cbb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4658c32d562f9d60c5ca1262a2e0df2375063bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f48f8b681a405bfeba5aadaef40f32367ec5cd2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4900c0a5c0d03dc17d7a907ab40652d9920e756.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4a6438394dd3427f29aa0bbe58ad1f797c3c38d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4b87f983a5e84582efa1663f84da76cf60b5f6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4c803838f5644ccc6f04f7c8a6233fed0b6639e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4df1cbfbaf67705820f125b474469ad7ebab0c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f50fa4ea674a590d0a817367ad9915a5fce20c51.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f51f1a11f778d99a00aa5959a3e58a41fcbfb1e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f525b59df454ccf53da6cb201e0aa8d09f52a2ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f57f84892e2a8496169b7406e63b0d4f5aa63aaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f5803aadd93e33567aa6b23100ce4fbb6c040dd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f5f1797f6b672a55476348571ce17645c8a62869.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f6566441ac3074578cfe45758ba0583c0da0a5ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f672bf80a78885428b2c02e522426470653a7351.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f682399cd6412fed6a1141296a7e4d42078f7b29.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f6856ca950bcf173571766c3f04de4163be0402e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f69548d6cced86c21c09c6475237a0cb926df0ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f69878f4ca8cfe6b8d8748766f66a1ef8eab20ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f6f102a388ffb05c690a20a29cfe0b35a35eed61.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f7035f4bfd8f2f427720a07e3c311bccc1dba683.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f71f96ce4dcc7f789a8ace73c230c203b05ff6dc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f727911254904ce4341e4ff5f8bafc430b8cfbbf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f731289837f915e2aec1bd01eef1b3c1b099864d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f79def2b4edf6d18f6ef1d6b141f9e0435441f6a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f7aa9c39b06e55bf4bc9f9a2a0fb075c9d4e69ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f7cf08242b3fb1c643d4149bec985b667b9d28fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f851da732f397624717160f89271514bc334b59b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f861d8693f82d22e2c5b1abbcbae5f30f4433e5e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f87790f260630f312b84888dcbdf849ce130ae59.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f87991cb7787a29d3ce4711b4ce04c5fb6a14ca9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f90410c26d7649e21e2ae5e32e7af89d84d2ea70.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f92e9a82c879051d6fe3c42108f8a574187704af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f93bc23b8a4f1e0fc5c5756c4e1c835bf59dea09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f93bf815b520a9d9e17b43bf9d7fb870751b6225.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f974b12e83e214c30995a25631d37df1478927af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f9824fb32933b27501ae8a7f43f460a2dda6a814.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f98a6b193fec3203eaa75819f6b51aa45a48f212.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f9c58761c927b222112cb5cb6c9acb5d3c915785.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fa16fa84278b489af253b52839786f94aeeac36f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fa62a97675719c2e8e9bb97361b92ff1c7b9d2ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fa85f869a92f0482605e52019828244b12e12b44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fabdc143c29d5ca50ab1e96a814bda6d05b0d5d2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fac5a0f98b94530befd634891e42c424bb86f0e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fac99c3c82b77946f6844699d2333cd532a78a26.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_faf56e45b2240515e97fc1bfd552eb03b6de5094.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_faf686067fa433cea5e95dd523846dc881eff635.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb2fbb135d59028afcf867c2cf08edc323565528.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb4c15452f9155c5966990f09432e5eb7e28e785.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb4c5f8fecfbbe16e6648becb3b5ca89fa3d8a94.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb5bb49928ce5515d7b297d5eadd4ec70a22d60b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb79e1f9231692d736dbada062ed6821f34927bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb9477a613665cebcad781389ba7c5a36f51efe2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fba36678d5047ded97ee7a7ba9feb9569afdb6ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fba47fa8d9b5375bc408af68b67345ab9dba2eb8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fbea85b766bf0c918ee0baf24dffc6a5563d5105.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fbeec221cd63adaedceec39db41ea942f99f5133.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc030b61ae20c4b7d9b2d10930a17e01e9e93328.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc1790325b59bd44b0a5f6cf9723a25fd845cba7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc1eb85a00017efdc610e4259d2abe935b85304f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc5841a729099340d608e31023acbeaeade3e886.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc5ebf0f2200f37ccc0849e0c3745f6e2f00111d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc7b0916744b593435d8e1e7b6d874d760cd5e3b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc86c13e933cba40553ffba31d53aad27415ce4b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fcb0b08e29b2e1bf181fceceb9dc416e54f52b00.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fcb6ef39c3db49f26f736d6c9221dd825409ec4e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fcbe827108d252b2f5847fa8e132c9c3e56a90a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fccabea88b8e290688c1b360875d228e6fdf1624.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd10a3b937e9659716925e39a01d794914b08e26.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd19d7614f2ed5da21a52ed172ef62cc07c9c01a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd26e43ca652e6f58ff48c356165aa4349833b55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd345632e0cae0d549ba79626a08b1885711deb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd3558b4c7a667dbc365c4c2ceda646975408f51.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd614df484b263deae3b3c20adb0ce7b62eaa651.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd9cd1305633b62b68fb8474ce021f639f8492e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fde12cd366d6850ce26afce98e5076b695b4875b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe245e9ea974adce2b9807d33b9ba12d916eaffb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe72cdd69944d2d765478d4aed13066a02b76f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe8b8c3525fe86a20a2d6c69585f3e36c16caabd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe97b7adcd67ed9bda8831d1f3f1ca7590c6d251.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe9d98dbec5096a89b116f85675af772f023014a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_feb5e77111fe1e20bafdb83a925b5faeeb6214af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fecd7501265b4c4dcf015485e63e2324304f70d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fecffa403b3631b1957e1a9a06f18fdb3b4eee5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ff453e3bdc9752cb7b81f7cc3056325a8b9a8ad4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ff6862dbdbb20bc63a650e1f93e9ac169bb702b2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffb5b7349a671b182d73c8016590f26fe06a4cba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffb8adef0cef91a86f36872407fea35df90e8f2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffc6056d9fe125a4dbe08c1d86354e51f7daadd5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffd868d49abdb769ab82c21508d655daf54b8a99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fff7aa57cca501f221077124359a589b3a6f9d0a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fffbfcac254e33926131a71905e93f9cc0aef89e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/launch_kernel_pt.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_fwd_ck.hip tools/amd_build/build_amd.py,https://github.com/pytorch/pytorch/pull/144777,alugorey,jeffdaily,jianyuh,xw285cornell,
c58198184b8,dynamo,not user facing,[dynamo][dicts] Insert LENTGH guard on an if condition on dict (#145432),test/dynamo/test_dicts.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/145432,anijain2305,jansel,williamwen42,,
f56c638849c,skip,not user facing,[c10/metal] Add a vectype variant for `short`/`int`/`long` (#145430),c10/metal/utils.h,https://github.com/pytorch/pytorch/pull/145430,dcci,jansel,malfet,,
28c251dd0b1,skip,not user facing,[BE] Remove test_modules from FIXME_inductor_dont_reset_dynamo (#145306),test/inductor_expected_failures/.gitkeep test/inductor_skips/TestModuleCPU.test_non_contiguous_tensors_nn_KLDivLoss_cpu_float32 test/inductor_skips/TestModuleCPU.test_non_contiguous_tensors_nn_KLDivLoss_cpu_float64 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm1d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm1d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm2d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm2d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm3d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm3d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Bilinear_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Conv1d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Conv2d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Conv3d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_ConvTranspose1d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_ConvTranspose2d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_ConvTranspose3d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Embedding_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_GRUCell_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_GroupNorm_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_LSTMCell_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_LayerNorm_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Linear_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_PReLU_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_RMSNorm_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_RNNCell_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCUDA.test_cpu_gpu_parity_nn_CTCLoss_cuda_float32 test/inductor_skips/TestModuleCUDA.test_cpu_gpu_parity_nn_CTCLoss_cuda_float64 test/inductor_skips/TestModuleCUDA.test_non_contiguous_tensors_nn_KLDivLoss_cuda_float32 test/inductor_skips/TestModuleCUDA.test_non_contiguous_tensors_nn_KLDivLoss_cuda_float64 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm1d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm1d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm2d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm2d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm3d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm3d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Bilinear_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Conv1d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Conv2d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Conv3d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_ConvTranspose1d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_ConvTranspose2d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_ConvTranspose3d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Embedding_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_GRUCell_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_GroupNorm_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_LSTMCell_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_LayerNorm_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Linear_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_PReLU_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_RMSNorm_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_RNNCell_swap_True_set_grad_True_cuda_float32 torch/testing/_internal/common_utils.py torch/testing/_internal/dynamo_test_failures.py,https://github.com/pytorch/pytorch/pull/145306,masnesral,zou3519,,,
f52901a0a71,onnx,improvements,[ONNX] Remove LegacyDynamoStrategy (#145442),torch/onnx/_internal/exporter/_capture_strategies.py,https://github.com/pytorch/pytorch/pull/145442,justinchuby,titaiwangms,,,
d3f196909dd,inductor,not user facing,[inductor] let inplace-padding support cpp-wrapper (#145325),test/inductor/test_inplace_padding.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_fx.py torch/_inductor/select_algorithm.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/145325,shunting314,desertfire,jansel,,
638903aeeea,skip,not user facing,Adapt Dynamo tests to HPUs using instantiate_device_type_tests (#144387),test/dynamo/test_debug_utils.py test/dynamo/test_repros.py test/dynamo/test_unspec.py,https://github.com/pytorch/pytorch/pull/144387,amathewc,EikanWang,ankurneog,guangyey,
b6941d4e42a,inductor,not user facing,[inductor] fix autotuning memory usage (#145410),test/inductor/test_inplace_padding.py test/inductor/test_triton_heuristics.py torch/_inductor/runtime/triton_heuristics.py torch/testing/_internal/inductor_utils.py,https://github.com/pytorch/pytorch/pull/145410,shunting314,jansel,masnesral,,
29f52e3972d,quantization,Untopiced,[2/N] Remove unnecessary once flag usage (#145057),aten/src/ATen/native/NNPACK.cpp aten/src/ATen/native/quantized/cpu/init_qnnpack.cpp torch/csrc/autograd/engine.cpp torch/csrc/cuda/Module.cpp torch/csrc/cuda/memory_snapshot.cpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/ProcessGroupMPI.cpp torch/csrc/distributed/c10d/ProcessGroupMPI.hpp torch/csrc/mps/Module.cpp torch/csrc/mtia/Module.cpp torch/csrc/xpu/Module.cpp,https://github.com/pytorch/pytorch/pull/145057,cyyever,albanD,,,
629840e038e,dataloader_frontend,not user facing,Backout PEP585 use of Iterable (#145438),torch/utils/data/dataset.py,https://github.com/pytorch/pytorch/pull/145438,aorenste,izaitsevfb,,,
0802e783154,releng,not user facing,[CD] Disable Kineto for XPU Windows CD (#145255),.circleci/scripts/binary_windows_build.sh,https://github.com/pytorch/pytorch/pull/145255,chuanqi129,atalman,xuhancn,,
768ad0886ff,skip,Untopiced,"Revert ""Binary upload checksum (#144887)""",.circleci/scripts/binary_upload.sh,,,,,,
bf4f8919df8,skip,not user facing,Fix test_modules_can_be_imported (#145387),test/test_public_bindings.py,https://github.com/pytorch/pytorch/pull/145387,H-Huang,albanD,,,
c6707734def,inductor,not user facing,Enable non power of 2 head_dim for FlexAttention (#133495),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/133495,drisspg,Chillee,,,
3a8d3785f7a,skip,not user facing,[ca][bug_fix] Fix ref counting of objects in the set_autograd_compiler function. (#145482),torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/145482,BartlomiejStemborowski,albanD,jansel,,
045698653a6,skip,not user facing,[BE] Remove test_ops_gradients from FIXME_inductor_dont_reset_dynamo (#145308),test/inductor_skips/TestBwdGradientsCPU.test_fn_grad_linalg_det_singular_cpu_float64 torch/testing/_internal/dynamo_test_failures.py,https://github.com/pytorch/pytorch/pull/145308,masnesral,zou3519,,,
a86fa779ce3,fx,not user facing,[BE] Fix edge case in translation validation bisector (#145414),test/dynamo_expected_failures/ExcTests.test_trigger_bisect_on_error torch/fx/experimental/validator.py,https://github.com/pytorch/pytorch/pull/145414,StrongerXi,ysiraichi,zou3519,,
b0f35971335,mps,improvements,Add fused rms_norm implementation for MPS backend (#145301),aten/src/ATen/native/layer_norm.cpp aten/src/ATen/native/mps/kernels/RMSNorm.metal aten/src/ATen/native/mps/operations/RMSNorm.h aten/src/ATen/native/mps/operations/RMSNorm.mm,https://github.com/pytorch/pytorch/pull/145301,manuelcandales,malfet,,,
3c247ee8c4d,skip,not user facing,[hop][be] add utils for more comprehensive input alias and mutation (#145298),test/functorch/test_control_flow.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/flex_attention.py torch/_higher_order_ops/hints_wrap.py torch/_higher_order_ops/scan.py torch/_higher_order_ops/utils.py torch/_higher_order_ops/while_loop.py,https://github.com/pytorch/pytorch/pull/145298,ydwu4,zou3519,,,
bdc2c2a2375,skip,not user facing,[be] fix flaky test aot_export_ cond caused by free symbol lifting and automatic dynamic shape (#145330),test/functorch/test_aotdispatch.py,https://github.com/pytorch/pytorch/pull/145330,ydwu4,zou3519,,,
9a5bc7b6ddc,inductor,not user facing,[BE] Type annotate metrics.py (#145418),torch/_inductor/choices.py torch/_inductor/metrics.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/145418,BoyuanFeng,Skylion007,,,
fef92c9447c,skip,not user facing,Fix IdentationError of code example  (#145251),docs/source/notes/get_start_xpu.rst,https://github.com/pytorch/pytorch/pull/145251,ZhaoqiongZ,mikaylagawarecki,,,
015c6d6fdb5,dynamo,not user facing,[dynamo][guards] Turn on profiling of guard manager (#145420),torch/_C/_dynamo/guards.pyi torch/_dynamo/guards.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/145420,anijain2305,ezyang,,,
629fb1590c6,inductor,not user facing,[BE] Type annotate pad_mm.py (#145409),torch/_inductor/fx_passes/pad_mm.py,https://github.com/pytorch/pytorch/pull/145409,BoyuanFeng,Skylion007,,,
34b8d8b0c0a,releng,not user facing,update compile time benchmarks to dump compile times to stdout and csv (#145447),.ci/pytorch/test.sh benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/145447,xmfan,ezyang,,,
41b38f755c4,linalg_frontend,Untopiced,"Revert ""Reverting the PR adding Kleidiai-based int4 kernels (#145392)"" (#145505)",.gitmodules BUILD.bazel CMakeLists.txt WORKSPACE aten/src/ATen/CMakeLists.txt aten/src/ATen/Config.h.in aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int4mm_kernel.cpp aten/src/ATen/native/cpu/int_mm_kernel.h aten/src/ATen/native/kleidiai/kai_kernels.cpp aten/src/ATen/native/kleidiai/kai_kernels.h aten/src/ATen/native/kleidiai/kai_pack.h aten/src/ATen/native/kleidiai/kai_ukernel_interface.cpp aten/src/ATen/native/kleidiai/kai_ukernel_interface.h aten/src/ATen/native/native_functions.yaml buckbuild.bzl cmake/Dependencies.cmake cmake/Summary.cmake cmake/TorchConfig.cmake.in docs/source/backends.rst setup.py test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_torchinductor.py test/test_linalg.py third_party/kleidiai torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/_inductor/quantized_lowerings.py torch/_meta_registrations.py torch/backends/__init__.py torch/backends/kleidiai/__init__.py torch/csrc/Module.cpp torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_quantization.py torchgen/aoti/fallback_ops.py,https://github.com/pytorch/pytorch/pull/145505,nikhil-arm,malfet,,,
2f317bbdbc4,mps,Untopiced,Missing autorelease in lstm_mps caused a ton of leaked memory (#145503),aten/src/ATen/native/mps/operations/RnnOps.mm,https://github.com/pytorch/pytorch/pull/145503,jhavukainen,Skylion007,malfet,,
6d4f5f76883,releng,not user facing,[Utilization][Usage Log] Add data model for record (#145114),.ci/docker/requirements-ci.txt .github/workflows/_linux-test.yml .lintrunner.toml tools/stats/monitor.py tools/stats/utilization_stats_lib.py,https://github.com/pytorch/pytorch/pull/145114,yangw-dev,huydhn,,,
5b37249259a,skip,Untopiced,Enable fp16 linear layers in PyTorch via ACL (#144992),aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/mkldnn/Matmul.cpp aten/src/ATen/native/mkldnn/Utils.h test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/144992,renato-arantes,,,,
991a4b59254,dynamo,not user facing,[dynamo] Add `--profile-details` and `--export-perfdoctor` option (#144751),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/144751,xuzhao9,drisspg,,,
66bf7da446f,inductor,Untopiced,Enable sleef for Win Arm64 (#144876),aten/src/ATen/CMakeLists.txt caffe2/CMakeLists.txt torch/_inductor/cpp_builder.py torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/144876,iremyux,albanD,malfet,,
527101fa952,releng,Untopiced,Move Windows arm64 scripts from pytorch/builder (#144317),.ci/pytorch/test_example_code/cnn_smoke_win_arm64.py .ci/pytorch/test_example_code/rnn_smoke_win_arm64.py .ci/pytorch/windows/arm64/bootstrap_apl.bat .ci/pytorch/windows/arm64/bootstrap_buildtools.bat .ci/pytorch/windows/arm64/bootstrap_git.bat .ci/pytorch/windows/arm64/bootstrap_libuv.bat .ci/pytorch/windows/arm64/bootstrap_openblas.bat .ci/pytorch/windows/arm64/bootstrap_python.bat .ci/pytorch/windows/arm64/bootstrap_rust.bat .ci/pytorch/windows/arm64/bootstrap_sccache.bat .ci/pytorch/windows/arm64/bootstrap_tests.bat .ci/pytorch/windows/arm64/build_libtorch.bat .ci/pytorch/windows/arm64/build_pytorch.bat .ci/pytorch/windows/arm64/smoke_test.bat,https://github.com/pytorch/pytorch/pull/144317,iremyux,Skylion007,seemethere,,
ce4a097bf76,skip,Untopiced,"Revert ""Added swizzle searching, disabled fp16 accum, and enabled ping-pong for cutlass (#144829)""",torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/config.py,,,,,,
d53f2067fec,export,Untopiced,"[BE][export] add ""+export"" logging to de/serialization (#145283)",torch/_export/serde/serialize.py torch/_logging/_registrations.py,https://github.com/pytorch/pytorch/pull/145283,pianpwk,angelayi,ydwu4,,
d7b6746470b,skip,Untopiced,"Revert ""Fix deprecated pytorch_sphinx_theme editable installation (#145347)""",.ci/docker/requirements-docs.txt docs/requirements.txt,,,,,,
5ebca3015d2,skip,Untopiced,[BE]: Simplify set add with set update (#145152),torch/_functorch/partitioners.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/145152,Skylion007,XuehaiPan,albanD,,
99367ecbedb,export,Untopiced,[draft export] count how many times a data-dep error shows up (#145030),torch/export/_draft_export.py,https://github.com/pytorch/pytorch/pull/145030,pianpwk,angelayi,,,
6a44a615144,releng,not user facing,[BE] Bump TIMM pin (#145320),.ci/docker/ci_commit_pins/timm.txt benchmarks/dynamo/ci_expected_accuracy/aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv,https://github.com/pytorch/pytorch/pull/145320,desertfire,Skylion007,,,
881eb86692a,distributed,Untopiced,Fix staging for CPU tensors in OSS DCP async_save (#145408),torch/distributed/checkpoint/staging.py torch/distributed/checkpoint/state_dict_saver.py,,,,,,
e924ddbef12,mps,not user facing,[BE] [mps] Refactor UnaryConstants to be its own kernel. (#145230),aten/src/ATen/native/mps/UnaryConstants.h aten/src/ATen/native/mps/kernels/UnaryKernel.metal aten/src/ATen/native/mps/operations/UnaryKernel.mm,https://github.com/pytorch/pytorch/pull/145230,dcci,malfet,,,
302b07f166e,inductor,not user facing,Implement deepcopy for AOTICompiledModel (#145423),test/inductor/test_aot_inductor_package.py torch/_inductor/package/package.py,https://github.com/pytorch/pytorch/pull/145423,yushangdi,desertfire,,,
e1407f5aeb6,skip,not user facing,[compiled_autograd] Rename interface to pyinterface (#145495),torch/csrc/autograd/custom_function.h,https://github.com/pytorch/pytorch/pull/145495,zou3519,albanD,xmfan,,
6f07847efe9,inductor,not user facing,Bail on checking internal overlap when dealing with unbacked symints (#145385),aten/src/ATen/MemoryOverlap.cpp test/dynamo/test_misc.py test/test_dynamic_shapes.py torch/_inductor/lowering.py torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/145385,bobrenjc93,ezyang,,,
42f4fda2ebb,skip,Untopiced,Enable clang-tidy on torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp (#143806),.lintrunner.toml torch/csrc/distributed/c10d/FileStore.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/exception.h torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/logging.h torch/csrc/distributed/c10d/reducer.cpp torch/csrc/distributed/c10d/reducer.hpp,https://github.com/pytorch/pytorch/pull/143806,cyyever,kwen2501,,,
dad9bc34610,skip,Untopiced,"Revert ""[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441)""",aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,,,,,,
a8b7cb6a2dd,skip,not user facing,Add multi env variable support to configs (#145288),test/test_utils_config_module.py torch/testing/_internal/fake_config_module2.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/145288,oulgen,c00w,,,
bf62222d817,skip,Untopiced,"Revert ""[compiled_autograd] Rename interface to pyinterface (#145495)""",torch/csrc/autograd/custom_function.h,,,,,,
0a310d73881,skip,not user facing,[dynamo] Log guard latency (#145132),test/dynamo/test_utils.py torch/_dynamo/guards.py torch/_dynamo/testing.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/145132,anijain2305,ezyang,,,
ab082863a16,skip,Untopiced,"Revert ""[compiled autograd] support Tensor Subclasses in AOTBackward (#144115)""",test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/subclass_utils.py,,,,,,
3f6cfd01564,skip,Untopiced,"Revert ""[compiled autograd] stop specializing on metadata during initial trace (#143417)""",torch/_dynamo/compiled_autograd.py torch/_dynamo/trace_rules.py torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/autograd/init.cpp torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,,,,,,
16c4f8c3955,skip,Untopiced,"Revert ""[compiled autograd] Always proxy autograd.Function nodes; handle AOT backwards (#143405)""",test/dynamo/test_backward_higher_order_ops.py test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/_dynamo/external_utils.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/autograd/function.py torch/csrc/autograd/python_function.cpp,,,,,,
9553301ade6,skip,Untopiced,"Revert ""[compiled autograd] Proxy nodes for user-defined C++ torch::autograd::Function (#143387)""",test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/autograd/custom_function.cpp torch/csrc/autograd/custom_function.h torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,,,,,,
c3fadacf84f,skip,Untopiced,"Revert ""[compiled autograd] Proxy a node for CopyBackwards into the graph (#143304)""",torch/csrc/autograd/functions/tensor.cpp,,,,,,
6dd82833817,skip,Untopiced,"Revert ""[compiled autograd] Proxy opaque nodes for built-in autograd nodes (#143296)""",aten/src/ATen/TensorGeometry.h build_variables.bzl test/dynamo/test_backward_higher_order_ops.py test/inductor/test_compiled_autograd.py test/inductor/test_distributed_patterns.py tools/autograd/gen_autograd_functions.py torch/_dynamo/compiled_autograd.py torch/csrc/autograd/engine.cpp torch/csrc/autograd/engine.h torch/csrc/autograd/function.h torch/csrc/autograd/function_hook.h torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/dynamo/compiled_autograd.cpp torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,,,,,,
f8a4f166345,distributed,Untopiced,[c10d] fix memory leak on shutdown (#145507),torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp,https://github.com/pytorch/pytorch/pull/145507,c-p-i-o,XilunWu,d4l3k,,
6aaae9d78f0,distributed,Untopiced,Make torchelastic etcd rendezvous publicly importable (#145396),test/test_public_bindings.py torch/distributed/elastic/rendezvous/_etcd_stub.py torch/distributed/elastic/rendezvous/etcd_rendezvous.py torch/distributed/elastic/rendezvous/etcd_rendezvous_backend.py torch/distributed/elastic/rendezvous/etcd_store.py,https://github.com/pytorch/pytorch/pull/145396,H-Huang,albanD,,,
f0e9f87a9b5,skip,not user facing,[BE/mps] Mark input args as `constant` to prevent incorrect usage. (#145535),aten/src/ATen/native/mps/kernels/UnaryKernel.metal,https://github.com/pytorch/pytorch/pull/145535,dcci,jansel,malfet,,
6f60c65a3a4,skip,Untopiced,"Revert ""[dynamo] Log guard latency (#145132)""",test/dynamo/test_utils.py torch/_dynamo/guards.py torch/_dynamo/testing.py torch/_dynamo/utils.py,,,,,,
6a2b4db0a1e,skip,Untopiced,"Revert ""Enable clang-tidy on torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp (#143806)""",.lintrunner.toml torch/csrc/distributed/c10d/FileStore.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/exception.h torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/logging.h torch/csrc/distributed/c10d/reducer.cpp torch/csrc/distributed/c10d/reducer.hpp,,,,,,
714f64329ba,skip,Untopiced,"Revert ""Add multi env variable support to configs (#145288)""",test/test_utils_config_module.py torch/testing/_internal/fake_config_module2.py torch/utils/_config_module.py,,,,,,
b963ab5325c,inductor,bug fixes,"[inductor][1/N] triton support post-#5512, main components (#145051)",test/inductor/test_torchinductor.py torch/_inductor/codegen/common.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_utils.py torch/_inductor/runtime/hints.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/145051,davidberard98,jansel,,,
b2c89bc1151,inductor,bug fixes,"[inductor][2/N] triton support post-#5512, user-defined triton kernels (#145348)",test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_inductor/codegen/triton_utils.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/145348,davidberard98,jansel,,,
54e2f4b2013,python_frontend,improvements,Fix lerp weight type promotion (#141117),aten/src/ATen/native/Lerp.cpp test/test_binary_ufuncs.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/141117,zeshengzong,janeyx99,,,
cd68d549111,inductor,not user facing,Inductor cache: Revamp how we handle frozen params (#143808),test/inductor/test_codecache.py torch/_inductor/codecache.py torch/_inductor/output_code.py,https://github.com/pytorch/pytorch/pull/143808,masnesral,eellison,leslie-fang-intel,,
47e65077b13,skip,not user facing,OpenReg: Remove REGISTER_GENERATOR_PRIVATEUSE1 (#144841),test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/OpenRegHooks.cpp,https://github.com/pytorch/pytorch/pull/144841,Zhenbin-8,albanD,,,
d5629889f12,inductor,not user facing,cpp_wrapper: Properly handle scalars when input to tensor arguments (#144910),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/ir.py torch/csrc/inductor/aoti_runtime/scalar_to_tensor.h torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/144910,benjaminglass1,desertfire,,,
d6bea398ac8,skip,not user facing,Only include RMSNorm.h in layer_norm.cpp for MPS (#145524),aten/src/ATen/native/layer_norm.cpp,https://github.com/pytorch/pytorch/pull/145524,manuelcandales,malfet,,,
386650353ba,linalg_frontend,not user facing,[ARM] Fix bf32 and tf32 precision for tensordot unit test (#141136),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/141136,robert-hardwick,malfet,,,
36fcf98db62,skip,not user facing,"[cutlass backend tests] Manually clear cache, test more tests in fbcode and limit configs in some tests (#145545)",test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/145545,henrylhtsang,ColinPeppler,coconutruben,,
a40ead1fd6c,inductor,not user facing,Don't fail if fresh_inductor_cache fails to clean up its tmp dir. (#145513),torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/145513,masnesral,eellison,,,
29ddf9a63eb,skip,not user facing,Document dispatch trace build flag (#145517),aten/src/ATen/core/dispatch/Dispatcher.h aten/src/ATen/functorch/ADInterpreters.cpp aten/src/ATen/functorch/DynamicLayer.cpp setup.py,https://github.com/pytorch/pytorch/pull/145517,albanD,bdhirsh,,,
68a15059854,export,Untopiced,serde and_ operator (#145506),test/export/test_export.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/145506,avikchaudhuri,Skylion007,zhxchen17,,
4799ebf3266,mps,not user facing,[MPS][BE] Turn `bicubic2d` into generic metal template (#145578),aten/src/ATen/native/mps/operations/UpSample.mm,https://github.com/pytorch/pytorch/pull/145578,malfet,Skylion007,,,
c1840557432,skip,not user facing,[BE] Use `value_or` in layer_norm.cpp (#145417),aten/src/ATen/native/layer_norm.cpp,https://github.com/pytorch/pytorch/pull/145417,malfet,Skylion007,huydhn,,
732c4998f34,build_frontend,Untopiced,[NVIDIA] Full Family Blackwell Support codegen (#145436),cmake/Modules_CUDA_fix/upstream/FindCUDA/select_compute_arch.cmake torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/145436,johnnynunez,drisspg,ezyang,,
9752c7c1c81,skip,not user facing,[CD] Fix slim-wheel cuda_nvrtc import problem (#145582),torch/__init__.py,https://github.com/pytorch/pytorch/pull/145582,atalman,eqy,kit1980,malfet,
53fc921ce2b,dynamo,not user facing,[dynamo][trace-rules-cleanup] Remove functools from the Builtins skiplist (#145519),test/dynamo/test_repros.py test/inductor/test_flex_attention.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/145519,anijain2305,yanboliang,zou3519,,
2ce70da96ca,distributed,not user facing,[cp] override compute_log_sumexp to True for aten._scaled_dot_product_efficient_attention.default if False (#145421),test/distributed/tensor/test_attention.py torch/distributed/tensor/experimental/_attention.py,https://github.com/pytorch/pytorch/pull/145421,XilunWu,H-Huang,tianyu-l,,
817fd147149,inductor,not user facing,[BE] Type annotation for `_inductor/dependencies.py` (#145311),torch/_inductor/dependencies.py torch/_inductor/ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/145311,BoyuanFeng,eellison,,,
0e98b26b28e,skip,not user facing,[CI][CUDA][Dynamic Shape] xfail: DynamicShapesCodegenGPUTests.test_linspace4_dynamic_shapes_cuda (#145204),test/inductor/test_torchinductor_codegen_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/145204,nWEIdia,eellison,,,
d62e900d8ce,skip,not user facing,[CI][CUDA][MultiGPU][Regression] Skip a failure due to https://github.com/pytorch/pytorch/issues/139520 (#145318),test/inductor/test_move_constructors_to_cuda.py,https://github.com/pytorch/pytorch/pull/145318,nWEIdia,eqy,,,
72da0a8a42c,skip,not user facing,[Submodule] Add flash as third-party submodule [Prep for later PRs] (#145502),.gitmodules third_party/flash-attention,https://github.com/pytorch/pytorch/pull/145502,drisspg,Skylion007,,,
10bdd0a1cc2,skip,not user facing,[BE][export] Fix hop tests with flaky memory leak (#145391),test/export/test_hop.py,https://github.com/pytorch/pytorch/pull/145391,yiming0416,ydwu4,,,
d3989ca636b,skip,not user facing,Add multi env variable support to configs (#145288),test/test_utils_config_module.py torch/testing/_internal/fake_config_module2.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/145288,oulgen,c00w,,,
f08b9bc7e4e,quantization,Untopiced,[WIP] Move XNNPACKQuantizer from PyTorch to ExecuTorch (#144940),torch/ao/quantization/quantizer/xnnpack_quantizer.py,https://github.com/pytorch/pytorch/pull/144940,digantdesai,jerryzh168,mcr229,,
6a35d9aaa4f,skip,Untopiced,Enable clang-tidy on torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp (#143806),.lintrunner.toml torch/csrc/distributed/c10d/FileStore.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/exception.h torch/csrc/distributed/c10d/init.cpp torch/csrc/distributed/c10d/logging.h torch/csrc/distributed/c10d/reducer.cpp torch/csrc/distributed/c10d/reducer.hpp,https://github.com/pytorch/pytorch/pull/143806,cyyever,kwen2501,,,
9d6927715f8,skip,Untopiced,"Revert ""Fix triton masked loading for non-block tl.loads (#144782)""",test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/codegen/triton.py,,,,,,
bc629307650,python_frontend,bug fixes,Work around buggy use_const_ref_for_mutable_tensors (#145530),aten/src/ATen/native/SpectralOps.cpp aten/src/ATen/native/cuda/DepthwiseConv2d.cu aten/src/ATen/native/native_functions.yaml test/test_fake_tensor.py torchgen/model.py,https://github.com/pytorch/pytorch/pull/145530,ezyang,albanD,bdhirsh,,
5c64aaea404,skip,not user facing,[triton] Update triton pin to include warp specialization support (#145120),.ci/docker/ci_commit_pins/triton.txt,https://github.com/pytorch/pytorch/pull/145120,htyu,bertmaher,,,
5d24a9a274d,releng,not user facing,Advance docker release latest verison to cuda 12.4 (#145566),.github/workflows/docker-release.yml,https://github.com/pytorch/pytorch/pull/145566,atalman,kit1980,malfet,nWEIdia,
7c314bfed4e,skip,not user facing,[Intel GPU] Add TORCH_API macro to export symbol NestedTensor_to_mask for libtorch_xpu (#145467),aten/src/ATen/native/nested/NestedTensorTransformerFunctions.h,https://github.com/pytorch/pytorch/pull/145467,min-jean-cho,ezyang,guangyey,,
1335882b2a7,skip,not user facing,If mypy fails it should report the error back to lintrunner (#145550),tools/linter/adapters/mypy_linter.py,https://github.com/pytorch/pytorch/pull/145550,aorenste,Skylion007,bobrenjc93,,
9132f4b7ce5,dynamo,not user facing,[dynamo][guards] Log guard latency to tlparse (#145509),torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/145509,anijain2305,ezyang,,,
c0861d092ce,distributed,Untopiced,[PGNCCL] Add an API to get the status/error code at the PG level (#144498),test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp test/distributed/test_c10d_nccl.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/144498,shuqiangzhang,kwen2501,,,
ce371ab4c6f,releng,not user facing,[ROCm] Create inductor-rocm-mi300 (#145621),.github/workflows/inductor-rocm-mi300.yml,https://github.com/pytorch/pytorch/pull/145621,amdfaa,jeffdaily,,,
547c18ee9fc,quantization,Untopiced,Add Torchao docs link to Pytorch libraries (#145412),docs/source/index.rst,https://github.com/pytorch/pytorch/pull/145412,jainapurva,svekars,,,
97c0b7cb0a9,inductor,not user facing,Add unique identifer to bmm thread_mm functions (#145303),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_bmm_template.py,https://github.com/pytorch/pytorch/pull/145303,dmpots,frost-intel,hl475,leslie-fang-intel,
74cfb4f3647,dynamo,not user facing,[dynamo][refactor] Move collections.namedtuple out of SkipFunctionVariable (#145547),torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/145547,anijain2305,zou3519,,,
b8087747f5c,skip,not user facing,[inductor][BE] Enable test_cpu_cpp_wrapper in fbcode (#145373),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/cpp_builder.py torch/testing/_internal/inductor_utils.py,https://github.com/pytorch/pytorch/pull/145373,desertfire,Skylion007,,,
e57cdb84026,releng,not user facing,[ROCm] trunk.yml only runs pre-merge via ciflow/trunk label (#145629),.github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/145629,amdfaa,jeffdaily,,,
8eea554332e,dynamo,Untopiced,[Dynamo] Fix names collisions with foreach decomps (#145479),test/dynamo/test_repros.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/145479,mlazos,yanboliang,,,
6cda572c988,mps,not user facing,[mps] Hoist erfinv logic out of the kernel in preparation for moving. (#145568),aten/src/ATen/native/mps/kernels/UnaryKernel.metal,https://github.com/pytorch/pytorch/pull/145568,dcci,Skylion007,malfet,,
a989a0b13a0,jit,not user facing,[NFC] Fix some minor typos. (#145599),torch/_inductor/codegen/wrapper.py torch/csrc/jit/python/pybind_utils.h torch/distributed/distributed_c10d.py torch/distributed/pipelining/_backward.py torchgen/gen_aoti_c_shim.py,https://github.com/pytorch/pytorch/pull/145599,c8ef,Skylion007,,,
ad36f4f42c4,skip,Untopiced,"Revert ""Add generator parameter to rand*_like functions (#136780)""",aten/src/ATen/native/TensorFactories.cpp aten/src/ATen/native/native_functions.yaml test/expect/HasDecompTest.test_has_decomposition.expect test/test_tensor_creation_ops.py torch/_subclasses/fake_impls.py torch/_torch_docs.py torch/overrides.py,,,,,,
2fd2a950e68,skip,not user facing,[torchbench] Add meta function for _cudnn_rnn_flatten_weight (#145488),aten/src/ATen/native/cudnn/RNN.cpp,https://github.com/pytorch/pytorch/pull/145488,IvanKobzarev,eqy,zou3519,,
96149a201a5,inductor,not user facing,[Inductor] be able to disable cache for test (#141195),torch/_inductor/test_case.py,https://github.com/pytorch/pytorch/pull/141195,shunting314,desertfire,masnesral,,
cf063d41f85,inductor,not user facing,Spruce up docs for emulate_precision_casts (#145579),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/145579,ezyang,gchanan,,,
c16866a5821,skip,not user facing,[BE] mv test/inductor_skips/* to test/inductor_expected_failures/ (#145572),test/inductor_expected_failures/.gitkeep test/inductor_expected_failures/TestModuleCPU.test_non_contiguous_tensors_nn_KLDivLoss_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_non_contiguous_tensors_nn_KLDivLoss_cpu_float64 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm1d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm1d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm2d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm2d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm3d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm3d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Bilinear_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Conv1d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Conv2d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Conv3d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_ConvTranspose1d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_ConvTranspose2d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_ConvTranspose3d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Embedding_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_GRUCell_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_GroupNorm_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_LSTMCell_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_LayerNorm_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Linear_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_PReLU_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_RMSNorm_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_RNNCell_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCUDA.test_cpu_gpu_parity_nn_CTCLoss_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_cpu_gpu_parity_nn_CTCLoss_cuda_float64 test/inductor_expected_failures/TestModuleCUDA.test_non_contiguous_tensors_nn_KLDivLoss_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_non_contiguous_tensors_nn_KLDivLoss_cuda_float64 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm1d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm1d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm2d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm2d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm3d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm3d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Bilinear_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Conv1d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Conv2d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Conv3d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_ConvTranspose1d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_ConvTranspose2d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_ConvTranspose3d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Embedding_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_GRUCell_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_GroupNorm_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_LSTMCell_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_LayerNorm_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Linear_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_PReLU_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_RMSNorm_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_RNNCell_swap_True_set_grad_True_cuda_float32 test/inductor_skips/.gitkeep test/inductor_skips/TestBwdGradientsCPU.test_fn_grad_linalg_det_singular_cpu_float64 test/inductor_skips/TestModuleCPU.test_non_contiguous_tensors_nn_KLDivLoss_cpu_float32 test/inductor_skips/TestModuleCPU.test_non_contiguous_tensors_nn_KLDivLoss_cpu_float64 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm1d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm1d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm2d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm2d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm3d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_BatchNorm3d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Bilinear_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Conv1d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Conv2d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Conv3d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_ConvTranspose1d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_ConvTranspose2d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_ConvTranspose3d_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Embedding_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_GRUCell_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_GroupNorm_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_LSTMCell_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_LayerNorm_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_Linear_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_PReLU_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_RMSNorm_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCPU.test_to_nn_RNNCell_swap_True_set_grad_True_cpu_float32 test/inductor_skips/TestModuleCUDA.test_cpu_gpu_parity_nn_CTCLoss_cuda_float32 test/inductor_skips/TestModuleCUDA.test_cpu_gpu_parity_nn_CTCLoss_cuda_float64 test/inductor_skips/TestModuleCUDA.test_non_contiguous_tensors_nn_KLDivLoss_cuda_float32 test/inductor_skips/TestModuleCUDA.test_non_contiguous_tensors_nn_KLDivLoss_cuda_float64 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm1d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm1d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm2d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm2d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm3d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_BatchNorm3d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Bilinear_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Conv1d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Conv2d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Conv3d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_ConvTranspose1d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_ConvTranspose2d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_ConvTranspose3d_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Embedding_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_GRUCell_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_GroupNorm_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_LSTMCell_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_LayerNorm_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_Linear_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_PReLU_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_RMSNorm_swap_True_set_grad_True_cuda_float32 test/inductor_skips/TestModuleCUDA.test_to_nn_RNNCell_swap_True_set_grad_True_cuda_float32,https://github.com/pytorch/pytorch/pull/145572,masnesral,Skylion007,zou3519,,
457facf7e2e,caffe2,not user facing,[caffe2] Use the manifold cache backend as the default (#144773),torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/144773,AishwaryaSivaraman,izaitsevfb,,,
caf60395f4c,skip,not user facing,[torchbench] Increase tolerance for amp only poolformer_m36 (#145375),benchmarks/dynamo/timm_models.py,https://github.com/pytorch/pytorch/pull/145375,IvanKobzarev,desertfire,,,
d79c6f49460,distributed,not user facing,Improve torchrun documentation (#144354),torch/distributed/run.py,https://github.com/pytorch/pytorch/pull/144354,fepegar,H-Huang,c-p-i-o,,
615bdd9c81d,skip,not user facing,Improve the caching allocator test for raw alloc (#145269),test/test_cuda.py,https://github.com/pytorch/pytorch/pull/145269,1274085042,albanD,eqy,jeffdaily,
2a70de7e925,build_frontend,Untopiced,[CUDA] Change slim-wheel libraries load order (#145638),torch/__init__.py,https://github.com/pytorch/pytorch/pull/145638,nWEIdia,atalman,kit1980,malfet,
d4171b724ed,python_frontend,bc breaking,Let `tensor_a.new_tensor()` be on `tensor_a.device` by default (#144958),test/test_tensor_creation_ops.py torch/csrc/utils/tensor_new.cpp,https://github.com/pytorch/pytorch/pull/144958,oraluben,ezyang,,,
5bf5ce0e15a,skip,not user facing,Modify enable logic of COLLECTIVE_COMM profiler activity type (#145478),torch/csrc/profiler/kineto_shim.cpp,https://github.com/pytorch/pytorch/pull/145478,jushg,sraikund16,,,
70577d335e9,skip,not user facing,[ATen][CUDA][Transformers] Add Blackwell support to SDPA (#145602),aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.cpp aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB.h aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_bf16_aligned_k128.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_bf16_aligned_k128_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_bf16_aligned_k32.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_bf16_aligned_k32_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_bf16_aligned_k64.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_bf16_aligned_k64_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_bf16_aligned_k65536.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_bf16_aligned_k65536_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_bf16_aligned_k96.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f16_aligned_k128.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f16_aligned_k128_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f16_aligned_k32.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f16_aligned_k32_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f16_aligned_k64.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f16_aligned_k64_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f16_aligned_k65536.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f16_aligned_k65536_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f16_aligned_k96.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f32_aligned_k128.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f32_aligned_k128_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f32_aligned_k32.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f32_aligned_k32_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f32_aligned_k64.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f32_aligned_k64_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f32_aligned_k65536.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassB_f32_aligned_k65536_dropout.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassF.h aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassF_bf16_aligned.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassF_f16_aligned.cu aten/src/ATen/native/transformers/cuda/mem_eff_attention/kernels/cutlassF_f32_aligned.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp test/test_transformers.py,https://github.com/pytorch/pytorch/pull/145602,Aidyn-A,Skylion007,drisspg,eqy,
81e370fc6b9,skip,Untopiced,Fix type annotation of `Linear.bias` (#142326),torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py torch/nn/modules/activation.py torch/nn/modules/linear.py torch/nn/modules/transformer.py torch/utils/_typing_utils.py,https://github.com/pytorch/pytorch/pull/142326,bluenote10,,,,
46e06e1d09b,skip,not user facing,Avoid data-dependent errors in NJT tests via capture_scalar_outputs=True (#144588),test/test_nestedtensor.py,https://github.com/pytorch/pytorch/pull/144588,jbschlosser,soulitzer,,,
57591edca1b,inductor,not user facing,[mps/inductor] Add support for `erfinv`. (#145643),aten/src/ATen/native/mps/kernels/UnaryKernel.metal c10/metal/special_math.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/145643,dcci,jansel,malfet,,
5b988ac4fa3,nn_frontend,not user facing,[Easy] Replace paper description with link to make a concise description. (#145031),torch/nn/modules/transformer.py,https://github.com/pytorch/pytorch/pull/145031,zeshengzong,mikaylagawarecki,,,
4cc5e880f9a,dynamo,Untopiced,Add accuracy issue support in AOTI Minifier (#145539),test/inductor/test_minifier.py torch/_dynamo/repro/aoti.py torch/_dynamo/test_minifier_common.py torch/_inductor/__init__.py torch/_inductor/config.py torch/_inductor/debug.py,https://github.com/pytorch/pytorch/pull/145539,yushangdi,desertfire,,,
0741963e019,skip,not user facing,[CI][CUDA][Blackwell] sm_\d\d no longer matches sm_100.  (#145641),test/test_cpp_extensions_jit.py,https://github.com/pytorch/pytorch/pull/145641,nWEIdia,eqy,malfet,,
f3304571fcf,distributed,not user facing,[BE][Ez]: FURB148 - remove useless enumerate calls (#145619),test/distributed/test_c10d_common.py test/distributed/test_fake_pg.py test/jit/test_alias_analysis.py test/profiler/test_profiler.py test/test_jit_fuser_te.py test/test_ops.py test/test_spectral_ops.py test/torch_np/numpy_tests/core/test_scalarmath.py tools/gen_vulkan_spv.py tools/test/test_set_linter.py torch/_higher_order_ops/utils.py torch/_inductor/codegen/simd.py torch/_inductor/fuzzer.py torch/_inductor/memory.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/145619,Skylion007,drisspg,,,
f2ad2cdf1cf,skip,not user facing,[utils] add try_import method for importing optional modules (#145528),test/test_utils.py torch/_utils.py,https://github.com/pytorch/pytorch/pull/145528,mhorowitz,albanD,,,
efebec5ef5e,skip,Untopiced,[dcp] Add ZStandard transformer (#143360),.ci/docker/requirements-ci.txt test/distributed/checkpoint/test_dtensor_resharding.py test/distributed/checkpoint/test_file_system_checkpoint.py test/distributed/checkpoint/test_file_system_checkpoint_cpu.py torch/distributed/checkpoint/_extension.py torch/testing/_internal/distributed/checkpoint_utils.py,https://github.com/pytorch/pytorch/pull/143360,mhorowitz,albanD,saumishr,,
b4fe3c159dc,skip,not user facing,inductor: Explicitly test that torch.compile(option=...) does something (#145321),test/inductor/test_config.py,https://github.com/pytorch/pytorch/pull/145321,c00w,jansel,,,
9e0ee152e5c,autograd_frontend,bug fixes,Fix allow_mutation_on_saved_tensors for inplace foreach (#145520),test/test_autograd.py torch/autograd/graph.py,https://github.com/pytorch/pytorch/pull/145520,soulitzer,albanD,,,
c7ca1df37ed,skip,not user facing,Disable slow gradcheck for nn.Transformer ModuleInfo (#145531),test/test_modules.py torch/testing/_internal/common_modules.py,https://github.com/pytorch/pytorch/pull/145531,soulitzer,mikaylagawarecki,,,
3a3e2cf90a0,distributed,not user facing,Remove det_singular OpInfo (#145533),test/distributed/tensor/test_dtensor_ops.py test/functorch/test_ops.py test/inductor/test_torchinductor_opinfo.py test/test_mps.py torch/testing/_internal/opinfo/definitions/linalg.py,https://github.com/pytorch/pytorch/pull/145533,soulitzer,lezcano,malfet,,
5725462cd86,skip,not user facing,Update NJT linear_backward to return non-aliased tensor bias grad (#145399),torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/145399,soulitzer,jbschlosser,,,
42b8e233d94,export,Untopiced,serde unbacked bindings (#144894),test/export/test_export.py torch/_export/serde/serialize.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/144894,avikchaudhuri,pianpwk,,,
ef60de07a0d,skip,not user facing,[dynamo] Log guard latency (#145132),test/dynamo/test_utils.py torch/_dynamo/guards.py torch/_dynamo/testing.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/145132,anijain2305,ezyang,,,
6939a56e135,skip,not user facing,[autocast][pytorch] Support autocast for MTIA (#145627),c10/core/DispatchKey.cpp c10/core/DispatchKey.h c10/core/DispatchKeySet.h torch/amp/autocast_mode.py,https://github.com/pytorch/pytorch/pull/145627,nautsimon,egienvalue,,,
392dc177a9d,skip,not user facing,OpenReg: Refactor impl_registry (#145465),test/cpp_extensions/open_registration_extension/pytorch_openreg/__init__.py test/cpp_extensions/open_registration_extension/pytorch_openreg/_aten_impl.py test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/Module.cpp test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/OpenReg.h test/cpp_extensions/open_registration_extension/pytorch_openreg/csrc/OpenRegHooks.cpp,https://github.com/pytorch/pytorch/pull/145465,Zhenbin-8,albanD,,,
b2a0feac854,nested tensor_frontend,docs,Update OSS nested tensor docs to focus on NJT (#145402),docs/source/_static/img/nested/njt_visual.png docs/source/nested.rst torch/nested/__init__.py,https://github.com/pytorch/pytorch/pull/145402,jbschlosser,soulitzer,,,
cc1ecead075,dynamo,not user facing,[Dynamo] Allow `format()` to handle int (#144956),test/dynamo/test_repros.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/144956,shink,jansel,,,
9007eb5f8e5,inductor,not user facing,[inductor] Kernel memory analysis for use in heuristics (#142026),torch/_inductor/codegen/simd_kernel_features.py,https://github.com/pytorch/pytorch/pull/142026,jansel,eellison,,,
3d506491b95,inductor,not user facing,[inductor] Fix duplicate detection in _dynamic_scale_rblock (#145577),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/145577,jansel,shunting314,,,
bbe7f532187,skip,not user facing,Save integral tensor data for ET (#144508),test/profiler/test_execution_trace.py torch/csrc/profiler/standalone/execution_trace_observer.cpp,https://github.com/pytorch/pytorch/pull/144508,shengfukevin,briancoutinho,,,
c5216d2b6c6,skip,not user facing,[ca] add test_reset for 2.6 release validation (#145549),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/145549,xmfan,atalman,,,
513f889a36f,inductor,not user facing,[Rocm][Inductor][CK] silence ck package not installed warning when CK backend is not used to autotune bmm (#145626),torch/_inductor/kernel/bmm.py,https://github.com/pytorch/pytorch/pull/145626,tenpercent,coconutruben,,,
0afdee4c39d,dynamo,not user facing,[dynamo] raise IndexError when inserting into a full `deque` (#139379),torch/_dynamo/exc.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/139379,XuehaiPan,jansel,,,
3cf7874ebea,mps,improvements,[MPS][BE] Implement bilineard2d as shader (#145581),aten/src/ATen/native/mps/kernels/UpSample.metal aten/src/ATen/native/mps/operations/UpSample.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/145581,malfet,Skylion007,,,
76bec878daa,skip,not user facing,Remove unnecessary HPUHooksInterface method (#145272),aten/src/ATen/detail/HPUHooksInterface.h,https://github.com/pytorch/pytorch/pull/145272,moksiuc,ezyang,,,
90448f0128d,dynamo,bug fixes,"Output of nonzero is transposed, fix fake tensor (#144695)",benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/inductor/test_auto_functionalize.py test/inductor/test_unbacked_symints.py test/test_fake_tensor.py torch/_dynamo/debug_utils.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_prims_common/__init__.py torch/_subclasses/fake_impls.py,https://github.com/pytorch/pytorch/pull/144695,ezyang,albanD,bobrenjc93,,
cb814c0b961,skip,not user facing,Align CPU behavior with CUDA for `ConvTranspose` when `out_channels=0` (#142859),aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp test/nn/test_convolution.py,https://github.com/pytorch/pytorch/pull/142859,chunyuan-w,malfet,mingfeima,,
73622fc5fa9,skip,not user facing,Fix Throughputbenchmark issue (#144669),test/test_throughput_benchmark.py torch/csrc/dynamo/guards.cpp torch/csrc/utils/throughput_benchmark-inl.h,https://github.com/pytorch/pytorch/pull/144669,shiyang-weng,jansel,leslie-fang-intel,williamwen42,
09ae69a3648,skip,Untopiced,"Revert ""Fix type annotation of `Linear.bias` (#142326)""",torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py torch/nn/modules/activation.py torch/nn/modules/linear.py torch/nn/modules/transformer.py torch/utils/_typing_utils.py,,,,,,
c4523999a15,fx,Untopiced,Fix incorrect type comparison (#145449),test/test_fx_experimental.py torch/fx/operator_schemas.py,https://github.com/pytorch/pytorch/pull/145449,aorenste,bobrenjc93,,,
ed015143ef1,skip,not user facing,Set RUNPATH on CUDA and XPU tests (#144305),c10/cuda/test/CMakeLists.txt c10/xpu/test/CMakeLists.txt caffe2/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/144305,oraluben,malfet,,,
5cd2b34e821,inductor,not user facing,[inductor] Adjust test_log_fp64 to only run when float64 is supported. (#145686),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/145686,dcci,jansel,malfet,,
6a4fb4b6153,skip,Untopiced,"Revert ""Align CPU behavior with CUDA for `ConvTranspose` when `out_channels=0` (#142859)""",aten/src/ATen/native/NaiveConvolutionTranspose2d.cpp aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp test/nn/test_convolution.py,,,,,,
f3ddc08ddc0,benchmark,Untopiced,Additional operators in operator benchmark (#145625),benchmarks/operator_benchmark/benchmark_all_other_test.py benchmarks/operator_benchmark/pt/arange_test.py benchmarks/operator_benchmark/pt/binary_inplace_test.py benchmarks/operator_benchmark/pt/binary_test.py benchmarks/operator_benchmark/pt/bmm_test.py benchmarks/operator_benchmark/pt/index_add__test.py benchmarks/operator_benchmark/pt/matrix_mult_test.py benchmarks/operator_benchmark/pt/mm_test.py benchmarks/operator_benchmark/pt/ternary_test.py benchmarks/operator_benchmark/pt/topk_test.py benchmarks/operator_benchmark/pt/unary_test.py benchmarks/operator_benchmark/pt/where_test.py,https://github.com/pytorch/pytorch/pull/145625,apakbin,jeffdaily,,,
c6ad08357bf,skip,Untopiced,pickler for GraphModule (#141659),test/distributed/_composable/fsdp/test_fully_shard_compile.py test/fx/test_graph_pickler.py torch/_dynamo/variables/misc.py torch/_inductor/compile_fx.py torch/_inductor/graph.py torch/_inductor/output_code.py torch/_inductor/utils.py torch/_subclasses/fake_tensor.py torch/_subclasses/meta_utils.py torch/fx/_graph_pickler.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/141659,aorenste,jamesjwu,,,
b75afa2e2e4,mps,improvements,[MPS] cholesky implementation (#145701),aten/src/ATen/native/mps/kernels/LinearAlgebra.metal aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py tools/autograd/derivatives.yaml,https://github.com/pytorch/pytorch/pull/145701,Isalia20,malfet,,,
87fdadde1df,dynamo,bug fixes,Remove FFT from stride incorrect ops (#145080),test/functorch/test_aotdispatch.py test/test_proxy_tensor.py torch/_meta_registrations.py torch/_subclasses/fake_impls.py,https://github.com/pytorch/pytorch/pull/145080,ezyang,Skylion007,albanD,,
ea141d8134a,distributed,not user facing,functional compiled autograd (#144707),aten/src/ATen/TensorGeometry.h build_variables.bzl test/dynamo/test_backward_higher_order_ops.py test/inductor/test_compiled_autograd.py test/inductor/test_distributed_patterns.py tools/autograd/gen_autograd_functions.py tools/autograd/templates/Functions.cpp torch/_dynamo/compiled_autograd.py torch/_dynamo/external_utils.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/trace_rules.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/subclass_utils.py torch/autograd/function.py torch/csrc/autograd/custom_function.cpp torch/csrc/autograd/custom_function.h torch/csrc/autograd/engine.cpp torch/csrc/autograd/engine.h torch/csrc/autograd/function.h torch/csrc/autograd/function_hook.h torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/autograd/init.cpp torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/dynamo/compiled_autograd.cpp torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/144707,zou3519,bdhirsh,jansel,xmfan,
71caac2b305,inductor,not user facing,[MPSInductor] Add rand support (#145705),c10/metal/random.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/145705,malfet,dcci,jansel,,
ddae87f7922,inductor,not user facing,[inductor] Add some typing to simd.py (#145690),torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/145690,jansel,malfet,,,
e90cf4abcff,inductor,not user facing,[inductor] Add some typing to common.py (#145691),test/inductor/extension_backends/cpp/extension_codegen_backend.py test/inductor/extension_backends/triton/extension_codegen_backend.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/wrapper.py torch/_inductor/metrics.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/145691,jansel,malfet,,,
e6c1e6e20ee,inductor,not user facing,simplify torch.utils.cpp_extension.include_paths; use it in cpp_builder (#145480),torch/_inductor/cpp_builder.py torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/145480,h-vetinari,ezyang,,,
7b6029dcc2c,skip,not user facing,Update slow tests (#145206),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/145206,pytorchupdatebot,pytorchbot,,,
0674ab7e33c,skip,not user facing,solve apl dependency issue (#145215),cmake/Modules/FindAPL.cmake cmake/Modules/FindOpenMP.cmake,https://github.com/pytorch/pytorch/pull/145215,alinpahontu2912,malfet,ozanMSFT,,
2e80093306d,fx,Untopiced,setitem node shouldn't be deadcode eliminated (#145714),test/fx/test_dce_pass.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/145714,leslie-fang-intel,ezyang,,,
639dd54ef7e,skip,not user facing,[BE] Use copy_method to import all tests (#145718),test/inductor/test_mps_basic.py,https://github.com/pytorch/pytorch/pull/145718,malfet,dcci,,,
5d01a2874f9,releng,not user facing,Increase the number of perf benchmark shards (#145534),.github/workflows/inductor-perf-test-nightly.yml,https://github.com/pytorch/pytorch/pull/145534,huydhn,jeanschmidt,,,
7db20ffd68a,skip,not user facing,Remove `public_allowlist` from `TestPublicBindings.test_correct_module_names` and ensure private_allowlist-ed things are actually private (#145620),test/test_public_bindings.py,https://github.com/pytorch/pytorch/pull/145620,mikaylagawarecki,albanD,,,
30dea8429d9,mps,not user facing,[MPS][BE] Use conveinence methods to set args (#145736),aten/src/ATen/native/mps/operations/LinearAlgebra.mm,https://github.com/pytorch/pytorch/pull/145736,malfet,Skylion007,,,
abf28982a8c,skip,not user facing,[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/144441,eqy,Chillee,malfet,,
835e770bad6,dynamo,Untopiced,Use `typing.IO[bytes]` instead of `io.BytesIO` in annotations (#144994),test/allowlist_for_publicAPI.json torch/_C/__init__.pyi.in torch/_dynamo/replay_record.py torch/_inductor/__init__.py torch/_inductor/compile_worker/subproc_pool.py torch/_inductor/debug.py torch/_inductor/package/package.py torch/export/__init__.py torch/onnx/_internal/fx/serialization.py torch/package/package_exporter.py torch/package/package_importer.py torch/serialization.py torch/types.py torch/utils/show_pickle.py,https://github.com/pytorch/pytorch/pull/144994,randolf-scholz,Skylion007,ezyang,,
1835e1eb987,skip,not user facing,[BE] Remove test_ops from FIXME_inductor_dont_reset_dynamo (#145307),test/inductor_expected_failures/TestCommonCPU.test_out__refs_bitwise_not_cpu_int64 test/inductor_expected_failures/TestCommonCPU.test_out_linalg_matrix_rank_cpu_float32 test/inductor_expected_failures/TestCommonCPU.test_out_linalg_matrix_rank_hermitian_cpu_float32 test/inductor_expected_failures/TestCommonCUDA.test_out__refs_bitwise_not_cuda_int64 test/inductor_expected_failures/TestCommonCUDA.test_out_linalg_matrix_rank_cuda_float32 test/inductor_expected_failures/TestCommonCUDA.test_out_linalg_matrix_rank_hermitian_cuda_float32 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_add_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_addbmm_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_addcdiv_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_addcmul_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_addmm_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_addmm_decomposed_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_addmv_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_addr_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_baddbmm_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_div_no_rounding_mode_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_index_add_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_index_copy_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_index_fill_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_index_put_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_lerp_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_masked_fill_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_masked_scatter_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_mul_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_pow_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_put_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_scatter_add_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_scatter_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_sub_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_conj_view_true_divide_cpu_complex64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_add_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_addbmm_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_addcdiv_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_addcmul_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_addmm_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_addmm_decomposed_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_addmv_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_addr_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_baddbmm_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_div_no_rounding_mode_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_float_power_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_index_add_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_index_copy_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_lerp_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_masked_scatter_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_mul_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_pow_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_put_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_scatter_add_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_scatter_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_sub_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_conj_view_true_divide_cpu_complex128 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_abs_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_acos_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_acosh_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_add_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_addbmm_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_addcdiv_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_addcmul_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_addmm_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_addmm_decomposed_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_addmv_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_addr_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_asin_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_asinh_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_atan2_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_atan_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_atanh_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_baddbmm_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_ceil_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_clamp_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_clamp_max_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_clamp_min_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_conj_physical_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_copysign_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_cos_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_cosh_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_cumprod_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_cumsum_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_deg2rad_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_digamma_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_div_floor_rounding_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_div_no_rounding_mode_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_div_trunc_rounding_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_erf_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_erfc_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_erfinv_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_exp2_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_exp_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_expm1_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_fill_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_float_power_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_floor_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_floor_divide_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_fmod_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_frac_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_heaviside_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_hypot_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_i0_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_igamma_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_igammac_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_index_add_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_index_copy_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_index_fill_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_index_reduce_amax_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_index_reduce_amin_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_index_reduce_mean_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_index_reduce_prod_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_lerp_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_lgamma_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_log10_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_log1p_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_log2_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_log_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_logit_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_masked_fill_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_masked_scatter_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_mul_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_mvlgamma_mvlgamma_p_1_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_mvlgamma_mvlgamma_p_3_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_mvlgamma_mvlgamma_p_5_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nan_to_num_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_neg_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nextafter_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_alpha_dropout_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_celu_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_dropout2d_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_dropout3d_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_dropout_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_elu_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_feature_alpha_dropout_with_train_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_feature_alpha_dropout_without_train_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_hardsigmoid_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_leaky_relu_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_mish_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_rrelu_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_selu_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_silu_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_nn_functional_threshold_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_polygamma_polygamma_n_0_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_polygamma_polygamma_n_1_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_polygamma_polygamma_n_2_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_polygamma_polygamma_n_3_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_polygamma_polygamma_n_4_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_pow_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_put_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_rad2deg_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_reciprocal_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_remainder_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_renorm_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_resize_as__cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_round_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_round_decimals_0_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_rsqrt_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_scatter_add_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_scatter_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_scatter_reduce_amax_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_scatter_reduce_amin_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_scatter_reduce_mean_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_scatter_reduce_prod_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_scatter_reduce_sum_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_sgn_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_sigmoid_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_sign_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_sin_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_sinc_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_sinh_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_sqrt_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_square_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_squeeze_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_squeeze_multiple_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_sub_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_t_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_tan_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_tanh_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_transpose_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_tril_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_triu_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_true_divide_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_trunc_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_unsqueeze_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_xlogy_cpu_float64 test/inductor_expected_failures/TestMathBitsCPU.test_neg_view_zero__cpu_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_add_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_addbmm_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_addcdiv_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_addcmul_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_addmm_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_addmm_decomposed_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_addmv_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_addr_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_baddbmm_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_div_no_rounding_mode_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_index_add_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_index_copy_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_index_fill_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_index_put_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_lerp_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_masked_fill_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_masked_scatter_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_mul_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_pow_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_put_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_scatter_add_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_scatter_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_sub_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_conj_view_true_divide_cuda_complex64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_add_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_addbmm_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_addcdiv_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_addcmul_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_addmm_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_addmm_decomposed_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_addmv_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_addr_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_baddbmm_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_div_no_rounding_mode_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_float_power_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_index_add_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_index_copy_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_lerp_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_masked_scatter_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_mul_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_pow_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_put_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_scatter_add_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_scatter_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_sub_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_conj_view_true_divide_cuda_complex128 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_abs_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_acos_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_acosh_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_add_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_addbmm_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_addcdiv_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_addcmul_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_addmm_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_addmm_decomposed_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_addmv_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_addr_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_asin_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_asinh_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_atan2_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_atan_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_atanh_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_baddbmm_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_ceil_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_clamp_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_clamp_max_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_clamp_min_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_conj_physical_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_copysign_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_cos_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_cosh_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_cumprod_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_cumsum_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_deg2rad_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_digamma_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_div_floor_rounding_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_div_no_rounding_mode_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_div_trunc_rounding_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_erf_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_erfc_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_erfinv_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_exp2_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_exp_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_expm1_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_fill_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_float_power_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_floor_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_floor_divide_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_fmod_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_frac_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_heaviside_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_hypot_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_i0_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_igamma_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_igammac_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_index_add_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_index_copy_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_index_fill_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_index_reduce_amax_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_index_reduce_amin_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_index_reduce_mean_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_index_reduce_prod_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_lerp_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_lgamma_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_log10_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_log1p_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_log2_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_log_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_logit_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_masked_fill_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_masked_scatter_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_mul_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_mvlgamma_mvlgamma_p_1_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_mvlgamma_mvlgamma_p_3_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_mvlgamma_mvlgamma_p_5_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nan_to_num_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_neg_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nextafter_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_alpha_dropout_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_celu_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_dropout2d_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_dropout3d_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_elu_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_feature_alpha_dropout_with_train_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_feature_alpha_dropout_without_train_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_hardsigmoid_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_leaky_relu_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_mish_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_rrelu_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_selu_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_silu_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_nn_functional_threshold_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_polygamma_polygamma_n_0_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_polygamma_polygamma_n_1_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_polygamma_polygamma_n_2_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_polygamma_polygamma_n_3_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_polygamma_polygamma_n_4_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_pow_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_put_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_rad2deg_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_reciprocal_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_remainder_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_renorm_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_resize_as__cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_round_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_round_decimals_0_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_rsqrt_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_scatter_add_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_scatter_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_scatter_reduce_amax_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_scatter_reduce_amin_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_scatter_reduce_mean_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_scatter_reduce_prod_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_scatter_reduce_sum_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_sgn_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_sigmoid_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_sign_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_sin_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_sinc_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_sinh_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_sqrt_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_square_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_squeeze_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_squeeze_multiple_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_sub_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_t_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_tan_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_tanh_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_transpose_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_tril_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_triu_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_true_divide_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_trunc_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_unsqueeze_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_xlogy_cuda_float64 test/inductor_expected_failures/TestMathBitsCUDA.test_neg_view_zero__cuda_float64 test/inductor_skips/TestMathBitsCUDA.test_neg_view_jiterator_2inputs_2outputs_cuda_float64 test/inductor_skips/TestMathBitsCUDA.test_neg_view_jiterator_4inputs_with_extra_args_cuda_float64 test/inductor_skips/TestMathBitsCUDA.test_neg_view_jiterator_binary_cuda_float64 test/inductor_skips/TestMathBitsCUDA.test_neg_view_jiterator_binary_return_by_ref_cuda_float64 test/inductor_skips/TestMathBitsCUDA.test_neg_view_jiterator_unary_cuda_float64 torch/testing/_internal/common_utils.py torch/testing/_internal/dynamo_test_failures.py,https://github.com/pytorch/pytorch/pull/145307,masnesral,FindHao,zou3519,,
f951d216e0e,skip,not user facing,[autocast][pytorch] Support autocast for MTIA (policy) (#145666),aten/src/ATen/autocast_mode.cpp aten/src/ATen/autocast_mode.h,https://github.com/pytorch/pytorch/pull/145666,nautsimon,chaos5958,,,
ec91b7720fc,skip,not user facing,[Custom Ops] Add a new API to allow users to register an autocast for the custom op (#145588),docs/source/library.rst test/test_custom_ops.py torch/_library/custom_ops.py torch/library.py,https://github.com/pytorch/pytorch/pull/145588,yanboliang,zou3519,,,
bc377c503e9,skip,not user facing,[Custom Ops] Fix f-strings in custom ops error message (#145673),torch/library.py,https://github.com/pytorch/pytorch/pull/145673,yanboliang,zou3519,,,
635b98fa087,releng,not user facing,Add nitpick warning that aoti_torch/c/shim.h is ABI stable (#145745),.github/nitpicks.yml,https://github.com/pytorch/pytorch/pull/145745,ezyang,albanD,,,
9728e900dcd,inductor,not user facing,[Inductor][CPP] fix torch logit decomposition (#145576),test/inductor/test_cpu_repro.py torch/_refs/special/__init__.py,https://github.com/pytorch/pytorch/pull/145576,leslie-fang-intel,eellison,jgong5,,
c986eba560b,skip,Untopiced,"Revert ""[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441)""",aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,,,,,,
66631bc84b4,dynamo,not user facing,[dynamo] Fix read/write conflicts in a cuda test (#145658),test/dynamo/test_ctx_manager.py,https://github.com/pytorch/pytorch/pull/145658,StrongerXi,yifuwang,,,
2f8ad8f4b90,releng,Untopiced,Run inductor perf benchmark on ROCm (#145763),.ci/pytorch/test.sh .github/workflows/_rocm-test.yml .github/workflows/inductor-perf-test-nightly-rocm.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/145763,huydhn,jeffdaily,,,
64cd81712dd,python_frontend,Untopiced,`torch.distributions`: replace `numbers.Number` with `torch.types.Number`. (#145086),torch/distributions/bernoulli.py torch/distributions/beta.py torch/distributions/cauchy.py torch/distributions/constraint_registry.py torch/distributions/continuous_bernoulli.py torch/distributions/exponential.py torch/distributions/fishersnedecor.py torch/distributions/gamma.py torch/distributions/geometric.py torch/distributions/gumbel.py torch/distributions/laplace.py torch/distributions/normal.py torch/distributions/poisson.py torch/distributions/relaxed_bernoulli.py torch/distributions/transforms.py torch/distributions/uniform.py torch/distributions/utils.py torch/distributions/wishart.py torch/types.py,https://github.com/pytorch/pytorch/pull/145086,randolf-scholz,malfet,,,
93dd6bc4d84,releng,not user facing,Add CUDA 12.8 installation and manylinux-cuda12.8 (#145567),.ci/docker/common/install_cuda.sh .ci/docker/common/install_cuda_aarch64.sh .ci/docker/common/install_magma.sh .github/workflows/build-manywheel-images.yml,https://github.com/pytorch/pytorch/pull/145567,tinglvv,atalman,nWEIdia,,
bfaf76bfc67,dynamo,not user facing,[dynamo] clear out traced frames at the start of `test_log_traced_frames` (#145640),test/dynamo/test_logging.py test/dynamo/test_repros.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/145640,StrongerXi,williamwen42,,,
60f98262f17,releng,not user facing,PEP585: .github (#145707),.github/scripts/cherry_pick.py .github/scripts/close_nonexistent_disable_issues.py .github/scripts/collect_ciflow_labels.py .github/scripts/delete_old_branches.py .github/scripts/file_io_utils.py .github/scripts/filter_test_configs.py .github/scripts/generate_binary_build_matrix.py .github/scripts/generate_ci_workflows.py .github/scripts/get_workflow_job_id.py .github/scripts/github_utils.py .github/scripts/gitutils.py .github/scripts/label_utils.py .github/scripts/pytest_caching_utils.py .github/scripts/runner_determinator.py .github/scripts/tag_docker_images_for_release.py .github/scripts/test_check_labels.py .github/scripts/test_filter_test_configs.py .github/scripts/test_trymerge.py .github/scripts/trymerge.py .github/scripts/trymerge_explainer.py .github/scripts/tryrebase.py .github/workflows/_runner-determinator.yml,https://github.com/pytorch/pytorch/pull/145707,aorenste,huydhn,,,
3a23d75b37b,mps,not user facing,[MPS] Fix `c0::metal::log_gamma` correctness on M4 (#145740),c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/145740,malfet,dcci,,,
1ba1b7b597d,nested tensor_frontend,improvements,Support remaining *_like factory functions for NJT (#144889),test/test_nestedtensor.py torch/nested/_internal/ops.py,https://github.com/pytorch/pytorch/pull/144889,jbschlosser,soulitzer,,,
7e1c7253e9d,dynamo,not user facing,[dynamo][builtin-skipfile-cleanup] Support tuple.__new__ (#145558),test/dynamo/test_functions.py torch/_dynamo/side_effects.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/145558,anijain2305,StrongerXi,jansel,,
993b2296657,dynamo,not user facing,[dynamo][dicts] Fix dict.__new__ bug (#145723),test/dynamo/test_dicts.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/145723,anijain2305,StrongerXi,jansel,,
69e82d02d37,inductor,bug fixes,"[inductor][3/N] triton support post-#5512, tt.divisibility format (#145575)",test/inductor/test_codegen_triton.py torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/145575,davidberard98,SamGinzburg,,,
006397fac3d,releng,not user facing,Remove FBGEMM sccache hack (#145664),.ci/pytorch/common_utils.sh,https://github.com/pytorch/pytorch/pull/145664,huydhn,wdvr,,,
2de53b3b659,skip,Untopiced,"Revert ""pickler for GraphModule (#141659)""",test/distributed/_composable/fsdp/test_fully_shard_compile.py test/fx/test_graph_pickler.py torch/_dynamo/variables/misc.py torch/_inductor/compile_fx.py torch/_inductor/graph.py torch/_inductor/output_code.py torch/_inductor/utils.py torch/_subclasses/fake_tensor.py torch/_subclasses/meta_utils.py torch/fx/_graph_pickler.py torch/fx/node.py,,,,,,
5534c270db7,distributed,Untopiced,[chore] fix new linter (#145756),torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp,https://github.com/pytorch/pytorch/pull/145756,c-p-i-o,Skylion007,XilunWu,fduwjj,
3ce68dc61ef,distributed,Untopiced,[c10d] Flush file in file recorder (#145458),torch/csrc/distributed/c10d/FlightRecorder.cpp,https://github.com/pytorch/pytorch/pull/145458,c-p-i-o,kwen2501,,,
7d01f6e6f25,skip,not user facing,Add ignorable commits on run_test.py to git blame ignore (#145787),.git-blame-ignore-revs,https://github.com/pytorch/pytorch/pull/145787,janeyx99,malfet,,,
c1161957a41,dynamo,not user facing,inductor_config_logging: Don't drop keys (#144700),test/dynamo/test_utils.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/144700,c00w,ezyang,,,
7db0afabaaf,skip,not user facing,Remove lexicographical sorting of storage keys in torch.save (#143879),test/test_serialization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/143879,mikaylagawarecki,albanD,,,
db3685a35cd,python_frontend,improvements,Add option to serialization config to reduce random reads from get_record_offset when loading with mmap=True (#143880),.ci/pytorch/macos-test.sh .ci/pytorch/test.sh .ci/pytorch/win-test.sh caffe2/serialize/inline_container.cc caffe2/serialize/inline_container.h docs/source/notes/serialization.rst test/test_serialization.py torch/csrc/jit/python/init.cpp torch/serialization.py torch/utils/serialization/config.py,https://github.com/pytorch/pytorch/pull/143880,mikaylagawarecki,albanD,,,
5a4d959cdb1,dynamo,not user facing,[dynamo] Properly model torch profiler context objects (#145537),test/dynamo/test_ctx_manager.py test/dynamo_expected_failures/TestAutograd.test_profiler test/dynamo_expected_failures/TestAutograd.test_profiler_aggregation_table test/dynamo_expected_failures/TestAutograd.test_profiler_propagation test/dynamo_expected_failures/TestAutograd.test_profiler_seq_nr test/dynamo_expected_failures/TestAutograd.test_profiler_shapes test/dynamo_expected_failures/TestAutograd.test_record_function_callbacks test/dynamo_expected_failures/TestAutograd.test_record_function_legacy test/dynamo_expected_failures/TestExperimentalUtils.test_profiler_name_pattern test/dynamo_expected_failures/TestExperimentalUtils.test_profiler_pattern_match_helper test/dynamo_expected_failures/TestExperimentalUtils.test_utils_compute_queue_depth_when_no_cuda_events test/dynamo_expected_failures/TestExperimentalUtils.test_utils_compute_self_time test/dynamo_expected_failures/TestProfiler.test_concrete_inputs_profiling test/dynamo_expected_failures/TestProfiler.test_export_stacks test/dynamo_expected_failures/TestProfiler.test_guarded_record_function_fast test/dynamo_expected_failures/TestProfiler.test_profiler_fwd_bwd_link test/dynamo_expected_failures/TestProfiler.test_profiler_op_event_args test/dynamo_expected_failures/TestProfiler.test_profiler_strides test/dynamo_expected_failures/TestProfiler.test_profiler_tracing test/dynamo_expected_failures/TestRecordFunction.test_datapipe_with_record_function test/dynamo_expected_failures/TestRecordFunction.test_datapipe_with_record_function_fork test/dynamo_expected_failures/TestRecordFunction.test_record_function test/dynamo_expected_failures/TestTEFuserDynamic.test_profiler test/dynamo_expected_failures/TestTEFuserStatic.test_profiler test/dynamo_expected_failures/TestTorchTidyProfiler.test_allocation_id_uniqueness test/dynamo_expected_failures/TestTorchTidyProfiler.test_allocation_ids test/dynamo_expected_failures/TestTorchTidyProfiler.test_allocation_ids_with_other_ops test/dynamo_expected_failures/TestTorchTidyProfiler.test_allocations test/dynamo_expected_failures/TestTorchTidyProfiler.test_extra_fields test/dynamo_expected_failures/TestTorchTidyProfiler.test_impl_reuse test/dynamo_expected_failures/TestTorchTidyProfiler.test_mkldnn_tensors test/dynamo_expected_failures/TestTorchTidyProfiler.test_pointers_and_ids test/dynamo_expected_failures/TestTorchTidyProfiler.test_scalar_ins test/dynamo_expected_failures/TestTorchTidyProfiler.test_tensor_lists test/dynamo_expected_failures/TestTorchTidyProfiler.test_tensor_properties torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/145537,StrongerXi,williamwen42,zou3519,,
6eb74fbec69,distributed,not user facing,Updates NCCL user buffer registration test for NCCL 2.24.3 (#145285),test/distributed/test_c10d_nccl.py,https://github.com/pytorch/pytorch/pull/145285,syed-ahmed,kwen2501,,,
f16ce3c7e96,inductor,improvements,Refactor fuzzer and add support for Dynamo (#145565),test/inductor/test_fuzzer.py torch/_inductor/fuzzer.py,https://github.com/pytorch/pytorch/pull/145565,exclamaforte,masnesral,,,
e3d3f2b22e4,skip,not user facing,[dynamo] save/restore system random state more carefully (#145750),test/dynamo/test_unspec.py torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/output_graph.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/utils.cpp torch/csrc/dynamo/utils.h,https://github.com/pytorch/pytorch/pull/145750,williamwen42,StrongerXi,,,
db33d23aa8d,distributed,Untopiced,[SymmetricMemory] fix an issue where rendezvous is performed with wrong device context when torch.cuda.set_device() is not callled (#144886),test/distributed/test_symmetric_memory.py torch/csrc/distributed/c10d/CUDASymmetricMemory.cu,https://github.com/pytorch/pytorch/pull/144886,yifuwang,awgu,,,
2df2f9d8956,inductor,not user facing,[inductor] Change type of get_backend_features to OrderedSet (#145692),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/triton.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/145692,jansel,yanboliang,,,
78a94c91143,inductor,not user facing,[inductor] Remove type ignores from scheduler.py (#145712),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/memory.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/145712,jansel,Skylion007,yanboliang,,
7c1fc0a047a,skip,not user facing,Log cache state for AOTAutograd in title of file (#145715),test/dynamo/test_structured_trace.py torch/_functorch/_aot_autograd/autograd_cache.py,https://github.com/pytorch/pytorch/pull/145715,jamesjwu,bobrenjc93,,,
4be831ba2d4,export,Untopiced,[draft_export] fix dense-in-memory check for inferring fakes (#145653),test/export/test_draft_export.py torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/145653,pianpwk,zou3519,,,
78f02bf07ce,distributed,Untopiced,[bug] handle case when remote peer closes connection (#145757),torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp,https://github.com/pytorch/pytorch/pull/145757,c-p-i-o,XilunWu,fduwjj,,
90106492928,skip,Untopiced,"Revert ""Add option to serialization config to reduce random reads from get_record_offset when loading with mmap=True (#143880)""",.ci/pytorch/macos-test.sh .ci/pytorch/test.sh .ci/pytorch/win-test.sh caffe2/serialize/inline_container.cc caffe2/serialize/inline_container.h docs/source/notes/serialization.rst test/test_serialization.py torch/csrc/jit/python/init.cpp torch/serialization.py torch/utils/serialization/config.py,,,,,,
1a26cdd5cb0,skip,not user facing,[cond] remove warning for unsupported tuple returns (#145766),torch/_higher_order_ops/cond.py,https://github.com/pytorch/pytorch/pull/145766,pianpwk,ydwu4,zou3519,,
01a4d86b313,dynamo,not user facing,add pt2 callbacks for backward pass and prevent duplicate callbacks (#145732),test/dynamo/test_callback.py test/run_test.py torch/_dynamo/callback.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/145732,burak-turk,mlazos,,,
67fcc7cf02b,skip,not user facing,[3/N] Remove unnecessary once flag usage (#145672),c10/core/impl/GPUTrace.cpp c10/cuda/CUDACachingAllocator.cpp c10/util/Logging.cpp,https://github.com/pytorch/pytorch/pull/145672,cyyever,albanD,,,
8e46d0f5951,skip,not user facing,[BE]: Update typing of OrderedSet ancestor (#145783),torch/utils/_ordered_set.py,https://github.com/pytorch/pytorch/pull/145783,Skylion007,eellison,,,
23eb0a32016,skip,not user facing,Improve typing in torch/types.py (#145237),test/allowlist_for_publicAPI.json torch/_C/_cudnn.pyi torch/types.py,https://github.com/pytorch/pytorch/pull/145237,cyyever,XuehaiPan,albanD,,
0f5a68344aa,inductor,not user facing,[BE][Inductor] Simplify `custom_op` tests (#145814),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/145814,malfet,jansel,,,
a699034eeca,skip,Untopiced,"Record inputs at time of tracing, constrain to them for triton fn (#145448)",test/inductor/test_triton_kernels.py torch/_inductor/graph.py torch/_inductor/lowering.py torch/fx/experimental/proxy_tensor.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/145448,eellison,zou3519,,,
0a8a0ef7673,inductor,not user facing,[inductor] Fix crash running wrapper_benchmark with no device (#145644),torch/_inductor/wrapper_benchmark.py,https://github.com/pytorch/pytorch/pull/145644,mgraczyk,shunting314,,,
45f64e770aa,export,Untopiced,relax assertion to warning for unbacked binding names (#145777),torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/145777,avikchaudhuri,angelayi,,,
5c5306e8bc1,dynamo,not user facing,[dynamo][builtin-skiplist-cleanup] Remove weakref (#145744),test/dynamo/test_dicts.py test/fx/test_source_matcher_utils.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/145744,anijain2305,jansel,,,
bdf6dfa17d4,distributed,Untopiced,[chore][ez] change alloc buffer size from 4000 to 4096 (#145759),torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp,https://github.com/pytorch/pytorch/pull/145759,c-p-i-o,XilunWu,fduwjj,,
a08f7f32663,skip,not user facing,OpenReg: fix issue of pin_memory (#145046),test/cpp_extensions/open_registration_extension/pytorch_openreg/_device_daemon.py test/cpp_extensions/open_registration_extension/test/test_openreg.py,https://github.com/pytorch/pytorch/pull/145046,Zhenbin-8,albanD,,,
97b3b73f3e9,skip,Untopiced,[Environment Variable][7/N] Use thread-safe getenv functions (#140211),aten/src/ATen/core/Vitals.cpp aten/src/ATen/cuda/CublasHandlePool.cpp aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/Tunable.h aten/src/ATen/mps/MPSAllocator.mm aten/src/ATen/mps/MPSFallback.mm aten/src/ATen/mps/MPSProfiler.mm aten/src/ATen/native/cudnn/Conv_v8.cpp aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm torch/csrc/autograd/engine.cpp torch/csrc/cuda/nccl.cpp torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/init.cpp torch/csrc/jit/codegen/fuser/compiler.cpp torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp,https://github.com/pytorch/pytorch/pull/140211,cyyever,eqy,ezyang,,
3a560892176,inductor,not user facing,fix unbacked + view incorrectness (#145548),test/inductor/test_unbacked_symints.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/145548,eellison,desertfire,,,
4307e6c008e,dynamo,not user facing,[dynamo][builtin-skipfile-cleanup] Remove signal (#145753),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/145753,anijain2305,zou3519,,,
6824a4a75d8,dynamo,not user facing,[dynamo][builtin-skipfiles-cleanup] Remove re (#145826),torch/_dynamo/trace_rules.py torch/_dynamo/variables/constant.py,https://github.com/pytorch/pytorch/pull/145826,anijain2305,zou3519,,,
80a0412b76d,dynamo,not user facing,[dynamo][builtin-skipfiles-cleanup] Remove posixpath (#145828),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/145828,anijain2305,zou3519,,,
6967ef1b074,skip,not user facing,[ROCm] fix test_cublas_workspace_explicit_allocation for gfx12 (#145227),aten/src/ATen/cuda/CublasHandlePool.cpp test/test_cuda.py,https://github.com/pytorch/pytorch/pull/145227,dnikolaev-amd,eqy,jeffdaily,,
ac87388e61e,skip,not user facing,[AOTInductor] Refactor CPU and GPU to remove ifdef macros (#145639),torch/csrc/inductor/aoti_runtime/model.h torch/csrc/inductor/aoti_runtime/model_container.h,https://github.com/pytorch/pytorch/pull/145639,muchulee8,desertfire,,,
c751541e79f,quantization,Untopiced,Fix cppcoreguidelines-init-variables ignorance (#141795),aten/src/ATen/native/AveragePool3d.cpp aten/src/ATen/native/BatchLinearAlgebra.cpp aten/src/ATen/native/BatchLinearAlgebraKernel.cpp aten/src/ATen/native/Copy.cpp aten/src/ATen/native/EmbeddingBag.cpp aten/src/ATen/native/FractionalMaxPool2d.cpp aten/src/ATen/native/FractionalMaxPool3d.cpp aten/src/ATen/native/GridSampler.cpp aten/src/ATen/native/LossCTC.cpp aten/src/ATen/native/LossMultiLabelMargin.cpp aten/src/ATen/native/LossMultiMargin.cpp aten/src/ATen/native/NaiveConvolutionTranspose3d.cpp aten/src/ATen/native/QuantizedLinear.cpp aten/src/ATen/native/SobolEngineOps.cpp aten/src/ATen/native/Sorting.cpp aten/src/ATen/native/SparseTensorUtils.cpp aten/src/ATen/native/TensorShape.cpp aten/src/ATen/native/TriangularOps.cpp aten/src/ATen/native/cpu/SoftMaxKernel.cpp aten/src/ATen/native/cpu/Unfold2d.cpp aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp aten/src/ATen/native/cuda/jit_utils.cpp aten/src/ATen/native/mps/operations/Distributions.mm aten/src/ATen/native/quantized/cpu/AdaptiveAveragePooling.cpp aten/src/ATen/native/quantized/cpu/AveragePool2d.cpp aten/src/ATen/native/quantized/cpu/Pooling.cpp aten/src/ATen/native/quantized/cpu/qconv.cpp aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp aten/src/ATen/native/quantized/cpu/qembeddingbag_prepack.cpp aten/src/ATen/native/quantized/cpu/qembeddingbag_unpack.cpp aten/src/ATen/native/quantized/cpu/qlinear_dynamic.cpp aten/src/ATen/native/quantized/cpu/qmul.cpp aten/src/ATen/native/xnnpack/Convolution.cpp aten/src/ATen/nnapi/nnapi_bind.cpp aten/src/ATen/nnapi/nnapi_model_loader.cpp aten/src/ATen/quantized/Quantizer.cpp c10/core/SymbolicShapeMeta.cpp c10/core/impl/SizesAndStrides.h c10/test/util/bfloat16_test.cpp,https://github.com/pytorch/pytorch/pull/141795,cyyever,albanD,,,
7eb51e5464c,releng,not user facing,Ensure GPU isolation for kubernetes pod MI300 runners. (#145829),.github/actions/setup-rocm/action.yml,https://github.com/pytorch/pytorch/pull/145829,saienduri,jeffdaily,,,
097ccd9c390,releng,not user facing,Move ROCm MI300 jobs to unstable to make CI green (#145790),.github/workflows/inductor-rocm-mi300.yml .github/workflows/rocm-mi300.yml .github/workflows/unstable.yml,https://github.com/pytorch/pytorch/pull/145790,ZainRizvi,yangw-dev,,,
dbef2a9bc93,skip,Untopiced,"Revert ""Remove lexicographical sorting of storage keys in torch.save (#143879)""",test/test_serialization.py torch/serialization.py,,,,,,
cfbb27462eb,skip,Untopiced,"Revert ""[inductor][BE] Enable test_cpu_cpp_wrapper in fbcode (#145373)""",torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/cpp_builder.py torch/testing/_internal/inductor_utils.py,,,,,,
56915b093a2,releng,not user facing,Fix environment deployment spam (#145823),.github/workflows/_rocm-test.yml,https://github.com/pytorch/pytorch/pull/145823,huydhn,clee2000,,,
5382ab57d7d,releng,not user facing,Move trunk windows builds to CUDA-12.4 (#145844),.github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/145844,atalman,janeyx99,malfet,,
3fd46919080,mps,not user facing,[MPS] Add `op_math_t` (#145808),c10/metal/utils.h,https://github.com/pytorch/pytorch/pull/145808,malfet,dcci,,,
8e258e2ecd6,inductor,not user facing,Parallelize epilogue/prologue benchmarking (#143408),test/inductor/test_codecache.py test/inductor/test_max_autotune.py torch/_inductor/async_compile.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/triton.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/143408,eellison,jansel,shunting314,,
eaec97ab1fb,dynamo,not user facing,[dynamo] Properly prune dead input cell object (#145781),test/dynamo/test_higher_order_ops.py test/dynamo/test_misc.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/145781,StrongerXi,jansel,williamwen42,,
621604ce464,inductor,not user facing,Maintain multiple configs (#145103),test/inductor/test_max_autotune.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/145103,eellison,jansel,shunting314,,
c32bafeb0be,skip,not user facing,[ROCm] Bump AOTriton to 0.8.2b (#145508),cmake/External/aotriton.cmake,https://github.com/pytorch/pytorch/pull/145508,xinyazhang,jeffdaily,,,
cbc40942984,export,Untopiced,[draft_export] add LOC for data-dep error logging (#145443),torch/export/_draft_export.py,https://github.com/pytorch/pytorch/pull/145443,pianpwk,angelayi,,,
6c09954a9e0,releng,not user facing,Windows builds with VS2022 (#145319),.circleci/scripts/binary_windows_build.sh,https://github.com/pytorch/pytorch/pull/145319,Camyll,huydhn,,,
d9ffa5da65f,skip,not user facing,Log info for AOTAutogradCache bypasses instead of warning (#145768),torch/_functorch/_aot_autograd/autograd_cache.py,https://github.com/pytorch/pytorch/pull/145768,jamesjwu,bdhirsh,eellison,,
eaff13275e4,dynamo,not user facing,[dynamo] Properly branch on an unspecialized NN module (#145786),test/dynamo/test_modules.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/145786,StrongerXi,williamwen42,,,
8d91bfd9654,skip,not user facing,[BE] Include CheckFunctionExists in `FindBLAS.cmake` (#145849),cmake/Modules/FindBLAS.cmake,https://github.com/pytorch/pytorch/pull/145849,malfet,Skylion007,,,
515e55e6927,python_frontend,Untopiced,Set -DPy_LIMITED_API flag for py_limited_api=True extensions (#145764),test/cpp_extensions/python_agnostic_extension/python_agnostic/csrc/ultra_norm.cu torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/145764,janeyx99,albanD,ezyang,zou3519,
edf266e9bbb,inductor,deprecation,inductor.config.descriptive_names = False is not actually supported (#145523),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/145523,exclamaforte,eellison,,,
28982ceb3b1,skip,not user facing,[aarch64] Rebuild everything with ArmPL (#145742),defs.bzl,https://github.com/pytorch/pytorch/pull/145742,psaab,malfet,,,
3481c2aec4c,skip,Untopiced,"Revert ""[dynamo] save/restore system random state more carefully (#145750)""",test/dynamo/test_unspec.py torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/output_graph.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/utils.cpp torch/csrc/dynamo/utils.h,,,,,,
ef28df5c9e3,distributed,Untopiced,[Reland][Environment Variable][4/N] Use thread-safe getenv functions (#140593),aten/src/ATen/cuda/tunable/TunableGemm.h torch/csrc/distributed/c10d/GlooDeviceFactory.cpp torch/csrc/distributed/c10d/Utils.hpp torch/csrc/lazy/core/ir.cpp torch/csrc/lazy/ts_backend/ts_backend_impl.cpp torch/csrc/lazy/ts_backend/ts_node.cpp torch/csrc/utils/cpp_stacktraces.cpp,https://github.com/pytorch/pytorch/pull/140593,cyyever,albanD,,,
ccc2878c978,inductor,Untopiced,Fix fractional_max_pool lowering in inductor (#144395),torch/_inductor/lowering.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/144395,isuruf,amjames,eellison,,
6bcb545d9c3,releng,not user facing,[CI][CUDA][cuSPARSELt] cusparselt 0.6.3 and cu121 related cleanups (#145793),.ci/docker/build.sh .ci/docker/common/install_cuda.sh .ci/docker/common/install_cuda_aarch64.sh .ci/docker/common/install_cusparselt.sh .github/scripts/test_filter_test_configs.py .github/scripts/test_trymerge.py .github/workflows/docker-builds.yml .github/workflows/target-determination-indexer.yml .github/workflows/torchbench.yml .github/workflows/trunk.yml test/test_sparse_semi_structured.py,https://github.com/pytorch/pytorch/pull/145793,nWEIdia,eqy,tinglvv,,
4f949f282dc,distributed,Untopiced,[c10d][ez] Remove goto in PGNCCL and make linter happy for PGNCCL and NCCLUtils (#145855),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/145855,fduwjj,c-p-i-o,kwen2501,,
9003d81144f,xpu,docs,change the test wheel to release wheel when release wheel available (#145252),docs/source/notes/get_start_xpu.rst,https://github.com/pytorch/pytorch/pull/145252,ZhaoqiongZ,atalman,seemethere,,
a06a18b1bb3,skip,not user facing,[ATen] Implement exception handling for hipsolver APIs (#145839),aten/src/ATen/cuda/Exceptions.cpp aten/src/ATen/cuda/Exceptions.h,https://github.com/pytorch/pytorch/pull/145839,danzimm,Mellonta,,,
1ffed44b420,inductor,not user facing,[aotinductor] update unbacked symint runtime assertion msg (#145569),test/inductor/test_aot_inductor.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/145569,ColinPeppler,chenyang78,,,
5aa5a5763e5,nn_frontend,Untopiced,[inductor triton] Disable incorrect TF32 usage on CUDA capability < 8 (#145684),docs/source/cuda.rst test/nn/test_convolution.py test/test_nn.py test/test_torch.py test/xpu/test_conv.py torch/_inductor/select_algorithm.py torch/cuda/__init__.py torch/testing/_internal/common_cuda.py,https://github.com/pytorch/pytorch/pull/145684,benjaminglass1,eqy,,,
af43b445a5b,onnx,bc breaking,[ONNX] Set USE_EXPERIMENTAL_LOGIC to True (#137296),test/onnx/dynamo/test_exporter_api.py test/onnx/dynamo/test_registry_dispatcher.py test/onnx/exporter/test_small_models_e2e.py test/onnx/test_fx_passes.py test/onnx/test_fx_to_onnx.py test/onnx/test_fx_to_onnx_decomp_skip.py torch/onnx/_flags.py,https://github.com/pytorch/pytorch/pull/137296,justinchuby,titaiwangms,,,
f388ba5986f,cudnn,not user facing,Update CUDNN frontend submodule to 1.10.0 (#145780),third_party/cudnn_frontend,https://github.com/pytorch/pytorch/pull/145780,Skylion007,albanD,atalman,malfet,
c26bb9ba5bd,build_frontend,improvements,[CMake] Find HomeBrew OpenMP on MacOS (#145870),cmake/Modules/FindOpenMP.cmake,https://github.com/pytorch/pytorch/pull/145870,malfet,Skylion007,atalman,,
eea7d395e5f,skip,not user facing,[CD] Install ninja and setuptools from PyPI (#145871),.ci/wheel/build_wheel.sh,https://github.com/pytorch/pytorch/pull/145871,malfet,Skylion007,,,
049f042e520,releng,Untopiced,Update build_wheel.sh,.ci/wheel/build_wheel.sh,,,,,,
02dd7a7803f,releng,not user facing,Extend abi-stable nitpick message to all the c stable files (#145862),.github/nitpicks.yml,https://github.com/pytorch/pytorch/pull/145862,albanD,ezyang,,,
7ca156f0ee5,composability,Untopiced,partitioner: avoid inserting duplicates into heap (#145082),benchmarks/dynamo/pr_time_benchmarks/benchmarks/aotdispatcher_partitioner2.py benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/145082,bdhirsh,xmfan,,,
82859f61857,dynamo,not user facing,[associative_scan] scan dim handling in user-facing associative_scan() (#139864),torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/associative_scan.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/139864,bohnstingl,ydwu4,,,
2f24f2eb461,skip,not user facing,Make sure to evaluate annotation strings in the context of where the prototype was created (#145667),test/test_custom_ops.py torch/_library/infer_schema.py,https://github.com/pytorch/pytorch/pull/145667,aorenste,zou3519,,,
9036a22c835,inductor,not user facing,[Inductor][Triton] Change propagated dtype for fp16/bf16 unwrapped 0d tensors (#145613),torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/145613,kundaMwiza,davidberard98,eellison,jansel,
ae0f305bf92,inductor,not user facing,[inductor] Make triton kernel autotune config defaults backward-compatible (#145494),torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/145494,bertmaher,aorenste,,,
a24b25942a3,skip,not user facing,Fix RMSNorm epsilon value type for BF16 or FP16 (#142848),aten/src/ATen/native/layer_norm.cpp,https://github.com/pytorch/pytorch/pull/142848,fmo-mt,albanD,,,
15e37e4253b,export,Untopiced,[export] don't always print GM in serdes logging (#145857),torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/145857,pianpwk,angelayi,yiming0416,,
6f5c8fb128b,distributed,not user facing,[DTensor] Add pointwise ops strategy for `aten.minimum` (#145816),test/distributed/tensor/test_dtensor_ops.py torch/distributed/tensor/_ops/_pointwise_ops.py,https://github.com/pytorch/pytorch/pull/145816,wz337,XilunWu,,,
331f49057dd,cuda,Untopiced,Removes threadfence from topk kernel to improve AMD performance (#145536),aten/src/ATen/native/cuda/TensorTopK.cu,https://github.com/pytorch/pytorch/pull/145536,ngimel,eqy,,,
29521256e10,inductor,Untopiced,[Customized Optimus][Inductor] Add split cat pattern in aten level (#145721),test/inductor/test_split_cat_fx_aten_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/145721,mengluy0125,Yuzhen11,,,
9fd6722fc90,skip,Untopiced,[c10d] Add NCCL memory allocator (#145675),test/distributed/test_c10d_nccl.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/145675,kwen2501,syed-ahmed,wconstab,,
9330b6d0987,skip,not user facing,"Added swizzle searching, disabled fp16 accum, and enabled ping-pong for cutlass (#144829)",torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/144829,masnesral,Chillee,,,
f4ca98950ed,releng,not user facing,Add CUDA 12.8 libtorch image (#145789),.ci/docker/libtorch/Dockerfile .github/workflows/build-libtorch-images.yml,https://github.com/pytorch/pytorch/pull/145789,tinglvv,atalman,nWEIdia,,
50f834f1341,export,Untopiced,[export] allow bit shift builtin ops (#145802),test/export/test_export.py torch/_export/verifier.py,https://github.com/pytorch/pytorch/pull/145802,ColinPeppler,pianpwk,,,
9be2e88d41d,inductor,not user facing,Fix lowering to inductor IR for triton CPU (#144389),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/144389,kundaMwiza,jansel,,,
90a6db4a9ca,skip,not user facing,[be][pytorch] Fix backend in autocast (#145859),aten/src/ATen/autocast_mode.h,https://github.com/pytorch/pytorch/pull/145859,nautsimon,jvandebon,,,
fd515e4f59b,skip,not user facing,Fix C++20 Wambiguous-reversed-operator warnings (#144126),aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h,https://github.com/pytorch/pytorch/pull/144126,cyyever,albanD,,,
776bdb962c7,onnx,bug fixes,[ONNX] Support subgraphs with 1+ outputs (#145860),test/onnx/exporter/test_small_models_e2e.py torch/onnx/_internal/exporter/_core.py,https://github.com/pytorch/pytorch/pull/145860,justinchuby,titaiwangms,,,
8696e59ae2b,skip,not user facing,add test for capture_dynamic_output_shape_ops=True changing expected output between eager and compiled versions (#145821),test/dynamo/test_misc.py,https://github.com/pytorch/pytorch/pull/145821,bobrenjc93,eellison,ezyang,,
7178b827d7f,distributed,Untopiced,PEP585: Missed conversions (#145342),torch/_jit_internal.py torch/_library/infer_schema.py torch/distributed/_functional_collectives.py torch/distributed/checkpoint/_extension.py torch/nn/modules/conv.py torch/nn/utils/clip_grad.py torch/optim/adamax.py torch/optim/nadam.py torch/optim/radam.py torch/utils/_cpp_embed_headers.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/145342,aorenste,bobrenjc93,,,
64ee57847b6,dynamo,not user facing,[dynamo][builtin-skipfiles-cleanup] Remove some builtins (#145856),test/dynamo/test_structured_trace.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/145856,anijain2305,zou3519,,,
a479656cd2e,dynamo,not user facing,[dynamo][builtin-skipfiles-removal] Remove logging (#145875),test/inductor/test_compiled_autograd.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/145875,anijain2305,williamwen42,,,
236793684d1,dynamo,not user facing,"[dynamo][builtin-skipfiles-cleanup] Remove threading, _collections_abc, _weakrefset, threading (#145878)",torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/145878,anijain2305,StrongerXi,williamwen42,,
3f77002b968,dynamo,not user facing,"[dynamo][builtin-skipfiles-cleanup] remove abc, enum, importlib (#145892)",torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/145892,anijain2305,StrongerXi,williamwen42,,
2e8c080ab16,inductor,bug fixes,"[inductor][4/N] triton support post-#5512, fix constexpr signatures (#145583)",test/inductor/test_torchinductor.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_bmm_template.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_utils.py torch/_inductor/codegen/wrapper.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/145583,davidberard98,jansel,,,
501c5972f02,skip,not user facing,[pytorch] raise exception when calling dim order on sparse tensor (#145888),test/test_torch.py torch/_tensor.py,https://github.com/pytorch/pytorch/pull/145888,Gasoonjia,Jack-Khuu,,,
354fe48db9e,releng,not user facing,Add magma cuda build 12.8 (#145765),.ci/magma/Makefile .github/workflows/build-magma-linux.yml,https://github.com/pytorch/pytorch/pull/145765,tinglvv,malfet,,,
81685d81eb8,skip,not user facing,[ATen][CUDA] Implement 128 bit vectorization v2 (#145746),aten/src/ATen/native/cuda/CUDAJitLoops.cuh aten/src/ATen/native/cuda/CUDALoops.cuh aten/src/ATen/native/cuda/Dropout.cu aten/src/ATen/native/cuda/MemoryAccess.cuh aten/src/ATen/native/cuda/jit_utils.cpp aten/src/ATen/native/cuda/jit_utils.h aten/src/ATen/native/cuda/thread_constants.h aten/src/ATen/test/cuda_vectorized_test.cu,https://github.com/pytorch/pytorch/pull/145746,Aidyn-A,eqy,ngimel,,
5e5da9bd9af,skip,not user facing,[triton] Update pin to tip of 3.2 release (#145867),.ci/docker/ci_commit_pins/triton.txt,https://github.com/pytorch/pytorch/pull/145867,bertmaher,Skylion007,exclamaforte,htyu,
3b3aac0cde4,xpu,not user facing,Filter out iGPU if dGPU is found on XPU (#144378),c10/xpu/XPUFunctions.cpp,https://github.com/pytorch/pytorch/pull/144378,guangyey,EikanWang,gujinghui,,
ed141d7d1a3,composability,Untopiced,dont assign a size to _assert_scalar in partitioner (#143877),torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/143877,bdhirsh,zou3519,,,
4499d60d56d,dynamo,not user facing,[dynamo][builin-skipfiles-cleanup] Remove types (#145909),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/145909,anijain2305,zou3519,,,
3e135993bd0,skip,not user facing,Update mi300 labels to account for multiple clusters. (#145923),.github/workflows/inductor-perf-test-nightly-rocm.yml .github/workflows/unstable.yml,https://github.com/pytorch/pytorch/pull/145923,saienduri,,,,
d1f82de2bf4,skip,not user facing,[dynamo] Use polyfill to implement comparison operators (#144485),benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv test/dynamo/test_misc.py test/dynamo/test_repros.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/utils.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/144485,anijain2305,jansel,,,
4abff4b2716,lazy,Untopiced,Introduce cache clearing APIs for the lazy graph executor (#144489),test/cpp/lazy/CMakeLists.txt test/cpp/lazy/test_lazy_graph_executor.cpp torch/csrc/lazy/core/lazy_graph_executor.cpp torch/csrc/lazy/core/lazy_graph_executor.h,https://github.com/pytorch/pytorch/pull/144489,rpsilva-aws,wconstab,,,
1a613c33424,export,Untopiced,bump counters for unbacked binding names (#145882),torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/145882,avikchaudhuri,pianpwk,,,
0d6343347fb,skip,Untopiced,"Revert ""Record inputs at time of tracing, constrain to them for triton fn (#145448)""",test/inductor/test_triton_kernels.py torch/_inductor/graph.py torch/_inductor/lowering.py torch/fx/experimental/proxy_tensor.py torch/fx/proxy.py,,,,,,
284f217011e,skip,Untopiced,"Revert ""[Environment Variable][7/N] Use thread-safe getenv functions (#140211)""",aten/src/ATen/core/Vitals.cpp aten/src/ATen/cuda/CublasHandlePool.cpp aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/Tunable.h aten/src/ATen/mps/MPSAllocator.mm aten/src/ATen/mps/MPSFallback.mm aten/src/ATen/mps/MPSProfiler.mm aten/src/ATen/native/cudnn/Conv_v8.cpp aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm torch/csrc/autograd/engine.cpp torch/csrc/cuda/nccl.cpp torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/init.cpp torch/csrc/jit/codegen/fuser/compiler.cpp torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp,,,,,,
e0525dbca9c,skip,Untopiced,"Revert ""inductor.config.descriptive_names = False is not actually supported (#145523)""",torch/_inductor/config.py,,,,,,
6371c25b91f,skip,Untopiced,"Revert ""[c10d] Add NCCL memory allocator (#145675)""",test/distributed/test_c10d_nccl.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp,,,,,,
1c9014a1358,export,Untopiced,[export] Add tlparse to draft-export (#145810),test/export/test_draft_export.py torch/export/_draft_export.py,https://github.com/pytorch/pytorch/pull/145810,angelayi,pianpwk,,,
082fab0fc7e,cuda,Untopiced,[64-bit] Int64 casting for UpSampleNearest3D (#144865),aten/src/ATen/native/cuda/UpSampleNearest3d.cu test/test_torch.py,https://github.com/pytorch/pytorch/pull/144865,jataylo,eqy,,,
b52e8d521e2,skip,Untopiced,"Revert ""[CD] Install ninja and setuptools from PyPI (#145871)""",.ci/wheel/build_wheel.sh,,,,,,
b80482988fa,skip,Untopiced,"Revert ""[CMake] Find HomeBrew OpenMP on MacOS (#145870)""",cmake/Modules/FindOpenMP.cmake,,,,,,
6aed6c042e5,skip,not user facing,[CD] Install ninja and setuptools from PyPI (#145871),.ci/wheel/build_wheel.sh,https://github.com/pytorch/pytorch/pull/145871,malfet,Skylion007,,,
2d5d0225944,inductor,not user facing,"Fix a number of flexattention issues (cse, cudagraph, etc.) (#145059)",aten/src/ATen/functorch/BatchRulesScatterOps.cpp test/inductor/test_flex_attention.py torch/_inductor/dtype_propagation.py torch/_inductor/kernel/flex_attention.py torch/_inductor/select_algorithm.py torch/nn/attention/flex_attention.py,https://github.com/pytorch/pytorch/pull/145059,Chillee,Skylion007,drisspg,,
2b8c28099a9,distributed,not user facing,[OSS] Add no dist as an argument to DCP top level apis (#145754),torch/distributed/checkpoint/state_dict_loader.py torch/distributed/checkpoint/state_dict_saver.py,https://github.com/pytorch/pytorch/pull/145754,ankitageorge,daulet-askarov,,,
bb4964013f8,nn_frontend,Untopiced,Add determinmistic kernel for reflection2d (#136241),aten/src/ATen/native/cuda/ReflectionPad.cu test/test_nn.py test/test_torch.py,https://github.com/pytorch/pytorch/pull/136241,ZelboK,albanD,eqy,,
8bd7bf3269d,inductor,Untopiced,[Inductor-CPU] Add profiling support for codegened flex attention kernels (#145894),torch/_inductor/codegen/cpp_flex_attention_template.py,https://github.com/pytorch/pytorch/pull/145894,sanchitintel,jansel,leslie-fang-intel,,
40ccb7a86d4,releng,Untopiced,cpp_wrapper: Move #includes to per-device header files (#145932),.github/labeler.yml .lintrunner.toml setup.py torch/_inductor/codecache.py torch/_inductor/codegen/aoti_runtime/implementation.cpp torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/device_op_overrides.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/wrapper.py torch/_inductor/codegen/xpu/device_op_overrides.py torch/csrc/inductor/aoti_include/array_ref.h torch/csrc/inductor/aoti_include/common.h torch/csrc/inductor/aoti_include/cpu.h torch/csrc/inductor/aoti_include/cuda.h torch/csrc/inductor/aoti_include/xpu.h torch/csrc/inductor/array_ref_impl.h torch/csrc/inductor/cpp_wrapper/array_ref.h torch/csrc/inductor/cpp_wrapper/common.h torch/csrc/inductor/cpp_wrapper/cpu.h torch/csrc/inductor/cpp_wrapper/cuda.h torch/csrc/inductor/cpp_wrapper/device_internal/cpu.h torch/csrc/inductor/cpp_wrapper/device_internal/cuda.h torch/csrc/inductor/cpp_wrapper/device_internal/xpu.h torch/csrc/inductor/cpp_wrapper/xpu.h,https://github.com/pytorch/pytorch/pull/145932,desertfire,benjaminglass1,yushangdi,,
a6e3f294f16,skip,not user facing,Don't use mypy daemon in CI (#145961),tools/linter/adapters/mypy_linter.py,https://github.com/pytorch/pytorch/pull/145961,ZainRizvi,malfet,,,
953e80936e8,skip,not user facing,[linter] Grep linter batches long command (#145950),tools/linter/adapters/grep_linter.py,https://github.com/pytorch/pytorch/pull/145950,clee2000,wdvr,,,
1185b81c514,skip,Untopiced,"Revert ""[dynamo] Use polyfill to implement comparison operators (#144485)""",benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv test/dynamo/test_misc.py test/dynamo/test_repros.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/utils.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/user_defined.py,,,,,,
d0aa1386b8c,skip,not user facing,Disable AOTAutogradCache for triton version < 3.2 (#145937),torch/_functorch/_aot_autograd/autograd_cache.py,https://github.com/pytorch/pytorch/pull/145937,jamesjwu,bdhirsh,,,
bb2fb554a95,skip,not user facing,[BE]: Update CUTLASS submodule to 3.7.0 (#145172),third_party/cutlass,https://github.com/pytorch/pytorch/pull/145172,Skylion007,eqy,henrylhtsang,,
23ff899164e,inductor,not user facing,[inductor] Fix handling of fixed XBLOCK larger than xnumel=1 (#145671),test/inductor/test_cooperative_reductions.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/145671,jansel,eellison,,,
5db0ad92e3d,inductor,not user facing,[inductor] Remove mask_str from IndexingOptions (#145695),torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/145695,jansel,eellison,,,
793dfc27e08,inductor,not user facing,[inductor] Add some typing to triton.py (#145688),torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/145688,jansel,Skylion007,eellison,,
e02c038a237,dynamo,not user facing,[dynamo][benchmarks] Stop benchmarking compile time of dead code (#145590),benchmarks/dynamo/common.py benchmarks/dynamo/timm_models.py benchmarks/dynamo/timm_models.yaml,https://github.com/pytorch/pytorch/pull/145590,xmfan,jansel,,,
2141c1aebe0,skip,not user facing,Better hop_db comment; move test to a non-export test file (#145938),CODEOWNERS test/export/test_hop.py test/test_hop_infra.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/145938,zou3519,ydwu4,,,
1e57154af38,skip,not user facing,Require that all HOPs be imported at `import torch` time (#145939),test/test_hop_infra.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/associative_scan.py torch/_higher_order_ops/wrap.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/145939,zou3519,ydwu4,,,
2e5886dcc45,composability,Untopiced,Add fake_impl for unique_consecutive (#145649),test/dynamo/test_misc.py test/test_proxy_tensor.py torch/_subclasses/fake_impls.py torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/145649,zou3519,eellison,ezyang,,
3df961d99b8,inductor,not user facing,give emulate_precision_casts an envar (#145948),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/145948,eellison,mlazos,,,
521588519da,skip,not user facing,re-use FloorDiv for RShift (#145898),test/inductor/test_aot_inductor.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/145898,ColinPeppler,bobrenjc93,desertfire,,
b60120d0df9,skip,Untopiced,"Revert ""[ATen][CUDA] Implement 128 bit vectorization v2 (#145746)""",aten/src/ATen/native/cuda/CUDAJitLoops.cuh aten/src/ATen/native/cuda/CUDALoops.cuh aten/src/ATen/native/cuda/Dropout.cu aten/src/ATen/native/cuda/MemoryAccess.cuh aten/src/ATen/native/cuda/jit_utils.cpp aten/src/ATen/native/cuda/jit_utils.h aten/src/ATen/native/cuda/thread_constants.h aten/src/ATen/test/cuda_vectorized_test.cu,,,,,,
18a7a04c4ad,skip,Untopiced,[c10d] Add NCCL memory allocator (#145675),test/distributed/test_c10d_nccl.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/145675,kwen2501,syed-ahmed,wconstab,,
a9ed7bd78ea,releng,not user facing,[utilization] pipeline to create clean db records (#145327),.github/workflows/lint.yml .lintrunner.toml tools/stats/monitor.py tools/stats/upload_stats_lib.py tools/stats/upload_utilization_stats/test_upload_utilization_stats.py tools/stats/upload_utilization_stats/upload_utilization_stats.py tools/stats/utilization_stats_lib.py,https://github.com/pytorch/pytorch/pull/145327,yangw-dev,huydhn,,,
6b41f310c20,skip,not user facing,config: Support str env variables (#145980),test/test_utils_config_module.py torch/testing/_internal/fake_config_module.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/145980,c00w,oulgen,yushangdi,,
72699950b06,skip,not user facing,Copy model before benchmark warmup runs (#145858),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/145858,angelayi,desertfire,,,
73dde451b7e,skip,not user facing,[pytorch] Sprinkle in a few `template` keywords (#145877),aten/src/ATen/core/boxing/impl/boxing.h aten/src/ATen/core/ivalue_inl.h,https://github.com/pytorch/pytorch/pull/145877,VasuAgrawal,Skylion007,,,
25ca05eebf8,distributed,not user facing,[PGNCCL] Correct some ifdef's (#145893),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/145893,kwen2501,c-p-i-o,,,
32bb6f83d5e,inductor,bug fixes,Make sure that benchmark_harness is set before running (#145532),test/inductor/test_aot_inductor.py torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/145532,exclamaforte,eellison,,,
933b6d98309,releng,not user facing,cpp_wrapper: enable in aarch64 and x86 nightly dashboard performance runs (#145791),.github/workflows/inductor-perf-test-nightly-aarch64.yml .github/workflows/inductor-perf-test-nightly-x86.yml,https://github.com/pytorch/pytorch/pull/145791,benjaminglass1,desertfire,,,
116af809ebd,skip,not user facing,Use std::string_view (#145906),aten/src/ATen/Dispatch.h aten/src/ATen/MapAllocator.h aten/src/ATen/PadNd.h aten/src/ATen/core/ATen_pch.h aten/src/ATen/core/function_schema.h aten/src/ATen/core/op_registration/op_allowlist.h aten/src/ATen/core/operator_name.h aten/src/ATen/native/BatchLinearAlgebra.h aten/src/ATen/native/Gelu.h aten/src/ATen/native/nested/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/sdp_utils_cpp.h aten/src/ATen/templates/Function.h aten/src/ATen/templates/Operator.h aten/src/ATen/templates/TensorMethods.cpp c10/core/impl/alloc_cpu.cpp c10/util/ConstexprCrc.h c10/util/StringUtil.h caffe2/serialize/inline_container.cc torch/csrc/utils/python_arg_parser.cpp,https://github.com/pytorch/pytorch/pull/145906,cyyever,albanD,,,
0d5f0a81c57,build_frontend,improvements,[CMake] Find HomeBrew OpenMP on MacOS (#145870),cmake/Modules/FindOpenMP.cmake setup.py,https://github.com/pytorch/pytorch/pull/145870,malfet,Skylion007,atalman,,
8cc6f173348,releng,improvements,[CD] Install OpenMP from homebrew (#145889),.ci/wheel/build_wheel.sh,https://github.com/pytorch/pytorch/pull/145889,malfet,atalman,,,
58cc6693cb4,inductor,not user facing,[BE] Type annotate wrapper_benchmark.py and cuda_combined_scheduling.py (#145542),torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/triton.py torch/_inductor/scheduler.py torch/_inductor/wrapper_benchmark.py,https://github.com/pytorch/pytorch/pull/145542,BoyuanFeng,eellison,,,
8a6e9a88e9c,cuda,bug fixes,Let PYTORCH_NO_CUDA_MEMORY_CACHING has effect only when value is 1 (#145905),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/145905,cyyever,eqy,janeyx99,,
6bd19e65b12,inductor,not user facing,add inductor_triton_kernel_mapping_post_grad.json to tlparseadd changes (#145954),test/dynamo/test_structured_trace.py torch/_inductor/compile_fx.py torch/_inductor/debug.py torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/145954,yushangdi,YUNQIUGUO,,,
ffa628169dd,cuda,not user facing,[ATen][Native][CUDA][SCALED_MM] limit f8f8bf16 rowwise scaled matmul to sm_90 (#145728),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/145728,Aidyn-A,Skylion007,eqy,,
894ef8c1e3b,dynamo,not user facing,[torchbench] Inductor freezing bfloat16 conv folding needs high tolerance (#145623),benchmarks/dynamo/common.py torch/_dynamo/utils.py torch/_inductor/fx_passes/binary_folding.py,https://github.com/pytorch/pytorch/pull/145623,IvanKobzarev,eellison,,,
50086ab537e,onnx,bug fixes,[ONNX] Delete `rename_dynamic_shapes_with_model_inputs` (#146002),torch/onnx/_internal/exporter/_compat.py,https://github.com/pytorch/pytorch/pull/146002,titaiwangms,justinchuby,,,
5fa28bbe407,skip,Untopiced,"Revert ""[c10d] Add NCCL memory allocator (#145675)""",test/distributed/test_c10d_nccl.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp,,,,,,
aaddfc5a7ff,quantization,not user facing,Add TORCHINDUCTOR_VEC_ISA_OK env var for vec_isa_ok (#134667),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/134667,matthewhagraphcore,eellison,,,
1c3df9ca8c1,inductor,not user facing,"Fix signif_strides_equal for symints, dedupe (#145953)",torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/145953,eellison,Skylion007,zou3519,,
967cf85f3a8,skip,Untopiced,"Revert ""Update mi300 labels to account for multiple clusters. (#145923)""",.github/workflows/inductor-perf-test-nightly-rocm.yml .github/workflows/unstable.yml,,,,,,
7796e308d06,skip,Untopiced,"Record inputs at time of tracing, constrain to them for triton fn (#145448)",torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/145448,eellison,zou3519,,,
51ee9b154e1,skip,Untopiced,[c10d] Add NCCL memory allocator (#145675),test/distributed/test_c10d_nccl.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/145675,kwen2501,syed-ahmed,wconstab,,
3ee655e4d4b,distributed,not user facing,[async-TP] Fix scheduling in matmul+reduce-scatter for 2 ranks (#145846),torch/distributed/_symmetric_memory/__init__.py,https://github.com/pytorch/pytorch/pull/145846,lw,yifuwang,,,
5a527fa5eed,inductor,bug fixes,Make sure not using cpp wrapper when setting nvtx training annotation (#145538),test/inductor/test_gpu_cpp_wrapper.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/145538,exclamaforte,desertfire,,,
f746bb6311b,skip,not user facing,config: Don't spam warnings about reference type configs (#145800),test/test_utils_config_module.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/145800,c00w,eellison,,,
d1143c4b373,fx,Untopiced,[export] fix non-strict pre_dispatch exporting while_loop (#145762),test/export/test_export.py test/export/test_hop.py torch/_higher_order_ops/while_loop.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/145762,ydwu4,avikchaudhuri,tugsbayasgalan,zou3519,
16420a78eb7,inductor,not user facing,[AOTI] Remove AOTI_USE_CREATE_TENSOR_FROM_BLOB_V1 (#146039),torch/_inductor/cpp_builder.py torch/csrc/inductor/aoti_runtime/model.h,https://github.com/pytorch/pytorch/pull/146039,desertfire,hl475,yushangdi,,
a3698ebd5c5,dynamo,not user facing,[while_loop] specialize when cond_fn return constants (#144515),test/inductor/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/utils.py,https://github.com/pytorch/pytorch/pull/144515,ydwu4,zou3519,,,
3ef1551f5a7,skip,not user facing,Advance past fc window for stft center (#145437),torch/functional.py torch/onnx/symbolic_opset17.py,https://github.com/pytorch/pytorch/pull/145437,jackzhxng,iseeyuan,justinchuby,,
1fdb4d65c02,mps,improvements,[MPS] Extend `torch.mm`/`torch.bmm` to integral types (#145809),aten/src/ATen/native/mps/kernels/LinearAlgebra.metal aten/src/ATen/native/mps/operations/Blas.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/145809,malfet,dcci,,,
e6c39d37e90,onnx,deprecation,[ONNX] Create deprecation warning on dynamo_export (#146003),test/onnx/test_pytorch_onnx_no_runtime.py torch/onnx/__init__.py torch/onnx/_deprecation.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_compat.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/146003,justinchuby,titaiwangms,,,
70f6aaa7864,distributed,not user facing,[OSS] Add kwargs to fsspec reader/writer (#145845),torch/distributed/checkpoint/_fsspec_filesystem.py,https://github.com/pytorch/pytorch/pull/145845,ankitageorge,mhorowitz,,,
9f9904172d5,dynamo,not user facing,[scan] scan dim handling in user-facing scan() (#145179),test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/scan.py,https://github.com/pytorch/pytorch/pull/145179,bohnstingl,ydwu4,,,
7e7341bddd7,dynamo,not user facing,[hop] fix unbacked_bindings meta for while_loop (#143559),test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/while_loop.py,https://github.com/pytorch/pytorch/pull/143559,ydwu4,zou3519,,,
d14046b58d2,releng,not user facing,Update fuzzer guidance to include rng (#145962),.github/ISSUE_TEMPLATE/pt2-bug-report.yml,https://github.com/pytorch/pytorch/pull/145962,mlazos,eellison,,,
1252c1933d3,releng,not user facing,Update to remind users to use torch.compile template (#145960),.github/ISSUE_TEMPLATE/bug-report.yml,https://github.com/pytorch/pytorch/pull/145960,mlazos,eellison,,,
08ff11e9d04,cuda,Untopiced,"initialize device when pinning memory on this device, short circuit i… (#145752)",aten/src/ATen/Context.h aten/src/ATen/EmptyTensor.cpp c10/util/CallOnce.h test/test_cuda.py,https://github.com/pytorch/pytorch/pull/145752,ngimel,albanD,,,
d100e9ae744,skip,not user facing,inductor: Don't throw an internal error when a nn.module is missing a attribute (#145122),test/dynamo/test_compile.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/145122,c00w,jansel,,,
f3120f6d262,dynamo,not user facing,Remove incorrect BuiltinVariable.call_hasattr() (#145551),torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/145551,aorenste,anijain2305,,,
ccbbc88bbbf,dynamo,not user facing,Turn on mypy for _dynamo/variables/builtin.py (#145552),torch/_dynamo/output_graph.py torch/_dynamo/replay_record.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py torch/_functorch/_activation_checkpointing/knapsack_evaluator.py torch/distributed/_tools/sac_estimator.py,https://github.com/pytorch/pytorch/pull/145552,aorenste,Skylion007,anijain2305,,
fbb076cc459,dynamo,not user facing,Fix call to create_load_global (#145553),torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/145553,aorenste,anijain2305,,,
23695ea0023,dynamo,not user facing,Fix dynamo use of `list[int]` in graph break (#145554),test/dynamo/test_repros.py torch/_dynamo/codegen.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/145554,aorenste,anijain2305,,,
7391cea857b,skip,Untopiced,"Revert ""[triton] Update pin to tip of 3.2 release (#145867)""",.ci/docker/ci_commit_pins/triton.txt,,,,,,
e2917245fb0,skip,not user facing,[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/144441,eqy,Chillee,malfet,,
99a09409919,mps,bug fixes,[MPS] Fix regression in con-contig bitwise ops (#146085),aten/src/ATen/native/mps/operations/BitwiseOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/146085,malfet,dcci,,,
2f60f12f8ba,skip,not user facing,[Torch] Extract arange_out resizing logic into a helper function that can be used by other devices (#145747),aten/src/ATen/native/RangeFactories.cpp aten/src/ATen/native/RangeUtils.h,https://github.com/pytorch/pytorch/pull/145747,PatriceVignola,mortzur,,,
f85e4c13601,skip,not user facing,Enable C++ API parity tests on AArch64 (#145370),test/test_cpp_api_parity.py,https://github.com/pytorch/pytorch/pull/145370,murste01,albanD,,,
4280232f214,skip,Untopiced,"Revert ""Advance past fc window for stft center (#145437)""",torch/functional.py torch/onnx/symbolic_opset17.py,,,,,,
9fdc20809a3,distributed,not user facing,[PGNCCL] Simplify support macro definition (#145964),torch/csrc/distributed/c10d/NCCLUtils.hpp,https://github.com/pytorch/pytorch/pull/145964,kwen2501,fduwjj,shuqiangzhang,,
81176561629,composability,not user facing,nonzero_static with symint size (#146006),aten/src/ATen/native/native_functions.yaml test/export/test_export.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/146006,avikchaudhuri,angelayi,,,
720b8d0d8da,skip,not user facing,[inductor/profiler] add kernel kwargs instrumentation (#145573),test/inductor/test_profiler.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/145573,briancoutinho,davidberard98,jansel,,
c72b5364205,skip,not user facing,Add manual override flag for core ATen op detection during bc check (#146052),test/forward_backward_compatibility/check_forward_backward_compatibility.py,https://github.com/pytorch/pytorch/pull/146052,soulitzer,albanD,,,
e6704a2447a,fx,not user facing,Allow replacing unbacked with very large upperbound by returning no-op for FloorToInt(int) (#146001),test/export/test_export.py torch/fx/experimental/symbolic_shapes.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/146001,ColinPeppler,bobrenjc93,,,
56307dc3708,dynamo,not user facing,[dynamo][dicts] Raise exception on pop (#145986),test/dynamo/test_exceptions.py torch/_dynamo/variables/dicts.py,https://github.com/pytorch/pytorch/pull/145986,anijain2305,Skylion007,StrongerXi,jansel,
4665bc2cc05,dynamo,not user facing,[dynamo][functions] Support `id` on function (#145987),test/dynamo/test_dicts.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/145987,anijain2305,StrongerXi,jansel,mlazos,
e7bb608d025,dynamo,not user facing,[dynamo][dicts] Support construction of types.MappingProxyType (#145994),test/dynamo/test_dicts.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/145994,anijain2305,StrongerXi,jansel,,
1e3d1738a44,dynamo,not user facing,[dynamo][polyfills]Support getrecursionlimit (#145989),test/dynamo/test_functions.py torch/_dynamo/polyfills/sys.py,https://github.com/pytorch/pytorch/pull/145989,anijain2305,StrongerXi,jansel,,
c70362fac8c,distributed,not user facing,[AsyncMM] re-enable and adapt to cutlass 3.6.0 (#144011),torch/csrc/distributed/c10d/cuda/AsyncMM.cu torch/csrc/distributed/c10d/cuda/cutlass/gemm/kernel/persistent_async_input_scheduler.cuh,https://github.com/pytorch/pytorch/pull/144011,yifuwang,Skylion007,drisspg,,
d94d816d966,skip,not user facing,Simplify handling of max jobs in CMake builds (#145820),tools/setup_helpers/cmake.py tools/test/test_cmake.py,https://github.com/pytorch/pytorch/pull/145820,cyyever,malfet,,,
3fae5c85091,skip,not user facing,torchgen: support exception boundary for ExecuTorch functions (#144341),tools/test/test_executorch_gen.py torchgen/gen_executorch.py,https://github.com/pytorch/pytorch/pull/144341,swolchok,Jack-Khuu,,,
ccd27e81296,dynamo,not user facing,Turn on fx graph cache and automatic dynamic pgo local caches in fbcode (#146065),torch/_dynamo/config.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/146065,oulgen,jamesjwu,,,
bf9d053fb8b,skip,not user facing,[Break XPU] Fix Inductor cuda bias UT (#145934),test/inductor/test_inplace_padding.py,https://github.com/pytorch/pytorch/pull/145934,guangyey,desertfire,jansel,shunting314,
2811f33d123,inductor,not user facing,Fix code cache + freezing compile-time regression (#145868),test/inductor/test_codecache.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/constant_folding.py torch/_inductor/freezing.py torch/_inductor/freezing_utils.py torch/_inductor/output_code.py,https://github.com/pytorch/pytorch/pull/145868,masnesral,eellison,,,
08d88127fe4,releng,not user facing,Use Magma-cuda 12.8 for libtorch (#146019),.ci/docker/common/install_magma.sh,https://github.com/pytorch/pytorch/pull/146019,tinglvv,eqy,,,
e01c898e51c,inductor,Untopiced,[Customized Optimus] Add select cat aten pass (#145918),test/inductor/test_split_cat_fx_aten_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/145918,mengluy0125,Yuzhen11,,,
ffb424eab6e,dynamo,Untopiced,[dynamo/export] call local_scalar_dense when full() value is scalar tensor (#144999),test/export/test_export.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/144999,pianpwk,angelayi,,,
27e35de6c28,skip,not user facing,[export] Add distributed test (#146050),test/export/test_export.py,https://github.com/pytorch/pytorch/pull/146050,angelayi,avikchaudhuri,,,
f358d4d0046,onnx,not user facing,[ONNX] Migrate test_torch_export_with_onnxruntime.py to test_small_models_e2e.py (#146095),test/onnx/exporter/test_small_models_e2e.py test/onnx/torch_export/test_torch_export_with_onnxruntime.py,https://github.com/pytorch/pytorch/pull/146095,titaiwangms,justinchuby,,,
cf2de4e2305,export,Untopiced,Introduce aoti_call_delegate HOP (#145630),torch/_export/serde/serialize.py torch/_export/verifier.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/aoti_call_delegate.py torch/_ops.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/145630,SherlockNoMad,zou3519,,,
f9227e7c337,jit,Untopiced,Expose ToIValueAllowNumbersAsTensors to TORCH_PYTHON_API so we can use it in monarch (#146087),torch/csrc/jit/python/pybind_utils.h,https://github.com/pytorch/pytorch/pull/146087,manav-a,suo,,,
57d8278ab90,skip,Untopiced,pickler for GraphModule (#141659),test/distributed/_composable/fsdp/test_fully_shard_compile.py test/fx/test_graph_pickler.py test/inductor/test_torchinductor.py torch/_dynamo/variables/misc.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/graph.py torch/_inductor/output_code.py torch/_inductor/utils.py torch/_subclasses/fake_tensor.py torch/_subclasses/meta_utils.py torch/fx/_graph_pickler.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/141659,aorenste,jamesjwu,,,
eeb5e1bf204,skip,not user facing,[AOTI] Cache treespec_loads calculation (#145815),torch/utils/_cxx_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/145815,henryhu6,henrylhtsang,,,
e84bf88dde5,skip,not user facing,[ATen][CUDA] Implement 128 bit vectorization v2 (#145746),aten/src/ATen/native/cuda/CUDAJitLoops.cuh aten/src/ATen/native/cuda/CUDALoops.cuh aten/src/ATen/native/cuda/Dropout.cu aten/src/ATen/native/cuda/MemoryAccess.cuh aten/src/ATen/native/cuda/jit_utils.cpp aten/src/ATen/native/cuda/jit_utils.h aten/src/ATen/native/cuda/thread_constants.h aten/src/ATen/test/cuda_vectorized_test.cu,https://github.com/pytorch/pytorch/pull/145746,Aidyn-A,eqy,ngimel,,
c55af2b567b,skip,not user facing,[CMake] Delete Caffe2 inspect_gpu binary (#146105),binaries/CMakeLists.txt binaries/inspect_gpu.cc,https://github.com/pytorch/pytorch/pull/146105,malfet,atalman,seemethere,,
1f1a9965d52,skip,not user facing,fix a small typo in comments (#145323),torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/145323,haifeng-jin,Skylion007,,,
7b07415aaa6,export,Untopiced,[export] nested terms in nn_module_stack deserialization (#145901),test/export/test_serialize.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/145901,pianpwk,angelayi,,,
2af876707be,inductor,bug fixes,[AOTI] Fix a memory leak in package boxed_run (#146100),test/inductor/test_aot_inductor.py torch/csrc/inductor/aoti_package/pybind.cpp,https://github.com/pytorch/pytorch/pull/146100,desertfire,cpuhrsch,,,
a7c2d85c18e,python_frontend,Untopiced,Add overloads to diagonal docs (#144214),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/144214,jackson-tsang578,albanD,,,
9232355bb0a,releng,not user facing,Add CUDA 12.8 manywheel x86 Builds to Binaries Matrix (#145792),.ci/manywheel/build_cuda.sh .github/scripts/generate_binary_build_matrix.py .github/scripts/generate_ci_workflows.py .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/145792,tinglvv,atalman,malfet,nWEIdia,
2d6f6637d31,skip,not user facing,Remove lexicographical sorting of storage keys in torch.save (#143879),test/test_serialization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/143879,mikaylagawarecki,albanD,,,
98f87edd233,skip,not user facing,Tensor .cuda() very slow with specific array sizes (#138964),aten/src/ATen/native/cuda/Copy.cu c10/cuda/CUDAFunctions.h test/test_cuda.py,https://github.com/pytorch/pytorch/pull/138964,tolleybot,,,,
001e355a56a,python_frontend,improvements,Add option to serialization config to reduce random reads from get_record_offset when loading with mmap=True (#143880),.ci/pytorch/macos-test.sh .ci/pytorch/test.sh .ci/pytorch/win-test.sh caffe2/serialize/inline_container.cc caffe2/serialize/inline_container.h docs/source/notes/serialization.rst test/test_serialization.py torch/csrc/jit/python/init.cpp torch/serialization.py torch/utils/serialization/config.py,https://github.com/pytorch/pytorch/pull/143880,mikaylagawarecki,albanD,,,
eb5a0718c25,releng,not user facing,S390x nightly builds timeouts (#146041),.github/templates/linux_binary_build_workflow.yml.j2 .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/146041,AlekseiNikiforovIBM,huydhn,malfet,,
f5a61ba0a3d,skip,Untopiced,"Revert ""inductor: Don't throw an internal error when a nn.module is missing a attribute (#145122)""",test/dynamo/test_compile.py torch/_dynamo/variables/nn_module.py,,,,,,
c3f71eb61b4,skip,Untopiced,"Revert ""[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441)""",aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,,,,,,
667b94d1c26,dynamo,not user facing,[hotfix][dynamo] Skip linecache due to a flaky issue (#146141),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/146141,anijain2305,StrongerXi,,,
44ecbcbd5ab,skip,not user facing,s390x: disable test_model_exports_to_core_aten.py test (#145835),test/run_test.py,https://github.com/pytorch/pytorch/pull/145835,AlekseiNikiforovIBM,huydhn,,,
d0748566b4d,releng,not user facing,s390x ci: ensure CI starts correctly if token pipe is not removed (#145840),.github/scripts/s390x-ci/self-hosted-builder/actions-runner@.service .github/scripts/s390x-ci/self-hosted-builder/helpers/gh_cat_token.sh,https://github.com/pytorch/pytorch/pull/145840,AlekseiNikiforovIBM,huydhn,,,
67ed47d8865,skip,not user facing,Binary upload checksum (#144887),.circleci/scripts/binary_upload.sh,https://github.com/pytorch/pytorch/pull/144887,clee2000,atalman,,,
16f44fee258,skip,Untopiced,"Revert ""[inductor/profiler] add kernel kwargs instrumentation (#145573)""",test/inductor/test_profiler.py torch/_inductor/runtime/triton_heuristics.py,,,,,,
aad9f44b2e9,export,Untopiced,[export] Sync model container types to schema.py (#145959),torch/_export/serde/export_schema.thrift torch/_export/serde/schema.py torch/_export/serde/schema.yaml torch/_export/serde/schema_check.py torch/csrc/utils/generated_serialization_types.h,https://github.com/pytorch/pytorch/pull/145959,zhxchen17,yiming0416,,,
68a36354840,skip,not user facing,[hop][inductor] track the dependency on unbacked symbols correctly with constant_args for hops (#143456),test/inductor/test_aot_inductor.py test/inductor/test_control_flow.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/143456,ydwu4,desertfire,,,
18380836eb7,package,not user facing,Remove outdated test skipif conditions for Python3.9 (#146144),test/dynamo/test_functions.py test/dynamo/test_misc.py test/jit/test_scriptmod_ann.py test/package/test_dependency_api.py test/package/test_directory_reader.py test/package/test_resources.py test/test_fx.py test/test_serialization.py test/test_testing.py test/torch_np/numpy_tests/core/test_dtype.py test/torch_np/numpy_tests/core/test_scalar_methods.py,https://github.com/pytorch/pytorch/pull/146144,cyyever,albanD,,,
6a0138fcc1a,skip,not user facing,Torch device backend autoload fix (#145611),torch/__init__.py,https://github.com/pytorch/pytorch/pull/145611,kundaMwiza,albanD,,,
ec2522e200d,mps,Untopiced,[MPS] optimize cholesky (#145722),aten/src/ATen/native/mps/kernels/LinearAlgebra.metal aten/src/ATen/native/mps/operations/LinearAlgebra.mm,https://github.com/pytorch/pytorch/pull/145722,Isalia20,malfet,,,
8b8c596503d,skip,not user facing,Remove trivial dispatch_key_allowlist_check function (#146169),aten/src/ATen/core/op_registration/op_allowlist.h torch/library.h,https://github.com/pytorch/pytorch/pull/146169,janeyx99,albanD,zou3519,,
3a4e7a589b2,distributed,not user facing,"[CI][Distributed] Fix edge case: One rank case (Rank 0) should get [False, False] (#146099)",torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/146099,nWEIdia,eqy,,,
49df8de8be6,dynamo,not user facing,[dynamo] disable eval_frame callback in _TorchDynamoContext __enter__/__exit__ (#145981),test/dynamo/test_repros.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/145981,williamwen42,jansel,,,
5b1abdbf5da,dynamo,not user facing,[dynamo] remove always-failing eval_frame.c debug check (#145982),torch/csrc/dynamo/eval_frame.c,https://github.com/pytorch/pytorch/pull/145982,williamwen42,StrongerXi,jansel,,
2e197c8a2da,dynamo,not user facing,[dynamo][hop] test torch.compiling all HOPs (#145422),test/dynamo/test_higher_order_ops.py,https://github.com/pytorch/pytorch/pull/145422,xmfan,ydwu4,zou3519,,
06850e624ad,skip,not user facing,[ca][hop] test CA on all HOPs (#145429),test/inductor/test_compiled_autograd.py,https://github.com/pytorch/pytorch/pull/145429,xmfan,zou3519,,,
a0d1393b1a4,distributed,Untopiced,[MTIA][FSDP2] Enable MTIA device in FSDP2 library code (#145842),torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py torch/distributed/fsdp/_fully_shard/_fsdp_param.py torch/distributed/fsdp/_fully_shard/_fsdp_state.py,https://github.com/pytorch/pytorch/pull/145842,jvandebon,chaos5958,nautsimon,,
781aceee9ce,dynamo,not user facing,[dynamo] Revert abc change due to internal failures (#146177),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/146177,anijain2305,StrongerXi,,,
8203894eff9,quantization,Untopiced,Resolve affine quantization namespace collision with torchao (#145941),torch/ao/quantization/pt2e/_affine_quantization.py,https://github.com/pytorch/pytorch/pull/145941,andrewor14,cccclai,,,
a7cc6d3e848,releng,not user facing,Manylinux 2.28 migration - remove pre-cxx11 abi libtorch builds (#146200),.github/scripts/generate_binary_build_matrix.py .github/scripts/generate_ci_workflows.py .github/workflows/generated-linux-binary-libtorch-pre-cxx11-main.yml .github/workflows/generated-linux-binary-libtorch-pre-cxx11-nightly.yml,https://github.com/pytorch/pytorch/pull/146200,atalman,huydhn,kit1980,,
c39c6798131,skip,Untopiced,"Revert ""Tensor .cuda() very slow with specific array sizes (#138964)""",aten/src/ATen/native/cuda/Copy.cu c10/cuda/CUDAFunctions.h test/test_cuda.py,,,,,,
af2a39849d9,inductor,not user facing,[AOTI] Refactor codegen_input_symbol_assignment (#146043),torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/146043,desertfire,angelayi,,,
eb029fba13d,distributed,Untopiced,[c10d][NCCL] Implement ncclCommInitRankScalable (merging #136789) (#144794),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/144794,fduwjj,atalman,eqy,kwen2501,
f25f1163dc5,dynamo,not user facing,[dynamo] Support frozenset({..}).__contains__ (#146062),test/dynamo/test_functions.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/146062,anijain2305,Skylion007,jansel,,
43372e70c27,fx,Untopiced,ehnace logging statically known by adding size_oblivious(..) (#145354),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/145354,laithsakka,ezyang,,,
a78c796f0bd,inductor,improvements,[AOTI] Support composed dynamic shape constraint (#146044),test/inductor/test_aot_inductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/wrapper.py,https://github.com/pytorch/pytorch/pull/146044,desertfire,angelayi,,,
35f113e2a00,skip,not user facing,torch/nn/utils/rnn.py: docs: improvements (#138628),torch/nn/utils/rnn.py,https://github.com/pytorch/pytorch/pull/138628,kuraga,mikaylagawarecki,,,
cde5ddfd146,export,Untopiced,fix internal error with reorder submodules (#146181),torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/146181,avikchaudhuri,angelayi,,,
73d90d66a40,inductor,not user facing,Cap size of thread pool in select_algorithm to cpu count (#146071),torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/146071,masnesral,Chillee,,,
57c45340e7c,fx,Untopiced,include entire GraphModule instead of current node when erroring inside of fx interpreter (#146197),torch/fx/interpreter.py,https://github.com/pytorch/pytorch/pull/146197,bdhirsh,jamesjwu,,,
6e734bab93d,skip,not user facing,execution trace export supports gzip format (#146179),test/profiler/test_execution_trace.py torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/146179,briancoutinho,davidberard98,shengfukevin,sraikund16,
8326d270932,inductor,bug fixes,"[inductor][5/N] triton support post-#5512, fix 1 and None handling (#145515)",test/inductor/test_torchinductor.py test/inductor/test_triton_kernels.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_utils.py torch/_inductor/codegen/wrapper.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/145515,davidberard98,SamGinzburg,,,
60236843110,export,Untopiced,[export] Fix symfloat serialization (#146112),test/export/test_serialize.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/146112,angelayi,pianpwk,,,
1de41e69182,dynamo,not user facing,[dynamo][exceptions][3.10] Clean symbolic stack on exception handling (#146198),test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/146198,anijain2305,williamwen42,,,
985a78e9df3,distributed,not user facing,Enable ruff F841 on distributed tests (#146131),test/distributed/algorithms/quantization/test_quantization.py test/distributed/elastic/rendezvous/c10d_rendezvous_backend_test.py test/distributed/fsdp/test_fsdp_optim_state.py,https://github.com/pytorch/pytorch/pull/146131,cyyever,albanD,rec,,
4d2056efb58,skip,not user facing,Enable ruff F841 on numpy tests (#146126),test/torch_np/numpy_tests/core/test_einsum.py test/torch_np/numpy_tests/core/test_multiarray.py test/torch_np/numpy_tests/core/test_numeric.py test/torch_np/numpy_tests/lib/test_function_base.py test/torch_np/numpy_tests/lib/test_histograms.py test/torch_np/numpy_tests/lib/test_twodim_base.py test/torch_np/numpy_tests/linalg/test_linalg.py,https://github.com/pytorch/pytorch/pull/146126,cyyever,albanD,rec,,
549e230c339,skip,not user facing,[draft_export] Clear pending unbacked symbols when overriding mismatched fake kernels (#146089),test/export/test_draft_export.py torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/146089,yiming0416,pianpwk,,,
bcd0ba0f696,inductor,not user facing,Adding the best autotuner config (#146121),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/146121,Mingming-Ding,oulgen,,,
a97a906dd9f,caffe2,not user facing,"Add ""//caffe2:libtorch""  to minifier TARGET file (#146203)",torch/_dynamo/debug_utils.py,https://github.com/pytorch/pytorch/pull/146203,yushangdi,desertfire,,,
f38d5b4a744,releng,not user facing,Update TorchBench commit to main (#145455),.ci/pytorch/common_utils.sh .github/ci_commit_pins/torchbench.txt benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_aot_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_aot_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/update_expected.py benchmarks/dynamo/common.py benchmarks/dynamo/torchbench.yaml,https://github.com/pytorch/pytorch/pull/145455,huydhn,kit1980,,,
a4e43681578,fx,not user facing,add node mapping processing (#146103),test/dynamo/test_structured_trace.py test/fx/test_fx_traceback.py test/inductor/test_provenance_tracing.py torch/_inductor/compile_fx.py torch/_inductor/debug.py torch/_inductor/graph.py torch/fx/traceback.py,https://github.com/pytorch/pytorch/pull/146103,yushangdi,chenyang78,,,
30f091da444,skip,not user facing,add speculation log divergence test (#145659),test/dynamo/test_misc.py,https://github.com/pytorch/pytorch/pull/145659,bobrenjc93,laithsakka,,,
f40e0137872,skip,not user facing,Fix aten.to when input is a tensor constant (#146220),test/export/test_export.py torch/_subclasses/functional_tensor.py,https://github.com/pytorch/pytorch/pull/146220,yushangdi,JacobSzwejbka,,,
2b00d211f0b,distributed,not user facing,Build RowwiseScaledMM.cu for SM89 (#145676),aten/src/ATen/native/cuda/RowwiseScaledMM.cu cmake/Codegen.cmake test/distributed/tensor/test_matrix_ops.py test/inductor/test_aot_inductor.py test/inductor/test_fp8.py test/inductor/test_max_autotune.py test/test_flop_counter.py test/test_matmul_cuda.py test/test_sparse_semi_structured.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/145676,alexsamardzic,drisspg,eqy,malfet,
2fd1b6b3610,skip,Untopiced,[Environment Variable][7/N] Use thread-safe getenv functions (#140211),aten/src/ATen/core/Vitals.cpp aten/src/ATen/cuda/CublasHandlePool.cpp aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/mps/MPSAllocator.mm aten/src/ATen/mps/MPSFallback.mm aten/src/ATen/mps/MPSProfiler.mm aten/src/ATen/native/cudnn/Conv_v8.cpp aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm torch/csrc/autograd/engine.cpp torch/csrc/cuda/nccl.cpp torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/init.cpp torch/csrc/jit/codegen/fuser/compiler.cpp torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp,https://github.com/pytorch/pytorch/pull/140211,cyyever,eqy,ezyang,,
373606928b0,skip,not user facing,Add torch.utils._pytree.register_dataclass (#146059),test/test_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/146059,zou3519,StrongerXi,angelayi,yanboliang,
0f768c7866f,fx,Untopiced,Barebones flat_apply HOP (#146060),test/dynamo/test_flat_apply.py test/test_pytree.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/flat_apply.py torch/testing/_internal/hop_db.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/146060,zou3519,StrongerXi,yanboliang,,
4c004caa76b,inductor,not user facing,[inductor] Add types to DeviceOpOverrides (#145913),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpu_device_op_overrides.py torch/_inductor/codegen/cuda/device_op_overrides.py torch/_inductor/codegen/mps_device_op_overrides.py torch/_inductor/codegen/xpu/device_op_overrides.py,https://github.com/pytorch/pytorch/pull/145913,jansel,Skylion007,,,
79f9f62e3a8,inductor,not user facing,[inductor] Combine regexp checks in OpOverrides.paren (#145914),torch/_inductor/codegen/common.py,https://github.com/pytorch/pytorch/pull/145914,jansel,Skylion007,,,
8e56d713c98,inductor,not user facing,[inductor] Add typing to common.OpDecompositions (#145915),torch/_inductor/codegen/common.py torch/_inductor/codegen/mps.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/145915,jansel,yanboliang,,,
68cf36d5ab6,skip,not user facing,[inductor] Add typing to common.KernelArgs (#145916),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py,https://github.com/pytorch/pytorch/pull/145916,jansel,yanboliang,,,
8c657ae4be5,skip,not user facing,[inductor] Add typing to common.CSE (#145993),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/145993,jansel,yanboliang,,,
e56dcf27722,inductor,bug fixes,[CPUInductor] Fix SVE256 detection (#146207),aten/src/ATen/cpu/Utils.cpp aten/src/ATen/cpu/Utils.h torch/_C/_cpu.pyi torch/_dynamo/trace_rules.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/cpu_vec_isa.py torch/cpu/__init__.py torch/csrc/cpu/Module.cpp,https://github.com/pytorch/pytorch/pull/146207,malfet,angelayi,,,
5ed57930163,python_frontend,bug fixes,Temp disable MKL in  DistributionKernels.cpp (#146174),aten/src/ATen/native/cpu/DistributionKernels.cpp test/distributions/test_distributions.py,https://github.com/pytorch/pytorch/pull/146174,malfet,ngimel,,,
f97307f4631,nn_frontend,not user facing,[Docs] Add clarification for target types in CrossEntropyLoss doc (#145444),torch/nn/modules/loss.py,https://github.com/pytorch/pytorch/pull/145444,spzala,mikaylagawarecki,,,
d4ad7b91adb,mps,not user facing,[mps] Move zeta() to special_math.h. (#146231),aten/src/ATen/native/mps/kernels/Gamma.metal c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/146231,dcci,Skylion007,malfet,,
07dbd539b4b,skip,not user facing,[BE][Ez]: Make c10/special arrays constexpr (#146246),c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/146246,Skylion007,dcci,malfet,,
dca5cc02558,mps,not user facing,[mps] Move polygamma to special_math.h. (#146253),aten/src/ATen/native/mps/kernels/Gamma.metal c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/146253,dcci,Skylion007,malfet,,
3a67c0e48d2,skip,not user facing,[inductor] Finish typing common.py (#146225),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_grouped_gemm_template.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/146225,jansel,Skylion007,,,
d89c7ea4019,skip,not user facing,add WaitCounter type interface and get rid of type errors (#146175),torch/_C/_monitor.pyi torch/monitor/__init__.py,https://github.com/pytorch/pytorch/pull/146175,burak-turk,Skylion007,andriigrynenko,,
7854299b27d,inductor,not user facing,[mps/inductor] Implement support for polygamma(). (#146259),c10/metal/special_math.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146259,dcci,jansel,,,
529eb8d5582,dynamo,not user facing,[dynamo] Add return to python_type (#146258),test/dynamo/test_repros.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/146258,anijain2305,jansel,,,
31fb691782c,dynamo,not user facing,[dynamo] Graph break on tensor.retain_grad (#146214),test/dynamo/test_misc.py test/inductor_expected_failures/TestModuleCPU.test_non_contiguous_tensors_nn_KLDivLoss_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_non_contiguous_tensors_nn_KLDivLoss_cpu_float64 test/inductor_expected_failures/TestModuleCUDA.test_cpu_gpu_parity_nn_CTCLoss_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_cpu_gpu_parity_nn_CTCLoss_cuda_float64 test/inductor_expected_failures/TestModuleCUDA.test_non_contiguous_tensors_nn_KLDivLoss_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_non_contiguous_tensors_nn_KLDivLoss_cuda_float64 torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/146214,anijain2305,jansel,,,
cef856faa93,dynamo,not user facing,[dynamo][enum] Trace through enum.py for enum construction (#146070),test/dynamo/test_misc.py torch/_dynamo/side_effects.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/146070,anijain2305,jansel,,,
8543d8395b0,distributed,Untopiced,[2/N] Enable ruff F841 on distributed tests (#146132),test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/distributed/_functional_collectives.py,https://github.com/pytorch/pytorch/pull/146132,cyyever,Skylion007,rec,,
a44a8a7d3a6,releng,not user facing,[audio hash update] update the pinned audio hash (#145988),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/145988,pytorchupdatebot,pytorchbot,,,
0144613e6ff,skip,Untopiced,move and fix logic to update unbacked bindings (#146115),test/export/test_export.py torch/_export/serde/serialize.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/146115,avikchaudhuri,pianpwk,,,
5d55a6585d5,mps,improvements,[MPS] lu factor ex implementation (#144651),aten/src/ATen/native/BatchLinearAlgebra.cpp aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py,https://github.com/pytorch/pytorch/pull/144651,Isalia20,malfet,,,
02fd4868d67,skip,not user facing,Fix unreachable code (#146262),aten/src/ATen/native/xnnpack/Linear.cpp,https://github.com/pytorch/pytorch/pull/146262,lancelotnd,Skylion007,,,
511d0dd5587,dynamo,not user facing,[Dynamo][Trace PyDispatcher] Support calling id function over class (#146269),test/dynamo/test_misc.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/146269,yanboliang,anijain2305,,,
1f21f699bae,skip,not user facing,[metal] Refactor digamma in preparation for moving it. (#146281),aten/src/ATen/native/mps/kernels/Gamma.metal,https://github.com/pytorch/pytorch/pull/146281,dcci,jansel,,,
d28fe3ed479,skip,not user facing,[metal] Move digamma to special_math.h (#146284),aten/src/ATen/native/mps/kernels/Gamma.metal c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/146284,dcci,jansel,,,
c0ec2e0a0dd,dynamo,not user facing,[dynamo][functions] Improve getattr on functions (#146075),test/dynamo/test_functions.py torch/_dynamo/source.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/146075,anijain2305,jansel,,,
d80eef7c6d2,inductor,not user facing,[inductor] Guard a member variable with a define. (#146278),torch/csrc/inductor/aoti_runtime/model_container.h,https://github.com/pytorch/pytorch/pull/146278,dcci,Skylion007,jansel,,
6ac8bc0cd2b,skip,not user facing,Remove unused import in tests (#146266),test/export/test_export.py test/export/test_hop.py test/export/test_swap.py test/export/test_unflatten.py,https://github.com/pytorch/pytorch/pull/146266,cyyever,Skylion007,,,
fa487571803,dynamo,not user facing,[dynamo] misc fixes for inspect (#146283),test/dynamo/test_repros.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/146283,anijain2305,jansel,,,
7b512095ef3,dataloader_frontend,not user facing,Enable some tests on MacOS (#146268),test/test_dataloader.py,https://github.com/pytorch/pytorch/pull/146268,cyyever,Skylion007,malfet,,
1580f47bf4c,export,Untopiced,[export][ez] Fix generated header file. (#146208),torch/_export/serde/schema_check.py torch/csrc/utils/generated_serialization_types.h,https://github.com/pytorch/pytorch/pull/146208,zhxchen17,yiming0416,,,
e3643e1e0e9,mps,Untopiced,[MPS] Add linalg det and fix lu factor for non contiguous tensors (#146279),aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py,https://github.com/pytorch/pytorch/pull/146279,Isalia20,malfet,,,
08b14936aea,dynamo,Untopiced,Disable has_relational_guards check for dict_tag optimization for now (#146232),torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/146232,isuruf,anijain2305,,,
550441a87b4,skip,not user facing,Update slow tests (#146301),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/146301,pytorchupdatebot,pytorchbot,,,
01554c7b5a2,dynamo,not user facing,fix incorrect literal strings / accidental tuples (#146037),test/dynamo/test_misc.py test/export/test_functionalized_assertions.py test/export/test_tree_utils.py test/xpu/test_gemm.py torch/_dynamo/variables/tensor.py torch/_inductor/cudagraph_trees.py torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/146037,haampie,Skylion007,,,
c0979d72b5f,skip,Untopiced,"Revert ""[hop][inductor] track the dependency on unbacked symbols correctly with constant_args for hops (#143456)""",test/inductor/test_aot_inductor.py test/inductor/test_control_flow.py torch/_inductor/ir.py,,,,,,
041e08f9dc9,export,Untopiced,Add buffers to parameterizaiton rule (#145991),test/export/test_export.py torch/_functorch/_aot_autograd/subclass_parametrization.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/145991,tugsbayasgalan,bdhirsh,,,
64fc9ff09ca,skip,Untopiced,"Revert ""[ONNX] Create deprecation warning on dynamo_export (#146003)""",test/onnx/test_pytorch_onnx_no_runtime.py torch/onnx/__init__.py torch/onnx/_deprecation.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_compat.py torch/onnx/utils.py torch/onnx/verification.py,,,,,,
57b1fc35f69,dynamo,not user facing,[dynamo] Disable compiling on elementwise_type_promotion_wrapper (#146219),torch/_prims_common/wrappers.py,https://github.com/pytorch/pytorch/pull/146219,anijain2305,zou3519,,,
1138d0c4f68,skip,not user facing,[hop] enable while_loop return torch.ones with unbacked symbol expression. (#146194),test/functorch/test_control_flow.py,https://github.com/pytorch/pytorch/pull/146194,ydwu4,zou3519,,,
674e0b668a6,skip,not user facing,Add non-strict export while_loop test back (#146195),test/functorch/test_control_flow.py,https://github.com/pytorch/pytorch/pull/146195,ydwu4,zou3519,,,
f2371727682,build_frontend,bug fixes,Fix not inlining functions used in metal files (#146316),c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/146316,Isalia20,atalman,malfet,,
e67ce674980,inductor,not user facing,[cutlass backend] update try_import_cutlass to accomodate for pip install (#145891),torch/_inductor/codegen/cuda/cutlass_utils.py,https://github.com/pytorch/pytorch/pull/145891,henrylhtsang,Chillee,ColinPeppler,,
11f69808c64,skip,not user facing,"[aoti] Assign proxy call args by name, and support default values. (#146263)",test/inductor/test_aot_inductor.py torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.h,https://github.com/pytorch/pytorch/pull/146263,zhxchen17,angelayi,,,
1d4adf4e1fe,dynamo,not user facing,[dynamo] log recompile reason to dynamo_compile (#146117),test/dynamo/test_utils.py torch/_dynamo/convert_frame.py torch/_dynamo/guards.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/146117,xmfan,bobrenjc93,,,
fd73ae20684,skip,not user facing,[Utilization] Convert timestamp to str for datetime64  (#145985),tools/stats/monitor.py tools/stats/upload_utilization_stats/test_upload_utilization_stats.py tools/stats/upload_utilization_stats/upload_utilization_stats.py tools/stats/utilization_stats_lib.py,https://github.com/pytorch/pytorch/pull/145985,yangw-dev,huydhn,,,
bd8d7b1b74b,dynamo,not user facing,[Dynamo][Trace PyDispatcher] Remove disable from HigherOrderOperator.__call__ (#146270),test/dynamo/test_higher_order_ops.py torch/_ops.py,https://github.com/pytorch/pytorch/pull/146270,yanboliang,zou3519,,,
15e12d5ec39,dynamo,not user facing,[Trace PyDispatcher] Support temporarily_pop_interpreter_stack ctx manager (#146271),torch/_dynamo/variables/__init__.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/torch.py,https://github.com/pytorch/pytorch/pull/146271,yanboliang,zou3519,,,
00dc5b10f69,skip,Untopiced,"Revert ""[Environment Variable][7/N] Use thread-safe getenv functions (#140211)""",aten/src/ATen/core/Vitals.cpp aten/src/ATen/cuda/CublasHandlePool.cpp aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/mps/MPSAllocator.mm aten/src/ATen/mps/MPSFallback.mm aten/src/ATen/mps/MPSProfiler.mm aten/src/ATen/native/cudnn/Conv_v8.cpp aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/operations/LinearAlgebra.mm torch/csrc/autograd/engine.cpp torch/csrc/cuda/nccl.cpp torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/init.cpp torch/csrc/jit/codegen/fuser/compiler.cpp torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp,,,,,,
0da07a6d1d3,dynamo,not user facing,[dynamo][skip-function] Add missing unimplemented line (#146322),torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/146322,anijain2305,StrongerXi,jansel,mlazos,
d69c181d77e,fx,not user facing,log out partial fx graph when guard on data dependent during non stirct tracing (#146298),torch/fx/_symbolic_trace.py,https://github.com/pytorch/pytorch/pull/146298,bobrenjc93,bdhirsh,,,
178531c95ee,onnx,improvements,[ONNX] torch.onnx.export(dynamo=True) changes optimization to default (#146187),test/onnx/exporter/test_api.py torch/onnx/__init__.py torch/onnx/_internal/_exporter_legacy.py,https://github.com/pytorch/pytorch/pull/146187,titaiwangms,justinchuby,,,
0463cb6ca5e,inductor,not user facing,[mps/inductor] Add support for digamma(). (#146292),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146292,dcci,jansel,malfet,,
35af193408c,inductor,not user facing,[easy] Add type annotation for autotune_num_choices_displayed (#146323),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/146323,henrylhtsang,ColinPeppler,,,
0bc036a9e98,skip,Untopiced,use copy2d in h2d/d2h copy when possible (#146256),aten/src/ATen/native/cuda/Copy.cu c10/cuda/CUDAFunctions.h test/test_cuda.py,https://github.com/pytorch/pytorch/pull/146256,ngimel,eqy,malfet,,
18380ab8777,skip,not user facing,[inductor] Refactor CSEProxy into global scope (#146226),torch/_inductor/codegen/common.py,https://github.com/pytorch/pytorch/pull/146226,jansel,shunting314,,,
204be4e0a2e,skip,not user facing,[inductor] Refactor op handlers part 1 (#146235),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/inductor/test_op_completeness.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/triton.py torch/_inductor/dtype_propagation.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/146235,jansel,shunting314,,,
3aeccf2a285,distributed,not user facing,DeepSpeed github repo move sync (#146320),torch/distributed/tensor/README.md,https://github.com/pytorch/pytorch/pull/146320,stas00,awgu,,,
2f40f789daf,skip,Untopiced,"Revert ""[inductor] Refactor op handlers part 1 (#146235)""",benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/inductor/test_op_completeness.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/triton.py torch/_inductor/dtype_propagation.py torch/_inductor/ops_handler.py,,,,,,
cf6c5b8fa84,skip,not user facing,[mps/inductor] Adjust more tests that expect float64 as input. (#146366),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/146366,dcci,malfet,,,
4168982dadb,releng,not user facing,PEP585: .github release triggers (#145708),.github/scripts/generate_docker_release_matrix.py,https://github.com/pytorch/pytorch/pull/145708,aorenste,malfet,,,
0e49f35e3d6,fx,not user facing,Integrate sympy expression provenance logging with structured logs (#145848),torch/_logging/_internal.py torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/145848,bobrenjc93,angelayi,,,
3dcbd04d1d9,inductor,not user facing,[cutlass backend] Add instantiation level for generating configs (#146230),torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/146230,henrylhtsang,Chillee,mlazos,,
71179772cdf,inductor,not user facing,[MPSInductor] Prep change for reduction support (#146369),torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146369,malfet,dcci,jansel,,
5451c9b7c9e,inductor,not user facing,[MPSInductor] Add support for any reduction (#146370),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146370,malfet,dcci,jansel,,
f397c72697b,quantization,Untopiced,Remove NOLINTNEXTLINE (#146238),aten/src/ATen/native/TensorShape.cpp aten/src/ATen/native/cpu/layer_norm_kernel.cpp aten/src/ATen/native/nested/NestedTensorBackward.cpp aten/src/ATen/native/quantized/cpu/TensorShape.cpp aten/src/ATen/native/quantized/cpu/fbgemm_utils.h aten/src/ATen/native/quantized/cpu/qsigmoid.cpp aten/src/ATen/test/tensor_interop_test.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/profiler/combined_traceback.cpp,https://github.com/pytorch/pytorch/pull/146238,cyyever,albanD,,,
0061eb5b70f,skip,Untopiced,"Revert ""[inductor] Refactor CSEProxy into global scope (#146226)""",torch/_inductor/codegen/common.py,,,,,,
ecbc725fad8,skip,Untopiced,"Revert ""[inductor] Finish typing common.py (#146225)""",torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_grouped_gemm_template.py torch/_inductor/ops_handler.py,,,,,,
d3c7e4bb9cb,skip,Untopiced,"Revert ""[inductor] Add typing to common.CSE (#145993)""",torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,,,,,,
7f796eb8b7e,skip,Untopiced,"Revert ""[inductor] Add typing to common.KernelArgs (#145916)""",torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py,,,,,,
762a05b3b3e,distributed,Untopiced,[DCP] Remove all-gather of state dict keys (#145998),test/distributed/checkpoint/test_save_load_api.py torch/distributed/checkpoint/state_dict_loader.py torch/distributed/checkpoint/utils.py,https://github.com/pytorch/pytorch/pull/145998,kwen2501,fegin,mhorowitz,,
5f538898507,dynamo,not user facing,[dynamo][builtin-skipfiles-cleanup] Remove inspect (#146116),test/test_serialization.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/146116,anijain2305,jansel,williamwen42,zou3519,
7997ecf8098,skip,not user facing,[BE] reduce log spew from test_triton_kernels.py (#145895),test/inductor/test_triton_kernels.py,https://github.com/pytorch/pytorch/pull/145895,davidberard98,nmacchioni,,,
a79d8f8ba42,cuda,not user facing,[ROCm] Tune 3d tensor sums when not using fastest dimension (#146170),aten/src/ATen/native/cuda/Reduce.cuh,https://github.com/pytorch/pytorch/pull/146170,doru1004,jeffdaily,,,
9756c7d7882,benchmark,devs,[benchmark] Remove ONNX (#146325),benchmarks/dynamo/common.py benchmarks/dynamo/runner.py scripts/onnx/test.sh,https://github.com/pytorch/pytorch/pull/146325,justinchuby,titaiwangms,,,
487400f47f8,dynamo,not user facing,[dynamo] Support functools.partial variables through inspect.signature (#146339),test/dynamo/test_dicts.py test/dynamo/test_functions.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/tensor.py torch/_functorch/vmap.py,https://github.com/pytorch/pytorch/pull/146339,anijain2305,jansel,,,
0c37c332da5,export,Untopiced,[export] Additionally save pytree namedtuple field names (#145956),test/export/test_serialize.py torch/_export/serde/export_schema.thrift torch/_export/serde/schema.py torch/_export/serde/schema.yaml torch/_export/serde/serialize.py torch/csrc/utils/generated_serialization_types.h torch/export/exported_program.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/145956,angelayi,zhxchen17,,,
425aca40a43,skip,not user facing,Fix random crash in PyPer (#146327),torch/csrc/profiler/standalone/execution_trace_observer.cpp,https://github.com/pytorch/pytorch/pull/146327,shengfukevin,sraikund16,,,
e68f5087d80,distributed,Untopiced,update _unsafe_set_version_counter to accept lists of tensors (#137921),test/inductor/test_distributed_patterns.py test/test_autograd.py torch/_C/_autograd.pyi torch/_dynamo/tensor_version_op.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/ctx_manager.py torch/autograd/grad_mode.py torch/csrc/autograd/init.cpp torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py,https://github.com/pytorch/pytorch/pull/137921,bdhirsh,albanD,awgu,,
71e35755252,dataloader_frontend,not user facing,Remove unactivated test (#146233),test/test_datapipe.py,https://github.com/pytorch/pytorch/pull/146233,cyyever,albanD,rec,,
1c16cf70c37,onnx,not user facing,Apply ruff fixes to tests (#146140),test/conftest.py test/dynamo/test_python_autograd.py test/functorch/discover_coverage.py test/onnx/internal/test_diagnostics.py test/test_fx_experimental.py test/test_overrides.py,https://github.com/pytorch/pytorch/pull/146140,cyyever,albanD,,,
54ceb7c5653,inductor,not user facing,[MPSInductor] Add support for `sum` reduction (#146380),c10/metal/reduction_utils.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146380,malfet,dcci,jansel,,
bb4bd5f00b3,inductor,not user facing,[Metal][BE] Fix the arguments of `polygamma` (#146382),aten/src/ATen/native/mps/kernels/Gamma.metal c10/metal/special_math.h torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146382,dcci,jansel,malfet,,
8444fe019a9,export,Untopiced,[export] Fix requires_grad deserialization (#146351),test/export/test_export.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/146351,angelayi,zhxchen17,,,
3f63f2bcedf,skip,not user facing,Use std::string_view in tests (#146120),test/edge/Evalue.h test/test_overrides.py,https://github.com/pytorch/pytorch/pull/146120,cyyever,albanD,,,
e0f22e54e8f,skip,not user facing,[ROCm][TunableOp] Support leading dimensions in TunableOp signature. (#146358),aten/src/ATen/cuda/tunable/GemmCommon.h,https://github.com/pytorch/pytorch/pull/146358,naromero77amd,jeffdaily,,,
106acf0eec8,skip,Untopiced,"Revert ""[aoti] Assign proxy call args by name, and support default values. (#146263)""",test/inductor/test_aot_inductor.py torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.h,,,,,,
bbe95341d92,inductor,not user facing,[MPSInductor] Implement `min` and `max` reductions (#146389),c10/metal/reduction_utils.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146389,malfet,dcci,jansel,,
5d81bc3696d,inductor,not user facing,[MPSInductor] Implement `prod` reduction (#146396),c10/metal/reduction_utils.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146396,malfet,dcci,,,
7a5239afd7a,skip,not user facing,[inductor] Add typing to common.KernelArgs (#145916),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py,https://github.com/pytorch/pytorch/pull/145916,jansel,yanboliang,,,
e9f6e273e7c,skip,not user facing,[inductor] Add typing to common.CSE (#145993),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/145993,jansel,yanboliang,,,
87a63a98864,skip,not user facing,Add `@nikitaved` to torch.linalg `CODEOWNERS/persons_of_interest` (#141803),CODEOWNERS docs/source/community/persons_of_interest.rst,https://github.com/pytorch/pytorch/pull/141803,nikitaved,albanD,,,
53759ccca8a,inductor,bug fixes,[AOTI] Fix an unaligned memory access issue in mm_template (#146293),test/inductor/test_aot_inductor.py torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/146293,desertfire,chenyang78,jansel,,
23fffb54d59,skip,not user facing,Use OrderedSet in _functorch/partitioners (#146102),.lintrunner.toml torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/146102,masnesral,oulgen,,,
157d81c2017,python_frontend,bug fixes,Move get accelerator to use build time flags when possible (#146098),aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/cuda/detail/CUDAHooks.h aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/xpu/detail/XPUHooks.cpp aten/src/ATen/xpu/detail/XPUHooks.h test/test_cuda.py,https://github.com/pytorch/pytorch/pull/146098,albanD,EikanWang,jeromean,malfet,
b0fe9755216,skip,not user facing,[hop][inductor] track the dependency on unbacked symbols correctly with constant_args for hops (#143456),test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_arrayref.py test/inductor/test_control_flow.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/143456,ydwu4,desertfire,,,
f38a2ea0d46,dynamo,not user facing,[Dynamo] Better unsupported message for Fake Tensor Exception (#146357),torch/_dynamo/output_graph.py,https://github.com/pytorch/pytorch/pull/146357,zou3519,williamwen42,,,
292af3cc894,distributed,Untopiced,[BE][Ez]: ISC001 Auto concatenate implicit one line strings (#146408),functorch/examples/dp_cifar10/cifar10_opacus.py functorch/examples/dp_cifar10/cifar10_transforms.py test/distributed/fsdp/test_fsdp_grad_acc.py test/jit/test_exception.py test/jit/test_tracer.py test/jit/test_union.py test/jit/test_union_pep604.py test/onnx/test_verification.py test/test_autograd.py test/test_xnnpack_integration.py test/torch_np/numpy_tests/core/test_numeric.py test/torch_np/numpy_tests/core/test_scalarmath.py tools/packaging/build_wheel.py torch/_dynamo/variables/higher_order_ops.py torch/_functorch/eager_transforms.py torch/_functorch/partitioners.py torch/_library/infer_schema.py torch/_tensor_str.py torch/distributed/_shard/sharded_tensor/utils.py torch/distributed/elastic/multiprocessing/errors/error_handler.py torch/distributed/elastic/timer/api.py torch/distributed/optim/utils.py torch/distributed/optim/zero_redundancy_optimizer.py torch/distributed/rpc/backend_registry.py torch/distributed/tensor/_sharding_prop.py torch/fx/experimental/symbolic_shapes.py torch/fx/passes/shape_prop.py torch/nn/modules/adaptive.py torch/nn/modules/module.py torch/nn/modules/normalization.py torch/onnx/_internal/fx/onnxfunction_dispatcher.py torch/optim/lbfgs.py torch/optim/optimizer.py torch/package/package_importer.py torch/utils/data/dataloader.py torch/utils/data/datapipes/iter/combining.py,https://github.com/pytorch/pytorch/pull/146408,Skylion007,janeyx99,justinchuby,,
b1663b31e11,skip,not user facing,[Metal][BE] Add `#pragma once` to all headers (#146423),c10/metal/special_math.h c10/metal/utils.h,https://github.com/pytorch/pytorch/pull/146423,malfet,Skylion007,dcci,,
7d60235aa66,skip,not user facing,[Metal] Small speedup for `sum`/`prod` (#146428),c10/metal/reduction_utils.h,https://github.com/pytorch/pytorch/pull/146428,malfet,Skylion007,dcci,,
8f861a8dfbb,skip,not user facing,[experimental] filter logs by subgraph (#146047),torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/146047,bobrenjc93,laithsakka,,,
c591ad0c030,dynamo,not user facing,dump partial fx graph to stderr when dynamo tracing fails with guard on data-dependent (#146296),torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/146296,bobrenjc93,zou3519,,,
3525b834f05,inductor,not user facing,[MPSInductor] Implement `argmax`/`argmin` (#146429),c10/metal/reduction_utils.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146429,malfet,dcci,,,
7f65a208848,export,not user facing,[BE]: Enable ruff SLOT checks (#146276),.github/scripts/pytest_caching_utils.py functorch/dim/reference.py pyproject.toml test/dynamo/test_functions.py test/torch_np/numpy_tests/core/test_indexing.py torch/_dynamo/polyfills/pytree.py torch/_export/serde/union.py torch/_utils.py torch/ao/quantization/fx/_equalize.py torch/ao/quantization/qconfig.py torch/distributed/distributed_c10d.py torch/fx/passes/infra/pass_base.py torch/nn/modules/module.py torch/testing/_internal/common_dtype.py torch/testing/_internal/jit_metaprogramming_utils.py torch/torch_version.py,https://github.com/pytorch/pytorch/pull/146276,Skylion007,aorenste,,,
d23e4f81090,skip,not user facing,use DTRACE_ENV_VAR as the trace logs directory of set (#146412),torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/146412,bobrenjc93,angelayi,,,
07b9fe0690c,dynamo,not user facing,[Trace PyDispatcher] Add CustomFunctionHigherOrderOperatorVariable (#146272),torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/146272,yanboliang,zou3519,,,
bc33d993acd,fx,not user facing,add support for capturing provenance of unary operations (#146413),torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/146413,bobrenjc93,angelayi,,,
4c5a9a5f949,skip,not user facing,[Testing] Reduce `test_exp` flakiness (#146436),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/146436,malfet,dcci,jansel,,
4e194bbfd65,distributed,not user facing,dynamo: fsdp throw unimplemented vs attribute error (#146188),test/distributed/test_dynamo_distributed.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/146188,c00w,jansel,,,
09b0dfdc90f,skip,not user facing,[metal] Add a missing cast to make the call to copysign unambiguous. (#146422),c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/146422,dcci,Samkm0084,Skylion007,,
7c0fe7a0456,inductor,not user facing,cpp_wrapper/aot_inductor: handle conjugation and negation dispatch keys (#145095),test/inductor/test_torchinductor.py torch/_inductor/ir.py torch/csrc/inductor/aoti_torch/utils.h torchgen/gen_aoti_c_shim.py,https://github.com/pytorch/pytorch/pull/145095,benjaminglass1,desertfire,,,
9873319a420,releng,not user facing,cpp_wrapper: fix set_.source_Tensor lowering (#145654),.ci/pytorch/test.sh test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h torchgen/aoti/fallback_ops.py,https://github.com/pytorch/pytorch/pull/145654,benjaminglass1,desertfire,,,
e2a029054de,releng,not user facing,cpp_wrapper: enable all CPU repro tests (#145655),.ci/pytorch/test.sh test/inductor/test_aot_inductor.py test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/graph.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/145655,benjaminglass1,desertfire,,,
317dae95fae,releng,not user facing,cpp_wrapper: fix CPU cpp_wrapper and max-autotune tests (#145683),.ci/pytorch/test.sh test/inductor/test_cpu_cpp_wrapper.py test/inductor/test_max_autotune.py torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/145683,benjaminglass1,desertfire,,,
aac0577796d,skip,not user facing,[TEST][Sparse] Force CUTLASS backend in TestSparseSemiStructuredCUTLASS (#146398),test/test_sparse_semi_structured.py,https://github.com/pytorch/pytorch/pull/146398,Aidyn-A,Skylion007,eqy,jcaip,
13e17aa1063,inductor,not user facing,Make the CUTLASS swizzle options configurable and default to 2. (#146088),torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/146088,masnesral,henrylhtsang,mlazos,,
7c8ec84dab7,skip,not user facing,[cutlass backend] fix bug for accuminator dtype (#146356),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cutlass_utils.py,https://github.com/pytorch/pytorch/pull/146356,henrylhtsang,Chillee,,,
5cac550ddfc,skip,not user facing,[inductor] Finish typing common.py (#146225),torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_grouped_gemm_template.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/146225,jansel,Skylion007,,,
ed03f9ca106,skip,not user facing,[inductor] Refactor CSEProxy into global scope (#146226),torch/_inductor/codegen/common.py,https://github.com/pytorch/pytorch/pull/146226,jansel,shunting314,,,
67be5953feb,skip,not user facing,[inductor] Refactor op handlers part 1 (#146235),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/inductor/test_op_completeness.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/triton.py torch/_inductor/dtype_propagation.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/146235,jansel,shunting314,,,
13f0436abdf,skip,not user facing,[inductor] Refactor op handlers part 2 (#146252),torch/_inductor/codegen/common.py torch/_inductor/dtype_propagation.py torch/_inductor/loop_body.py torch/_inductor/ops_handler.py torch/_inductor/select_algorithm.py torch/_inductor/subgraph_lowering.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/146252,jansel,yanboliang,,,
8e9bda8d895,skip,not user facing,[inductor] Refactor op handlers part 3 (#146254),torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/146254,jansel,shunting314,,,
7aced455c54,skip,not user facing,[inductor] Refactor op handlers part 4 (#146255),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/codegen/common.py torch/_inductor/codegen/triton.py torch/_inductor/dependencies.py torch/_inductor/index_propagation.py torch/_inductor/loop_body.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/146255,jansel,shunting314,,,
d3dd3eeb7f5,skip,not user facing,[inductor] Refactor op handlers part 5 (#146257),test/inductor/test_op_completeness.py test/test_sympy_utils.py torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/bounds.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/triton.py torch/_inductor/dependencies.py torch/_inductor/index_propagation.py torch/_inductor/loop_body.py torch/_inductor/ops_handler.py torch/_inductor/output_code.py torch/_inductor/subgraph_lowering.py torch/_inductor/virtualized.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/146257,jansel,shunting314,,,
b8a529cca18,skip,not user facing,[inductor] Minor compile time optimizations in DefaultHandler (#146282),benchmarks/dynamo/pr_time_benchmarks/benchmark_runner.sh torch/_inductor/codegen/common.py torch/_inductor/dtype_propagation.py torch/_inductor/loop_body.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/146282,jansel,shunting314,,,
7288950bcd4,skip,not user facing,[inductor] Refactor CaptureIndexing into global scope (#146297),torch/_inductor/loop_body.py,https://github.com/pytorch/pytorch/pull/146297,jansel,shunting314,,,
84ba9c6e784,skip,not user facing,[inductor] Pre-populate cache for simplify_with_ranges return value (#146373),torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/146373,jansel,shunting314,yanboliang,,
6e03f4f90eb,export,Untopiced,[export] Include metadata in FlatArgsAdapter (#146107),test/export/test_unflatten.py torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/146107,angelayi,pianpwk,,,
658e22d4956,skip,Untopiced,"Revert ""add support for capturing provenance of unary operations (#146413)""",torch/fx/experimental/sym_node.py,,,,,,
9d5bf38decd,inductor,not user facing,[cpp_builder] refactor to reduce libcudart_static logs (#146394),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/146394,henrylhtsang,benjaminglass1,chenyang78,,
8177fc4d332,distributed,not user facing,Make regex error catching compatible with Python 3.12+. (#145945),torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/145945,haifeng-jin,H-Huang,,,
616ac941752,dynamo,Untopiced,[Dynamo] Fix spammy optimizer warning (#146374),torch/_dynamo/variables/optimizer.py,https://github.com/pytorch/pytorch/pull/146374,mlazos,anijain2305,,,
0e060342b60,skip,not user facing,[triton] Update pin to tip of 3.2 release (#145867),.ci/docker/ci_commit_pins/triton.txt,https://github.com/pytorch/pytorch/pull/145867,bertmaher,Skylion007,exclamaforte,htyu,
52aaadf3792,skip,not user facing,[BE][Ez]: Enable ruff rule E731. use `def` instead of anonymous lambda  (#146410),pyproject.toml test/functorch/test_parsing.py,https://github.com/pytorch/pytorch/pull/146410,Skylion007,albanD,aorenste,,
001ad5bef52,skip,not user facing,[MPSInductor] Scope-down test_prod running in MPS (#146460),test/inductor/test_mps_basic.py,https://github.com/pytorch/pytorch/pull/146460,malfet,dcci,,,
9e45bc82e94,releng,not user facing,[aarch64] CUDA 12.8 aarch64 builds to nightly binaries (#146378),.ci/aarch64_linux/aarch64_ci_build.sh .ci/aarch64_linux/aarch64_wheel_ci_build.py .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/146378,tinglvv,atalman,,,
aafaf4016fe,mps,improvements,[MPS] Add error checking when dispatching kernel (#146458),aten/src/ATen/native/mps/OperationUtils.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/146458,malfet,dcci,,,
3c0d2bc262b,skip,Untopiced,"Revert ""[Testing] Reduce `test_exp` flakiness (#146436)""",test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,,,,,,
f35e60b21c8,skip,Untopiced,"Revert ""[cutlass backend] fix bug for accuminator dtype (#146356)""",test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cutlass_utils.py,,,,,,
c6ea4425e59,skip,not user facing,Enable some tests on Windows (#146243),test/test_testing.py test/test_torch.py test/test_unary_ufuncs.py,https://github.com/pytorch/pytorch/pull/146243,cyyever,albanD,,,
f242da41c7c,skip,Untopiced,"Revert ""move and fix logic to update unbacked bindings (#146115)""",test/export/test_export.py torch/_export/serde/serialize.py torch/export/_trace.py,,,,,,
eb832b7bcc7,export,Untopiced,[export] Fix draft-export logging (#146106),torch/export/_draft_export.py,https://github.com/pytorch/pytorch/pull/146106,angelayi,yiming0416,,,
93d98aca310,skip,not user facing,inductor: Don't throw an internal error when a nn.module is missing a attribute (#145122),test/dynamo/test_compile.py test/dynamo/test_modules.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/145122,c00w,jansel,,,
b0b3fe8bcf0,skip,not user facing,[inductor] use ftz variant of exp (#146216),test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/146216,shunting314,eellison,jansel,,
e5ea7e9cdc6,fx,not user facing,add support for capturing provenance of unary operations (#146413),torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/146413,bobrenjc93,angelayi,,,
6293d1446b6,cpp_frontend,Untopiced,[2/N] Remove NOLINT suppressions (#146402),torch/csrc/api/include/torch/nn/modules/batchnorm.h torch/csrc/api/include/torch/nn/modules/conv.h torch/csrc/api/include/torch/nn/modules/dropout.h torch/csrc/api/src/nn/modules/activation.cpp torch/csrc/api/src/nn/modules/embedding.cpp torch/csrc/api/src/nn/modules/linear.cpp torch/csrc/api/src/nn/modules/loss.cpp torch/csrc/api/src/nn/modules/normalization.cpp torch/csrc/api/src/nn/modules/pooling.cpp torch/csrc/api/src/nn/modules/rnn.cpp torch/csrc/api/src/nn/modules/transformer.cpp torch/csrc/jit/runtime/argument_spec.cpp,https://github.com/pytorch/pytorch/pull/146402,cyyever,soulitzer,,,
e20b0c82d17,dynamo,Untopiced,[ca] no longer require is_traceable annotations for c++ autograd functions (#146229),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/autograd/custom_function.h torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/146229,xmfan,jansel,zou3519,,
fd0cd6a08f7,skip,not user facing,[ROCm][TunableOp] Improve identification of fastest solution (#144942),aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/GemmRocblas.h aten/src/ATen/cuda/tunable/StreamTimer.cpp aten/src/ATen/cuda/tunable/StreamTimer.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/cuda/tunable/TunableOp.h,https://github.com/pytorch/pytorch/pull/144942,naromero77amd,jeffdaily,,,
8a2000fd423,mps,improvements,[MPS] Implement support for zeta (both eager and inductor). (#146465),aten/src/ATen/native/mps/kernels/BinaryKernel.metal aten/src/ATen/native/mps/operations/BinaryKernel.mm aten/src/ATen/native/native_functions.yaml test/inductor/test_mps_basic.py test/test_mps.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146465,dcci,malfet,,,
282d185ec19,skip,Untopiced,"Revert ""[inductor] use ftz variant of exp (#146216)""",test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py torch/_inductor/config.py,,,,,,
1bb977a2a49,inductor,not user facing,[auto_functionalized] Support `Tensor(a!)[]?` (#145400),test/inductor/test_torchinductor.py torch/_higher_order_ops/auto_functionalize.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/145400,zou3519,laithsakka,,,
cd6c0707a86,skip,not user facing,"[aoti] Assign proxy call args by name, and support default values. (#146263)",test/inductor/test_aot_inductor.py torch/csrc/inductor/aoti_torch/oss_proxy_executor.cpp torch/csrc/inductor/aoti_torch/oss_proxy_executor.h,https://github.com/pytorch/pytorch/pull/146263,zhxchen17,angelayi,,,
db22e9d5a29,intel,bug fixes,"Implement blend operation for float, double, int in VEC ATen backend for SVE (#146479)",aten/src/ATen/cpu/vec/sve/vec_double.h aten/src/ATen/cpu/vec/sve/vec_float.h aten/src/ATen/cpu/vec/sve/vec_int.h aten/src/ATen/test/vec_test_all_types.cpp,https://github.com/pytorch/pytorch/pull/146479,maajidkhann,malfet,,,
f55c0af37f7,inductor,Untopiced,[inductor] Support non-power-of-2 cooperative RSPLIT (#145689),test/inductor/test_cooperative_reductions.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/triton_helpers.py,https://github.com/pytorch/pytorch/pull/145689,jansel,eellison,,,
f27220e32af,skip,Untopiced,"Revert ""Move get accelerator to use build time flags when possible (#146098)""",aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/cuda/detail/CUDAHooks.h aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/xpu/detail/XPUHooks.cpp aten/src/ATen/xpu/detail/XPUHooks.h test/test_cuda.py,,,,,,
44248c44ebd,nn_frontend,Untopiced,[ROCm] miopen benchmark behavior now better aligns with cudnn (#145294),aten/src/ATen/Context.h aten/src/ATen/miopen/Descriptors.h aten/src/ATen/miopen/miopen-wrapper.h aten/src/ATen/native/miopen/Conv_miopen.cpp test/nn/test_convolution.py,https://github.com/pytorch/pytorch/pull/145294,jeffdaily,BrianHarrisonAMD,malfet,,
d2a2b9f8a74,fx,Untopiced,Fix constants with non-functional operators (#145593),test/export/test_export.py torch/_export/passes/lift_constants_pass.py torch/_export/utils.py torch/export/_trace.py torch/export/exported_program.py torch/fx/graph_module.py,https://github.com/pytorch/pytorch/pull/145593,tugsbayasgalan,avikchaudhuri,,,
6f7fda3f494,python_frontend,not user facing,Bump `nn.functional.conv3d` tolerances for `test_comprehensive` (#135719),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/135719,eqy,Chillee,albanD,,
9091096d6c4,distributed,not user facing,Refactoring Distributed test cases to be device agnostic [1/n] (#145222),test/distributed/_composable/test_replicate_with_compiler.py,https://github.com/pytorch/pytorch/pull/145222,AnantGulati,kwen2501,,,
dd349207c5d,skip,not user facing,Add check that envvar configs are boolean (#145454),test/test_utils_config_module.py torch/utils/_config_module.py,https://github.com/pytorch/pytorch/pull/145454,Raymo111,c00w,jamesjwu,,
9da376daa65,skip,not user facing,Add retain-output argument (#145921),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/145921,kfojcik-intel,jansel,,,
1f6b566d747,releng,not user facing,[ONNX] Bump onnx and onnxscript versions in CI (#146097),.ci/docker/common/install_onnx.sh .ci/docker/requirements-ci.txt torch/onnx/_internal/fx/fx_onnx_interpreter.py,https://github.com/pytorch/pytorch/pull/146097,justinchuby,malfet,,,
98b5d455fd5,composability,Untopiced,[opcheck] Improve error reporting; allow atol/rtol overrides (#146488),test/test_custom_ops.py torch/library.py torch/testing/_internal/optests/aot_autograd.py torch/testing/_internal/optests/generate_tests.py,https://github.com/pytorch/pytorch/pull/146488,zou3519,williamwen42,,,
4ee7d0de86e,distributed,Untopiced,Add generate_stage_to_rank_mapping utility (#146193),test/distributed/pipelining/schedule_registry.py test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/146193,H-Huang,wconstab,,,
9b6d6801317,distributed,Untopiced,Remove stage_index_to_group_rank from schedule (#146217),test/distributed/pipelining/schedule_registry.py test/distributed/pipelining/test_schedule.py test/distributed/pipelining/test_schedule_multiproc.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/schedules.py,https://github.com/pytorch/pytorch/pull/146217,H-Huang,wconstab,,,
97b64f2e5c4,releng,not user facing,Fix workflow for closing nonexistent disable issues (#146447),.github/scripts/close_nonexistent_disable_issues.py .github/workflows/close-nonexistent-disable-issues.yml,https://github.com/pytorch/pytorch/pull/146447,clee2000,huydhn,,,
8af31e30d7e,caffe2,not user facing,[Codemod][AddExplicitStrictExportArg] caffe2/torch (#146439),test/dynamo/test_export.py test/fx/test_fx_xform_observer.py test/inductor/test_aot_inductor.py test/test_fx.py,https://github.com/pytorch/pytorch/pull/146439,gmagogsfm,avikchaudhuri,,,
9555bfce88f,skip,Untopiced,"Revert ""[inductor] Pre-populate cache for simplify_with_ranges return value (#146373)""",torch/_inductor/sizevars.py,,,,,,
7dc5cfe2ad3,skip,Untopiced,"Revert ""[inductor] Refactor CaptureIndexing into global scope (#146297)""",torch/_inductor/loop_body.py,,,,,,
93e1e6e07cb,skip,Untopiced,"Revert ""[inductor] Minor compile time optimizations in DefaultHandler (#146282)""",benchmarks/dynamo/pr_time_benchmarks/benchmark_runner.sh torch/_inductor/codegen/common.py torch/_inductor/dtype_propagation.py torch/_inductor/loop_body.py torch/_inductor/ops_handler.py,,,,,,
49effa0deb4,skip,Untopiced,"Revert ""[inductor] Refactor op handlers part 5 (#146257)""",test/inductor/test_op_completeness.py test/test_sympy_utils.py torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/bounds.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/triton.py torch/_inductor/dependencies.py torch/_inductor/index_propagation.py torch/_inductor/loop_body.py torch/_inductor/ops_handler.py torch/_inductor/output_code.py torch/_inductor/subgraph_lowering.py torch/_inductor/virtualized.py torch/utils/_sympy/value_ranges.py,,,,,,
68304dba7ac,skip,Untopiced,"Revert ""[inductor] Refactor op handlers part 4 (#146255)""",benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/codegen/common.py torch/_inductor/codegen/triton.py torch/_inductor/dependencies.py torch/_inductor/index_propagation.py torch/_inductor/loop_body.py torch/_inductor/virtualized.py,,,,,,
72405b0c0f4,dynamo,Untopiced,[ca] refactor compile reasons and log to tlparse (#146386),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/146386,xmfan,jansel,,,
2001066c617,skip,Untopiced,"Revert ""[inductor] Refactor op handlers part 3 (#146254)""",torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/virtualized.py,,,,,,
54ef029532e,mps,not user facing,[BE][EZ][Metal] Mark constant inputs as constant (#146521),aten/src/ATen/native/mps/operations/BitwiseOps.mm,https://github.com/pytorch/pytorch/pull/146521,malfet,dcci,,,
c7087d6b14f,mps,not user facing,[BE][EZ][Metal] Do not pass tensor length as arg (#146522),aten/src/ATen/native/mps/operations/BitwiseOps.mm,https://github.com/pytorch/pytorch/pull/146522,malfet,dcci,,,
e0cf519adec,skip,Untopiced,"Revert ""[inductor] Refactor op handlers part 2 (#146252)""",torch/_inductor/codegen/common.py torch/_inductor/dtype_propagation.py torch/_inductor/loop_body.py torch/_inductor/ops_handler.py torch/_inductor/select_algorithm.py torch/_inductor/subgraph_lowering.py torch/_inductor/virtualized.py,,,,,,
495049860b1,skip,not user facing,[BE][Metal] Fix signed unsigned comparison warning (#146549),c10/metal/reduction_utils.h,https://github.com/pytorch/pytorch/pull/146549,malfet,dcci,,,
0dc03134d9e,mps,improvements,[MPS] linalg solve implementation (#146531),aten/src/ATen/native/BatchLinearAlgebra.cpp aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py,https://github.com/pytorch/pytorch/pull/146531,Isalia20,malfet,,,
6a985d8b2e0,skip,not user facing,Make `inductor_utils.requires_gpu` accept MPS (#145156),test/inductor/test_custom_lowering.py test/inductor/test_group_batch_fusion.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_config_overrides.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/testing/_internal/inductor_utils.py,https://github.com/pytorch/pytorch/pull/145156,malfet,Skylion007,dcci,jansel,
c5062cca98a,export,Untopiced,[export] make stack_trace optional in insert_custom_op_guards (#146438),torch/_export/passes/_node_metadata_hook.py torch/_export/passes/insert_custom_op_guards.py,https://github.com/pytorch/pytorch/pull/146438,pianpwk,angelayi,yiming0416,,
3a6a203b987,fx,Untopiced,[dynamic shapes][real tensor tracing] propagate unbacked hint when creating mod replacement (#146381),test/export/test_draft_export.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/146381,pianpwk,angelayi,,,
425804db2b5,jit,Untopiced,[torch] fix exception types in custom class magic setattr/getattr (#146516),test/jit/test_torchbind.py torch/csrc/jit/python/script_init.cpp,https://github.com/pytorch/pytorch/pull/146516,suo,zdevito,,,
389c5c0842d,dynamo,Untopiced,print out partial fx graph for all data-dependent errors (#146363),torch/_dynamo/symbolic_convert.py torch/fx/_symbolic_trace.py,https://github.com/pytorch/pytorch/pull/146363,bobrenjc93,angelayi,bdhirsh,,
e01a5e9e1e8,nested tensor_frontend,bug fixes,Small improvements to NJT matrix multiplies (#146405),torch/nested/_internal/ops.py torch/testing/_internal/opinfo/definitions/nested.py,https://github.com/pytorch/pytorch/pull/146405,michael-diggin,jbschlosser,soulitzer,,
1f8baf09ea5,skip,not user facing,[DTensor][Test] Create a simple unit test for tensordot (#146514),test/distributed/tensor/test_matrix_ops.py,https://github.com/pytorch/pytorch/pull/146514,wz337,XilunWu,tianyu-l,,
36c6e09528a,inductor,not user facing,[MPSInductor] Fix min/max for bfloat16 (#146552),c10/metal/utils.h test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146552,malfet,dcci,,,
5f0901e5734,skip,not user facing,[cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces (#145130),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDAContextLight.h aten/src/ATen/cuda/CublasHandlePool.cpp benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/145130,eqy,ngimel,,,
aab7925418b,skip,Untopiced,[dynamo] check for incompatible configs (#146513),test/dynamo/test_repros.py torch/_dynamo/config.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/146513,xmfan,williamwen42,,,
4a545eb85d6,nn_frontend,not user facing,Fix torch.nn.functional.one_hot param num_classes optional description (#146470),torch/nn/functional.py,https://github.com/pytorch/pytorch/pull/146470,zeshengzong,albanD,,,
bd7d4fb2b5f,skip,Untopiced,"Revert ""[DTensor][Test] Create a simple unit test for tensordot (#146514)""",test/distributed/tensor/test_matrix_ops.py,,,,,,
340cfe4f284,dynamo,not user facing,[dynamo][fbcode] Turn on inline_inbuilt_nn_modules (#145407),torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/145407,anijain2305,ezyang,jansel,,
1b79d476353,skip,Untopiced,"Revert ""[dynamo] check for incompatible configs (#146513)""",test/dynamo/test_repros.py torch/_dynamo/config.py torch/_dynamo/eval_frame.py,,,,,,
ed309b9156b,composability,Untopiced,Re-add stft option to align window for center = false (#146379),aten/src/ATen/native/SpectralOps.cpp aten/src/ATen/native/native_functions.yaml test/test_spectral_ops.py torch/_refs/__init__.py torch/_tensor.py torch/_tensor_docs.py torch/functional.py torch/onnx/symbolic_opset17.py torch/overrides.py,https://github.com/pytorch/pytorch/pull/146379,jackzhxng,iseeyuan,justinchuby,,
8a4dd763b87,skip,not user facing,[CCA] remove TODO for hardware_destructive_interference_size (#145591),c10/cuda/CUDACachingAllocator.cpp,https://github.com/pytorch/pytorch/pull/145591,1274085042,albanD,,,
6ff3383157b,profiler,improvements,Enable CUPTI on Windows (#141454),caffe2/CMakeLists.txt cmake/Dependencies.cmake test/profiler/test_profiler.py,https://github.com/pytorch/pytorch/pull/141454,taras-janea,malfet,,,
a14c780c4c0,dynamo,not user facing,[dynamo] fix dynamo_compile logging on RecompileLimitExceeded (#146544),test/dynamo/test_repros.py torch/_dynamo/convert_frame.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/146544,xmfan,masnesral,,,
46390e9a373,mps,Untopiced,[mps] Implement support for sinc() operator (inductor and eager). (#146539),aten/src/ATen/native/mps/kernels/UnaryKernel.metal aten/src/ATen/native/mps/operations/UnaryKernel.mm aten/src/ATen/native/native_functions.yaml c10/metal/special_math.h test/inductor/test_mps_basic.py test/test_mps.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146539,dcci,jansel,malfet,,
1090e58687c,mps,not user facing,[mps] Remove a stale comment. (#146619),aten/src/ATen/native/mps/operations/UnaryKernel.mm,https://github.com/pytorch/pytorch/pull/146619,dcci,Skylion007,malfet,,
e2e265e27b8,skip,not user facing,[dynamo] Use polyfill to implement comparison operators (#144485),benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_inference.csv test/dynamo/test_misc.py test/dynamo/test_repros.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/utils.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/144485,anijain2305,jansel,,,
7725d0ba12d,skip,not user facing,[METAL] inline bfloat min/max (#146588),c10/metal/utils.h,https://github.com/pytorch/pytorch/pull/146588,Isalia20,FFFrog,Skylion007,malfet,
49082f9dba3,releng,Untopiced,parallelize sort (#142391),cmake/Codegen.cmake,https://github.com/pytorch/pytorch/pull/142391,Ryo-not-rio,malfet,,,
15b1ac3e86d,functorch,Untopiced,Add torch.func.debug_unwrap (#146528),docs/source/func.api.rst test/functorch/test_eager_transforms.py torch/_functorch/eager_transforms.py torch/func/__init__.py,https://github.com/pytorch/pytorch/pull/146528,zou3519,Chillee,,,
99ddbb48025,dynamo,not user facing,[dynamo][fullgraph] Do not skip frame with fullgraph=True (#146527),test/distributed/tensor/test_dtensor_compile.py test/dynamo/test_functions.py test/dynamo/test_reconstruct.py test/dynamo/test_repros.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/146527,anijain2305,StrongerXi,zou3519,,
07b214402a5,cuda,not user facing,[CUDA][B200] Update the number of threads in `avg_pool2d` backward for SM 10.0 (#145669),aten/src/ATen/native/cuda/AveragePool2d.cu,https://github.com/pytorch/pytorch/pull/145669,eqy,nWEIdia,ngimel,,
9ee506bd938,skip,not user facing,[CUDA][cuBLAS] Add fp16 accumulate option to cuBLAS/cuBLASLt (#144441),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp docs/source/notes/cuda.rst test/test_cuda.py test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/backends/cuda/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/144441,eqy,Chillee,malfet,,
992388c1003,skip,not user facing,[inductor] use ftz variant of exp (#146216),test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/146216,shunting314,eellison,jansel,,
1b879fd0ea3,inductor,not user facing,[Inductor] Add a JIT Inductor unit test following #146293 (#146529),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/146529,desertfire,eellison,shunting314,,
5ecdc428b23,skip,not user facing,[while_loop][inductor] support sym expression as cond_fn output (#146222),test/inductor/test_aot_inductor.py test/inductor/test_control_flow.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/146222,ydwu4,desertfire,,,
6220c64aeaf,distributed,not user facing,[1/N][cp][example] flex attention in context parallel (forward pass) (#145896),torch/distributed/tensor/examples/flex_attention_cp.py,https://github.com/pytorch/pytorch/pull/145896,XilunWu,Skylion007,fegin,,
5cc1b54a91e,distributed,not user facing,[2/N][cp][example] flex attention in context parallel (backward pass) (#146397),torch/distributed/tensor/examples/flex_attention_cp.py,https://github.com/pytorch/pytorch/pull/146397,XilunWu,fegin,,,
44b69b80c21,linalg_frontend,not user facing,[ROCm][TunableOp] Future proof TunableOp unit test. (#146548),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/146548,naromero77amd,jeffdaily,,,
3008368b126,releng,not user facing,Honor Dr.CI classification results on auto commit hash update (#146337),.github/merge_rules.yaml,https://github.com/pytorch/pytorch/pull/146337,huydhn,clee2000,,,
99dd8466725,jit,Untopiced,[torch] fix builds for older pybind (#146630),torch/csrc/jit/python/script_init.cpp,https://github.com/pytorch/pytorch/pull/146630,suo,colin2328,ngimel,,
0c81b398aba,skip,not user facing,[BE][Ez]: Enable some additional pylint ruff warnings (#146609),pyproject.toml,https://github.com/pytorch/pytorch/pull/146609,Skylion007,aorenste,,,
2834fe5e938,inductor,not user facing,[inductor] Fix test error test_force_cutlass_backend_aoti_cexpr_codegen (#146564),torch/_inductor/codegen/cuda/cuda_kernel.py,https://github.com/pytorch/pytorch/pull/146564,jansel,yanboliang,,,
7bd7f735d4b,skip,not user facing,[CUDA][SDPA] Compute reference in `test_triton_scaled_dot_product_attention_block_size_16_cuda_float32` in `float64` (#146461),test/test_sparse_csr.py,https://github.com/pytorch/pytorch/pull/146461,eqy,drisspg,,,
e13a544b542,skip,not user facing,fix tf32 issue in test_inductor_freezing.py unit tests (#146444),test/inductor/test_inductor_freezing.py,https://github.com/pytorch/pytorch/pull/146444,Fuzzkatt,eellison,eqy,jansel,
3f5ed056888,skip,not user facing,[Windows][ROCm] Fix c10 hip tests (#146599),c10/cuda/test/CMakeLists.txt cmake/Dependencies.cmake cmake/public/LoadHIP.cmake,https://github.com/pytorch/pytorch/pull/146599,m-gallus,jeffdaily,,,
0d8fc00e0ae,cuda,not user facing,[ROCm][Windows] Fix isnan integer overload errors on MS STL (#146605),aten/src/ATen/native/cuda/SummaryOps.cu,https://github.com/pytorch/pytorch/pull/146605,m-gallus,jeffdaily,,,
3379c65de6c,skip,not user facing,[ROCm][Windows] Fix unrecognized _BitScanReverse intrinsic (#146606),c10/util/Float8_e4m3fn.h,https://github.com/pytorch/pytorch/pull/146606,m-gallus,jeffdaily,,,
9ea1823f966,jit,not user facing,[ROCm][Windows] Remove external linkage from an anonymous namespace (#146607),torch/csrc/jit/runtime/static/native_ops.cpp,https://github.com/pytorch/pytorch/pull/146607,m-gallus,jeffdaily,,,
624d94bdb8a,mps,improvements,[MPS] Extend `torch.special.sinc` to complex (#146648),aten/src/ATen/native/mps/kernels/UnaryKernel.metal c10/metal/special_math.h test/test_mps.py,https://github.com/pytorch/pytorch/pull/146648,malfet,dcci,,,
fa0592b568c,distributed,Untopiced,Remove some NOLINT (#146610),aten/src/ATen/native/ao_sparse/quantized/cpu/qlinear.cpp aten/src/ATen/test/legacy_vmap_test.cpp torch/csrc/autograd/python_engine.cpp torch/csrc/distributed/c10d/GroupRegistry.cpp torch/csrc/distributed/c10d/reducer.cpp,https://github.com/pytorch/pytorch/pull/146610,cyyever,Skylion007,malfet,,
41e6d189a39,onnx,deprecation,[ONNX] Create deprecation warning on dynamo_export (#146425),test/onnx/test_pytorch_onnx_no_runtime.py torch/onnx/__init__.py torch/onnx/_deprecation.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_compat.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/146425,justinchuby,atalman,titaiwangms,,
ecf44d10026,dataloader_frontend,not user facing,Fixed a typo in dataset.py (#146600),torch/utils/data/dataset.py,https://github.com/pytorch/pytorch/pull/146600,Zhou32,Skylion007,,,
bc40ccf6aab,skip,not user facing,[BE]: Inline special functions for MPS (#146627),c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/146627,Skylion007,dcci,,,
1c872803cb4,fx,Untopiced,[export][dynamic shapes] log provenance for locals & symbols for non-strict (#143378),torch/export/_draft_export.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/143378,pianpwk,avikchaudhuri,bobrenjc93,,
f6bd20e8a25,skip,not user facing,Enable TemporaryFileName tests on Windows (#146311),test/test_autocast.py test/test_content_store.py test/test_jit.py test/test_serialization.py,https://github.com/pytorch/pytorch/pull/146311,cyyever,albanD,,,
71e8a2bda40,inductor,not user facing,"Expand inductor codegen dtype asserts, fix scan (#146067)",torch/_inductor/codegen/common.py torch/_inductor/codegen/triton.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/146067,eellison,jansel,shunting314,,
002accfb8d2,inductor,not user facing,Check meta strides for expanded dims in effn_attn_bias (#146054),test/inductor/test_cuda_repro.py test/inductor_expected_failures/TestCommonCPU.test_out__refs_bitwise_not_cpu_int64 test/inductor_expected_failures/TestCommonCUDA.test_out__refs_bitwise_not_cuda_int64 torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/146054,eellison,shunting314,,,
5d7532140f3,skip,not user facing,[CUDA][CUDA Graphs] Fix debug mode warning message (#145996),aten/src/ATen/cuda/CUDAGraph.cpp,https://github.com/pytorch/pytorch/pull/145996,eqy,eellison,ptrblck,,
076717785c1,skip,Untopiced,"Revert ""[while_loop][inductor] support sym expression as cond_fn output (#146222)""",test/inductor/test_aot_inductor.py test/inductor/test_control_flow.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py,,,,,,
25aa7ca62d7,distributed,Untopiced,Cleanup CallOnce.h (#146700),torch/csrc/autograd/engine.h torch/csrc/cuda/Module.cpp torch/csrc/distributed/c10d/ProcessGroupMPI.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupUCC.cpp torch/csrc/jit/codegen/cuda/interface.cpp torch/csrc/lazy/core/internal_ops/ltc_ops.h torch/csrc/xpu/Module.cpp,https://github.com/pytorch/pytorch/pull/146700,cyyever,albanD,,,
fa0956951cb,dynamo,not user facing,[dynamo] Remove the suggestion to use suppress_errors on compiler error (#146553),torch/_dynamo/exc.py,https://github.com/pytorch/pytorch/pull/146553,anijain2305,jansel,zou3519,,
ee45ea599d7,dynamo,not user facing,[dynamo] Actionable message on recompilations for fullgraph=True (#146550),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/146550,anijain2305,StrongerXi,zou3519,,
f138b18d183,skip,not user facing,[inductor/profiler] add kernel kwargs instrumentation (#145573),test/inductor/test_profiler.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/145573,briancoutinho,davidberard98,jansel,,
76c8a2dc481,dynamo,not user facing,"Fix get_top() to return the base level event of the stack, not the most recently started event (#146649)",torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/146649,jamesjwu,anijain2305,masnesral,,
68631f6e87f,distributed,Untopiced,PyWork: preserve Python reference counting when used in functional collectives (#146376),test/distributed/test_c10d_functional_native.py torch/csrc/distributed/c10d/PyProcessGroup.hpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/146376,d4l3k,yifuwang,,,
23af9dde4dd,distributed,not user facing,distributed/serialization: add experimental streaming torch.save/load methods (#146555),test/distributed/test_serialization.py torch/distributed/_serialization.py,https://github.com/pytorch/pytorch/pull/146555,d4l3k,fegin,mikaylagawarecki,,
9e27d36e2b2,skip,Untopiced,windows Magma build for cu128 (#146653),.ci/pytorch/windows/cuda128.bat .ci/pytorch/windows/internal/cuda_install.bat .github/scripts/windows/build_magma.bat .github/scripts/windows/cuda_install.bat .github/workflows/build-magma-windows.yml,https://github.com/pytorch/pytorch/pull/146653,tinglvv,atalman,,,
b60f630de80,inductor,not user facing,"fuzzer: disable ""fail_on_recompile_limit_hit"" and ""suppress_errors"" (#146650)",torch/_inductor/fuzzer.py,https://github.com/pytorch/pytorch/pull/146650,exclamaforte,xmfan,,,
bc0191802f1,inductor,not user facing,[inductor] add size-asserts for fallback ops (#145904),test/inductor/test_control_flow.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_opinfo.py test/test_ops.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/csrc/dynamo/guards.cpp torch/testing/_internal/common_utils.py torch/testing/_internal/inductor_utils.py,https://github.com/pytorch/pytorch/pull/145904,shunting314,jansel,,,
f17109bd96d,skip,Untopiced,"Revert ""windows Magma build for cu128 (#146653)""",.ci/pytorch/windows/cuda128.bat .ci/pytorch/windows/internal/cuda_install.bat .github/scripts/windows/build_magma.bat .github/scripts/windows/cuda_install.bat .github/workflows/build-magma-windows.yml,,,,,,
206ad9f4ad0,skip,not user facing,"[cutlass backend] Set no fallback to aten, disabled a few broken tests, default to test on H100 (#146554)",test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/146554,henrylhtsang,chenyang78,,,
80a16966795,skip,Untopiced,"Revert ""[cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces (#145130)""",aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDAContextLight.h aten/src/ATen/cuda/CublasHandlePool.cpp benchmarks/dynamo/common.py,,,,,,
04ce02182b6,inductor,not user facing,[inductor] Use index_dtype (int32/int64 depending on size) for argmax accumulators (#146651),test/inductor/test_cooperative_reductions.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/146651,jansel,eellison,shunting314,,
579b9f2ed9e,inductor,not user facing,[inductor] Better exception error messages for cache_on_self (#146652),torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/146652,jansel,yanboliang,,,
91dfa829811,inductor,not user facing,[FlexAttention] Fix dynamic shapes in max-autotune (#146657),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/146657,drisspg,Chillee,yanboliang,,
908133f6820,skip,not user facing,[TreeSpec] Add custom comparision function (#146442),torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/146442,henryhu6,angelayi,,,
45d35f5f5af,skip,not user facing,Clean up op BC check list (#146577),test/forward_backward_compatibility/check_forward_backward_compatibility.py,https://github.com/pytorch/pytorch/pull/146577,houseroad,hl475,,,
103c8b44bcb,skip,Untopiced,move and fix logic to update unbacked bindings (#146115),test/export/test_export.py torch/_export/serde/serialize.py torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/146115,avikchaudhuri,pianpwk,,,
fa341284356,distributed,Untopiced,revert PTD's change that leads to signature mismatch of printNcclCommProxyTrace (#146453),torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp,https://github.com/pytorch/pytorch/pull/146453,dmwu,c-p-i-o,,,
dcac3c3e065,skip,not user facing,[MTIA] (2/n) Implement PyTorch APIs to query/reset device peak memory usage (#146659),torch/mtia/memory.py,https://github.com/pytorch/pytorch/pull/146659,chaos5958,nautsimon,,,
803661526eb,releng,not user facing,Update ET pin to 41e7ffa (#145831),.ci/docker/ci_commit_pins/executorch.txt .ci/docker/common/install_executorch.sh,https://github.com/pytorch/pytorch/pull/145831,GregoryComer,metascroy,,,
0ab67299c35,mps,improvements,[MPS] lu unpack (#146681),aten/src/ATen/native/mps/kernels/LinearAlgebra.metal aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py,https://github.com/pytorch/pytorch/pull/146681,Isalia20,malfet,,,
6cb2f737ee6,skip,not user facing,Enable Windows tests (#146666),test/test_functional_autograd_benchmark.py,https://github.com/pytorch/pytorch/pull/146666,cyyever,albanD,,,
9c78fb920d5,inductor,not user facing,Fix assertion failure in gemm template lowering (#146353),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_template.py,https://github.com/pytorch/pytorch/pull/146353,dmpots,frost-intel,hl475,leslie-fang-intel,
69feef5a947,nn_frontend,bug fixes,Fix broken meta function for flex-attention backwards (#146563),test/inductor/test_flex_attention.py torch/_higher_order_ops/flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/146563,drisspg,Chillee,,,
2328dcccb9e,inductor,not user facing,[MPSInductor] Implement Welford reduction  (#146703),c10/metal/reduction_utils.h test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/146703,malfet,dcci,jansel,,
63c2909ae3e,onnx,deprecation,[ONNX] Adjust and add deprecation messages (#146639),torch/onnx/__init__.py torch/onnx/_deprecation.py torch/onnx/_exporter_states.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/146639,justinchuby,titaiwangms,,,
a3ca5c7f4ed,python_frontend,Untopiced,remove incorrect warnings from min/max documentation (#146725),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/146725,ngimel,malfet,wdvr,,
46e83bb6377,skip,not user facing,Fix linter F821 error (#146665),test/test_sort_and_select.py test/test_transformers.py torch/testing/_internal/jit_utils.py,https://github.com/pytorch/pytorch/pull/146665,cyyever,Skylion007,,,
71498aeae3b,skip,not user facing,[inductor] Refactor op handlers part 2 (#146252),torch/_inductor/codegen/common.py torch/_inductor/dtype_propagation.py torch/_inductor/loop_body.py torch/_inductor/ops_handler.py torch/_inductor/select_algorithm.py torch/_inductor/subgraph_lowering.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/146252,jansel,yanboliang,,,
0e31e5932b5,skip,not user facing,[inductor] Refactor op handlers part 3 (#146254),torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/146254,jansel,shunting314,,,
403db2faee8,skip,not user facing,[inductor] Refactor op handlers part 4 (#146255),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/codegen/common.py torch/_inductor/codegen/triton.py torch/_inductor/dependencies.py torch/_inductor/index_propagation.py torch/_inductor/loop_body.py torch/_inductor/virtualized.py,https://github.com/pytorch/pytorch/pull/146255,jansel,shunting314,,,
06604c4ec1e,skip,not user facing,[inductor] Refactor op handlers part 5 (#146257),test/inductor/test_op_completeness.py test/test_sympy_utils.py torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/bounds.py torch/_inductor/codegen/common.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/triton.py torch/_inductor/dependencies.py torch/_inductor/index_propagation.py torch/_inductor/loop_body.py torch/_inductor/ops_handler.py torch/_inductor/output_code.py torch/_inductor/subgraph_lowering.py torch/_inductor/virtualized.py torch/utils/_sympy/value_ranges.py,https://github.com/pytorch/pytorch/pull/146257,jansel,shunting314,,,
d35f6b23393,skip,not user facing,[inductor] Minor compile time optimizations in DefaultHandler (#146282),benchmarks/dynamo/pr_time_benchmarks/benchmark_runner.sh torch/_inductor/codegen/common.py torch/_inductor/dtype_propagation.py torch/_inductor/loop_body.py torch/_inductor/ops_handler.py,https://github.com/pytorch/pytorch/pull/146282,jansel,shunting314,,,
c098385cb3e,skip,not user facing,[inductor] Refactor CaptureIndexing into global scope (#146297),torch/_inductor/loop_body.py,https://github.com/pytorch/pytorch/pull/146297,jansel,shunting314,,,
eee5622b98d,skip,not user facing,[inductor] Pre-populate cache for simplify_with_ranges return value (#146373),torch/_inductor/sizevars.py,https://github.com/pytorch/pytorch/pull/146373,jansel,shunting314,yanboliang,,
a1bfb39a31a,inductor,not user facing,[Inductor] Expand Identity ops prior to block pattern matching (#146000),test/inductor/test_block_analysis.py test/test_sympy_utils.py torch/_inductor/codegen/block_analysis.py torch/_inductor/codegen/triton.py torch/testing/_internal/inductor_utils.py torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/146000,blaine-rister,eellison,jansel,,
92b7e610abf,dynamo,Untopiced,[Inductor changes] Invoke Quant (#139102),test/dynamo/test_higher_order_ops.py test/higher_order_ops/test_invoke_quant.py torch/_dynamo/trace_rules.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/_invoke_quant.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/lowering.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/139102,eellison,Chillee,,,
ade8fee5120,build_frontend,not user facing,Use c10 version of half/bfloat16 in executorch (#144111),buckbuild.bzl c10/build.bzl c10/core/build.bzl c10/util/build.bzl,https://github.com/pytorch/pytorch/pull/144111,swolchok,malfet,,,
8603a1c8703,dynamo,not user facing,Suport generators (#141055),test/dynamo/test_ctx_manager.py test/dynamo/test_exceptions.py test/dynamo/test_generator.py torch/_dynamo/codegen.py torch/_dynamo/config.py torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/trace_rules.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/141055,guilhermeleobas,zou3519,,,
d7988311674,dynamo,Untopiced,Implement `generator.__iter__()` (#144421),test/dynamo/test_generator.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/144421,guilhermeleobas,zou3519,,,
ca9b16e0700,dynamo,Untopiced,Implement `generator.send(..)` (#144422),test/dynamo/test_generator.py torch/_dynamo/exc.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/144422,guilhermeleobas,zou3519,,,
8ee095f7c1e,dynamo,Untopiced,Implement `generator.close()` (#144423),test/dynamo/test_generator.py torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/144423,guilhermeleobas,zou3519,,,
53ab82d8f5c,dynamo,Untopiced,Implement `generator.throw(exception)` (#144424),test/dynamo/test_generator.py torch/_dynamo/exc.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/144424,guilhermeleobas,zou3519,,,
68cfd36c111,dynamo,Untopiced,Add `CLEANUP_THROW` bytecode (#144420),test/dynamo/test_generator.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/144420,guilhermeleobas,zou3519,,,
580a3056818,dynamo,Untopiced,Raise MutationError if there are side effects when returning generator (#145223),test/dynamo/test_generator.py torch/_dynamo/exc.py torch/_dynamo/output_graph.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/145223,guilhermeleobas,zou3519,,,
6a9a02acbe3,dynamo,not user facing,Set `enable_faithful_generator_behavior` flag to True (#142513),test/dynamo/test_ctx_manager.py test/dynamo/test_misc.py test/dynamo/test_repros.py test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm1d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm1d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm2d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm2d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm3d_eval_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_BatchNorm3d_train_mode_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Bilinear_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Conv1d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Conv2d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Conv3d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_ConvTranspose1d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_ConvTranspose2d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_ConvTranspose3d_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_GRUCell_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_GroupNorm_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_LSTMCell_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_LayerNorm_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_Linear_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCPU.test_to_nn_RNNCell_swap_True_set_grad_True_cpu_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm1d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm1d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm2d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm2d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm3d_eval_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_BatchNorm3d_train_mode_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Bilinear_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Conv1d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Conv2d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Conv3d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_ConvTranspose1d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_ConvTranspose2d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_ConvTranspose3d_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_GRUCell_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_GroupNorm_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_LSTMCell_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_LayerNorm_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_Linear_swap_True_set_grad_True_cuda_float32 test/inductor_expected_failures/TestModuleCUDA.test_to_nn_RNNCell_swap_True_set_grad_True_cuda_float32 torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/142513,guilhermeleobas,zou3519,,,
0e83e7d56e6,skip,not user facing,[EZ] Add logic to build Metal shader with debug info (#146768),tools/build_with_debinfo.py,https://github.com/pytorch/pytorch/pull/146768,malfet,dcci,,,
91c4bf39d39,mps,not user facing,[mps] Add a shader for spherical_bessel_j0. (#146771),c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/146771,dcci,malfet,,,
b133907d0ab,skip,not user facing,Update strided test to float32 (#146748),test/inductor/test_flex_attention.py,https://github.com/pytorch/pytorch/pull/146748,drisspg,BoyuanFeng,leijurv,,
2a55311773b,cuda,not user facing,[cuda] Simplify the sinc function a bit. (#146774),aten/src/ATen/native/cuda/Math.cuh,https://github.com/pytorch/pytorch/pull/146774,dcci,malfet,,,
298226f3582,skip,Untopiced,[dynamo] check for incompatible configs (#146513),test/dynamo/test_repros.py torch/_dynamo/config.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/146513,xmfan,williamwen42,,,
e8304f08fed,python_frontend,Untopiced,Fix torch.take_along_dim param type and default description (#146474),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/146474,zeshengzong,mikaylagawarecki,,,
387c993c3b0,dynamo,Untopiced,[ca] remove private API: _compiled_autograd_should_lift (#146720),torch/_functorch/_aot_autograd/runtime_wrappers.py torch/autograd/function.py torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h,https://github.com/pytorch/pytorch/pull/146720,xmfan,zou3519,,,
effc5452748,distributed,Untopiced,[DDP] Use NCCL allocated memory for gradient bucket (#146589),test/distributed/test_c10d_nccl.py torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/distributed/c10d/NCCLUtils.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/reducer.cpp,https://github.com/pytorch/pytorch/pull/146589,kwen2501,c-p-i-o,fduwjj,syed-ahmed,
c4d835fbab5,distributed,not user facing,[DTensor][conv] add DTensor convolution_backward op support for case where the input Tensor has requires_grad=False (#142278),test/distributed/tensor/test_convolution_ops.py torch/distributed/tensor/_ops/_conv_ops.py torch/distributed/tensor/_sharding_prop.py torch/distributed/tensor/_tp_conv.py,https://github.com/pytorch/pytorch/pull/142278,XilunWu,bdhirsh,,,
6667e5d786f,skip,not user facing,[dim order]  solve broken doc (#146641),torch/_tensor.py,https://github.com/pytorch/pytorch/pull/146641,Gasoonjia,Jack-Khuu,svekars,,
c88ae006923,skip,not user facing,fix: replace stderr with stdout for download messages in hub.py (#146475),torch/hub.py,https://github.com/pytorch/pytorch/pull/146475,yousoumar,mikaylagawarecki,,,
c24038025d5,skip,not user facing,[ROCm] Unskip std:bad_alloc failures (#146407),test/inductor/test_cpu_repro.py,https://github.com/pytorch/pytorch/pull/146407,jataylo,jeffdaily,,,
6f15a609d39,skip,not user facing,Test typing of arithmetic operators on Tensor (see #145838) (#146426),test/typing/fail/arithmetic_ops.py test/typing/pass/arithmetic_ops.py,https://github.com/pytorch/pytorch/pull/146426,rec,Skylion007,,,
d0e70c4fd33,skip,Untopiced,windows Magma build for cu128 (#146653),.ci/pytorch/windows/cuda128.bat .ci/pytorch/windows/internal/cuda_install.bat .github/scripts/windows/build_magma.bat .github/scripts/windows/cuda_install.bat .github/workflows/build-magma-windows.yml,https://github.com/pytorch/pytorch/pull/146653,tinglvv,atalman,,,
611ca163fd8,mps,improvements,[MPS] Add bilineard2d_aa implementation (#145526),aten/src/ATen/native/mps/kernels/UpSample.metal aten/src/ATen/native/mps/operations/UpSample.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py test/test_nn.py,https://github.com/pytorch/pytorch/pull/145526,malfet,dcci,,,
899066eedf8,dynamo,Untopiced,Fix round(...) with constants (#146495),test/dynamo/test_functions.py torch/_dynamo/variables/constant.py,https://github.com/pytorch/pytorch/pull/146495,guilhermeleobas,anijain2305,,,
a36c22f2eda,inductor,not user facing,"futher scheduler changes for invoke_quant: prologue low prec, (slightly) more aggressive fusion (#145104)",test/higher_order_ops/test_invoke_quant.py torch/_higher_order_ops/_invoke_quant.py torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/codegen/triton.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/graph.py torch/_inductor/lowering.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/145104,eellison,jansel,shunting314,,
1557b7bf9a8,skip,Untopiced,"Revert ""[ONNX] Adjust and add deprecation messages (#146639)""",torch/onnx/__init__.py torch/onnx/_deprecation.py torch/onnx/_exporter_states.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,,,,,,
6aa924af688,skip,Untopiced,"Revert ""[ONNX] Create deprecation warning on dynamo_export (#146425)""",test/onnx/test_pytorch_onnx_no_runtime.py torch/onnx/__init__.py torch/onnx/_deprecation.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_compat.py torch/onnx/utils.py torch/onnx/verification.py,,,,,,
68c9e22ef77,distributed,Untopiced,FSDP: avoid resetting version counter of all_gather_output in inference_mode (#146709),torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py,https://github.com/pytorch/pytorch/pull/146709,bdhirsh,awgu,,,
5f621c5879d,skip,not user facing,[MTIA] (3/n) Implement PyTorch APIs to query/reset device peak memory usage (#146710),aten/src/ATen/detail/MTIAHooksInterface.h torch/_C/__init__.pyi.in torch/csrc/mtia/Module.cpp,https://github.com/pytorch/pytorch/pull/146710,chaos5958,nautsimon,,,
dfe3b642826,mps,not user facing,[mps] Implement eager support for spherical_bessel_j0 (#146818),aten/src/ATen/native/mps/kernels/SpecialOps.metal aten/src/ATen/native/mps/operations/SpecialOps.mm aten/src/ATen/native/native_functions.yaml c10/metal/special_math.h test/test_mps.py,https://github.com/pytorch/pytorch/pull/146818,dcci,malfet,,,
3cadce7af2a,nested tensor_frontend,bug fixes,[NJT] Fix inference mode for composite implicit ops without nested-specific kernel (#146633),test/test_nestedtensor.py torch/nested/_internal/nested_tensor.py,https://github.com/pytorch/pytorch/pull/146633,soulitzer,jbschlosser,,,
de6efa1feb0,skip,not user facing,cpp_wrapper: Precompile device-specific header files (#144002),tools/generate_torch_version.py torch/_inductor/codecache.py torch/_inductor/config.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/144002,benjaminglass1,desertfire,,,
ee8a06f1f62,dynamo,not user facing,[dynamo][user-defined] User class.__new__ instead of special casing (#146677),test/dynamo/test_dicts.py torch/_dynamo/side_effects.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/146677,anijain2305,jansel,,,
cbbb11d9674,dynamo,not user facing,[dynamo][user-defined] Unify standard and non-standard __new__ codebase (#146737),torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/146737,anijain2305,jansel,,,
b8261358ca8,skip,Untopiced,"Revert ""windows Magma build for cu128 (#146653)""",.ci/pytorch/windows/cuda128.bat .ci/pytorch/windows/internal/cuda_install.bat .github/scripts/windows/build_magma.bat .github/scripts/windows/cuda_install.bat .github/workflows/build-magma-windows.yml,,,,,,
5fd15a04b7a,releng,not user facing,[ROCm] Enable inductor-periodic testing for MI300 (#144594),.ci/docker/build.sh .ci/docker/common/install_ucc.sh .ci/docker/ubuntu-rocm/Dockerfile .ci/pytorch/common_utils.sh .ci/pytorch/test.sh .github/ci_commit_pins/fbgemm_rocm.txt .github/workflows/inductor-perf-test-nightly-rocm.yml .github/workflows/inductor-periodic.yml benchmarks/dynamo/check_accuracy.py benchmarks/dynamo/check_graph_breaks.py benchmarks/dynamo/ci_expected_accuracy/rocm/aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/aot_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/aot_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/aot_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_aot_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_aot_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_aot_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_inductor_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamo_eager_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamo_eager_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamo_eager_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamo_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamo_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/inductor_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/inductor_huggingface_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/inductor_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/rocm/inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/inductor_torchbench_training.csv,https://github.com/pytorch/pytorch/pull/144594,BLOrange-AMD,huydhn,malfet,,
374b762bbf3,inductor,not user facing,[ez][BE] get rid of the extra printf('\n') (#146726),torch/_inductor/codegen/debug_utils.py,https://github.com/pytorch/pytorch/pull/146726,YUNQIUGUO,ColinPeppler,,,
6b3f51f870e,inductor,not user facing,use None to slice when list has one element only (#146638),torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/146638,henrylhtsang,drisspg,,,
ddcc97bb8ce,inductor,not user facing,Make sure cutlass kernel .cu file has configuration name and nvcc compile command (#146668),torch/_inductor/codecache.py torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/146668,henrylhtsang,chenyang78,,,
ad847da0cf3,skip,not user facing,[cutlass backend] fix bug for accuminator dtype (#146356),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cutlass_utils.py,https://github.com/pytorch/pytorch/pull/146356,henrylhtsang,Chillee,,,
519f547d052,skip,Untopiced,windows Magma build for cu128 (#146653),.ci/pytorch/windows/cuda128.bat .ci/pytorch/windows/internal/cuda_install.bat .github/scripts/windows/build_magma.bat .github/scripts/windows/cuda_install.bat .github/workflows/build-magma-windows.yml,https://github.com/pytorch/pytorch/pull/146653,tinglvv,atalman,,,
0486a996d29,export,Untopiced,[sigmoid] Implement a OSS only model runner. (#146440),torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/146440,zhxchen17,SherlockNoMad,,,
9b7d0506006,fx,Untopiced,Move capture_provenance to make_node_impl (#146625),torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/146625,angelayi,bobrenjc93,,,
c02a1ecc1d6,export,Untopiced,[export][ez] Allow math.trunc for serialization. (#146715),torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/146715,zhxchen17,angelayi,,,
b45e6fa707c,releng,Untopiced,Cleanup VS 2019 refs in pytorch (#145863),.ci/pytorch/windows/internal/smoke_test.bat .ci/pytorch/windows/internal/static_lib_test.bat .ci/pytorch/windows/internal/vc_install_helper.bat .ci/pytorch/windows/internal/vs2019_install.ps1 .circleci/scripts/binary_windows_build.sh .circleci/scripts/binary_windows_test.sh .github/ISSUE_TEMPLATE/disable-ci-jobs.md .github/scripts/test_trymerge.py .github/workflows/pull.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/145863,Camyll,Skylion007,atalman,huydhn,
3822a88d211,fx,Untopiced,[symbolic shapes] Log symnode id (#146583),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/146583,angelayi,bobrenjc93,,,
97f6480cf55,distributed,Untopiced,Fix an issue where functional collectives don't force fx stride on inputs when compiled (#146467),test/distributed/test_c10d_functional_native.py test/distributed/test_inductor_collectives.py torch/_inductor/comm_lowering.py torch/csrc/distributed/c10d/Functional.cpp,https://github.com/pytorch/pytorch/pull/146467,yifuwang,Chillee,lw,shunting314,
3d604b17d91,skip,Untopiced,Exclude upsample_bilinear2d.vec from default core ATen decomposition table (#141791),test/export/test_export.py torch/export/decomp_utils.py,https://github.com/pytorch/pytorch/pull/141791,GregoryComer,digantdesai,tugsbayasgalan,,
86b52f42098,fx,Untopiced,Fix lint (#146846),torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/146846,angelayi,clee2000,huydhn,,
da216baaa21,inductor,not user facing,Optimize inductor `Self` typing (#146669),torch/_inductor/codecache.py torch/_inductor/dependencies.py torch/_inductor/package/package.py,https://github.com/pytorch/pytorch/pull/146669,zeshengzong,jansel,,,
97d4753bd3b,inductor,not user facing,[hop][inductor] don't promote arg type for cond and while_loop (#146660),torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/146660,ydwu4,eellison,zou3519,,
8f073065d5b,skip,not user facing,[while_loop][inductor] support sym expression as cond_fn output (#146222),test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_arrayref.py test/inductor/test_control_flow.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/146222,ydwu4,desertfire,,,
4d626c261b2,linalg_frontend,Untopiced,Fix workarea compute in lapackSyevd (#146456),aten/src/ATen/native/BatchLinearAlgebraKernel.cpp test/test_linalg.py,https://github.com/pytorch/pytorch/pull/146456,wdvr,malfet,,,
e65b89e4cd1,intel,performance,[Feat]: Improve KleidiAI 4 bit kernel performance (#146476),aten/src/ATen/native/kleidiai/kai_kernels.cpp,https://github.com/pytorch/pytorch/pull/146476,nikhil-arm,malfet,,,
652880e8403,linalg_frontend,Untopiced,"Fix logging and test files which misspell ""precision"" (#146113)",aten/src/ATen/native/cuda/Blas.cpp test/test_linalg.py,https://github.com/pytorch/pytorch/pull/146113,danielvegamyhre,drisspg,,,
681894546bd,skip,not user facing,Fix bazel job after #144489 (#146840),BUILD.bazel,https://github.com/pytorch/pytorch/pull/146840,huydhn,atalman,,,
bab35eb26aa,inductor,bug fixes,fix intermediate debug information with cpp_wrapper (#145527),test/inductor/test_gpu_cpp_wrapper.py torch/_inductor/codegen/debug_utils.py,https://github.com/pytorch/pytorch/pull/145527,exclamaforte,desertfire,,,
0d5fb0941fb,inductor,not user facing,[cutlass backend] check against arch >= 100 (#145812),torch/_inductor/codegen/cuda/cutlass_utils.py,https://github.com/pytorch/pytorch/pull/145812,henrylhtsang,Aidyn-A,ColinPeppler,chenyang78,
c2bf3be0112,inductor,not user facing,[inductor] Remove _get_grid_fn_str (#146800),tools/build_with_debinfo.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_split_scan.py,https://github.com/pytorch/pytorch/pull/146800,jansel,yanboliang,,,
aa1622c0b6f,distributed,Untopiced,Support ignoring parameters in FSDP2 (#146631),test/distributed/_composable/fsdp/test_fully_shard_ignore_params.py torch/distributed/fsdp/_fully_shard/_fsdp_init.py torch/distributed/fsdp/_fully_shard/_fully_shard.py,https://github.com/pytorch/pytorch/pull/146631,ckluk2,awgu,,,
b6273d7f4ba,skip,not user facing,[ROCm] Update periodic.yml to use 2GPU runners (#146839),.github/workflows/periodic.yml,https://github.com/pytorch/pytorch/pull/146839,amdfaa,,,,
937b41e3b55,distributed,not user facing,Refactoring pipeline parallelism test cases to be device agnostic [1/n] (#146472),test/distributed/pipelining/test_backward.py test/distributed/pipelining/test_microbatch.py test/distributed/pipelining/test_transformer.py test/distributed/pipelining/test_unflatten.py,https://github.com/pytorch/pytorch/pull/146472,AnantGulati,H-Huang,,,
d763093b49a,mps,bug fixes,[MPS] fix lu factor for large tensors with bs>1 (#146753),aten/src/ATen/native/mps/operations/LinearAlgebra.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/146753,Isalia20,malfet,,,
2fafcd37c38,skip,Untopiced,"Revert ""cpp_wrapper: Precompile device-specific header files (#144002)""",tools/generate_torch_version.py torch/_inductor/codecache.py torch/_inductor/config.py torch/_inductor/cpp_builder.py,,,,,,
0c9fdd6cfb3,python_frontend,docs,[Docs] Fix description of `input` in `torch.addbmm()` (#146664),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/146664,shink,mikaylagawarecki,,,
f38f1dcd825,skip,Untopiced,"Revert ""move and fix logic to update unbacked bindings (#146115)""",test/export/test_export.py torch/_export/serde/serialize.py torch/export/_trace.py,,,,,,
5205158c1b0,skip,Untopiced,Replace is_same with is_same_v for concise syntax (#145450),aten/src/ATen/core/List_inl.h aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h aten/src/ATen/cuda/cub.cuh aten/src/ATen/native/cuda/ScanUtils.cuh aten/src/ATen/native/sparse/cuda/SparseMatMul.cu aten/src/ATen/templates/TensorBody.h,https://github.com/pytorch/pytorch/pull/145450,zeshengzong,huydhn,,,
275c034b164,dynamo,not user facing,[SkipFiles] remove some stuff from MOD_SKIPLIST (#146854),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/146854,zou3519,anijain2305,yanboliang,,
1d81ecfc54d,dynamo,Untopiced,Rename PrimHOPBase to BaseHOP + minor changes (#146727),test/dynamo/test_base_hop.py test/dynamo/test_prim_hop_base.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/__init__.py torch/_higher_order_ops/_invoke_quant.py torch/_higher_order_ops/base_hop.py torch/_higher_order_ops/foreach_map.py torch/_higher_order_ops/invoke_subgraph.py torch/_higher_order_ops/prim_hop_base.py,https://github.com/pytorch/pytorch/pull/146727,zou3519,ydwu4,,,
30cbf13544b,distributed,Untopiced,[PGNCCL] Associate tensor allocation support with NCCL version (#146842),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp,https://github.com/pytorch/pytorch/pull/146842,kwen2501,XilunWu,fduwjj,wconstab,
fe94ece375f,skip,Untopiced,"Revert ""Exclude upsample_bilinear2d.vec from default core ATen decomposition table (#141791)""",test/export/test_export.py torch/export/decomp_utils.py,,,,,,
23524699d5b,inductor,not user facing,"Only call triton in worker process, kick off worker processes earlier, during inductor codegen (#146417)",test/inductor/test_codecache.py test/inductor/test_max_autotune.py torch/_inductor/async_compile.py torch/_inductor/codecache.py torch/_inductor/codegen/triton.py torch/_inductor/runtime/autotune_cache.py torch/_inductor/runtime/compile_tasks.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/146417,jamesjwu,jansel,,,
001ebbf734a,skip,not user facing,[MTIA] (4/n) Implement PyTorch APIs to query/reset device peak memory usage (#146751),torch/mtia/__init__.py torch/mtia/memory.py,https://github.com/pytorch/pytorch/pull/146751,chaos5958,nautsimon,,,
a7fe384d0ed,dynamo,not user facing,Remove torch._higher_order_ops from MOD_SKIPLIST (#146853),test/functorch/test_control_flow.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/146853,zou3519,williamwen42,,,
29523aa113f,dynamo,not user facing,[Dynamo][autograd.Function] Relax backward speculation strict mode a bit (#146571),test/dynamo/test_autograd_function.py test/dynamo/test_repros.py torch/_dynamo/_trace_wrapped_higher_order_op.py torch/_dynamo/config.py torch/_dynamo/exc.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/tensor.py,https://github.com/pytorch/pytorch/pull/146571,yanboliang,zou3519,,,
f2da810516c,dynamo,not user facing,[Dynamo][autograd.Function] Relax backward speculation strict mode: support .data (#146741),test/dynamo/test_autograd_function.py torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/146741,yanboliang,zou3519,,,
229fb0bc83b,dynamo,not user facing,[Dynamo][autograd.Function] Relax backward speculation strict mode: support .requires_grad (#146742),test/dynamo/test_autograd_function.py torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/146742,yanboliang,zou3519,,,
69301fb10eb,skip,not user facing,Introduce new template heuristic for triton autotune configs (#144985),torch/_inductor/choices.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_plus_mm.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/kernel/unpack_mixed_mm.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/template_heuristics.py,https://github.com/pytorch/pytorch/pull/144985,jataylo,jansel,,,
15635b14ce9,distributed,Untopiced,[4/N] Remove unnecessary once flag usage (#146783),aten/src/ATen/cuda/CUDAContext.cpp aten/src/ATen/cuda/CUDAGeneratorImpl.cpp aten/src/ATen/native/NNPACK.cpp aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip aten/src/ATen/xpu/XPUContext.cpp aten/src/ATen/xpu/XPUGeneratorImpl.cpp aten/src/ATen/xpu/detail/XPUHooks.cpp torch/csrc/distributed/c10d/logger.cpp torch/csrc/utils/tensor_types.cpp,https://github.com/pytorch/pytorch/pull/146783,cyyever,albanD,,,
fc5913b6bf7,jit,Untopiced,[StaticRuntime] Fix a bug that memory planner ignores subblocks (#146728) (#146855),benchmarks/static_runtime/test_static_module.cc torch/csrc/jit/runtime/static/impl.cpp torch/csrc/jit/runtime/static/impl.h,https://github.com/pytorch/pytorch/pull/146855,coufon,swolchok,,,
5235a18cd66,dynamo,not user facing,[SkipFiles] remove some more stuff from MOD_SKIPLIST (#146876),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/146876,zou3519,anijain2305,,,
861bf892fb9,releng,not user facing,Set USE_CUFILE=1 by default and add pypi package to binary build matrix (#145748),.ci/manywheel/build_cuda.sh .ci/pytorch/smoke_test/smoke_test.py .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml CMakeLists.txt test/test_cuda.py torch/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/145748,mikaylagawarecki,atalman,,,
e3839bd6036,skip,not user facing,[BE] Strip `#pragma once` when embedding the headers (#146871),torch/utils/_cpp_embed_headers.py,https://github.com/pytorch/pytorch/pull/146871,malfet,dcci,,,
c7515da7b00,skip,Untopiced,Implement cuda graphs implementation of torch.cond and torch.while_loop (#140979),aten/src/ATen/cuda/CUDAGeneratorImpl.cpp aten/src/ATen/cuda/CUDAGraph.cpp aten/src/ATen/cuda/CUDAGraph.cu aten/src/ATen/cuda/CUDAGraph.h docs/source/notes/cuda.rst docs/source/torch.compiler_cudagraph_trees.rst test/functorch/test_control_flow.py test/functorch/test_control_flow_cuda_initialization.py test/run_test.py torch/_C/__init__.pyi.in torch/_dynamo/backends/cudagraphs.py torch/_dynamo/backends/debugging.py torch/_dynamo/variables/torch_function.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/cudagraph_conditional_nodes.py torch/_higher_order_ops/while_loop.py torch/_inductor/cudagraph_trees.py torch/_subclasses/fake_tensor.py torch/csrc/cuda/Graph.cpp torch/csrc/cuda/shared/cudart.cpp torch/cuda/graphs.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/140979,galv,eellison,eqy,,
d5d3bdb55a2,releng,not user facing,Fix var CUDA_PATH_V128 in cuda128.bat file (#146906),.ci/pytorch/windows/cuda128.bat,https://github.com/pytorch/pytorch/pull/146906,atalman,malfet,tinglvv,,
ebd992724f5,export,Untopiced,Implement serializable getattr support for tensor subclasses (#145772),docs/source/export.rst test/export/test_export.py torch/_export/verifier.py torch/export/__init__.py torch/export/_trace.py torch/export/custom_ops.py,https://github.com/pytorch/pytorch/pull/145772,tugsbayasgalan,bdhirsh,,,
af349047c3e,inductor,not user facing,[FlexAttention] Bug fix broken flag (#146872),torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/146872,drisspg,BoyuanFeng,,,
b18e3c01aa8,inductor,not user facing,[Inductor] Unifiy Low Precision FP Legalization for to_dtype_bitcast & constant (#144646),test/inductor/test_torchinductor.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/144646,DDEle,jansel,jgong5,leslie-fang-intel,
110638f7027,inductor,not user facing,[inductor] skip _test_insignificant_strides on rocm (#146849),test/inductor/test_fused_attention.py,https://github.com/pytorch/pytorch/pull/146849,shunting314,atalman,eellison,jansel,
664550ecbf2,export,Untopiced,[export] Serialize special values of float into strings for json. (#146490),test/export/test_serialize.py torch/_export/serde/schema_check.py torch/_export/serde/serialize.py torch/csrc/utils/generated_serialization_types.h,https://github.com/pytorch/pytorch/pull/146490,zhxchen17,yiming0416,,,
b1ff90ae8a8,releng,not user facing,remove Windows XPU build workaround. (#144644),.ci/pytorch/windows/xpu.bat cmake/Dependencies.cmake,https://github.com/pytorch/pytorch/pull/144644,xuhancn,EikanWang,atalman,chuanqi129,
443437648a8,skip,Untopiced,"Revert ""Introduce new template heuristic for triton autotune configs (#144985)""",torch/_inductor/choices.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_plus_mm.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/kernel/unpack_mixed_mm.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/template_heuristics.py,,,,,,
a20055288f1,skip,not user facing,[DTensor][Test] Create a simple unit test for tensordot (#146514),test/distributed/tensor/test_matrix_ops.py,https://github.com/pytorch/pytorch/pull/146514,wz337,XilunWu,tianyu-l,,
f59a56e56f8,releng,not user facing,[ARM] Fix `test_float_to_int_conversion_nonfinite` (#145367),.ci/pytorch/test.sh test/test_tensor_creation_ops.py,https://github.com/pytorch/pytorch/pull/145367,robert-hardwick,malfet,,,
3a29992ee6c,dynamo,not user facing,[associative_scan] Lifted arguments (#140043),test/export/test_export.py test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/associative_scan.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/140043,bohnstingl,ydwu4,,,
f50d359ce29,distributed,Untopiced,[ c10d ] modify API to get device string from device with torch.device (#146290),torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/146290,ankurneog,H-Huang,guangyey,,
7aa629f1268,skip,not user facing,Update octokit/request-action to 2.4.0 (#146940),.github/workflows/upload-test-stats.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/146940,huydhn,izaitsevfb,,,
281249ba543,skip,not user facing,[torch][amdsmi] Avoid ODR violation when loading amdsmi (#146324),torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/146324,danzimm,malfet,,,
a58f421f4bb,skip,not user facing,[CUDA][CUDNN][SDPA] Pass dropout seed and offset to cuDNN in `int64` (#146734),aten/src/ATen/native/cudnn/MHA.cpp,https://github.com/pytorch/pytorch/pull/146734,eqy,Skylion007,,,
df5e232563e,releng,improvements,[BE] Delete NCCL slimming (#146943),cmake/External/nccl.cmake,https://github.com/pytorch/pytorch/pull/146943,malfet,Skylion007,atalman,,
78ebd3c5020,releng,not user facing,Revert commit that removed windows testing in VS2019-> update  (#146920),.github/scripts/test_trymerge.py .github/workflows/pull.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/146920,Camyll,atalman,huydhn,malfet,
0acbf8039ab,skip,not user facing,[BE] Unskip some tensor creation tests on Mac (#146952),test/test_tensor_creation_ops.py,https://github.com/pytorch/pytorch/pull/146952,malfet,atalman,seemethere,,
28a2ab6b848,inductor,not user facing,Clear CompiledTritonKernel cache after each inductor compile (#146925),torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/146925,jamesjwu,jansel,laithsakka,,
06f8f9a0177,skip,not user facing,Update instructions about faster linker (#146750),CONTRIBUTING.md,https://github.com/pytorch/pytorch/pull/146750,oraluben,albanD,,,
683bb1242c1,export,Untopiced,[export][ez] Update tag_ for union setters. (#146912),torch/_export/serde/schema_check.py torch/csrc/utils/generated_serialization_types.h,https://github.com/pytorch/pytorch/pull/146912,zhxchen17,yiming0416,,,
eb655a2d5fa,inductor,not user facing,Fix CUTLASS 2.x kernels for auto-tuning (#146755),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/146755,alexsamardzic,henrylhtsang,,,
5a1c7c424d0,inductor,not user facing,Fix standalone runner for CUTLASS auto-tuning backend (#146764),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/146764,alexsamardzic,henrylhtsang,,,
6105b6f15f8,skip,Untopiced,"Revert ""Update octokit/request-action to 2.4.0 (#146940)""",.github/workflows/upload-test-stats.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,,,,,,
b30bad710d6,skip,not user facing,Update octokit/request-action to 2.4.0 (#146940),.github/workflows/upload-test-stats.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/146940,huydhn,izaitsevfb,,,
8c80c13b34c,releng,not user facing,[CD] Add python 3.13t build for xpu (#146614),.github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/146614,chuanqi129,atalman,,,
ae5cc19ba71,nn_frontend,Untopiced,[pytorch][cuda] Improve softmax backward pass native CUDA implementation (#145866),aten/src/ATen/native/cuda/SoftMax.cu test/test_nn.py,https://github.com/pytorch/pytorch/pull/145866,ahmadsharif1,ngimel,,,
d774a6333d1,jit,Untopiced,[StaticRuntime] Support a new pattern for ClipRangesToGatherToOffsets (#146931),torch/csrc/jit/runtime/static/passes.cpp,https://github.com/pytorch/pytorch/pull/146931,coufon,hanyilou123,,,
ac0f206f3cb,distributed (dtensor),not user facing,[dtensor] fix side-effect on dtype for _like ops (#146869),test/distributed/tensor/test_tensor_ops.py torch/distributed/tensor/_ops/_tensor_ops.py,https://github.com/pytorch/pytorch/pull/146869,tianyu-l,janeyx99,ngimel,yifuwang,
98e16012ec8,linalg_frontend,Untopiced,[Quant][CPU] add a wrapper op for _weight_int4pack_mm_for_cpu with tensor args (#145245),aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/quantized/cpu/qlinear.cpp aten/src/ATen/native/quantized/cpu/qlinear.h aten/src/ATen/native/quantized/library.cpp test/test_linalg.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/145245,Xia-Weiwen,jerryzh168,jgong5,leslie-fang-intel,
b0042286d48,dynamo,not user facing,[Dynamo] Allow dynamo to handle `str.xxx()` (#146587),test/dynamo/test_repros.py torch/_dynamo/utils.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/146587,shink,zou3519,,,
ce80865f134,skip,Untopiced,"Revert ""Replace is_same with is_same_v for concise syntax (#145450)""",aten/src/ATen/core/List_inl.h aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h aten/src/ATen/cuda/cub.cuh aten/src/ATen/native/cuda/ScanUtils.cuh aten/src/ATen/native/sparse/cuda/SparseMatMul.cu aten/src/ATen/templates/TensorBody.h,,,,,,
b61032fcf74,skip,not user facing,[BE][Ez]: Remove unnecessary type ignores from orderedset (#146902),torch/utils/_ordered_set.py,https://github.com/pytorch/pytorch/pull/146902,Skylion007,eellison,,,
1f8ff94d4fe,onnx,not user facing,PEP585: Add noqa to necessary tests (#146391),test/onnx/test_onnxscript_runtime.py test/test_cpp_extensions_aot.py test/test_fx.py test/test_fx_experimental.py torch/autograd/grad_mode.py torch/fx/graph.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/146391,aorenste,Skylion007,justinchuby,,
9abaaad6a89,skip,not user facing,[pytree][Easy] preserve `dict` keys in insertion order in CXX pytree (#130140),test/test_pytree.py torch/utils/_cxx_pytree.py,https://github.com/pytorch/pytorch/pull/130140,XuehaiPan,zou3519,,,
b894c2824b1,onnx,new features,[ONNX] Support custom axis name through dynamic_shapes (#146321),test/onnx/exporter/test_compat.py test/onnx/exporter/test_dynamic_shapes.py test/onnx/exporter/test_hf_models_e2e.py test/onnx/exporter/test_ir_passes.py test/onnx/exporter/test_small_models_e2e.py torch/onnx/_internal/exporter/_compat.py torch/onnx/_internal/exporter/_dynamic_shapes.py torch/onnx/_internal/exporter/_ir_passes.py torch/onnx/_internal/exporter/_onnx_program.py,https://github.com/pytorch/pytorch/pull/146321,titaiwangms,justinchuby,,,
6c81435f16f,releng,not user facing,[ONNX] Update CI transformers cache (#146926),.ci/docker/common/install_onnx.sh,https://github.com/pytorch/pytorch/pull/146926,titaiwangms,justinchuby,,,
d6513f3246e,dynamo,not user facing,[dynamo] Support list subclasses and fix dict subclasses mutation bugs (#146819),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/dynamo/test_comptime.py test/dynamo/test_dicts.py test/dynamo/test_functions.py torch/_dynamo/guards.py torch/_dynamo/side_effects.py torch/_dynamo/source.py torch/_dynamo/trace_rules.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/146819,anijain2305,StrongerXi,jansel,,
04011304e55,skip,not user facing,Update dynamo expected 20250210 (#146856),benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_amp_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/cpu_inductor_freezing_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamo_eager_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/dynamic_inductor_torchbench_inference.csv benchmarks/dynamo/ci_expected_accuracy/rocm/inductor_torchbench_inference.csv,https://github.com/pytorch/pytorch/pull/146856,huydhn,yanboliang,,,
bfcce6984b0,skip,not user facing,[ROCm][TunableOp] Close offline tuning results file when offline tuning is disabled. (#146574),aten/src/ATen/cuda/tunable/Tunable.cpp,https://github.com/pytorch/pytorch/pull/146574,naromero77amd,jeffdaily,,,
5f2714d5e7c,skip,not user facing,[cutlass backend] Do not change dtype of GEMM template (#146877),torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/146877,henrylhtsang,ColinPeppler,mlazos,,
7f62616a585,onnx,Untopiced,[ONNX][reland2] Create deprecation warning on dynamo_export (#146923),test/onnx/test_pytorch_onnx_no_runtime.py torch/onnx/__init__.py torch/onnx/_deprecation.py torch/onnx/_exporter_states.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_compat.py torch/onnx/symbolic_opset9.py torch/onnx/utils.py torch/onnx/verification.py,https://github.com/pytorch/pytorch/pull/146923,justinchuby,titaiwangms,,,
352484cc83c,mps,not user facing,[BE] Unify kernel templates instantiation (#146965),aten/src/ATen/native/mps/kernels/BinaryKernel.metal,https://github.com/pytorch/pytorch/pull/146965,malfet,Skylion007,dcci,,
0de27ee7e0d,distributed,not user facing,Let _create_cpu_state_dict and _copy_state_dict support DTensor (#146852),test/distributed/checkpoint/test_state_dict_utils.py torch/distributed/_state_dict_utils.py,https://github.com/pytorch/pytorch/pull/146852,fegin,d4l3k,,,
8a975cb247d,skip,Untopiced,"Revert ""[cutlass backend] Do not change dtype of GEMM template (#146877)""",torch/_inductor/codegen/cuda/gemm_template.py,,,,,,
ec0b318ddb4,composability,Untopiced,[poc] force UntypedStorage.from_buffer(buf) to return meta storage under FakeTensorMode (#146642),test/test_fake_tensor.py torch/csrc/StorageMethods.cpp,https://github.com/pytorch/pytorch/pull/146642,bdhirsh,zou3519,,,
5cda021cac9,composability,Untopiced,support meta_tensor.to(device='cpu') under fake_mode (#146729),test/test_fake_tensor.py torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/146729,bdhirsh,SherlockNoMad,albanD,zou3519,
de964b9f8bb,dynamo,Untopiced,dont specialize symints when testing truthiness (#146731),test/dynamo/test_repros.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/146731,bdhirsh,bobrenjc93,,,
06a07f6018d,mps,Untopiced,[BE] Towards MetalTensorIterator (#146993),aten/src/ATen/native/mps/kernels/BinaryKernel.metal,https://github.com/pytorch/pytorch/pull/146993,malfet,Skylion007,dcci,,
995f607c743,distributed,Untopiced,fix doc string (#146968),torch/distributed/elastic/multiprocessing/errors/__init__.py,https://github.com/pytorch/pytorch/pull/146968,probli,H-Huang,zackycao,,
f655f840b8d,onnx,not user facing,[ONNX][dort] Remove reference to onnxscript rewriter (#147003),torch/onnx/_internal/onnxruntime.py,https://github.com/pytorch/pytorch/pull/147003,justinchuby,gramalingam,shubhambhokare1,titaiwangms,
fd211260074,onnx,docs,[ONNX] Deprecation message follow up (#147005),torch/onnx/__init__.py torch/onnx/utils.py,https://github.com/pytorch/pytorch/pull/147005,justinchuby,titaiwangms,,,
f954aac6be0,skip,not user facing,Add `make_dynamo_test` (#146491),torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/146491,guilhermeleobas,anijain2305,malfet,zou3519,
c60f587c048,distributed,Untopiced,Fix shape_inference for V-schedules (#147000),torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/147000,H-Huang,wconstab,,,
076215944a4,skip,not user facing,Turn on autograd local caches in fbcode (#146996),torch/_functorch/config.py,https://github.com/pytorch/pytorch/pull/146996,oulgen,jamesjwu,,,
ad4e5bf705d,releng,not user facing,cpp_wrapper: handle mixed-device C-shim fallbacks (#146449),.ci/pytorch/test.sh torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/146449,benjaminglass1,desertfire,,,
d5a2e4c754d,fx,Untopiced,[oncall] Change error message to be more readable (#146934),torch/fx/passes/split_utils.py,https://github.com/pytorch/pytorch/pull/146934,jingsh,ColinPeppler,,,
0344bf8a5ae,releng,not user facing,[cuDNN] cuDNN to 9.7.1.26 for CUDA 12.8 (#146957),.ci/docker/common/install_cuda.sh .ci/docker/common/install_cuda_aarch64.sh .ci/docker/common/install_cudnn.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/146957,tinglvv,Skylion007,atalman,eqy,
21c2565f35f,dynamo,not user facing,Document dynamo (#146736),torch/_dynamo/__init__.py torch/_dynamo/_trace_wrapped_higher_order_op.py torch/_dynamo/backends/common.py torch/_dynamo/backends/cudagraphs.py torch/_dynamo/backends/debugging.py torch/_dynamo/backends/distributed.py torch/_dynamo/backends/inductor.py torch/_dynamo/backends/registry.py torch/_dynamo/backends/tvm.py torch/_dynamo/bytecode_analysis.py torch/_dynamo/bytecode_transformation.py torch/_dynamo/callback.py torch/_dynamo/code_context.py torch/_dynamo/codegen.py torch/_dynamo/compiled_autograd.py torch/_dynamo/comptime.py torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/current_scope_id.py torch/_dynamo/debug_utils.py torch/_dynamo/decorators.py torch/_dynamo/device_interface.py torch/_dynamo/distributed.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/external_utils.py torch/_dynamo/funcname_cache.py torch/_dynamo/graph_deduplication.py torch/_dynamo/graph_region_tracker.py torch/_dynamo/guards.py torch/_dynamo/hooks.py torch/_dynamo/logging.py torch/_dynamo/metrics_context.py torch/_dynamo/mutation_guard.py torch/_dynamo/output_graph.py torch/_dynamo/pgo.py torch/_dynamo/profiler.py torch/_dynamo/replay_record.py torch/_dynamo/repro/after_aot.py torch/_dynamo/repro/after_dynamo.py torch/_dynamo/repro/aoti.py torch/_dynamo/resume_execution.py torch/_dynamo/side_effects.py torch/_dynamo/source.py torch/_dynamo/symbolic_convert.py torch/_dynamo/tensor_version_op.py torch/_dynamo/test_case.py torch/_dynamo/test_minifier_common.py torch/_dynamo/testing.py torch/_dynamo/types.py torch/_dynamo/utils.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/distributed.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/iter.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/script_object.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/146736,Raymo111,StrongerXi,anijain2305,jansel,
be387f57b1a,fx,Untopiced,[symbolic shapes] Log SymNode id for provenance (#146532),test/export/test_draft_export.py torch/fx/experimental/sym_node.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/146532,angelayi,bobrenjc93,,,
b4bdbce1ac6,fx,Untopiced,[export] Use custom stream logger in draft-export (#146533),test/export/test_draft_export.py torch/export/_draft_export.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/146533,angelayi,avikchaudhuri,,,
43f5566c92f,fx,Untopiced,[export] Add additional tlparse logging (#146534),torch/export/_trace.py,https://github.com/pytorch/pytorch/pull/146534,angelayi,pianpwk,,,
59bc5d0d71a,fx,Untopiced,[tlparse] Add stacktrace filter utility (#146858),test/export/test_draft_export.py torch/_logging/structured.py torch/export/_draft_export.py torch/fx/experimental/sym_node.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/146858,angelayi,bobrenjc93,,,
67cbbb29e07,export,Untopiced,[export] Dedup expression_created logs (#146859),test/export/test_draft_export.py torch/export/_draft_export.py,https://github.com/pytorch/pytorch/pull/146859,angelayi,pianpwk,,,
2ce6de2415f,skip,not user facing,[cond] make cond re-dispatch in proxy mode (#146954),torch/_higher_order_ops/cond.py torch/_higher_order_ops/utils.py,https://github.com/pytorch/pytorch/pull/146954,ydwu4,zou3519,,,
b77a6eb1849,dynamo,not user facing,[dynamo] Fix tensordict regression (#146995),test/dynamo/test_functions.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/146995,anijain2305,StrongerXi,,,
87ebd77b344,dynamo,not user facing,Add some more docs to trace_rules.py (#147016),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/147016,zou3519,yanboliang,,,
fddaa2958bf,dynamo,not user facing,[SkipFiles] Some more cleanup (#147012),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/147012,zou3519,yanboliang,,,
5dab0aeef0e,dynamo,not user facing,[SkipFiles] Some more cleanup (#147013),torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/147013,zou3519,yanboliang,,,
80f146dedfb,python_frontend,Untopiced,"Update addbmm, addmm, addmv and baddbmm description (#146689)",torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/146689,zeshengzong,mikaylagawarecki,,,
e0ca041ae36,mps,improvements,[BE] Toward Metal Iterator (step 2) (#147023),aten/src/ATen/native/mps/kernels/BinaryKernel.metal aten/src/ATen/native/mps/operations/BinaryKernel.mm,https://github.com/pytorch/pytorch/pull/147023,malfet,dcci,,,
aaa46c06250,mps,Untopiced,Add missing autoreleasepool around runUniqueGraph to prevent leaks (#145512),aten/src/ATen/native/mps/operations/Unique.mm,https://github.com/pytorch/pytorch/pull/145512,jhavukainen,malfet,,,
54e28b2a71d,mps,improvements,[BE] Turn nextafter into functor (#147018),aten/src/ATen/native/mps/kernels/BinaryKernel.metal aten/src/ATen/native/mps/operations/BinaryKernel.mm,https://github.com/pytorch/pytorch/pull/147018,malfet,dcci,,,
821422018a1,inductor,not user facing,[FlexAttention] Make zero_length sequence handiling better (#147010),test/inductor/test_flex_attention.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/147010,drisspg,Chillee,,,
c159723c390,composability,not user facing,Fix meta impl for topk (#147017),torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/147017,tugsbayasgalan,ydwu4,,,
6ca497a8e58,skip,Untopiced,Replace is_same with is_same_v for concise syntax (#145450),aten/src/ATen/core/List_inl.h aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h aten/src/ATen/cuda/cub.cuh aten/src/ATen/native/cuda/ScanUtils.cuh aten/src/ATen/native/sparse/cuda/SparseMatMul.cu aten/src/ATen/templates/TensorBody.h,https://github.com/pytorch/pytorch/pull/145450,zeshengzong,huydhn,,,
a9598337b7d,inductor,Untopiced,[Optimus] Include more corner cases in the select cat aten pass (#146662),test/inductor/test_split_cat_fx_aten_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/146662,mengluy0125,Microve,,,
936df4571b1,distributed,not user facing,Update test_c10d_object_collectives.py with DistributedTestBase class (#145056),test/distributed/test_c10d_object_collectives.py,https://github.com/pytorch/pytorch/pull/145056,amathewc,guangyey,kwen2501,,
2ff3fdfdaea,releng,not user facing,[audio hash update] update the pinned audio hash (#146738),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/146738,pytorchupdatebot,pytorchbot,,,
88d0bb0fee4,skip,not user facing,"[aoti_debug_printer][BE] explicitly dumping float32, bfloat16, float16 data type (#147020)",torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/147020,YUNQIUGUO,ColinPeppler,jingsh,,
43eb39d7c83,releng,not user facing,[executorch hash update] update the pinned executorch hash (#145128),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/145128,pytorchupdatebot,pytorchbot,,,
66fb10fc53b,skip,not user facing,[BE][OpInfo] Introduce generic `dtypesIf` (#146905),torch/testing/_internal/opinfo/core.py,https://github.com/pytorch/pytorch/pull/146905,malfet,janeyx99,,,
aa20b4b6cf8,xpu,Untopiced,Friendly handle mem_get_info's runtime error message (#146899),torch/csrc/xpu/Module.cpp,https://github.com/pytorch/pytorch/pull/146899,guangyey,EikanWang,,,
3e4172d985a,skip,not user facing,[BE][Ez]: Update fmtlib submodule to 11.1.3 (#146985),third_party/fmt,https://github.com/pytorch/pytorch/pull/146985,Skylion007,drisspg,,,
4879f8f919f,distributed,Untopiced,[TP] Add warning when module is distributed twice (#147006),test/distributed/tensor/test_api.py torch/distributed/tensor/_api.py,https://github.com/pytorch/pytorch/pull/147006,kwen2501,XilunWu,,,
17a808557c4,mps,improvements,[MPS] cholesky ex version (#146799),aten/src/ATen/native/BatchLinearAlgebra.cpp aten/src/ATen/native/mps/kernels/LinearAlgebra.metal aten/src/ATen/native/mps/operations/LinearAlgebra.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py tools/autograd/derivatives.yaml,https://github.com/pytorch/pytorch/pull/146799,Isalia20,malfet,,,
b9a22b3f370,mps,bug fixes,bug fix: ensure 4d input in _scaled_dot_product_attention_math_mps (#146623),aten/src/ATen/native/mps/operations/Attention.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/146623,hellopahe,malfet,,,
43496e9b907,nested tensor_frontend,bug fixes,[NJT] fix flop counter for SDPA & test (#147032),test/test_nestedtensor.py torch/nested/_internal/sdpa.py torch/utils/flop_counter.py,https://github.com/pytorch/pytorch/pull/147032,davidberard98,jbschlosser,,,
17d3a69c324,xpu,Untopiced,[Intel GPU] fix memory leak in deconv backward (#144385),aten/src/ATen/native/mkldnn/xpu/detail/Deconv.cpp,https://github.com/pytorch/pytorch/pull/144385,jianyizh,EikanWang,liangan1,,
ca3aabc8e6f,quantization,not user facing,[Inductor][CPU] Add a lowering pass for _weight_int4pack_mm_for_cpu (#145250),aten/src/ATen/native/quantized/cpu/qlinear.h test/inductor/test_cpu_cpp_wrapper.py test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/quantized_lowerings.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/145250,Xia-Weiwen,jerryzh168,leslie-fang-intel,,
e21181642f6,skip,not user facing,[AOTInductor] Align behavior between CPU and GPU (#145459),torch/csrc/inductor/aoti_runtime/model.h torch/csrc/inductor/aoti_runtime/model_container.h,https://github.com/pytorch/pytorch/pull/145459,muchulee8,desertfire,hl475,,
f95bdf5e6c8,skip,not user facing,Make GetCPUAllocatorMaybePinned to be Device-Agnostic (#146687),aten/src/ATen/DeviceAccelerator.h aten/src/ATen/EmptyTensor.cpp,https://github.com/pytorch/pytorch/pull/146687,FFFrog,ngimel,,,
858bc0cea50,skip,not user facing,Use 2022 as default VC_YEAR for windows builds (#147053),.github/workflows/_win-build.yml,https://github.com/pytorch/pytorch/pull/147053,atalman,huydhn,,,
8d94eb1e3bf,skip,not user facing,[BE]: Make OrderedSet reversible (#146904),torch/utils/_ordered_set.py,https://github.com/pytorch/pytorch/pull/146904,Skylion007,eellison,,,
d1997b610f5,skip,not user facing,update kineto submodule (#147015),third_party/kineto,https://github.com/pytorch/pytorch/pull/147015,briancoutinho,Skylion007,sraikund16,,
67c4c39b4f7,inductor,not user facing,[docs] Minor fixes to export and aoti docs (#144513),docs/source/export.rst docs/source/torch.compiler_aot_inductor.rst torch/_inductor/__init__.py,https://github.com/pytorch/pytorch/pull/144513,angelayi,desertfire,yushangdi,,
aeabbffe153,skip,not user facing,Disable test with dynamo for schema gen (#146865),test/functorch/test_control_flow.py,https://github.com/pytorch/pytorch/pull/146865,ydwu4,zou3519,,,
e2479d78097,skip,not user facing,Update slow tests (#146822),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/146822,pytorchupdatebot,pytorchbot,,,
7c3b2a29ec1,skip,not user facing,"[subclass] testing WrapperSubclass respect outer_size, outer_stride (#146897)",torch/testing/_internal/subclasses.py,https://github.com/pytorch/pytorch/pull/146897,IvanKobzarev,bdhirsh,,,
345f5566282,skip,not user facing,Fix `DispatchStub.cpp` compilation for gcc 14 (#146512),aten/src/ATen/native/DispatchStub.cpp,https://github.com/pytorch/pytorch/pull/146512,anmyachev,Skylion007,,,
df776d64f7b,distributed,Untopiced,chore: fix typos in error messages in FSDP (#146805),torch/distributed/fsdp/_fully_shard/_fsdp_init.py torch/distributed/fsdp/_init_utils.py,https://github.com/pytorch/pytorch/pull/146805,universome,Skylion007,awgu,,
020232ec9f8,skip,not user facing,[Submodule]: Update KleidiAI submodule to v1.3.0 (#146480),third_party/kleidiai,https://github.com/pytorch/pytorch/pull/146480,nikhil-arm,digantdesai,malfet,,
683178fabc2,cuda,Untopiced,[cuda] fix printing of num_gpus (#146838),aten/src/ATen/cuda/CUDAContext.cpp,https://github.com/pytorch/pytorch/pull/146838,wconstab,Skylion007,eqy,,
938209fb6ff,skip,Untopiced,"Revert ""Use 2022 as default VC_YEAR for windows builds (#147053)""",.github/workflows/_win-build.yml,,,,,,
7077d0ac8c2,distributed,not user facing,[DCP] Introduce modules metadata in the storage_meta (#146654),torch/distributed/checkpoint/metadata.py,https://github.com/pytorch/pytorch/pull/146654,saumishr,MeetVadakkanchery,,,
447a142de2e,composability,Untopiced,support input mutations on tangents in compile (#141131),test/dynamo/test_aot_autograd.py test/functorch/test_aotdispatch.py torch/_functorch/_aot_autograd/traced_function_transforms.py,https://github.com/pytorch/pytorch/pull/141131,bdhirsh,zou3519,,,
1f8ff6812dc,skip,not user facing,[Fix]: Disable KleidiAI if unsupported gcc/clang compiler is detected (#146836),CMakeLists.txt,https://github.com/pytorch/pytorch/pull/146836,nikhil-arm,malfet,,,
65e8862b9a8,skip,Untopiced,"Revert ""[cond] make cond re-dispatch in proxy mode (#146954)""",torch/_higher_order_ops/cond.py torch/_higher_order_ops/utils.py,,,,,,
9a883007a2f,skip,Untopiced,"Revert ""Implement cuda graphs implementation of torch.cond and torch.while_loop (#140979)""",aten/src/ATen/cuda/CUDAGeneratorImpl.cpp aten/src/ATen/cuda/CUDAGraph.cpp aten/src/ATen/cuda/CUDAGraph.cu aten/src/ATen/cuda/CUDAGraph.h docs/source/notes/cuda.rst docs/source/torch.compiler_cudagraph_trees.rst test/functorch/test_control_flow.py test/functorch/test_control_flow_cuda_initialization.py test/run_test.py torch/_C/__init__.pyi.in torch/_dynamo/backends/cudagraphs.py torch/_dynamo/backends/debugging.py torch/_dynamo/variables/torch_function.py torch/_higher_order_ops/cond.py torch/_higher_order_ops/cudagraph_conditional_nodes.py torch/_higher_order_ops/while_loop.py torch/_inductor/cudagraph_trees.py torch/_subclasses/fake_tensor.py torch/csrc/cuda/Graph.cpp torch/csrc/cuda/shared/cudart.cpp torch/cuda/graphs.py torch/testing/_internal/common_utils.py,,,,,,
92d448ff623,fx,not user facing,Add self to CODEOWNERS for fx/proxy.py; warn against adding new node arg types (#147031),CODEOWNERS torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/147031,zou3519,StrongerXi,,,
260b21b8bca,skip,not user facing,[cutlass backend] Do not change dtype of GEMM template (#146877),torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/146877,henrylhtsang,ColinPeppler,mlazos,,
b0553cee6bb,releng,not user facing,[Utilization] post-test-process workflow (#145310),.github/actions/upload-utilization-stats/action.yml .github/workflows/_linux-test.yml tools/stats/monitor.py tools/stats/upload_stats_lib.py tools/stats/upload_utilization_stats/upload_utilization_stats.py,https://github.com/pytorch/pytorch/pull/145310,yangw-dev,huydhn,,,
2d3db4509aa,quantization,Untopiced,fix pt2e block wise quantization test (#147035),test/quantization/pt2e/test_quantize_pt2e.py torch/ao/quantization/pt2e/_affine_quantization.py,https://github.com/pytorch/pytorch/pull/147035,cccclai,andrewor14,,,
7b4efb492b3,inductor,not user facing,[inductor][refactor] Make _compile_file only used for fbcode (#147106),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/147106,desertfire,yushangdi,,,
5469e5c5566,fx,Untopiced,[export] Minor fix to locals (#146955),torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/146955,angelayi,bobrenjc93,,,
1f41ceb7133,skip,not user facing,[BE][Ez]: Enable ruff rule banning print in assert (#146615),benchmarks/dynamo/runner.py pyproject.toml,https://github.com/pytorch/pytorch/pull/146615,Skylion007,jansel,,,
cdbf677cddb,skip,not user facing,Remove outdated comment in ATen/mkl/Sparse.h about lack of Windows support (#147125),aten/src/ATen/mkl/Sparse.h,https://github.com/pytorch/pytorch/pull/147125,gajanan-choudhary,janeyx99,,,
76dacd5fc76,dynamo,not user facing,[ca] log graph before reodering passes (#146735),torch/_dynamo/compiled_autograd.py,https://github.com/pytorch/pytorch/pull/146735,xmfan,jansel,,,
057bcd3a454,dynamo,not user facing,[ca] eliminate duplicate getitem graph nodes for shape inputs (#146875),torch/_dynamo/compiled_autograd.py,https://github.com/pytorch/pytorch/pull/146875,xmfan,jansel,,,
20a369aa3ab,skip,not user facing,[Intel GPU] Avoid copy when the input of Matmul is broadcasted (#143784),aten/src/ATen/native/mkldnn/xpu/detail/Matmul.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.h,https://github.com/pytorch/pytorch/pull/143784,jianyizh,EikanWang,,,
6419076db90,skip,not user facing,[torch][amdsmi] Look for amdsmi in ROCM_HOME/ROCM_PATH before using rpath (#147117),torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/147117,danzimm,malfet,,,
972e9271341,skip,not user facing,[Break XPU][Inductor UT] Fix XPU Inductor UT failures introduced from community. (#146762),test/export/test_export.py test/higher_order_ops/test_invoke_quant.py test/inductor/test_aot_inductor.py test/inductor/test_inplace_padding.py test/inductor/test_max_autotune.py test/inductor/test_torchinductor_dynamic_shapes.py test/inductor/test_torchinductor_opinfo.py test/inductor/test_unbacked_symints.py,https://github.com/pytorch/pytorch/pull/146762,etaf,EikanWang,desertfire,jansel,
9befdf565aa,skip,not user facing,[Break XPU][Inductor UT] Set input tensors to corresponding device for test case in test_aot_indutor.py (#145248),test/inductor/test_aot_inductor_utils.py test/inductor/test_torchinductor.py torch/testing/_internal/inductor_utils.py,https://github.com/pytorch/pytorch/pull/145248,etaf,EikanWang,desertfire,jansel,
ade5af9430c,skip,not user facing,[XPU] Align XPU convolution_backward output layout between fake tensor and real output tensor. (#146880),aten/src/ATen/native/ConvUtils.h aten/src/ATen/native/mkldnn/xpu/detail/Utils.cpp test/xpu/test_conv.py,https://github.com/pytorch/pytorch/pull/146880,etaf,EikanWang,jansel,,
d3524ecdd6b,composability,not user facing,[Break XPU] Align meta calculation for fft_r2c with _fft_r2c_mkl (#146763),torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/146763,etaf,jansel,,,
c5a9e4a6a0f,inductor,not user facing,[Inductor][CPP] Fix a CPP GEMM Template output data type issue (#146958),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/146958,leslie-fang-intel,jgong5,,,
f0bdc27f74f,skip,Untopiced,Add torch._scaled_mm for CPU (#139975),aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/_inductor/codegen/cpp_prefix.h torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/139975,yanbing-j,jgong5,malfet,mingfeima,
486fc12d7e9,inductor,not user facing,torch: Log a unified waitcounter for torch.compile and triton.autotune (#146723),torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/146723,c00w,davidberard98,,,
05001f04599,skip,not user facing,Add Structured Tracing for Traced Graph Edge Details for AC Debugging (#146634),test/functorch/test_ac_logging.py torch/_functorch/_activation_checkpointing/ac_logging_utils.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/146634,basilwong,Yuzhen11,jansel,,
0b84311842e,export,Untopiced,[export] Generate printers/parsers for serialization enum values. (#147126),torch/_export/serde/schema_check.py torch/csrc/utils/generated_serialization_types.h,https://github.com/pytorch/pytorch/pull/147126,zhxchen17,yiming0416,,,
880e1765446,inductor,Untopiced,[inductor] Fix for pattern file contains 'getitem' fails during impor… (#144980),torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/144980,kareemshaik80,eellison,,,
d473c212fd5,dynamo,Untopiced,Remove code for Python < 3.9 (#147097),torch/_dynamo/bytecode_analysis.py torch/_dynamo/bytecode_transformation.py torch/_jit_internal.py torch/fx/experimental/unification/multipledispatch/core.py torch/package/_stdlib.py torch/serialization.py torch/testing/_internal/common_optimizers.py,https://github.com/pytorch/pytorch/pull/147097,cyyever,albanD,,,
6344ca1dd44,onnx,Untopiced,[BE][Ez]: Apply FURB188: use str remove(pre|suf)fix (#146997),docs/source/scripts/onnx/build_onnx_torchscript_supported_aten_op_csv_table.py tools/autograd/gen_autograd_functions.py tools/testing/clickhouse.py tools/testing/target_determination/heuristics/filepath.py torch/_dynamo/trace_rules.py torch/cuda/__init__.py torch/distributed/distributed_c10d.py torch/distributed/fsdp/_state_dict_utils.py torch/export/_trace.py torch/onnx/_internal/fx/passes/modularization.py,https://github.com/pytorch/pytorch/pull/146997,Skylion007,XuehaiPan,cyyever,jansel,
2d089a5697c,dynamo,not user facing,[dynamo] Remove unintended lru_cache (#147147),torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/147147,anijain2305,williamwen42,,,
92f669e39ce,mps,not user facing,[BE] Use `c10::multiply_integers` in cholesky_impl (#147163),aten/src/ATen/native/mps/operations/LinearAlgebra.mm,https://github.com/pytorch/pytorch/pull/147163,malfet,huydhn,,,
ce38bfd2999,releng,not user facing,[executorch hash update] update the pinned executorch hash (#147157),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/147157,pytorchupdatebot,pytorchbot,,,
331d5cf5608,inductor,not user facing,[inductor] [cpp] Support vectorization for score and mask in FlexAttention CPU (#143638),torch/_inductor/codegen/cpp_flex_attention_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/143638,chunyuan-w,drisspg,jgong5,leslie-fang-intel,
2ef51cfb9db,mps,not user facing,[BE][MPS]  Infer results of functor (#147182),aten/src/ATen/native/mps/kernels/BinaryKernel.metal,https://github.com/pytorch/pytorch/pull/147182,malfet,dcci,,,
278ffd84fcb,mps,not user facing,[MPS][BE] Add copysign integral flavors as functor (#147183),aten/src/ATen/native/mps/kernels/BinaryKernel.metal aten/src/ATen/native/mps/operations/BinaryKernel.mm,https://github.com/pytorch/pytorch/pull/147183,malfet,dcci,,,
10bc8f25b28,mps,not user facing,[MPS][BE] Migrate polar to use functor (#147184),aten/src/ATen/native/mps/kernels/BinaryKernel.metal aten/src/ATen/native/mps/operations/BinaryKernel.mm,https://github.com/pytorch/pytorch/pull/147184,malfet,dcci,,,
bd019c0bb48,inductor,not user facing,[Inductor][CPP] Fix node name for wgt delete (#147056),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/147056,leslie-fang-intel,jgong5,,,
de26ddfbdc7,skip,not user facing,Update torch-xpu-ops commit pin (#146671),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/146671,xytintel,EikanWang,,,
ea188ac0c76,skip,not user facing,[export] Add meta for aten.bincount (#147129),test/export/test_export.py torch/_subclasses/fake_impls.py,https://github.com/pytorch/pytorch/pull/147129,angelayi,pianpwk,,,
dbb86b78adf,dynamo,Untopiced,Add `sys.exc_info` and `sys.exception` (#146498),test/dynamo/test_sys.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/146498,guilhermeleobas,anijain2305,zou3519,,
134723ee1cf,dynamo,Untopiced,Add `WITH_EXCEPT_START` opcode (#146492),test/dynamo/test_ctx_manager.py test/dynamo/test_exceptions.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/146492,guilhermeleobas,anijain2305,zou3519,,
cefd9805de4,dynamo,Untopiced,Add `RAISE_VARARGS 0` (#146493),test/dynamo/test_misc.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/146493,guilhermeleobas,zou3519,,,
06f4a5c0e57,skip,not user facing,Nccl update to 2.25.1 for cuda 12.4-12.8  (#146073),.ci/docker/ci_commit_pins/nccl-cu11.txt .ci/docker/ci_commit_pins/nccl-cu12.txt .ci/docker/common/install_base.sh .ci/docker/common/install_cuda.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/build-manywheel-images.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml .gitignore .gitmodules cmake/External/nccl.cmake setup.py test/dynamo/test_repros.py third_party/nccl/nccl torch/testing/_internal/common_distributed.py,https://github.com/pytorch/pytorch/pull/146073,atalman,Skylion007,fduwjj,kwen2501,
059dfe20818,skip,Untopiced,"Revert ""update kineto submodule (#147015)""",third_party/kineto,,,,,,
e06ee4aa9fb,skip,Untopiced,"Revert ""Nccl update to 2.25.1 for cuda 12.4-12.8  (#146073)""",.ci/docker/ci_commit_pins/nccl-cu11.txt .ci/docker/ci_commit_pins/nccl-cu12.txt .ci/docker/common/install_base.sh .ci/docker/common/install_cuda.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/build-manywheel-images.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml .gitignore .gitmodules cmake/External/nccl.cmake setup.py test/dynamo/test_repros.py third_party/nccl/nccl torch/testing/_internal/common_distributed.py,,,,,,
c3853d924f2,skip,not user facing,Introduce new template heuristic for triton autotune configs (#144985),torch/_inductor/choices.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_plus_mm.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/kernel/unpack_mixed_mm.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/template_heuristics.py,https://github.com/pytorch/pytorch/pull/144985,jataylo,jansel,,,
e8fbc86de02,cuda,new features,Make torch.cuda.gds APIs public (#147120),.ci/pytorch/smoke_test/smoke_test.py docs/source/cuda.rst test/test_cuda.py torch/cuda/gds.py,https://github.com/pytorch/pytorch/pull/147120,mikaylagawarecki,albanD,,,
0d16188c064,releng,not user facing,[CI] Use job name to index into test times json (#147154),.github/workflows/upload-test-stats.yml test/run_test.py,https://github.com/pytorch/pytorch/pull/147154,clee2000,huydhn,,,
8b5ee275fb4,mps,bug fixes,[MPS] Fix cholesky_ex for empty inputs (#147159),aten/src/ATen/native/mps/operations/LinearAlgebra.mm,https://github.com/pytorch/pytorch/pull/147159,malfet,albanD,,,
20a9938069f,inductor,not user facing,try print stacktrace for error (#147061),torch/_inductor/codegen/common.py,https://github.com/pytorch/pytorch/pull/147061,henrylhtsang,Skylion007,,,
aac5d1a2890,skip,Untopiced,"Revert ""Add torch._scaled_mm for CPU (#139975)""",aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/_inductor/codegen/cpp_prefix.h torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,,,,,,
5517eb4398b,skip,Untopiced,"Revert ""[cutlass backend] Do not change dtype of GEMM template (#146877)""",torch/_inductor/codegen/cuda/gemm_template.py,,,,,,
9a1eac67046,onnx,bug fixes,[ONNX] Handle number of outputs in builder (#147164),torch/onnx/_internal/exporter/_building.py,https://github.com/pytorch/pytorch/pull/147164,justinchuby,titaiwangms,,,
765bc30ab9e,onnx,improvements,[ONNX] Set warning stacklevel so it appears at the torch.onnx call site (#147165),torch/onnx/_internal/exporter/_compat.py,https://github.com/pytorch/pytorch/pull/147165,justinchuby,Skylion007,,,
58f654b5ad8,onnx,not user facing,[ONNX] Consolidate constants to a single location (#147166),torch/onnx/_internal/exporter/_constants.py torch/onnx/_internal/exporter/_core.py,https://github.com/pytorch/pytorch/pull/147166,justinchuby,titaiwangms,,,
272ead7b5e9,fx,not user facing,Make fx.node.map_arg() and .map_aggregate() generic (#146248),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/test_fx.py test/typing/pass/arithmetic_ops.py torch/_tensor.py torch/distributed/pipelining/stage.py torch/fx/node.py,https://github.com/pytorch/pytorch/pull/146248,rec,Skylion007,XuehaiPan,,
6f035d8462e,cuda,Untopiced,[torch] Make amdsmi cdll hook private (#147207),torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/147207,danzimm,PaulZhang12,,,
dd86491b351,skip,not user facing,[cutlass backend][BE] refactor tests to remove duplicate logic (#146743),test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/146743,henrylhtsang,ColinPeppler,chenyang78,,
d38db94689f,inductor,not user facing,[inductor][refactor] Move _compile_file to cpp_builder (#147202),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/147202,desertfire,yushangdi,,,
eecee5863e6,skip,not user facing,Nccl update to 2.25.1 for cuda 12.4-12.8  (#146073),.ci/docker/ci_commit_pins/nccl-cu11.txt .ci/docker/ci_commit_pins/nccl-cu12.txt .ci/docker/common/install_base.sh .ci/docker/common/install_cuda.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/build-manywheel-images.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml .gitignore .gitmodules cmake/External/nccl.cmake setup.py test/dynamo/test_repros.py third_party/nccl/nccl tools/build_pytorch_libs.py,https://github.com/pytorch/pytorch/pull/146073,atalman,Skylion007,fduwjj,kwen2501,
85a82c5bc8a,skip,not user facing,[cond] make cond re-dispatch in proxy mode (#146954),torch/_higher_order_ops/cond.py torch/_higher_order_ops/utils.py,https://github.com/pytorch/pytorch/pull/146954,ydwu4,zou3519,,,
12247652863,dynamo,not user facing,[cond] make cond call fake kernel in dynamo (#147045),test/functorch/test_control_flow.py test/inductor/test_aot_inductor.py torch/_dynamo/utils.py torch/_dynamo/variables/higher_order_ops.py,https://github.com/pytorch/pytorch/pull/147045,ydwu4,zou3519,,,
a5c0dab9007,skip,not user facing,[AOTInductor] Guard RAII_cpuMalloc with macro (#147150),torch/csrc/inductor/aoti_runtime/model.h,https://github.com/pytorch/pytorch/pull/147150,muchulee8,henrylhtsang,,,
76f57e184a1,dynamo,not user facing,[dynamo] Make SliceVariable a subclass of VariableTracker (#147046),torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/147046,anijain2305,StrongerXi,,,
49727bbc9d8,inductor,Untopiced,Turn on prologue fusion (#147008),test/inductor/test_max_autotune.py torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/codegen/simd.py torch/_inductor/config.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/147008,eellison,masnesral,,,
a9ae3340ca9,skip,Untopiced,Fix triton masked loading for non-block tl.loads (#144782),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/144782,isuruf,eellison,,,
933f921b366,distributed,Untopiced,[PT][FSDP] support custom all reduce hook across FSDP units (#147114),test/distributed/_composable/fsdp/test_fully_shard_comm.py test/distributed/_composable/fsdp/test_fully_shard_init.py torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py torch/distributed/fsdp/_fully_shard/_fully_shard.py,https://github.com/pytorch/pytorch/pull/147114,xunnanxu,awgu,,,
bf0c89a72f2,dynamo,not user facing,[dynamo] fix error message when logging graph that contains hops (#147227),torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/147227,ydwu4,zou3519,,,
c1fcba36480,inductor,not user facing,[Inductor] Fix the lowering of squeeze when input is not contiguous (#146746),test/inductor/test_unbacked_symints.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/146746,leslie-fang-intel,eellison,jgong5,sanchitintel,
4233a779603,skip,not user facing,update kineto submodule to include fix for windows build (#147195),third_party/kineto,https://github.com/pytorch/pytorch/pull/147195,briancoutinho,cyyever,davidberard98,sraikund16,
8f291e8c001,jit,Untopiced,Fix clang-tidy warnings in torch/jit (#146963),torch/csrc/jit/backends/coreml/objc/PTMCoreMLTensorSpec.h torch/csrc/jit/backends/nnapi/nnapi_backend_preprocess.cpp torch/csrc/jit/backends/xnnpack/compiler/xnn_compiler.h torch/csrc/jit/backends/xnnpack/xnnpack_backend_preprocess.cpp torch/csrc/jit/codegen/fuser/kernel_spec.h torch/csrc/jit/ir/irparser.cpp torch/csrc/jit/mobile/module.cpp torch/csrc/jit/passes/freeze_module.cpp torch/csrc/jit/passes/hoist_conv_packed_params.cpp torch/csrc/jit/passes/onnx/function_substitution.cpp torch/csrc/jit/passes/onnx/list_model_parameters.cpp torch/csrc/jit/passes/onnx/remove_inplace_ops_for_onnx.cpp torch/csrc/jit/runtime/argument_spec.h torch/csrc/jit/runtime/interpreter/code_impl.h torch/csrc/jit/runtime/jit_exception.cpp torch/csrc/jit/runtime/logging.h torch/csrc/jit/runtime/register_prim_ops.cpp torch/csrc/jit/serialization/import.cpp torch/csrc/jit/serialization/python_print.cpp torch/csrc/jit/serialization/unpickler.h torch/csrc/jit/tensorexpr/bounds_overlap.h torch/csrc/jit/tensorexpr/expr.h torch/csrc/jit/tensorexpr/loopnest_randomization.cpp torch/csrc/jit/testing/file_check.cpp,https://github.com/pytorch/pytorch/pull/146963,cyyever,davidberard98,,,
9919375cf17,releng,not user facing,[executorch hash update] update the pinned executorch hash (#147241),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/147241,pytorchupdatebot,pytorchbot,,,
8daa742e8b0,skip,not user facing,Remove code for Python < 3.9 (#147181),cmake/Dependencies.cmake torch/csrc/dynamo/debug_macros.h torch/csrc/dynamo/eval_frame.c,https://github.com/pytorch/pytorch/pull/147181,cyyever,albanD,,,
9dc702875de,dynamo,not user facing,[dynamo][mappingproxy][inspect] Support existing types.MappingProxyType (#147217),test/dynamo/test_dicts.py torch/_C/_dynamo/guards.pyi torch/_dynamo/guards.py torch/_dynamo/side_effects.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/dicts.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/147217,anijain2305,williamwen42,,,
a30f145101d,inductor,not user facing,[inductor] Don't leak pointers to cpp_wrapper with lru_cache (#147233),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_gpu.py,https://github.com/pytorch/pytorch/pull/147233,jansel,yanboliang,,,
9e0b3e9b6c1,inductor,not user facing,[Inductor] Fix Inplace Buffer inner name conflict (#147199),torch/_inductor/codegen/common.py,https://github.com/pytorch/pytorch/pull/147199,leslie-fang-intel,jansel,,,
86be5d4421e,skip,not user facing,remove unnecessary xpu availability check when retrieving aot flags (#146966),torch/xpu/__init__.py,https://github.com/pytorch/pytorch/pull/146966,jingxu10,EikanWang,albanD,dvrogozh,
6ca5c22e316,skip,Untopiced,"Revert ""Enable fp16 linear layers in PyTorch via ACL (#144992)""",aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/mkldnn/Matmul.cpp aten/src/ATen/native/mkldnn/Utils.h test/inductor/test_mkldnn_pattern_matcher.py,,,,,,
75a4b738161,python_frontend,not user facing,utils: Update md5 call to be fips compliant (#147252),torch/utils/hipify/hipify_python.py,https://github.com/pytorch/pytorch/pull/147252,seemethere,Skylion007,huydhn,malfet,
4ab967c44d4,distributed,Untopiced,all reduce non strict (#147133),test/export/test_export.py torch/_export/non_strict_utils.py torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/147133,avikchaudhuri,tugsbayasgalan,,,
44ee9ca5932,inductor,not user facing,[inductor] Add type annotations to _inductor/utils.py  (#144108),torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/simd.py torch/_inductor/compile_fx.py torch/_inductor/graph.py torch/_inductor/lowering.py torch/_inductor/metrics.py torch/_inductor/mkldnn_ir.py torch/_inductor/output_code.py torch/_inductor/select_algorithm.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/144108,rec,eellison,,,
1677a310195,inductor,not user facing,[Inductor] Fix 3D tiling with permute (#147249),test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/codegen/simd.py,https://github.com/pytorch/pytorch/pull/147249,blaine-rister,jansel,,,
8f20026bcb5,sparse_frontend,not user facing,[Intel GPU] Support SparseCsrXPU codegen (#144722),aten/src/ATen/SparseCsrTensorImpl.cpp aten/src/ATen/native/sparse/SparseCsrTensor.cpp torchgen/model.py,https://github.com/pytorch/pytorch/pull/144722,cfgfung,EikanWang,albanD,guangyey,
4bacd13c92d,releng,not user facing,[executorch hash update] update the pinned executorch hash (#147273),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/147273,pytorchupdatebot,pytorchbot,,,
0fb5b224b78,distributed,not user facing,[DCP] Cache save plans: planner helpers and interface updates (#147116),test/distributed/checkpoint/test_planner.py torch/distributed/checkpoint/planner.py torch/distributed/checkpoint/planner_helpers.py,https://github.com/pytorch/pytorch/pull/147116,saumishr,MeetVadakkanchery,huydhn,,
ed3b119c40d,skip,not user facing,Skip unsupported types by MPS in `test_torchinductor.py` (#147211),test/inductor/test_mps_basic.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/147211,malfet,Skylion007,dcci,jansel,
607379960bc,skip,Untopiced,xpu: support sycl with torch.utils.cpp_extension APIs (#132945),docs/source/cpp_extension.rst test/cpp_extensions/setup.py test/cpp_extensions/xpu_extension.sycl test/test_cpp_extensions_aot.py test/test_cpp_extensions_jit.py torch/utils/_cpp_extension_versioner.py torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/132945,dvrogozh,albanD,guangyey,malfet,
ee38a32c55b,dynamo,not user facing,[Dynamo] support `isinstance(...)` check for type tuple (#146984),test/dynamo/test_functions.py test/dynamo_expected_failures/TestScript.test_isinstance_dynamic torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/146984,XuehaiPan,jansel,,,
b16ae97ad03,nn_frontend,Untopiced,Generalize mixed precision in DDP (#146808),torch/nn/modules/batchnorm.py torch/nn/parallel/distributed.py,https://github.com/pytorch/pytorch/pull/146808,zhangxiaoli73,guangyey,gujinghui,wconstab,
dd5d0ea6bb0,skip,Untopiced,"Revert ""xpu: support sycl with torch.utils.cpp_extension APIs (#132945)""",docs/source/cpp_extension.rst test/cpp_extensions/setup.py test/cpp_extensions/xpu_extension.sycl test/test_cpp_extensions_aot.py test/test_cpp_extensions_jit.py torch/utils/_cpp_extension_versioner.py torch/utils/cpp_extension.py,,,,,,
d27ecf85db3,skip,Untopiced,xpu: support sycl with torch.utils.cpp_extension APIs (#132945),docs/source/cpp_extension.rst test/cpp_extensions/setup.py test/cpp_extensions/xpu_extension.sycl test/test_cpp_extensions_aot.py test/test_cpp_extensions_jit.py torch/utils/_cpp_extension_versioner.py torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/132945,dvrogozh,albanD,guangyey,malfet,
5c0c99f658a,mps,not user facing,[MPS][BE] Use stubs for floor/ceil/round/trunc (#147286),aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/operations/BinaryOps.mm aten/src/ATen/native/mps/operations/UnaryOps.mm aten/src/ATen/native/native_functions.yaml,https://github.com/pytorch/pytorch/pull/147286,malfet,Skylion007,,,
a8fa4bcfd27,jit,Untopiced,[StaticRuntime] Support a new pattern (aten::to with 5 inputs) for ClipRangesToGatherToOffsets (#147189),torch/csrc/jit/runtime/static/passes.cpp,https://github.com/pytorch/pytorch/pull/147189,coufon,hanyilou123,,,
e738f7ba23d,quantization,not user facing,[BE]: Enable ruff rule SIM113 (#147290),benchmarks/dynamo/parse_logs.py benchmarks/fastrnns/custom_lstms.py benchmarks/sparse/dlmc/utils.py pyproject.toml test/dynamo/test_aot_autograd_cache.py test/functorch/test_eager_transforms.py test/onnx/test_pytorch_onnx_onnxruntime.py torch/_strobelight/examples/cli_function_profiler_example.py torch/ao/ns/fx/mappings.py torch/ao/quantization/_correct_bias.py torch/export/unflatten.py torch/testing/_internal/common_quantization.py torch/testing/_internal/distributed/rpc/rpc_test.py torch/utils/_strobelight/examples/cli_function_profiler_example.py torchgen/_autoheuristic/train_decision.py,https://github.com/pytorch/pytorch/pull/147290,Skylion007,jansel,,,
198ffbdf11b,mps,Untopiced,[MPS] Implement and test round.decimals (#147266),aten/src/ATen/native/mps/kernels/UnaryKernel.metal aten/src/ATen/native/mps/operations/UnaryKernel.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py,https://github.com/pytorch/pytorch/pull/147266,malfet,Skylion007,,,
ae351d4d0ee,xpu,Untopiced,[Intel GPU] allow_tf32 for oneDNN backend - XPU part (#137570),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/native/mkldnn/xpu/detail/Conv.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.h test/xpu/test_conv.py torch/_C/__init__.pyi.in torch/backends/mkldnn/__init__.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/137570,ZhiweiYan-96,EikanWang,atalman,guangyey,
359165734b1,releng,not user facing,[executorch hash update] update the pinned executorch hash (#147294),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/147294,pytorchupdatebot,pytorchbot,,,
424c1b82e05,inductor,not user facing,[Inductor][CPP] Add the legalize low fp support for index expr (#147298),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/147298,leslie-fang-intel,jgong5,,,
3d251e6512e,mps,not user facing,[BE] Switch all structured funcs to stubs (#147296),aten/src/ATen/native/mps/operations/UnaryKernel.mm aten/src/ATen/native/native_functions.yaml,https://github.com/pytorch/pytorch/pull/147296,malfet,dcci,,,
2b30e94fc04,mps,not user facing,[BE] Make `exec_unary_kernel` take TensorIterator as argument (#147297),aten/src/ATen/native/mps/operations/UnaryKernel.mm,https://github.com/pytorch/pytorch/pull/147297,malfet,dcci,,,
ae5f7fec823,xpu,Untopiced,[Intel GPU] Enable fp64 GEMM (#140677),aten/src/ATen/native/mkldnn/xpu/Blas.cpp aten/src/ATen/native/mkldnn/xpu/detail/Matmul.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.cpp test/inductor/test_torchinductor_opinfo.py test/xpu/test_gemm.py,https://github.com/pytorch/pytorch/pull/140677,ZhiweiYan-96,EikanWang,desertfire,guangyey,
e8b20f6ef39,mps,Untopiced,[MPS][BE] Turn `exec_unary_kernel` as MetalShaderLibrary method (#147299),aten/src/ATen/native/mps/MetalShaderLibrary.h aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/kernels/SpecialOps.metal aten/src/ATen/native/mps/operations/SpecialOps.mm aten/src/ATen/native/mps/operations/UnaryKernel.mm,https://github.com/pytorch/pytorch/pull/147299,malfet,dcci,,,
71855a1cad1,skip,not user facing,Update slow tests (#147308),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/147308,pytorchupdatebot,pytorchbot,,,
1b29de5c053,quantization,performance,Add NEON implementation for 8 bit quantized embedding bag on aarch64 (#147322),aten/src/ATen/native/quantized/cpu/qembeddingbag.cpp,https://github.com/pytorch/pytorch/pull/147322,annop-w,malfet,,,
22fae4c5f94,skip,Untopiced,Add torch._scaled_mm for CPU (#139975),aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/_inductor/codegen/cpp_prefix.h torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/139975,yanbing-j,jgong5,malfet,mingfeima,
516133ddb03,quantization,Untopiced,Fix arvr macOS buck pytorch builds (#147292),aten/src/ATen/native/quantized/cpu/qnnpack/buckbuild.bzl,https://github.com/pytorch/pytorch/pull/147292,stepanhruda,Skylion007,,,
6c0e7463af6,skip,not user facing,Fix test_device_memory_allocated (#147311),test/test_xpu.py,https://github.com/pytorch/pytorch/pull/147311,Stonepia,Skylion007,guangyey,,
1393f9a76ca,releng,not user facing,[ROCm] Update inductor-perf-test-nightly-rocm.yml to use the correct labels & frequency (#147221),.github/workflows/inductor-perf-test-nightly-rocm.yml,https://github.com/pytorch/pytorch/pull/147221,amdfaa,huydhn,pruthvistony,,
59b7e52ad8f,skip,not user facing,Fix non-bitwise type annotations for Tensor operators (see #145838) (#146845),test/typing/pass/arithmetic_ops.py torch/_decomp/decompositions.py torch/_prims/rng_prims.py torch/_tensor.py torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py torch/distributions/exp_family.py torch/optim/adam.py torch/optim/nadam.py torch/optim/swa_utils.py,https://github.com/pytorch/pytorch/pull/146845,rec,Skylion007,,,
708428704e5,quantization,Untopiced,patch for block-wise quantization + pt2e (#146946),test/export/test_passes.py torch/_export/passes/constant_folding.py torch/ao/quantization/pt2e/utils.py torch/ao/quantization/quantize_pt2e.py,https://github.com/pytorch/pytorch/pull/146946,cccclai,andrewor14,jerryzh168,,
71484a21067,skip,not user facing,[pt2-benchmarks] Compiler reset on every run (#147313),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/147313,anijain2305,jansel,,,
bb8c4ecc6d7,sparse_frontend,Untopiced,Allow XPU device for validating the arguments to sparse compressed tensor factory functions (#147306),aten/src/ATen/native/sparse/SparseCsrTensor.cpp,https://github.com/pytorch/pytorch/pull/147306,xytintel,EikanWang,Skylion007,guangyey,
59915b8dece,quantization,not user facing,[Intel GPU] qlinear at XPU backend (#133307),aten/src/ATen/native/mkldnn/xpu/detail/QMatmul.cpp aten/src/ATen/native/mkldnn/xpu/detail/oneDNN.h aten/src/ATen/native/mkldnn/xpu/qlinear.cpp test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py torch/_inductor/mkldnn_ir.py torch/_inductor/mkldnn_lowerings.py torch/_meta_registrations.py torch/ao/quantization/quantizer/xpu_inductor_quantizer.py,https://github.com/pytorch/pytorch/pull/133307,ZhiweiYan-96,EikanWang,guangyey,jerryzh168,
6a2bb629ec6,skip,not user facing,Update torch-xpu-ops commit pin (#147302),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/147302,xytintel,EikanWang,,,
59a08138c58,releng,not user facing,[executorch hash update] update the pinned executorch hash (#147345),.ci/docker/ci_commit_pins/executorch.txt,https://github.com/pytorch/pytorch/pull/147345,pytorchupdatebot,pytorchbot,,,
49e8f9c9651,skip,Untopiced,"Revert ""Add torch._scaled_mm for CPU (#139975)""",aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/_inductor/codegen/cpp_prefix.h torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,,,,,,
d9cf1debf98,jit,not user facing,[ROCm][Windows] Fix clang-cl error related to -Wmissing prototypes enabled (#146981),torch/csrc/jit/codegen/fuser/cpu/fused_kernel.cpp torch/csrc/jit/codegen/fuser/cpu/temp_file.h,https://github.com/pytorch/pytorch/pull/146981,m-gallus,Skylion007,cyyever,,
b10ba0a46cb,releng,not user facing,Unify all sympy versions to avoid conflicts within PyTorch (#147197),.ci/docker/requirements-ci.txt .github/requirements/pip-requirements-macOS.txt .lintrunner.toml setup.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/147197,FFFrog,malfet,,,
0c8028e8772,export,Untopiced,[export] Loosen symint input serialization (#147237),test/export/test_serialize.py torch/_export/serde/serialize.py,https://github.com/pytorch/pytorch/pull/147237,angelayi,avikchaudhuri,,,
6edc419d696,skip,not user facing,Update torch-xpu-ops commit pin (#147358),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/147358,xytintel,EikanWang,,,
a00d2b51447,releng,not user facing,s390x: add cleanup for cancelled docker image builds (#147110),.github/workflows/build-manywheel-images-s390x.yml,https://github.com/pytorch/pytorch/pull/147110,AlekseiNikiforovIBM,huydhn,,,
5d675de7547,distributed,Untopiced,Update ck (#144799),.gitignore aten/src/ATen/CMakeLists.txt aten/src/ATen/native/transformers/hip/flash_attn/ck/CMakeLists.txt aten/src/ATen/native/transformers/hip/flash_attn/ck/add_make_kernel_pt.sh aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_bwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_00042c36bc588e60a7c8a9ba297a8a25d8ac0660.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0029076f83a3dc695a167beda6fe19230a2b114b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_006c417a52a1bd7c55e45d111483d26f4480caeb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_008f2429c678d13386a06e8d8b15c4b480940ff3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_00a2adbe938d458d51ca5fc4020667a215b672a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_012c0f480917c329f4c3c6c666cf32af2d82b294.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_014c209d5cfc6b965bfd78c64bf132c0154e32be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0153ec18d3ded0f8bdc6459ea5757ebd94d9faf2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01ac1a2ecf9a487809e46faa92e267df2d47de91.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01ca79005067e20e4eed5a72ff9187cde702cd1c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01cb354dddef6e99e4ac843f2adafcddfc58d520.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01d12033d59ce2799a2a024e5d9232325ccf1320.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01d3b034a2d8d0b83c0aefa4faac6c3f28ce737f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01e2428c5447aa9a78f79f73f31cf685c586872d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01e8aedb7b7d77f44a46b2e9b7a826f245aaf4a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01e8f0df0c54ce619e5b66441b3c96a5e18b05d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01ee0083f6df962c4a754cd3295b1a436c590a0e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_01f74764c3c3284fdd1b67d0ea781c2261ed0de6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0225857454eaab2eb664aef7a0849ce12c32fdf9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0237c76137df14fb808ade8bd6837045f2aaa5c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0271bd8b7c270e1593871b638288a4923342c446.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_02d88a03cd3966dd0cff550065f58c3ffecfff6c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_02ff94e3c787a7b06ffc90c25777fa74f225e32c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_030a759dcc92028b4c6f317fc230b98cb929e806.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_031b12f9fd94e01aaff2c0da4f35f346822087e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_036887daf6cc092e7422a17882488e59cecfb643.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_037c6c80fcec3eb8b0bef50ad6af6d27bf5447f5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0392491c5a6dfc742c2be483419a40f6a7a7ea56.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_03a71615a088e972c998f9c7cb44566c268c5124.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_03ff035717140f7385282419598cb4fb2881ce8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_041a0718891596ddac1fb0088637029233ccbe60.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_042a156e9eb935555ab14a84461959b466c2fb5b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04641230fe9a50a221047f7a1df8a370f72805b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04c363e11d202c6d2f4bb753661c5a2043edc0ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04caeecbc01667ec6f5599358a0a20423aa9a00b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04f39b453505f68a5091f68b1c3de48369d1e7ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_04ffca078cfab8bc6c4ccd1cc8994a1bb4a88ea7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0502e718337eab7d47aa65cea7d3c5f641484520.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0513b2f3bd8ad51315aadb7f63737201898adca8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_053981d9e7af2ebc0f91e61ac5e25cbe68c95bd8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_054fda16133a0d25077967b05425f9128e1fe1a5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05538339c21c92c53d237865d72debaaf2ee5075.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0595316f0dfffda03e5296b959a49ec3f3c48d67.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05dfe927fd64a564c5fad537fb7c41ee9c94c2c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05e60b3ab7477f9edc8576a8bf43e3a62b8d5ef8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_05f794c7023cbb7e35f1fd1ae45bd2377bfbc520.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0628931bf5cc1daa6e106cf60bb21fa1aac6b1df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_062c8c3c1cf6c33af4574099e9b6ac54a55ad776.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0682150e93f547e00f13cd8984779bf49b91e50c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_069c663be0267c009be4814e9e4e7c13ec999411.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_06ae52ef937cc27c544e32025ea0dadb7fad982d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_06b74acd9abfbd1c4ec2f4c718eeb92a0bca7bab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_06ba94794a14f0f0022af6f5f3c16e1e16959d4c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_071751b1012b90f7b57f8591cd06ae1fd27d9cd3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0766e7aa4b263a811408b285213e47176ee2bdaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_076b3beb57b30afb30636f948e3989b346b38d20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0789852b0cd3cc030c78b28f2fd5b6b0546382a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_078b96ad691a85eebd18586db0b62b8911016d9c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_07c3fc96d2bebe546dce6ebf46e5c7a519959599.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_07ff04fcc273e469737512893ea3fb5876ac131d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0801c56831b4c6428200db6318638a2129bb197a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0836d5dfc0f939ab9a4064b403339373caf35b56.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0842c4e3aabdf55405b3ce09ce1899245ddf11ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_085722b43cde5f37242edb071f639da7c4a0bd48.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0878b9aa31429d23a93cd953cc6a2fc5f43d0d3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_089a347aef8a920e3b59d5ffe71fc5bfe002609c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_089de13222caec1483207d4a54249f8da4f9c151.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_091cb49c1958fb4342d79f367ea93cf2b472f785.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_093834d4d3fe76e1745e4482c6b51b550c6f3dfc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09513bff5c1da6aadf11d2e8272a422eabff21bc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_096863cd93d1b105a617d0daa1d4f37d7fb6b893.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0968cebd81ade762c2f92fffc0153fa7a2b91eb5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_096e888c52d0f4a5847d7515fcc66208b1ff40d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_097b3e1dae9bfb2e89398706508f8e01966fd4ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09d76cca48b71dbcc9bd96734787209fee4c9a74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09e50367b62bb09071e28b44235a7c112645a706.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_09ecb6347009f6a5d5530a6acf90f9f40288cbcf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a2b116fd5065109aae46ee547e4f49ad0e9d6e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a4e76d89b175e1d9fd2e9fb908d5fce1ebb945d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a55ed15ef58c941e06dda890aeb530e28eb7bba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a672fca51de618e3441cf8764e8e83eb782f2c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a68c2f9a3acdd787b81be455cbc7836c8bfd90c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a89417a043556970f72eebd48b4f3e7ac15377a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0a92671b6ea99891c0d69b1c793f4d131b9a82ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0aafb881e34a3794970a1282af740b3f19c138b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ace6e29e1d3060c3086c08fe27b471e375f9c75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ad9d68fcee021437e13ffdf94d78252205f5a31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b2647b5982405a48e8c8888552a4b89386ccdd9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b2efefea81036641561bed80c75d77651176f74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b3153af7bcdba33115a0d31f121fd76be2ffbcc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b532fcf26f90c82a792cde7943634f667c1d033.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b90a0186d8b8004e3f19886c7992c8e04d0e066.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0b9585ba1c10acf67115c5899b3546608541820d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0bb81407c8a2b3cdc5fecf655b3ad64d5d729cc9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0bc7910aac798f0555e9e505ad7f177c9fbbd92c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0be8cf70c6be969ecfca675782c860b5b75ac089.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0befed50a89d80c22b2c8c3d5ba67d73c3d0190e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c32a2d9701e23dd930119c4ee8089042b5b0ac5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c3b2ec99fa7b09c7f78dcc3142a661d686044ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c8a0bb89a6f05289c0405df5126fa0cc16252e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c93c65e5942a2f43f2e491547add02777dd2eee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0c9bd38b8f9009d932ec49204fdea39a52885246.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0caeedaa7d50f1741d618fb6c573529eebb075b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0cdef49859c80c6b3ba18eb2fb4c35c72abc1cf2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0cee6b9427c164d78994150305a47f73954a67c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0d0e0147a92061d32608a34e7b47bd534eb787fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0d13a4c8d169877da6408584dc1f20a6f7c5e3aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0dde401aa76cb5425563cbbdb0362748148da3ca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e007c36231ccdae12f102eacca1f74b0711b9c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e0a2370f2a320484d8f9f21e3197425c2dbe9ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e1dbc9c433ce8ec33ace9e62550261d613db582.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e3f4cd28a4c06cc109f6a0798a77844bcc750b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0e661b5f30566d1f159f060c264849c7ae4772f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ebacd06455ab20eba78b389462946716b5819f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ef309b923172f4c0fb38d9b9f5325b33b4877c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0ef9b9413697d6f4573c6605bff6f58d027c5016.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0efdaa9266a5a464009297dc59db92504f8bf1a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0f0c699d9c3b0ed62097e38ba05e40e815cf474e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0f588dcb2ef86677ebf84e406eb802e9921d1f1e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fbb0bef3b388867e75d7a8a187b8b4b650a42ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fbddf533661642d84bf5a16149692d5a892182a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fcb7492feb79e27e0bda73e57ef7dab410e2bb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_0fd4068ea93fcf4df463e3bf3a6898d23b65da7f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_103186dbad604763008e0204a1ea90baecef8877.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1037f1bc50c4a65dac09ba56b701256b701c4322.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_10a055e5c3d6a953d470db5dc21449766248058a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_10c24f1f9009e46afa3a59193784cc2575f79056.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_10ceed95b0a0a01f844678717c88e0426fb503fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1132b11429034d96d82c82dbfdb69e460ad8a564.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_11e7df31541c3aa919e9825ad7dc4432f9a03c0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_11ff174ff2175e9ec22ac3a0fa59dd7713b79643.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1211733062ed30b876f1d63bffa642d77e258dd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_12207f4b6e7fac27d6c16493a5373f448a2aaae8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1241814f76107d74ed069ecec99a248676487eee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_12d5c8a4988efe60ef7943ecd73e18a28a736583.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_12d60c8abecb3bc9b84b0ea7851628ab17d8b0b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_131691f01cc7f29affb88152dd48c7a484315dcd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_131c1fdc4206bb952b2fea675f24e3b09f605eef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_133c51948cf8584900807998da14d788039f53b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_135ea67de101135ed5fe04f5cab1ec1d7b3714bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_137fa6780d9e6bde10aec10a875c039fdbbc652e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1386cd75411e61a8dbbaf2b916e62f4f5f99104f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_13d5f2ec83b3331654e37ea0b44d88cd98abaa37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_13f747525ad31e76c88774fb2208e470da9c2310.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14221590b90c48d3cf259fb4e834ccfaf7f3209b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_144f19363ef26efd36f0436cfa9f84f181a8824c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_146eb8c40e3146e06936f3141b2c4d92a578ddec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14baaaf1e90a075ab802c6e7d97c4b1605c8bd72.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14c4ebd1792c781d219bd21b691b575f64635730.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14d11aad7b666f500f68b264a2fcca6dfc5f1a05.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14d4630876785655bd4950566e81ae0b645c0d3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14f77aeeafe4b28f314fde5ebccfd2a554872781.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_14fea611f3c253aebf726af3e5fdb7e63e18e13a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_151a4425b411596c46c7032f6b83d3152a0e0cd4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_153e897098539c3466da9d7a37234daf16476277.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1552dc38d26f6badb7a9bcb5ce9124d54cc45ed3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_155bafb551768855c8c01faa63e44764ebe6c110.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_155c3549d067464d186a99b8205317cc000d4898.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1573e3d855d28c54af612ab950b081302891d56d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_157768cd725813f8111d265cfdfea7f42034e5e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_157b89d8d625b8244b5cceaa4d3e5fc5a09c8989.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_158d5ce564c3ae1eefb54e3d41dde2604560ef4a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_159ee1f1b44d1a8fbaead65d8449413bb616d15e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15b255dde1a9d915e582ee2a83de7d83190c6a24.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15cf7068183421b141ed5d6e7fe902d06b6492a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15dc02ea7e0908cf0bd48034f5a49debfaa36219.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15e8e1ab8c63db96843054bb7a98d708ae6a9c44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_15fe3e8f4add16a088fe44458353fa7c0c4f9658.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_16047b5544acef40e39932672cac6f562e200948.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1621507cf219fe608715d4e5bb6e5764022e2d61.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_162b0dfbe3f615b1d164290799b2457437a0044b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_164a947a6c2ba83a5b1cb7074aee0bdac6c9c64e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_165dfb45658df8f1ae8dc0738ac9614740f2576c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_167f5328b035ed59a6f05dfee31edd704c4b07ee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1687ddf65ce4ed2997583e20fee9f201e86633b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_16f94f5c65c37624f5458c165daf83517d9e3c81.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_173c44dd85077e6b12dd06fdcf6b11ba349e1866.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_17b9b96edda151072215502cc2b606bf1f6f0b03.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1847fef2c06ea581b0ab31af1cb0556c572696ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_187963e1969301abfa61d06afc97faea2bb4efb1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1886d4bf54b3a4a9e093360998b2059b3c03d072.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_188a70d526394e254274df95de0727850820326c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1899e28aff2fb168cdc3af7132dd7fd09c2e1ced.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_18a4d71b31c451a50df7996e3db864bc3c3882ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_18b92b4e249195ac3e0c74d246585a4c9e0992fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_18ed7195a9443c84956c3f32839cb3ab9056bdfc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1914250fce818584291c69a5f058a58cfbd83df9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_193699a5daa14ca2def07489e0b563149bc403f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_19af6a7f9e5020e8d0f0ca0f6258001f6ce592c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_19cd9f7b08cec83736605af63d9fcaf463a1aea4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_19df4e13108e043361e9528b71df56f04f696a0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a11dd5ebb989503a1c182684e7f247e2f8cd9c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a236be9da05a07d11cd28034d90cdf89941a172.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a5e18f6333ed2cce509f07cb8bd5868951d66a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a6785392af35e27d6697b584cb6f17a766d3fee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a6bc2762b95d550485aa720edaf71138d94cd07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a8da3e6ab050262b659c801ccf9a14787d7f176.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a96f0ac76f117e66eba97cb990c2350561ec2ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a98bcbe900f8c141136d18c114b02fffbe8bca1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1a99b2625adffa8215276bb88fc65bae944b846b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1acf2f892742b1d236d2b31a8185c6869126adad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1b3e7c8969027d3316875f33dc50fe022e05ce37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1be43f8b629e7039f57b95866d5777273377470d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1be746990a2032f0363ad9f9112cc994983f4706.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1bf767e7104cfc8322f26df35907fbf04b8948f3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1c1b0f85e085dd0769c566fb16aafe5ab5952714.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1c2a2d78176e3f0a78e3ad78217e75a4430c0de5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1c65ba6dba01da9caa84ba89453b61d81376763f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1ca3f45d0be2d1119cccd0af042a3e8adeda2ed7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1cbf88db44aa5f884438288a325270d29c7a04b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1cc459e57bfed5ec7f40ea4a4dd9f72f3ad7a709.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d02609fb803ea2697e2c2cef35e6f923d2578cf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d0b822743e0205f60521d38d7c64f589fdf0f58.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d21263e16dafe79b9fe2f998847296e575c14e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d3ef3d5ded0dfe2a0bafb52ea8f841658db35fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1d498e418ebbf33bed58b4074d1edf3d9bdd07c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1da23de9604b5d98fe02529075bad995954c12ca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1db03461737f1e359f389a8d297476f9b60faabd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1dc6e599144a093203fd7f92ac6d3c2cd7180d49.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1de2f97d49f015b9af0b186801e939c6f357a0c4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1df893ee660d37fba7eaca452ae65b3e45a73087.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e22f2d99804198c61251b4629a3f18ed3dcd42e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e33ce1fa113b221e5303b4093c2c4e748ce8298.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e42736d4f677a59a172bd6f162616a437696351.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e7d7888480b83c78833214b32e10f37a6e20301.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e9130607a2d24cb0662a47e9cf12c6602143838.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1e943fcc2e64c618fc1415b3f1a0db4d70aa8494.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1edaf9d4270d2ac61c299320e06ba73f44730364.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f0cad6ad5b172e51c569e84cd54a19b4eb0ed05.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f13a6d0f8c798c0c4ba4ad202d081899fe081ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f6bc5faf18be193212217788d476ce6fd384bfb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f7faa0b33a9aada86f032174afd40d18efa7715.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1f81f8cce0d77dec9f977b9eeb0778b70a13fa75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1fcdcb750f382fc7828a9886585f50efbe5be735.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1fd9fa7c2e13d0bad5fddb2b5a316bbc09d397ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_1fda1c96568eab89a8f6498f8bb23c1223cdc7b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2005aca3520b171bb82d10ad70fef44f28c19776.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_204a573ce6b7d2f90aede543939315561cc43177.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_20588bcac681a5d69f252d7523a3681a0c6b6181.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2081430c92864c29bb9f409e7c27caee1de00749.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_20d5c3c86398f6ce55abc90db3e362dbf9f457f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_20f7ea0aabd069362ba4bbd66623cea5b6e1a6bd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_210ef512b7862837f54acbc3b21e135a192647a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2122c973581930ab7a4ebc90b3bf1cdaa229a87f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21411df58165946bf02942b597d94de7dd856987.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_216806a4598c885e517e664fc8280c59ec3cbf11.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2173b7c710d418f44dc2b41bec5905024334eae5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2177d95cdf45f6fec95d1812f2ef183a75259e38.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21828c7d3f5574690f12f841c27f025206e6165b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2184fba2eec5899bb40d49d4508196e6be1ec1b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21e235e31d6955393ac8e825bd69ead70687b7c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_21f860d42fdc2cc6bd743d53ba546e332c22fedf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22105635385fbfb5d2f330df83ba6747bcb27f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_224f9af5e5ca519b21b71a54acb49f50b4999c47.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22511de2592b6e350737e44865e1fed6496e3f32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22632f996eb63fbe4bc5748c5897b775087446a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_226662cf1c9900a4334d2cadcc5f5ac3ad355f05.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2273457ac3be01cc1595a015a5f598f8290c77e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22a07ecf1a59f72ec6bef3e970d7f33cf54c5f44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_22c142d869ef940ca876c93033ad53b576ed34f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_23047ea90076e3b0a3eb0586d49b9ee74ca6d279.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_230861e81e5acc523fa680534eed757b7b4a4e1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_232f61bf31dbb5de5d7039d5ff2338068a759b68.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_233132e712eba8972ba444c604f89e01c5b84cc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_235bf652702c2976551778b9159e09188575c63c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_236b3eef02b904304348b9d35f715b639d63218f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_238e4c1ca112afec494fbe47a85b553302c43395.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_23914c00690ac5c4f89cdbbaf00732ba66c5c0ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_23c9b46da8774462de8c24e14b12df3ed596eb57.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_242013527a0266ad479715ee3e6ae01c45de29d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_24410fd9a4150c33186a2a365d06d8f6ea621c20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_245d90000b55ab8b6055b1934880fc6c4870b34b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_24643917fc970c043d1c80d8d4b17ec92deeb8a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_249668a3212cd00edaae871758be30a5a1fea589.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_249e6b93baae25dff97a0bc9145a8d328ed3f317.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2543da478310245e19e6c6a0d9ed7ad99540b3bc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_256ef175029a43e64164176d4eb212baf9d27bb9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_258d747083272ea657604ac84867ecea17bd65da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_25938733446b6c0dcd159719f08d04a9aa467967.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_25b3225da1e1842f83592971a1f62a0fe30aa9d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2660282ad39ef034fecbdb74acedfb48620b7dfd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26835ba70606c769e56d19dbfe74061361aa855e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2695783ae8f0034692efd6563f789ef03fd0f4f3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26d77b228420a3ead919474ec9c6fb2800f86890.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26ea90eb5a527434c1740933a1d2dd863eccf14c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_26f90358e522d7bb7c76c3a2c6010f0f38788bb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2703018e71d57d3266fc35e2e18a78faa3dd52ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_278639d44a4a8372a627a7c31e9527c8faa26f97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_27c2000d32c230a57a6712f27bc0fba02722f5fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_280bfced8745fbd9266207463fb41476dc23afff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_281d897ad17d7f6db2741b396e6b85a9b8f35286.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_285e61dad8f63fb973cb2eb899c959e400622652.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_288458c5a0720ef152848713119ebce6d76db6d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_289071756e7d0582eb61ce6483fa3c988d2e10b5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28e4d2c757e4b8c366a2c320360e21ff0ef671a8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28f1ef32c4384ec26f3dc5e3af6a74fc8cebae92.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28f2e2b108a53308a0cb6c123c8d318cbc2eadb4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_28f7634d29bef11fd466b452a46b0612f38c949b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_290c484c2a366258941ee0051e139ea716a9de2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_291a8bdf9d63b112e7fe5fa7e8835a6789cb8ecf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_292454f2d82184ab0491ea0675750c6ec55d659c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_292b4f995d622826af5d1f2bffa7ba68467c841a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_295a523f815eb822d66162d4feb75fe0bc50b648.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_296c5836ba118969c4ba89ed62a98dffe3105738.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2995d39cd62f20622a31f11a292ed175abb5fdf9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_29bffc159b0bb826ba489ae763dae141bfe8e802.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_29c9e5384809b21f39e78bb2e43af345a9a21d19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_29fe68ba10b3480dddc9866c51ca8b5efe962cc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a3a980a26682d879c3a3425f3ba5be3f5761adf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a45129fc4995abcb8f880692f11c6186fc01641.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a833fc01e88bd8e256ef64ae8251dd0ed10720b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2a97c457144cb63a9c6c3d6be613b47bd0df9928.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ad492377add5c8f6d0d2dbf9ee9e4338bbd9f1f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ae344010d49f7f9a6caab2cb84be7f87d2d96bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2af6c5be53732eb1939a2f93232af7dc011dec1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b0bcb241e5a1be1d35366461408d06e095a26ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b3326e055da32cc979892a2fbd0f7b003cb9f98.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b3af90387f1d227119c5dcd4b71362940bbce52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b4050988e5790a28dbe10b4c20e14f10f6cf85c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b49a9b0801a06dd89c7f7182d7590b515df1592.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b50073f6dfeb7ea77d5dce288a1d2f08f8f6362.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b5317b6cde327a842170ebff20c2b03d81379ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b8169ce4b4b9a17ac96fbb232e6a93f22071ab4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2b823c3b99e7c8d1cdc39a5dbc7365a383bf9ccb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ba934408c75da5479cc41f96b98ea7d333635ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2bb6da1095bd8669c0e48b5cd808cf0dcefa2674.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c0bda0feaade2b554d648d72f219ac9c389bf09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c2e75e6f659a500dd3cf2cfd65118f111342119.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c77bd7e89ed832cc31b2995566a49bec6e4cb52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c7aede7762a524a7a424cc4dc46e43fdedf73a2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c808da5c2514806c2953bb77d5692e5d7c97aa3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c82e3c4e445e1e02f14435e4ca01a90850139a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2c9756060ac0e73dbcfc58a9222a78f0283cd029.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2caba3ab83239e474412fcf89fe0fbef97e51bf1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2cf351fc2c2da4a8e1760a3affc9a5947c6b3bda.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d06f77a4054ca615d96636c0e2eba2a89850142.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d1f2d1e57095f756ddd11e8e9d4f6f253e3ffa3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d23a26e0a59a8323dd97632e610d24624143fbe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d43460c011b8d5e01ea98c9b8ddce962de59a96.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d446754d7000673779d15d3e73039fd3c10a720.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d7b637e0313cb423b22cd8844cc2997b3ff73e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d9a04b7f41dd6f0db017157a44790f35c626e2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2d9c659ba43bb907fd4e3e36a50958288bafd1a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2da2b905c4ce32234c2af62328adae6b1f9217a8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2db33b5442d2e0948762b1f2147a321a9d6907be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2dfac5a83def98340c8786d55a30a98ad68b9eed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2e30f50071113dc4ab59468d568ac9deb06b0342.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2e43e401abbfb1b6737e4dc822f68421abbc648a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2e8b4260626beeac76c26dbcee3cba1457b30e99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2ea394a09c8691a534ad2219bedf73724b6dd5ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2eba937ff6d0302ab013db7349d4feb914107f1f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2f0247e301a7b076b6ec8a778c3b47e330638963.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2f32f2d658f1f69840fbad511ce8a3851c859d52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_2f55a23a0f24ff7062a4c286944f25d2db3e20a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_30024440e780fdf9ec94deccc85216d8bbb5788a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_303b7b04496e4db7c1ba2436485dc7c8a4c88448.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3076a6de0e2612279e0ed64612f7393856bcc9ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_30c8e4d5c761fda50e010da779e8e4730051d403.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_30f0200092b0e18d57a9f5e512d565f1c0229436.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3108502fd29d3a24b32177bcea968121ee809115.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3110540b50e95e99a5cccebe47d9d3a83093c2fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_311104394c8bef8d4ecff35c1409221e723a5a8a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_311731442b756308c0a869f21b7b8b103aa613e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31222e158484773d2257f4a31e3dfbdb68336a8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3163272d25bc2db2ffaa1fea87648b45ee68d408.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_319df310195191895005b30151da8c1afab6c82f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31a968898f0bc6366313e41eddb5e3a3ed12dc98.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31b807c48c472e9b1311a6037cd98e21d6706889.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31c3760f5978baf9780ce4587ae4c768af0e49d1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_31c4b866692ba5c3d115482bef4790733863c1fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3206cc121ce8955ed59ea3b12b858ee2e0cf82f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_320a6196b662a1d3dc7441a9536d825dc356b95d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_321500dd4c41e4d68834814a48a639f5ca36a2fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_322a86568f89a5a5a165cfffbae9ca6949f2477e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_32438250078ba2a47345ec4955dafb4e4de78a25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_32527660fa7aeb9a951a9f2fc3c53989bd141c48.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_325fbcb9e503e68fafea08abf86a4951f440850f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_32652a27e8605cef59c8341813b68e7513be23c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_327e27892bc57f3dec0da24f94f2a483d6c9321b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_328a311bafd1c153525393b252e4170f8aafb370.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_33099fcfc218ffdf69edb4f2f0e46121bea9fafc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_33746071156e9ad46f403a539dc237e0a44122a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_33e7c1e5f41a451c7baff54f7238b220f1bdf8a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3400f0af03743dce328486f8fc805dd30bd6da31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3408103188e27b3bc55dce0c1716c0b4d32d6494.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_342d29c85070f488a14b1915f948e5fd69019c99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_344932e2655d7b32704be8de9a63bbd8c3369f02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_345a939a2491166dc520e9a2b9de7e43671e0c2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_345ea796c8d97bfe3b7c9663bf15e2e5e7696235.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_34807a8e90bf1cd839f32fd718afa6469c35a4fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_349241529745bf138552f49d9a93db418663ad65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_34c2db98d8e2e690f499f41cfd5afb831b756f54.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3511c54e6a6f9eec378d8b661121066536195d3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_351425a006aeeff4d69c8570cb6bf1e1427d2c21.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_354121d3bad1d448bd413718fa096f54faa12e95.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_356f83cb96d0313abcdb24955edd4264df72aed7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_357f7e626135cc9176a295f3d1f336a7c3852688.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_358399e756ed5026baf3ab78af17489dc07b9532.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_358d28c958c0a831a615a4811d13279b18db09c4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3642b78913a853a62dbff8b99d9ae3fa458f461d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_366662dccf2f650bcd8123c49006c759cd4c0ef6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_367e58867c46d96c9bbaa96eaaa9f93595c9e099.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_36a0a960541bd8a2dc6741579de685b7c0a5f6d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_377b70f54cb2778b5ce3df936b477f775eea8b3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_378759ae25465c32960487375828e23c5f1ac869.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_378bf438642e5d863e31145ada2a0688059aa5d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_37ad61bf8427a26775969f8a9166fd0bfb7446b4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_37fe04467e87ec2110f60c7aea0cc9bf2ca07481.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38010c9bf7341588f071f889b7a0b4dcc4e7a14c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_381b29d9888365bff0f109d897b508eebfd8a61f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3824e97d5ecba46e06d5ec1a9456c810d80227a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38273a2f8e6bbb42ba0b0871b6c95abb34531f33.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38a5ff72f22e0ad040a281e66b1aca0bf3a2aadb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38abcbeaa4d33d3150f2b0238bb62ebbfe960980.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38b94d76503e13c911781169fbc378517332c42e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38bb367362fe2c4849ded728ec5dd00969ce188f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38e12dad9e3bafe177ed3c27c833825813e18fc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_38f8a89468cf9c8606cf12a930db062a83cd0ea0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3937d9dfb68351de2942e32f35e2ca1ce71edfa8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_39422621a00ff79b2f5ec0dafb957c77693537b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3967a8807c9451b09227c0f685c18aafeb062fd2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3992d5df4ba2e999caf6889a852db4e1ba078e65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_39d3071347a0c98f3221104036f477aa13bffa4d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a1dca5feb864e8981387c2d07e62acef1730aa8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a2280997eb6f1d091094fc54cecf42b7c9c3a2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a2643099365d0903c799585f41dc1a525ac9f9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3a6b9566559ed2b1c85f2bea1c55e72c41dc47bd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3af86f458fb4dfcceb7db3357fbae0dc15142a15.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3afbb5ac9048a962a60f48886728220ae6c2aeaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b26eafe76cca8e74e819220b6de1f4279d48e43.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b4ecb47f9ebe8c2784976c3e9bbe4834b475cf1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b508b92f7e123b21658f6e17d624ffa87831fee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3b5b3c218e4a7b459e54080e24c5b730221eac02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3bb129e6dee6848043dd0e8fa812ae80fec4d014.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3bb3b682eab96e4e173affad75b9d8e73f1dd690.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3be7cea6df8e6dd56194e1172f28943667f1c4ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3bed3aaf24c73073c604a3b23bb4b0358b8e3490.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c1454ffc1418dac641f63671e947d9f550b1f0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c38bb80e9880335faaea81985ed5d0e713ecb08.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c3b7e4b8c1efe59f79a15512716fce2282a79a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3c64c33870ebc329921cfa3867d58b1857421f65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cb0cee09d633b6f70febbba63a1e090522cfb4a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cce3baac1e3ca03af0c3f4ee4d0158ad1031e9f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3ccf0a9d5a5451da5dbf6075ccea45e4a140550a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cd7a9ca49c1149d46f6b05b0fefc41ecaeb6ea1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3cf45927b6d931e31e2209685d787efa28eed8ba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d1cea88a2277b87d405025ba256272a1720f88d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d289100991d4c8c362f64c8f6c4ba395c2f3495.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d3f3eb2f5eb1f3287879604892b1c230df85f1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d45624dc6e33c477c73a155500b015b6c010de8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3d55cb42b0096a8ae338ce100f86e378aa1a04c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3da8c31f6d5bcaacfa4a21aed4d1d3caecb48922.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3dba3cd44f78c950fe7ceaa5f0629dfc607b30f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3dff884e176ec7cff86d17c6afe1ddaa4dd6007d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e143d88eaa0d9cfea856b2f3a57d1275a656627.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e2557f206fd81d82a3b9d59113105040beb891f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e562e6c3af28b8478020ce3c3bf73c036001c93.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e61b019e1398a6a3c36143fb84b5ff22c9f4508.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3e839660557dee9d5bcda9b56940ce23236c5f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3eb2ea922daabbba131b90713e06d8caf5f30662.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3ecf565a5a1c4a09887c67ac3b9a019dca427ac0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3f34433b784d1e405ade3378918641372a30bf6b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3f5e01b4f2ca8ea10898c39d6570bd74e85f46ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3f7315955f555768f24585a50d75e216c40f062d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3fad30ff0739ab5dede67a96e859f8c474c245f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_3fcc6893456a559c7d22714116022fc69b372266.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4018b1fcee808b6cccd131418b6ae9e8bf900d8f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4018f690b6322588041bb467beabd8a7bc79a2e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_40357c5e9739eae136a7abf92bc38d3ac94753f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4052ca6a3ec02f6559e4bbf1edde42ad2d127c26.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_405e7efa263223148318ae96bd1929b382e994e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_40aa64439b80ff8dd12498b3e5f6b625da16e285.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_40db688a9189e1c47c300d474df946a248a63303.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4118e3ab290263ed2576feaf22a1944bf2ddcb7a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_415b183c50dd2663dabe3eb8b780913b778c54ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4160f6b6d0869740a5a411abd80108f729f810eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_417b1cb14b67dc82f614831550f7deb0895bd7e4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_419461cdb5687ebbb7bf0be136071d70420c1619.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_41b68458076e6cb129d3ec793e95b91430a0c8a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_41db3f29d1940e59dadc357c040ea37a6ff208d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4217a48a1677bd26cd48e512f1fc8830a8a551b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_428ce4e14cf94b284ffa735fe03d923cc74c9fe0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_429b82a27571ac91e3631cbdb7e0a58155abf962.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_42e2326066c91452335eac05f25a6311376bd9e5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4306c6c37cf472ad262f53941611b5e60072bdf6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4347e039c003489dd528faf5d710e687321a3fd7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4356b3a2ff49f72b91a6b9c215df285f2798ad47.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4377ac04be3a6cbdbfbe57612a469412812fb5b5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_438e3565f4c720e6c9691b0d33c1392936e2e7ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4395d3c96b3f4556b9765fd0a3b5701b2fb10948.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_43e7c78e8f65be35e2753a0ad5123118555c56b2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_43f2156a04b18bab55af60e9357f28d8a4604e8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4409f2a7deb027e864afdfc9975d3ab93c5dcc9a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4432c5214c4d40c54ca2d02f0d4785c6d6902370.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44462715ed5f192532760d6f4c66ff9d4e20e254.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44564dddf8b492d80be54854abb8d1d831e42679.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_445cd8fa559588f4264ce6192f2de3e3065365ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_445e28a8a51cd435130ded2abc9fc606e522c713.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4462b192a64efb60d5484798526278ac7a0fb9fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4466b6c6b2ec3acb40ac1cda432efa1e4e62d9d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44690e48f30657b0fcfa26fb3b9af3ef76e792e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44c181996532676f2140fd026707135144e9d37b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44cc95831c347212021c0bab7b43acd7daabce42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_44d82b58fdc3e5b7a7c20490ce7f5acce4e6ec79.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_451fbbdc2dcf2ec81efce34673ee6c425cc16ca2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4568af1b2f104664fd05d21ad789aed39ecfa42b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_457eaffbff3c58183a656687010daa2c16cfc26e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_458d708d13577f2b92e6d5adfe952a87e0cf7be5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_459c8fb6028991321b09a990c2188d854d940268.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_459ea3713aef9b916e1b38a882a45012930924d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_45b9871c220c0065d74bffeed4021d0304a9625c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_45f4363f50af1e7ccd24751d5f5b181bf32c604f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4601680af41c8738089ff377147e0547dcad114d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_461737a13e24009bf1a5a4b780175043a9f2e33e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4666db0ff7b035e54f2c0e59acedc2131b722a55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_468a5f057fd5cef2df5f919f5102f47e86901e3b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_474fe2d739eca8c93fdcb2c105d4154cee6ca1c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_47548aa042c69bb9c59a8bf706b44028aaa41830.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_47f3ced9b5ddb0dfee8ed5e7df8eca0bbe273047.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_47fe73f04cef91cd2a0682e905483968ff80eadb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_481415463f0316ebe25ff2fda47c68cc54db3359.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4824e1f8cda50f80988857611da766685da94494.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48280c91d7cd8712fd533e246a6b0f758834abc9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_482e34930d11ff493007b1613993e01acc1af78d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48300e0aeabe337785d4c7b41796ce65df6cc42a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_483eaea4096c8f5bee16a64860432f0634a253d8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48435e5dd23e49e19dd313f9891ffec800ce74c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_486f6c7c7655c34b7b9973ff357b0813f0a3fd7c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_487724686efd35731e5335efa949486c93ae26e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_489e7be0f85656d012a6451b65f6c1d2613b187d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48ae3af78583258c4b13c11a442022e0e058bb85.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48d7d145f96aa8958a9208d0c8887742a8c834fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_48e9e858abf6f77489f3fadc4ee81edacd26705a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4904c5910a2d0595b39a3f87652a9d1ef4fcbe80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_490a68220a7b621ae9817d7b77f55de239b0a4f3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4911bdd71351610d55916d452495e599960d0a41.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_492fbc418e829f89bcb8d93f8afd2869dd8dfccc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_49d4c005d723cdab9fbc307933c1257d114b539e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_49f5017cc0f5c8c8dc71492e7765cf729c1f225c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a06b5b153ea6e8b1e20d9aad9d4633333fd98f5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a2e6b05e7e4de2cb23d815f8b2c8adf22131c0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a4a00bd6ea27ff20a2903d619e1361b5e27672a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a5dbf601de5754c03a03a1a42395dc0766fb8ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4a9f3da698a6103caf25d785928dd9f814ac27b4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ab5d6e8fbfd92e9f7e47bda5cfbb0d4162a6319.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4afd02981f92fbef6277c1985cc479c12bae9239.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b1eaca3c37a82d19f8dc91f06764170069ca3af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b2e7f96b095ebfb66ecc7a75752fba2a63e4f37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b30f472f00bec9da0564ddc40e07112b5f9a117.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b45948f2795293e72530b02669c4f549608ea7f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b4c03c916393d6be7c5181369ebcef949eaa763.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b68e4d00295b294320b94bc777d7d34609127e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b7393d55600c9892558248f4131fc06a6cf3309.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b74439f42140cdda9bb0f78d995d741212a35f4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4b76e5dce9af523422782dd25d8dcf6f25edc68f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4baf664bfdf070362bcc91af77d1bc406f744351.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4bc48576f285325345fa1205e5e7e01787b74f71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4bd4d46397a3749646b232b306688e52b8c6e584.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4be4a98f150f3f9ab6f03b5fd0968c5454565c9a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4beca56234ff6fb4f23b9b24822887fd9a3d0df9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4bef4d120e71bfcfe61d67aa44d24ceb907c2b9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4c0c50a1fac82d47dff2357ee3ddbfa0b2c8d487.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4c69d06e3f32e3b6d28d3e54ad764b472741c193.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4c8720923c3452e3aebd7b9c1b4b23f0c35d7e4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4cabdafad0bf803223ba5e8f474cd59233dc48cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4cb1861e31df98bdfd731efc3d335055090d83af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4cd3de43cc1f7588d62a10362f59d113ee818846.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ce03571f1d2779bdeaf0a6a2d617e236d191c11.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ce671f5defd76ca08614a7a1f184c36c0f1e2ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d3b1ae63e127b6e6afe39e354d4995afc5faeaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d5f3cf0f78f73df79665c26b20b0805615e1b04.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d65e58c9f147498ed04dd51fe1393770603a6d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4d7dc0f356b630179916f8fc2041b7f1402b46df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4da9e9b7277bc90518ab92860bef2097ba96d982.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4db2e63cfebcf84043f79be0321708cd159c62b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4dbdd9c3f496a27bde68cf86374999ff2dd53505.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4dc87b7d385e7b092e4706c464217b004fd8a6a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4dde56efe17f4fd36a11cc959320a5e43f1dc232.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e0a88ccef04e81b8c684b695f7cb4310e448915.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e15e4f16de26068cba30ef12fc29332d45e460e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e47f8fa40332c6ed12d9971e0b539049a871c34.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e760de14b71a41882ec4a2c7362565af36d1a5d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e79dce18e49ffe024fe4cd0693ad3399f5edaee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4e9a933b916285d9580a76df543cfafc88a536cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ec2075f394acfb14fae7b1ef4304fd9b654ba0d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ed6da5357b67cc28aee4afa9523adaf055c4e32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ef35d82ceb4af2e07719c16109c6d72eaedce67.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f0aded9d1baec3125ce8e176248cb146ca580fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f1e1c969b57659e7e1367ac9ba10ed5ef5b69a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f44435491aa68acb3217b0e693232c67641a2db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f4a5d56721bb1a1332a65882132a8c5763932ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4f6243c6850c0a2d2b7bf1476e12f95f187257b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fa4d21931b9afcbd70b1567995d3eeb6f9308aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fa883a36a76edb276a66c5d779294f170d6d4b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fd34faa8b168e2ac7862641229e6146d3e28aee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4fe530cbf6363a8f08a94728e45e88ecde299e7b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_4ff20bafbf156fe8fb80bdd84a5d2f3a4a944c1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_501dcf3213efd214cc2ce8c9ba0027f991d241b4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5052b2318dbb78b1a82ef03666a35a623f44481b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5093976cb7b32a8bd28ce92fc13af00a3e21f737.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50e59bd079f4d205b613056f975fd2b4e372ab10.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50e7b11019fc2299d70869253877319b03388244.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50f887556a3540609649744957651ca667b91774.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_50f915b4d9bd18a3c25a85917392ea4a5e88b349.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_515128c6978449b33ce0c35b02a9e9aaad65ef7a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_522a2a9435103ed405dc1500d31652f1d431a49d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_523e5bf45ec5008aa3aba4773e68a78e122b2fe7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_52688999141a72e61322140db29043ef9f7fbc3d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_526c89b7a04758b4badbf9695b316f877b8bb053.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_528db08068589c6e4c096054d26a2e5be63285b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_52a89981a05963efcea7ba5c1e967638beeebbbb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_52a8a323414448c50571a334f29bc0a38919b61d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_532a6ffd8a21d3e98342fd401f0247f62ca4e038.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5344427df3ae9392c4fc4c25c232196828e70648.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5382a30dcf702daae19bd6705864bfe36e09502c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_53bd60bd2afee49b30a583c32a45ae9f2076db08.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5403eec1cdd216d5c4a7ba977e2ef92a0d7fcc8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_540bd57333c6839ccf5cf2e928edb996bc60c371.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_541874a7633e5713720b9d084b6d1c6715a51a17.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54208a6e8c5263e38f9ffcb062564ab61d2785ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5435b4651a90e331fcdcf224282457e3dc038a30.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54402a22ceee3b665a3f24edb98b8398c35c6f5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54548ad36fb92d0963893146c8db20f53cbf0c8f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5467aea26852aa9a9e3dae76b906005ddf6fbae1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_548b347672451e8391388a400d016803f4c4cf8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54940ce53998becf9bddf56df7d19894a7658168.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_549b6956eaf678f7eb901567d1a515eddbedae5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54b6e18b10d529eb6b32d7c19c59eaefc7184376.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_54ff49018f1c12b9fa31e523ad40b9cc162ba34d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_555ba79201a585bc091ccfc326fd24e851d1eecc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_556cd05288e1666f5c67fb87ad02ce660e4c589c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55b14cf2998a61611d1de2594e926fcdc378999c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55bd9c4f1b7a0621c67f3e964d946ce22fb2fc80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55bf8444c1c26b91fd490c7216f4d0f8aa0a1f1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55cda610c235987e13232e828f8d86fa88030560.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_55ea83a47c6299fefa4220ed88f7a8e1dd938215.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_566b4782793c6526bfce7362efbf6bf069928b2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_566e26d4969bc6bbe9b092bedab11cddb3360c0f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56964a17f902257aca9d08c736516a2c67d9a0e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56cc4399c5567a9495f17d54c712cc9e65e57521.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56de9a7dfb1201b56528740e9d8a07b62710fcaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_56ffe9e21362afe9c3a407c09d5de186954931a6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5724d91c1fd6290a6cf8d52a3801ac6b921dc7d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_572e68bd619e118292768f0925ccf92cbfa68415.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5732094f5917e9164ee0f973ac6ec47245a69101.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5789f267d34c9961ced63ad07ffea2c6d2911415.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5854f09511778dd1779a839b0b194896070f69ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58679919fcd292a2a69543de0db94e2985c9d364.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58762476c7f2bb05dce92ec22c0acbeb03676746.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_587fc33d02b1932235b8d152e57559060211d591.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58a784fb478ff5b3f1e2da9765a3a777efda92e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58a7ab44bbd9fbc97c7805860d5f6ac81d6ae468.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_58eb2edc7738d8d18ac359691da261ceaaf71788.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5919133d2ed892745013b2fc5d503414cf0a4d83.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5939e6610e41aff8d1ccdb66d9e84d3e48e8d379.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_594929c433b049a8cf949ff476309a8faf5c25fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_597a0276ec419f18f060a5186e6bb703ae434ac8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_59901147b7188212b8d8feea15831a11425fe4b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_59beb9cb4e161f9dcff79080149076488d436301.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_59d366421e0b51c90fa53c366d47ed8d51b3a329.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a05b4e7782bd0e29ca9f6d33fc59d4304136d41.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a216f777feec4752f5882677b18168225da4b53.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a29b93cee012c79d4364502f1d90f947c73641d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5a85ae0a16e4b293b549bcb6a3ee52df7fccca32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5aba1183efe205af38e79a1b2dccea5fa515d02e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5ace1c9b00f160a17355d4583d49c47887ac33c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5af96b404feac271dac8f4190180754480d3ba80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5b413bdc825ae863d53dab548f2145dc0de8fd37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5b55946ff3c15a44b9c741e9f6bbbcb5bd4c8577.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5b7a4ea3bb8905a22ae97a94c354b1cbe38093bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5ba578c0e7abf1127dd0370f06d7278656c93ab9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5bc803342862aa30e23e5be7d84e611bc571c529.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5be9ed84ad9be1627db7a66af9370679816c0897.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5bead6be6e39ece0e5d44335083336f7f546d2f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5c36fc744dfb0d985c9113175e76c7ec1c935054.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5c742b9ac6749f189d597ac97d46d35189472c50.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5cd03e29403ad53d6d52e5e81182ea6ff5aff2be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5cd41b6f578f3c903eb9d58ebfab62eb296044e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5d707d065ae152450f9def619ddc3dddb9089e88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5d7ed4c885fb32a0b548186e56d64bab98071d30.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5daedab8931f2eefb649b91e80145cb71b63360c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5de27c4081377f59363c2bf2ea8624217566d2d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e0abf4e2b6be3e2c555c2134705b9dcaee617ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e62968de58d9df7d687d671f37d63393f189321.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e735b12d130ebf849ac5d6752e413ecf3e69fbf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5e840be0741afa4d41fd4789c8300223fdc63ddc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5ea53f7c6370845fa94aa9b395c52fd1900b62de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5efe77ca5c394a60af0313072cdd132216a52bf3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f20263fd84776f155519b3481be5e2c5b035585.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f3c3bed2b584ea2031debf9f953f5f8f7012171.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f71e663978dbcba859c5114ec675a712e343fd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f8925f929a5b26f3544ca31938aa75b3c59d34d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5f954a393b7b5a7131c13d0c4578443f468a738d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fa19223cf296d7fd10e15e2571e63c84a80fbb1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fa7fafd4227918e0c7f0c6ca3b2bd673cd07279.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fb062527121e627871b3f1b2a94b96c42e51205.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_5fc66c5b53f83bf1e023e81e9d51f0285b3ae731.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6018ab272d7306689c7dc5a6d5326efea1471235.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6049c01db99fce654e9351e711b113cf7424550a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_606f5e0b99814b0a82a731de36f28024bc317801.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_60801d21c14796c08377349ec86a6c800af497b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6082d55544b5280b49b071ea277fb1827193fa2a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_609616f72bf16a060fa50091ac139ddc06bf9d88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_609f68180582384ba81aae2b1d4a4c52dde2c68c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_60efa9c427dc278c0d1bc31189f683cd45e4d873.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61204f6805d5d830aa6fca2a9b5f238ed63c3a73.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61220f6dca850a5b5ccf1f619a267c40c37efeca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_614a9f10ebc51bde3f580ef527c17f89489c12c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_615430cb65d8d540836c7f12b3367abd3c8e63d2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_618031345ea71cc17e458eb97a559b7c94d3ae43.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61896aa9e4e4d7e494c1755b1e77a08e0e264f8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61a44ac409e914c12281f1d26e5b52d8bfd0df75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_61a9e92183ba87924e73ff0b5e25bd12d6038e69.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62048a8ae1c0096f3372b0114c15edbe813425fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6214f820b39a8ba81e547a78ed19a909ac13221c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_621da34ee666903307d3a09b7a032f2a70054759.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_628b28f65f19e7d1b22fb3b85b7cf3d09cd54ebc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_629e0b97b3fece7c12504f4c8f1860d611b57269.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62ab710e4acc711430745e05e036dd6a4d6bcdca.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62ba7a5a0f3a714eb5f9f2af20f7bfbc82a30350.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_62eb2f81e73d65fddce7ff43c397da6529317607.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_634d530731c7ade2c7beecfd1bbbca8583032217.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6360621af3f7e1e81a8be48fea8d2750fdecbbf4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6376eb68c550b50b9aea42a7a2cc3bda186b0e40.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_63c411351ec59bdbed2590c599f9eddf7807b371.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_63f121a3c8928c10a2d86b487cd13fa995da670d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_643b3798f11997d33ccb58d90ed6c10d5411b735.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_649336d59a8b35919e593217b6fd4314a04ea359.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64a0ca185449a49fa485892fde6af745ba758167.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64b3488ddf3bb1a4870371882f0a5d267bdfdf73.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64c3c1e3dac623f07c2dc1b934ccb868cafcb38c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64cf03c0aa3f1b2a7b76b4e3418eb5063b982a29.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_64fe2db75cb20428856b02cd1cc8d7b393a6ad9c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_65794d9c185b21f59274ac5d4db10a7abc0be968.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_658552954505a2092662071401e135e84956c4c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_65910c8b7a30acc731948ab58467fdbe4fe32f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_661b49505cfecbe4ec3e5c7371de3aaaa85ac9d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_661ffaf653085dd7f122d603bb3ba4b001e5f3c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_662767e588220d0dc6137b00cc1d8dcc91e97134.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6649f19deeaea20663bee781af7edced7f7a4fc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66968bbf7e210911fcb95ba90c79837230ab1ce3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66a020f728df204ff51e37d2ddc21afb0aad5e7b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66be70b088b20fc8de464167c35745461ddab640.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_66f651d3415562206c1049b172261fddba01ea6c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_671828f15eec2a58be23063a1a8132d337cd26de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6767cce35ab784aa42ebcb75af7305bc38a8721a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6785dcec0197fdbb50124ab06efa627f1a2c0567.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_678a4a8210a972bb2ed89d6ac754fb79438ab2da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_67fb736c61088b8dd92fe0371f5c98e23bf9077f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_680e81c3700f130df142c9a37a368944ca548721.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_683e8a33fdb7053760c9c135002b0a94facbe015.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_687f4aaafd1a5b9ee85aadc6fab79ad0c27a2ea2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_688aaa193f332ed13e017e78ec07a7c80e45f6c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6905ba47078abd7a5b6a51eb93b26095517e7f70.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_69214eb450c3b249017480efb8d092b0edad6dc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6979ef43adffdb62100270a62706fb811963925a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_69cbe8eca7e3510f5caa7f13419cfbefbf031754.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a3f42d5c9ccdd3807e488b00f02bc6ab5d8d99a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a4b6226b355bf35d4d07aaef1828091f03ad2ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a66604bb15f97a56847a7c968dbe32d247cbc13.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a7b6781ffff9a42beebb4d73f0d15461ddd4479.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a7eb3d86aa385f9ecffbc5ba10489e56856f918.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6a95543aeed81adfb6d847f78212585a36122ae3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6abeb7b50ae6a1fc62535b9a1dabbde6f177a9d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6af23d1460abfe875e71f7911697c42fef0f41c5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6af4c15a119e805e4407b184625f57966f8833d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6b0ef67ce0f178aa2863c4909f5bdd7f766c9b2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6b638314efcc4f16aa4a6e58e6caf2fda1711519.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6bad2ed9f91bc1efd89ea66cd5c775fa140cf931.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6cfb7075345704340ff33dc0ef7c04ef127f26ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d07bf9c05e41dcf2416e05dab4bdde17158db76.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d17b92fab5bee7717bf9aff6a6bef7cee3816e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d307974bdeeef95cca0d130ebb7aeb77fb1b6eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d40d762ed576832b3a752453e9881b5fe6d2650.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d470f5c6fb81032fcd7974180297d4bb2a8427d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6d5aad18f59e47a3fa3278c7ef1a6372830c33d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6db86621d626722434f2ae9b7b8ab435a8dd8827.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6dd707cf48a17d31abef94215c5720419faa0a39.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e240106c771ebea461fc2a87b6da68e510aba70.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e6a4475ea795935f4cbf2dc0ac156a33d754587.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e7e1d245baabe2f6293e3d85318f9936b333500.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6e8cda718e10824956f0ee39bbb0891eafa45a7b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6eca9cd905ea8b0454cf9564643894682b08cb97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6eebd0c2fbfc85f938b10535855c388971129a28.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6ef5803b33d97db72eb8a8528aeb3fc956a938cc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f31b3345893eec8ed1ddf1d8de2512b46ff6187.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f3d098f8bb63133924aab70d26a6ed64018c13b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f8788c537cbf6833c58a6ca15c0a36de33c9fbd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6f88527a2cdb5adf51407f4661a254bb32d7de23.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6fa6478cc27e52fd9511fbff38369c921155cfb9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6ff4605d82507fc4bd6e96095eaee5173ea41973.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_6ff58a5186d69efd6062f3717bd315394ea6592b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_703246f1f53a988cf252eff88bdf814bd382d3ac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_70586668a61ab88bc46b763df8f1c2ea52001ea0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_70c8e45f6ea7cf5dba9eeadd0b19481d9f5defb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_70cf755f1485c065222be4daab84283a9c3d0eb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_714c5369aa848021e020d874289e3ae4e0f74d77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7177f939ac3dae8749cbf4232dcf04d2cf63b48f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71a2d046629a4b65c90d0e18d061c4984062f844.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71b6100efe30d836dab557ea4ac54c4b9d35c6aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71dcbe9f481c92215f3b636bc0e86ce8f65e6472.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71e3980331dc4bcec6ab6f4c345c7b5f71356979.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_71e5fb3544dafa9da03fd2de4bb9bd0718f6009f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7237ce5f3cf13ace3efc0b0227ae5a8c1fdfce1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_724d1d4408196d611b2e0535bf8833652acbd6ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7264e378e1ea1d4dd97f6949d66f3492883b663e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_72abb25dba0c48b380b2dabeb6ab7efaa706d180.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7309c38fc8a2d5ad6efd449107dc54a7509624fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7344f96bed2f56793b1c2583485aa161cdf30379.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7393267865f1c2b0aa1a09a586f54cec98eea4ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_73d4901b8ef034590314048de7223a572d61ee0f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_73ec21ed6e040260c4f04ef68ef9307aa86985a7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_741401abfbbbdf0dd1d62df8bc3e85371ead71d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_743176ecb1f0bc800c870861585edf56f88d7739.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_744ec604c577a27e0aae5b39711a9e2eb82801b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_745705ae121a1a331527cedfe4d31218a428a0df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_748a3d76e8ab73af9a5d2302d33e3b1d1b866dd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7497eca4d1a18306b406b367653622a8d64095bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_74ba59d347ce8916a22b40e6f22a3c89e13db4d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_74d5f2aef029f2103bb419cc982cae99fd1a9253.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7524904ac5a2040c7ea72aef5942212f291a21bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_758b211174da0f398b2a093e7389905b4f9c4060.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7596c14b8fee751d03f42ca48ea4f66e87fc2e2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7597ce4d2e5264bdeda47487d5bdb55a014c6616.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75a310a6eb86e3e8baac7a930c3ffbef372942b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75c38912947881caa14b3fc7ab7bca317e296dc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75f2010bf6c478d2f0eba77e912697661306c1cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_75f21e38ad01fade35b1db40adabd75eb602410c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7601e6aea44b96e94fb019501be6b102c6e6a654.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_761bde840c0c8149b24a8f6f264e963c4e9e8ceb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_765940baaaa2ae6ade43ef4c94a220eaa63702b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76674fc182dfa6329c73a354aa3adf458429444a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76704ca28a4877a1e84022e022614709adabb280.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_768c80fd3ea17813df1bf19a158186834fd00780.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76be322fc072ca19baa82707e260c6eba936ae19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_76f884e9ca116ee47b446efe9fc770c178a858d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_770ad1eb1b30ad8f1e7c17df486093129b2d5630.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_77200e875e0ef160b311c7de450c137772312d0d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_772016803aa3ca6ebe785557118365f9be7c4339.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7726be8909f631c04d4395fa4ffd03a736f447f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7728d5bec7941c9b6d5632bee8d67ed92b9c03ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7764814a0de7702f0b7b5ce9dede6440603f4853.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_77a814291d8f01870274149b9d82fb75921d6e20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_77d0223697ed41c4c2fd8830f8df6e5620db547f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7831ce329f2a0812ebb1dd103ea4ba8cb7ba531d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7838849e57ee9cd292e588f587a8079b57becfc8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_783ec08544591a22f59dc12f169b7327b4185a1a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_784c35fee4d372123631312f1051c43e1fa12378.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78663faeb0425f45e8a0da0f7b1a5ddbee5e07e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7872c45ba170f2782c4b5b75cfc78ac79a4cf157.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7878e2a4d3b96a552e03d1ffc33debfd50c9f7f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78e1edca5abe1bb3e7aa946eab6484b7bed806a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78e945db4afa1330fe3978bc1bc9ae99828ae287.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_78f7e2a2c08cd87702793f91b6935cbe4c22be55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_797750ac0b18b48f56ceb4640256e9bd3a36621a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7993fc08ac5c6ce7a2eceb1227f4e3718dc4cf5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_79a7dce707954e765d97cb22e57d9bd6168860d9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_79d0b8053ddf99a4d4447656d733c2da026b3a7c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_79f182ae021e23869d7bebf2a9b4575bdc910ed0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a0ab620e6d62259a559e329460e46e6e3f7c3f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a13d62a715fd717f0d4101f787349cb49cbe70f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a242e5953f44316b6a4f6587ec26283ed6cbcae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a2e032f6500fbc5468183415b6dd1d3e43f0bee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a890b126da2d8cfbf84f048b779cac2dd56b509.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7a902ed4ae3cc6558c73b730ff3949778007a230.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7aa14aa94d625b33df1adfa30ef4d91769592608.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ab03a62e064864e1e9c1cd506c1b2e1786a777c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7adf69b51f0a8cc9ae7e250e60df38758230fe4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7afd1a756247b15b078d15a39e350a07c22982da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b2d3680c3578c7292349b58843aef7a82e0087d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b5680f97836be4a369802e8115617a83875703e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b67045d438a7e4b8f3a313a5df5a85f351c1be5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b7fa76609243a8709f349ffc0d9d88157f28dc9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7b9a3bf1a9b37e0bd9bae6249609e5994dc0dba1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7bb7b63e8a4c1df4eac4d978e166867195bd6e53.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c19fc90e5a9c422dbf529d2def286f47dea0f50.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c23dde1a386436e9864c8fa5f1706c0d2fbfd0d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c3d8ef4da515960bf40eb1feb04d21950ad5ae5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7c4710e8f4e27fae4ae079f1667c3a1879cb6da8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7cbe4562c51d6829ec5942e11035c452fe318b3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7cdc419d4248dfdeeab1f0980aec35fa134e52e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d08373ace7087bdaca4ce8b0bc329f553f88d77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d0f767c17385eb7d756cbe8ed444d7cef72dea5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d12e9cb599d24631c082e3cf65d2c58b6d4d44f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d2f87c021e0b6a27b2d7e30351fd50f06414b5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7d5667b27f15a06d4040354fba3601d48bb9c045.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7dac5d4cf103d658e129673549549f1276f134e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7dd260849b86c46b685955cab54ba07d49b47954.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ddd621da88c57798db1e689b93b692b6519ff96.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7dfe21ee27f8a0ca0407ef0dea73cd73ae6940db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e1bdde812c332c9fc58613698568a04771b9fa8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e332a6aeecfb12dcf70c69157fd3137343fb9f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e6129eead18d13a4a6cb9550384fddabc7a2a16.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e89f79217037e361bb0909d06534e40f5026b4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e9519dd0d0f940fd5efd61bd32df7528ba7e3fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7e9c7feb747241c9c7de2adf3a19933a1c4c0995.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ea9c37d92e344f3cc58cd4d1d00f19167e3623e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ec038393ec329a894aee9bbac078a40f57a4684.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ec04763d635c5bc3e810737b5d948c59f117d5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ee953cb24e28bcdc8f05783894b23cbf83bdf35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f6ccdb3c2d595fffd05bc5e6417b157276547fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f80d44e82e601dc48d4c8b4e710ef7265894b6c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f9403cb91d6aabebf081afae94a8ba397d8d24f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7f9bb3486fee7b7c9e24300b8a4e4ce88a11bfc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7fa76fc1b066a15b08dc6c24a7cf33a58b4cb6cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7fe409f4421193fb48a54aa5f26bd6229d23204c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_7ff65c7abd9b0d8a2df9302d6dc167637b3a72f0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8004763f674dfb3f14b66dfdeb2a046e413ce2cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8007bf7ae1b71bf8ac4a793aa519ad333aa7a7ba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8021fa266c77e6b5bd1af2a9c22c686e5a6eac78.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_802b21f9588d72c3c3e3b9a3b269f19c484d5aa4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8046f566fa7188c92568b277354e8b06ad382544.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_806f9ab9baf631df1d3a8d801e4cf93a102526cf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_807545400aa6e70ff49a5f38ed6a218a180bd87f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80987e2d765efc320eaee813607c94c80ee35aa4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80a72d70d80b66c19e85daa00497308381050048.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80bfb0e6032892cc58cef4dd403f305a5b76851b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80cf0997573f4bcfbaaf75e40f519580a7495a17.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80efc341089a50ed5669b3c86f6ddd9b124d1442.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80f51f0e178c33e6196df1d2e47bd38bf5391cc8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_80fb694fce7b4c3c459fca43c89c6002fbfdaef5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_810dd4e870ceda3ba9b5f0084a4b025b2e609d57.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_811db756577b61cde9fe8279d956980db9ee21a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_813e60e8405aca3f7fbed19452ae37574ada9a77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_815918206483d2ae04a45aa67d69dfb986587214.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_816c48e129a0235cb3a19124ddb28cce286fb368.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81acf1d17650712b71a499bb66909bfcfcb6aecb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81bb8f13b6f20a72c9ce6d0b53f81eddbf05f1c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81dd3ea61bb61de02667b14f5a94198f48c7307b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_81f6c575c3fa2ccc7e65022f1ba65c8cfc16541e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82048cf91270631f98ac37dc488a1fb2e00ce004.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8250f27341241086515d833aa53ae873d4ece3fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8278845045d68027dcf3bf867ecde2fb12ec51d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82ad0c0580516485ea432d98f53e73f6dfec548c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82c932e6eaaf44861c794539d9caf8b50192fc44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82d7f61e6313930f063758b61102e7a43b118beb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82f0f3d71108dcc49234a258f0f3b21ea2123cc0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_82f1d7e1a93bf2fa80c409e6827ea88af56c44f0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8301bfc0394936a68fa0098580f06e77c88ebed9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83080406598df6bd3102db70a554e496e29db96a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_830e3532f27b391585d5de90f3bdf97992b67651.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8352031044ef2e4a22e27ad04ab5d2c02121faee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_835a906031a258c6362313eec783678bd8125c91.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_836a308c2d2afd6e0dfbfda61984b631c4ccffc6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83d580a612af85533c87aecdd7b0345c71b75980.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83d920a76114c63156740ba5dd6f3846c4b21c28.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83ddca2c6ecbba4314c434e7471ffb8fa642f936.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_83f6a1837a65df12b7c55d25ca28cc939c2a6328.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_843e7888cba5f463d19fcb71aaaab25dc3d2c09d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8441910c34830ad2459fb85c2c14af02da718fdc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8457ea5726149efb8778e6d90798b8e48288fc9a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_847feaf237911478173377a501ee19ee325b012b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84cca7528c7d1bf49ba79625733ff0ae7522c096.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84dc4af43de08130a04bfa06df9799b6e9e96900.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84e8ae99e184013739019c93d07caddce532382b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_84fc5e94f89d6a9287cf64662a372784511468dd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8513d96a66a4d9fb8dfc84afba7e1d8c200248a6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85156f2c556c6ef6180608c361b7b35ede71ffea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_854c8003a508ed3f8cbe6967c4ae2635a491c721.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85908fe6dc9c629c82d6953081b10021e64583b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85960fe542635079de5eca3c7785890cd4740005.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_85fdde4b25e2fc8cbdd46c2850c19eac8d9af8f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86309c036d96367939ccc3e8922595ac35a3e179.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86513d6e065a44bcb0c789eed1e7e5456e800ab6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_865eb90b1a2d64acc0f6fbe1d807c501fd4be3cd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8689126a7eb09d81baaf8f99dbff8932fbeab3cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86d73393d0d8b769f30222f7817563a955c36dfc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_86fa51b8c7a2f3fac5cf4cd2951ed2ede5c35450.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_875b08ca602fe48840c72cd61798acb98540fcd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_876a418fbe6183d0392b7a7d9986d067e323e2b9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_877e33463b3bf1853c6d2d2009af8d27bf88abbe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8793dc3217e154b65ebba065aa10ab4dc2374ae8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_87e3a06266deda093bdf28af82d8666066157fc6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8840e8899b4e632714632450bcef001c6070f955.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88ac7f6cbdfca2e397bcb86af4216e87166601c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88c04463f9c5ce565a9daa8c22e16de80fadd707.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88d52c5f70abb525b9c8aa8fc1cb3997c33ed67c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88ea5b5346c87cc4fc1e841c518080df4ab811a2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_88ed7f650c958a644c8031aeb88688b1e42458e5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_890aa875ac13957f00b30210477924697abf0c9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_89617bdea526d12d6a33ed42b9b0018c0b173722.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_89a3327da9a3411ff1cddc67eb647083cd947a92.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a1fd28acfe85b3adac859c4bbffa4d28fe634fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a58d4bca33c4c0e79141a56688049237d170d1b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a824621a50cdc3cbadc4b1f9ef18e1325385082.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8a980749c6b2a18c80426dd189e5506334343ca4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8adbdcd28cb2f078f89adf9aad2b3d4a0a477823.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8b17c082f249649eca733a8f0cdf9a1205c3e3d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8b9043572cabb65435627a3faf23b18d039bbcd8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8b92990df507e82f96eeb7aa3ec00c01437566fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8bd1a40b12ce927323594fcce61eb9c20cc5e3d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8bd7b8c63a51c8639b3cf27ad09d41ae47c480d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c074afcf33e3f3534ac3577484237fcfd2ca48e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c13c4f3f645a2bb475eb1c55ce1de452f0e2332.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c3bd4e029bba76ebfc79e6522dbc8ca0bba5dd2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c4688cbd23727dd0ea9a36fb977b31aeae98d65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8c7970957024de050748d3e31cef434f582d968b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8cdcdeb845e7bcdb89ef70ab2a97157d4db3cb52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8cf1007430da272174d3476d042f398627e83512.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8d079c1eb36db8461fa8b861c56760afcd97cc34.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8d7549e66ef309e32779ddc2a1f14e79bae53754.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8d79fe8a600c3b4e0ec9aa510f8036ba2b608985.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8da8285bd6182355e3164cdc5a983375cdf0a61d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e1b48a28b71c7f4c78eb14321b39951a7c5e903.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e2c587db8bd9f1b551624e0cf8b67a90245d7da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e2d5f979fc4fbd0991581a020a414f9c8656ae2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e431313fe082958d31b68d2fd0d61df0fe56736.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e50ea8dd480012cbe10be392cd26d1870e6ef9b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e675919a6c7758cbbeecb83b7ac6c62f95cdb46.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e812705ae3e452810794fa7caceef2ef6066dfb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e816fcad5e9ecfca94a6491eb2274bcc41e558b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8e938d0e3ad30db201880642e57758285b2ec4cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8efb5fc2ace6839eac741c5e6616665845f43566.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8f607ee20c0d92b6dbd0338f139517fdcce98d0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8f6e463eedd3e65b9c79feed3cd92ad8cbc9f036.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8f7166d4bb0c1c9b9999ba16a1adbf09ebfdb6f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fa4c40e244b412a07933d369704bcdaa6d5e74c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fb224b40a7be7db0a9c5c08cc5ab05b526c14e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fb33fc20f2e85e915f1b1529ae87981dfcaf86d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_8fc08b4f3959a2375ac03f40c4ce12d70cdc2d80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9009b7d39346537aa6c4a4e46b81139f603edb60.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_900d7f81c73b35ea64095d01c5d48d9190839e0a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9068ba8df8b0e977e9769f6acf6cfee6b00b9922.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_906fa8bf5e992ddc25815486ae9c24d8bfba7227.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_90b17d8cba28cceddb3ef907df878aeef0762d15.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_90da0d469cca5c8481504148468460c85a15c559.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_90e5c56e92712d00092ba102a5eb5176a3e5d471.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_910cb8bd09d287a1566265eb1e8894fe68d3cc81.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_915b75db795dbef037b14b003ee073665fe35d3e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9163ae070075f26926a86d39e15c27e6edb1f1cf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91695dea4171747fb3cc6d910459f800608d07c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_919ae177b7a793fa352c4f6bb8e4175f3064d814.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91a6200e36944b1f11106c02f7fcee053f01ee71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91b9e2616c2fe0480096b1ccf0f74d584b220146.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_91c916e14198f6d18dc89915e379b01070434e91.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9207a63fc55c411c73e4f93306c5ffed800dd249.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92121fd448b4640a17e1a7fe73bb7b58714c0afb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_921f789d619db6f225e8e9d646e93bbc9dc1a669.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92739f4464512feee083b875e11e11eee4f5b448.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92992be6252f2afdc368bd4baec4b8a55ae0abf8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92b0770fe64e3c60b9e56170aa88bbf74802a813.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92b722cdabcfaa388ccc6ccceb7e42462f3bdcd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92ba64cdf615c1be2865f027a293cb530fc07dc6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92d841e6d783bb46d841aafd9027f92dd1b61b88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92e53359c69bbe4d7405d45261a8a62008eb7d06.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_92f9ad0fb65638cfffb3e7786f2cbf01d9585b23.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_93054acb8a9508fd0f0f486367fb62454de47c39.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_931cf8d05cfa45319f4e5bb49334d35a530bffcf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_93728d999ae43ee1b5a16e60b90cf8533c7d303f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_937801fbb43fb6797f0425f08d13926b74d87c4a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_937c48d0b7096ad6c8bc445f13f2c8c1934695ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_93b885d6869400b0dc2ef1b2c2636ddfd21cde31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_942439e4f5644a3a4630481bc7d98834b29b6e1c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_94a94d145e575747c8956ac703810582c819e2e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_94aa519eb57e5797125728492d9330f5c0f0670a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_94f6f9dee9f0c3825d91f4d320a5280070e60ee7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_95061acc6650fc7b79fa1fe5b2b1e083555eec2c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_951343832a5bfd060c8d12da0d8a090f070a717d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9545f95c1093c60f0fb6c794636f79aaeb53b733.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_95530399ad7b43d8ce2c89da24c71056f2146b18.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9583148fd684a7e6a312127e023798278415bd27.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9594816877815bc0294610ca24f986fdccdc7c6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_960ecb3013071fb65f2d5ed4c947c4bf303e5308.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9638c9618dbf2af119e37596f7eb0fd3f8d72748.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_963986150adcd6e1d3886bacf2166de1252e14df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_964f916d3484295b5918e2e4c22c5529588a5662.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9689ecd7bf51bcffe9f5002959bdda41c50a3c8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_968fc75a7d102aca068e3ceb6111728c280fa837.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96c129dd4c798343d6f78ab78056f0faf2f1c9d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96c5e79f54b71677124f555b0ae4bfd27248d099.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96caa2056d99eb67ada498e287b4fae984397691.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96dee49ec6755006d67f0c30c65f50558bba69b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_96f1bb85dff8c97846f6b2e8796a6289bcd0d9d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_970073c70133ff2ee4737f803a0ac43801c47242.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_971a08c2e48d805b295d979b24173a04cf58def0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_97246460c21bc66c0f13936d27477a9fca1c44d1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9745b04a8026a01828c5dd606d89d044d3ed1d99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_976cf509d9c2bf86ba6ee5ded544fa8e6717f590.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_977137b371df841993c8d0584be7d83aca6add78.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_97851d5ecbf02f8af623988b1a39c0b91e51533a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9801b25e0f132d647934deb395b62a3f70cc7c88.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_987a617fae00fa90a1ba60937b0312c81087c19e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_987f00dd759d9714693e7517dfaa8bb427294d42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9893336a4b00b2a63f23ed7e13ec54c82d9e5063.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_98e484adeddf3394d8d7693b808d83b64c71ee69.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_98f5efcd500ce6b9ffc14bc9877e0ba457539925.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_98f9a4f4d85f292b78123599a2e1798f12aa545b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9990e6ad243a48b84304b5cad0c663c0802aedfd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_99ae680eed89ea93a3a94586bd5a68dbc5439f37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_99e2f290b962f1617b0a9d4fd6d55c43e4439d6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_99f8352674bd6bbe98944a1c0a769a4fc028a623.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a0a70932bd587759df1e5e150b25b0126d7b529.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a20fa19d8d30654602e363806f559113218d66d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a8e04fe9432a60f86ff0369e8c1851821074a04.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9a9edbe35a8fac7796f00bde836bd547044770ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ab73ea77ec20ea3bfaf995dacf93a6960ecdca0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ad1f99284aafc8d7908d062f179a056eb314925.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ae866c7db36286876818bfb718ac35204fa3843.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9afe4b6f3b901ff4af81bd4f1cd8ff19f09d0b07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b062dd633645772e4f2caffd111af73184f7657.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b327f0fa1155f2235d76be45cd22e3db5a69429.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b4dcde1ae3446b825dea739d4295c1d1ec5c4be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b6d08e63b9a90f2524cbfa8c5fcf8b82a1d2d36.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b73c92a13757877f34bd8a13c6fb29b60999020.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9b841b7cf5da31f0c30ec42c91cc8d5bd3fedd03.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9bcc791049e3ff9ebc1a9085d2d20efcc2f99b71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9bf235679af1ca03a6e601b4cf6cd0416d1c9091.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9c4fc7cda4b560040cec93f63021b529aa1ee3fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9ca3b1d36d777213eb381b47871bf15dd163c994.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9cc3ef3d3b36f52089548e9dce522b0448e2c26a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9d3d274058bc0a3d4d35d90669587761fdfbdba1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9d6759d8855c4c6289f1f241a1628cf0406c1b64.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9d69d441f48f9ea346dd8e00376a9a708da3ad87.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9dc424f0e192155e3c4e786e5b87d5a1a3e6c4ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9e51083e13aa4dfa8c969f8f916835a8e5e9ca39.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9eef1b54d5d3841f3fa6b84cca6c7ad33efa2d9f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9f0517550c7a23882b95de451e8099ea2186b4ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_9fb389d4b5ba590baa951f17da06f0e53d2bfa55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a017be7b8bcf303b30a147f41346898acc5fab7d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a02a71fdd587e47ee68e0cc76c3c4494ce06c359.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a02f152e9184af0b3d77082d8bdf519dbbfceb2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a046e888e3836b0bd3c49fec8e1872e880798f0c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a0874fc5ac87a1ec487c7722bf3b1bdaa924ee09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a094599fb5caf5e7aba728cd4713a8d0c6368a46.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a0a556c9358ddd6db719458c81d2d6d822a895da.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a103cd47156a98ad2cf2c325ea00df3f1d67fb72.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a189292c81a18d21a2921ce6740f81ebf4c046ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a1c71e7d33f0597fe090a3524e33e18b2e562680.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a1cba1509c413c870c5d784410855ee1bd737da2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a1d6ad9de7ac7993ae1923a2ef070b7dacb8c563.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a20c91b2f11bb7e5058ca7935b0bda4f5558a9dc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a21f3637624762547af1292e1b85e640b1d329dc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a225c4f1f3c7b271957768bb9235131c67afb48a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a2482a64659c838f3da55f56e3cbbee1dbfe6722.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a25e2aed617e1ff31f93ae7e054313ee0dceee97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a2a715b7e9c1a576f011dfe5769c5b392e984f82.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a2ef5d30a2318ae06430d17f84878800c4ca7364.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3339150d8bf9d073827738527f6cbe15b854607.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3709e4fc53d2254a03ea7660b8c72d2f47cf1ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a388a284f45f711d82a6ed87036d87cef1872eb1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3ac4f93722dc314086f1b7d7b8adc687cd75f82.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3d7aa46528ee74e2bef1e87c1feceacfa55e173.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3dc780b17152f696f9b957432c2eae8fb16e85e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3f9c236d24b30bc9c3fad90cfd6eb00da835de2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a3ff8445ba691807caadd9f26e7eb90851875280.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a421c2ed6b295c458071f1988b9d6f7b46e8992c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a4700d87a19a173e84d64e43cffabbed52366e35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a487f617c4b84c6a0328fedac750d41dc3dafe27.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a48843d844f78690c7a45b730652f0f763c595c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a4980becb0d3149fee575bad1fc3b463d08aabf5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a4b7f10440331a8a88ff93ba253217c2832bcf9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a55b47aafc4340e69e300ac61a7601a5c14513b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a55c7dd576e5b1061c059e5e99aeedf4389e2d25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a59423c095db052603d77073d409534bceef425f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5a7833f4597bb03a3e845d5580d677e97421040.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5bdc110955c05c6c6ea236a6f60266a4a6dce5e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5c0109313de1f6245d2a80f8539485b849e9d55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5c4dc0d70c547dbbfb661e879ba7f9adfafc2ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5d4eb673bafd81e3a0ee213da4603d88b8460ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5e5cae764142683b70d3344cf07dd1edb7d69e2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5f2f0cef657ae5e333d65ae4ab20529a43cd7de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5f8b7b2a891aa9f2ab49762eb31d835efdf18b6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a5fa94bb32a80e81886b711ebfcf2df5f5405866.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a622fa57764ec746e02f6d4bd4846b48c722b807.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a62a2ab489839ea1a1bfd1b24e54a3c232ed934f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a6461d72fb6ba50e81de3f661528c96dcfdc3f3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a64b4cf3f6706e4b4e0af4402e2263b9a1585f9b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a65c43b870705c780d734f9ef063f55cf8b3b52d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a673f35edd69241c6b921d6712dfd064d78ecbad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a71305f191f06cd53b7563971c706e8b71b19e2f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a74b0e7dd816ad08eec5a1bba6e227afee9813ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a7784b03ad757d51c234fa86ea9891f055ecd5c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a78fecb9725ceb4bcf2aa037d43bc43efeb1c3fd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a7f7553a7d2f6d42fe695cdc64423c85223af440.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a821661d8280c6e9d27f2c9ce1b3c855387b5a76.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a85d35b2fd98742427930eb536e346ffb005edd8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a8a4af070ee46d802cb11086b93daf91538f8a04.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a8a744edfa3a19d1493611df5bd0d4d59b707d43.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a92b43d374642df991edef1f6036dc898bf77cf8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a93324ccf11b273ed20fd960c61df897c8890b1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a93a03b33305b33055273711ab31a5b8d8298d5d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a968df29f5ae1463706b7981b3bde55918e1aa65.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a98925d99dc484da41dd55700e151cf545cf821d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9b50c6ebb27986ce5b378d8c39315eb9cb91dea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9d2be18e2d53a5144f97dfdebb225fcb6d611d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9df9ac4ee78e5f4d5bd0567e58a7090907c61e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_a9f00f270680de81df7737e848e0408cb070e68b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa1041530f794c7b8dc4a8321ea0fcdd338fff35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa522b43c5e5ea69bcabb4c0fe28def2bd081a12.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa6d13b09f85ee62bb5018608812181fb43afc86.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa82d20635e592edbf00439294835f6f39ad54a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aa996b9c843200a2ec33ed4319b48106cd7c6384.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aafe891dad43815e635f81225705ff944f990d75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab09941bddfa9d61985b55f9b6bf0edec9bb89f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab0be5a2072b5e87f5ee58149688796b6513219f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab0c3fe9529e24327686070731d0ac3ada76245e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab1ca4ce061f7f69a250356f613cab00d1e2ac71.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab1d7f93427095e39bfc1d986b3d7fe54073ec75.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab43f4a56c166dad0113f51b337a083f4df7cdb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab56e886d53a1d88fada0f10f00b9f398dc54568.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab6cd5c9242f8278c8f3d9ce57b97d605c7e5a3e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ab877ae2a1aab04498bf2b26b3fe99d6488ef151.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_abf6c6412f9853855b74a96e862935ddef66f763.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_abf92a5314fd33491b5eb6ebd2418b7e0d5db774.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ac1ccde31b47e0e56ee0daab6403fed7895208c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ac5e9aee85cd16903bf7b82a4ac10402b0b26e22.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ac9382cf8bb56ffd962c99329bf67da992f8810d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aceb0641213e9a45ba48bcf72bb23845720d8b79.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad091c69d19b27f7ad50ef6311532ad8b642a9c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad82071cc074fd30437f6158b5eb2c6df1f8c587.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad989d2ce769f20e175fa88f4082c1c25fe03062.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ad9b99a194b59d3149842c15733394da275b12c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ada016be2bd0e377fbe01fa7adb9bbb8febce100.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adae2d4f8b2dac799e03ea6f279e6ecdf66f5381.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adaef10ff2c5d89530310bdf1d53a194f06a94ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_add29e3e9828911a117dccaa5650e77805730d14.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adda7ad787524e3e47dcc1b65c41b2faea38f55f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_addb6a14043c5a4df0f5042b3770b40c4e90795c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_adf160741a4f751d2f15d6eb23d4121cdca62b55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae1ab1f4bbe86bb9bbc22e4774648076c321136f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae1afeb6cfdf860ff08e4c2f11c922fd5bfa621a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae239476d61f48379754b97f29d7a285cc3192de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae4e7253ad4873576052ec0a9400597bb7975753.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae4e80cb185759dd9b3eb3c67c239964b3694caa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae51b30c7e1cd30e550187458350c8db7c59a9ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae7899b1ef159ecbf01f27014601eb79b31b49b3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae87b1d5c50606430b544ed650d87df24366e7d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ae8d0bdde763e617beafc0365ec4a3cd11df6c55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aebb2441e6cc1ccba4a391566e547402bcf7ced2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aebd5fed34ebceb879ae3dffaf58c7c04ab5fe80.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aebff7e6605b273bad844b8f70ef031625bff48e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aec87e65afa93e84d7a947c52f291c1c7360033c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_aece14f7a220222eb4ce6783ec2b9fce6fde94b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_af06c0dae15684f83e15722a4c07342af9ea011c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_af6ccfa11add1ae49888337e84d9c446d2f67da4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afadc4f76e237514db0bc0203102297b79730bd0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afc4b47a6fa62a4ca5cff6a7e01c9f6b371d2215.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afcafd07c1f56e74373ccf37db35976023456d50.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afccf699f593c828e11efc053b144044e45b32d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afda8f46b5ded4c2aa9d722fec17b75004b59f7d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_afdab954fd111ec48721f25710d61c0c8affd8db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b00e062055933388e37525df5766f3c14cd3538a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b01dc872c24db4db0c9179fc07e17f41060390de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b03ab68e33844f97aa58d463e00037bc11c50da0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b04f14f829eff73afaa57a875f74ebd1e6860979.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b0544a38dfdf4d81dc95894387845f48435e299a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b0dd965d5d9080ed5c6a04b7eea9890f3a264f20.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b0f555b74ed36f1bef8f47880b3edc6760f27788.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b1766695dbb790bd614b83dc7569ad449404cc89.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b18a615e66d7cd739ce35412811359a03cb23a8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b192c55f002d8540d5f965cc4df0c2e33f4b9ff9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b19f05f6848403480ba41d37cdbf44ccca1b1f8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b1ad101ce91348266d3885afdf2996a0fdb72135.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b1c5d55d47d6038e9162d32ac968ff58c0942938.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b20c6252863a73341b0010191fad4c834860f884.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b20e314642cf565e4f32bceffdb5c0e653ab627b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b24f91dec2029b25d0d96962528410df55a468ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b285e2f1970b78e18002464eeda63798229bbc3a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b298e213f927b518c693660110f08bdd94990ef0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b2af5f5b5ee3ae964824a3e9c7bbeb5bb39c557c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b2f91e937b427ecc932c0cb0c90b2c2378db0be6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3063d06723ac70c5f8802ab49c5c35e1debf56e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b31f56244076c501cb09b4b90975132cae4c4386.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3486244e0b7d6dbcaa1951e8b8883ce441c3f99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b34c1ce348c3d9cdf6bbec9758de9d5fe94c43fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b38a1d3cffae01332a3a9d9472ff1b2c443e82af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3a104733f678193068d8642d6560faa03897258.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b3da22d3482738a8474ae15e8e5fca9020c4e195.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b41735d250b5a16967281a5f07873b9cde3df4d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b41a30092e8138877c1f6c25656e0f8ae2c2444e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b41ea5293bc1c56efa2c4b5681d965aa6f2ce6c3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4588379eaa268d79fe8f8e4457b009f204a5fb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b493c99888d82cd2852bfb101f99a2e6a27665b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4a5715b550f67b8870ba66e1e6282a26cc1dbf3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4b037a2e262d11d3ed7d9feeb41b9e05427a739.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4bd2d206ceb237ed2c51f58abb5cbf96e39d07b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4ec377c44ac18527ca6a01bc3b146706a6e1e09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b4f12f10d7b968e0d8e7c23f36d3a360de74a905.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b50e6df20a2426abd3d2ff2262a37c009196024c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b513834918d5ea789e2db21abece7c2d3532a7e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5248f443a12d96815c04409a00102923c717023.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5371415448fffffd58bf014dac9f4876153657b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5ac596c636df55e81293228cbc53dcbb3024e5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5ba2e73df35f6e0f7317303823fde92a42b1a35.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5bccc85f74f54a2ceb17fe3040b04fe306c53f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5c3131fb8e5a25bd4a14bc9075eb6fa01b61d02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5c7fca1f76a31b0390e92d90d569fab94d4f783.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b5db3d5b1d8af89381fc4b8073f84c5fa25fdef5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b60a4e87a7aabfe3c1ce02b408522f3ec862e3d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b6b17ae67adee9e56a022cd2a5514fb9c4e99920.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b72a804bb3c99830653d41ac0bd49943c801b89a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b737410b404a51043fc3bd503c0b107c297e4c9f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b75843bb13058ffe29251e053800c509c7590544.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b774450ebadaacf23e944aaf8ca90eada01e8a5a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b779cc0b0380e1e6a2b51fc6216fdd72215b882b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b7a03ab0b7887cc7ed0cb40e56360a8d36c0bb8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b80d0828ba6d24ea3c1a97bd9835ee937b4b32fb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b872f9e6ebe330cc1818ea82b53acec79a2f672c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b8fbc6f6e9c515edce3c7a438b3bc308b30d3857.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9385db12001110c42eff6aabad935a69ad3afe2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9559dd36a0a4f5e068a722e285f485137bd5ef0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9627f9c8d0088df0364a64643f2b5dcd951f2bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9a742ceeb6736a2c8f9439d0b05e10d3e0c5c6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9baf70220079e6d4e87eb01a7259923d8a01e29.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9d00ab8373747a5c6b9d2f8dd50ceb14db4163c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_b9ed0a64deb55616646ea98b21a891c971cd98ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ba145535e53899fe127987aa854f81234a9c51c4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ba8b09f0aaa40a7c9ad5f0458b460d3e328f3c74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bafbef3f13d429ec3e9f4672218998d5669d79f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bb111b7acc269f8d5e70915d3efde4c425aa5f5c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bb28a4e95723e3df380f98b5ac107c4df353850b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bb35c86443cc9ea38c06ebc0656306483c95ef67.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bba10ecb79ede07324e1198a71a95ff26e9eb235.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bbe23201fbebed25781f249e5c77c31e0e7f9ddb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bbfd025488e52b97c04995c4c5faff371b77e4d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc1ae1dddb8cc5d78196da6b26ebe66c1ce7e567.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc238fd2095b26a167b41cdec8280182330b7b25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc4425e30a0b17e8b31726817e8d3177b5c51934.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc4e0f0496a34d2fb43c80ce0162ad4183f29064.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc6ce17223d8d83a64b8c96ac88223e4441a4692.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc744db85d4237ee9640f1658e0caab7648e3bb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc79e255d25744725e2a9db9f90d5cc2b8a0e0c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bc897852a4ca992961843144f4ec4f8b86dd5e9d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bcb6f0730fd09b4c6c60913425927dfdb8f83d82.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bcd7ccdceb7baf3b986f2a0248827822a5f72e47.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bcf8836c8cf932cc2748e313885003f0e11a887f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd064e302ff5b983dbdb4ccf51383fb29ddff44f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd28203f47b6a48e9b66302cf8312f3796ca500c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd37f4f7914805a97d5073f1ebf8a8b8c2648d31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd3daa5f99b4522d932334924347353ce2854821.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd6aa39d0ae3c87d011610cdb5e2e317f337c454.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd80a1774d8b7d8bee4e8663392b97cda11dcbf5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd8bf7c572c1984ca3061062cf3c31d993f6762d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bd9c47f3305e47db6ab6bc627fb3d80269633074.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bdab172627718278a71a93e3737ef08ad9259a4f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bde24a8dbe6add6f2dd2beb48b1280f3a84a9b2a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_be1e1533fc37b41838bd37edc2b6d2f2e76ae1c6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_be4dd90ccb2f258029d0156cf23f940b694cf08d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_be8ec1163a01b9cd9a802d8b44669e8770c20234.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_beae876d6da465687f162136231f15767cc7bb14.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_beb9afccc15de7dfcb2e7d898abc0d61201de73e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bec30e7107c5dce3fe6aa87d83ed96da75478da0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bec9e4c0317e8d351f60258ed6611fbf365c4024.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_becc2a4d7ac045365300bf8bd45fc6d3e1e1c8b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bed5a8c5cf683f6dfaefad72c2e2f5c2f2b2732f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bef3bd014a918feddadc98eed92a7734f9bcd890.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_bf9cdf86a7944cd690b0fcbbaec235863acd10bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0338fbc05f86270ded7df2bd3e2758a03961b62.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0342686e4efd26413c6719782ed13603479c4e0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c063318cb851ccaa923be12d34c84d839bc64bb8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c08095341ca7e3a1debeb780c1878e351692bee2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0a3c4ac0a50bb9b7ad764929dbee98c856b1210.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c0f76aff077c28f8afd7b22f284cf2894e08a043.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c112c01d201c366bdd7acccf2e1b18b00f671153.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c11d68fe766fc753c657362673704005b538660b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c137c03bf161b2ec6a9a046fa49d7bbf80ae47b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c197d1f050f42d82e6851fa286db6f81ba197f40.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1b76bc7a17f573c0d52c07ae9ff4302662ae61f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1b94e19d762ddc33cc4e94c6675d93cbde21e3d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1f40c3421b9ad8cf43940530ec50bcf620058f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c1f721a330b2d0fac13b22061616d7b10c0f91e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c250ea59ab6e1ee39cce15cbd3f181047cdee31a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2541b6b5cf27de3f45f60671d36602f07ce1783.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c27b3026f1dc3056dee3a3e64bf31c45683607c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c28de8f96c8315877031a2d56261e95fee6aef44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c29110dd501853e87ebc122dd1971b0bb1bcd92f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2940fd05efd52bdf8a3f9aa4b78bde9b5809b34.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2a2856bf9a81544a30d535a13554e3a8107c476.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2b719893a4d8a1e71857966d399f06c0a41749c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2f04447e6a94c94a2315454e71d7d607a9fd0f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c2fcced07cc194a8050bc7b2f791453b3f5b2064.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c323a4d1f24d59bddd20ed2f2fb6446627b0ae8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c355189ade9b1a8269230232db754a3881b53168.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c35ea54eb6cd0f3756c462c66d9be956279b46ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c363ee1b087f6b504a3dd3972b96e77db02b0582.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c3cfaf0d53869c373f6d0ec821b008dbb819141a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c3d0eaf9399c863d672e8c08d123739bab837d4b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4015f0d0a7a5173810f6f17c00065e03fc61a89.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c402e84359b2037a29efd1d6ce7213ba7605ab25.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c41b6eda4f250da059fe0c428428219ff5a250ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c42ab428503e8f8bfa78c8cb8d9afad9f5185118.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4376ac8d82db1bc25fa273a80dfbf8b71ee5e2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c45a5e40f6a66bc5292a56e0097c69fe37cedfb3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c487a1a9933239270f44b1e08e1cf5323521c089.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4997f79435cf64add10506acb97d0647cfbb3d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4b34d3cb673447773f6da23e9cf52b98e99f718.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4c3425fe683d35dc3335db77d183ad1620b7a92.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4c6c405cefe204824e8fad1b3dd34bba87e796a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4de1bc135191f3c2aff740f4c6bb7e98da42f84.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c4dec99707511cebd9188d216ee0a148d729b470.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c538dc4f65d02776875627cbd20a9c794d70b043.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c53e295b68e807774ed31bb914e4bc59312a77d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c56aa150611b0d4800470c1493dc907082a5c23f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c581974c8b6f43f60d0af29c350d850b55c03121.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c59937be2b9a13d6520fdcc922e4e75c9fa085ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c59a22c6efd8bb8815887325aa0b739e260cc754.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c59ab718fa23f24f09a713ac28a339208a7a5802.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c5b440ca9a5196ee1e72c878c87d96934e9273c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c5fcdea177734366d3bf283317a65cc3fffda611.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c5fef330a975002ed15670e8e7b26a10376d3cb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c64f4cdce32189065362a502105c31bd2d9d99a4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c6e2da8b791d31f4ba05ef5f833fd6dea9e35f1c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c7568e11e44ce70924d27e683190422cfae5c31d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c7af2bbfac25de2853be344b9f636226c1c0112d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c806d7803d06ef8aac1d5caac9f36aafd47653d5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c80dce1a17d073259250ec0c87ade69e639ffa8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c8dbfaffc8a9b573f194f9c63f1175d9725f8950.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c8f6461673882d636772ae4d26e78eabcb568f31.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c919b8ed877d4244d01a17ecb948b459e361ff24.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c921a4790f982d48bcaf950123c699647afb739b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9312d7159369d13f3148a6f0882dfad6921ceec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9530e20038eb40c49bc8b045be0cf4e7e6b4eac.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c977735a36c325706bd19a12df66ed0839b032b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9ad71883a19b522486706d3705700c012a6fc19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9ba0a3369d4e4eaea1c902a90e6501f232dd57c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9f1e7e478a2208c4d32e2d7e6abebdc16bcc5fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9f28230817c9d9805c41dfcd4e834fe302e1df1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9fb8343e623e46f01893a2b61345d1ca5928671.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_c9fe51f982abd60e567d4238d3266fb60e45814b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca00cfdc5592b7440d72482a18781e9cf3afb05a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca1992a2634cd6674076611be54197c715ad8271.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca3975efd767ddf7c12e308d948bdcaf0968493a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca3d98ff43fbb80ceb82fc22ab039bee898969b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca4c6ad28aff1976c6dd36974ec3b339aa3090e9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca5681d4e5871aacef74bdba9e368445875252d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ca920c3239bb5796b1ab2fc75177eb3b820aa784.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cabb7b12cdd9b8b522af577e13232b2459dbd38d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cae6c7efbfc831e2bcfc8c1efa1a486c02627cbf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_caede7a18f3e3d5e24f6c70392413a2cda16ac15.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb10303a0b79f2710eb7c66896d3c1f8b12c04dd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb1a0ce432c27f4cfa51731c3ef181bf60c8a727.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb1b91c16e0255fe7a0a85638b98d94634e143a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb1deea4f4fab0db31d46a91228601f0c272d6e6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb20538073888bdb3174a8e9c32d7449072aa753.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb3d5273945c5d40cc05c2660af2df1fb7a15f3c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cb4576e8ea5d59d7663f3760009a00a19e1b0667.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cbd571f4fe576fdb17d5f75a558cb6747087c7f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cbe5a98163e878c7697e554758ebd0597c2c1760.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cbf3e4d4d4837a0cb33b78c4f2767b1d93da0850.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cc127a63d56099e08125b16939dac82f0173122b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cc4ac5a18f57f2ebb65f7e356e858ab0d59b2133.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cc54b107e1b557ea36b5cbaf7fe3dfce05415c86.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ccac6c0e61b65c9422c7f30fbd979031698370a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ccd0b777df1328bf24e070ed4cdf8615bb2199fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cd0453a5c3828c1358360f31f5d3b7258e17fdb9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cd4efcdd12184211c74e7b3f2f30fecf1041ca32.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cd757a8bbeabd16a44d149ab188430f6d79ddcaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cde0582e1aef74f9209de638b553ec0671476258.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce4714e4f33340859c106a3129993e22652262e2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5064e27ba427cb951f7e1b01328b0beb6b2b7c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5ad502dd40353312d561e9f40aa478c16ef5b1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5b5932f6df9a194ceb0d69220fba9596528eec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce5c161b725becf059fb4439c668edd454ac77d1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ce909cb5f96a4884caa0d2eb8c5e6bc7fa352797.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ceb9544e2a0caae2c9e3dd8bbd2c509e8dca1379.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cee81ab2e2678816c7b516d2d4c50e8cb5874c68.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cf5c6c0bfaf98f6e655fc443246b81fcc730fe97.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cf73e1fc0015094861ca0c1c81bacdbe0c5b8f37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cfda56a4eb08b803332f25bda6209932d9624acc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_cfec97bdfb6fa95e057eaf5a8138853e1c0884f2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d00f65bc99ca08eba66564d34f72f2769bff9491.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d036096f49a89730f8af7e75457c88cb8ae64165.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d049a1b8f4c1c6d37973ce38593efda1de8ce0cd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d04dc4ed02eb42c3fe303342801ed3073a0dcb8e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d06ba4c996570ddab77b6ff1e2a0101b638543eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0863830fc5d43dc6d6400280e892bb7de2892d4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d090b771a4f9750132f549c82a88b4ab00dce5c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0b09e8513646fbb2a007544a63ec9e2b04dc4c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0daa59f5dce6fc3965193ae37d8c82a3d1834e6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0dd0165ee91c095a19ceddf08789e3576912590.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0de618ff3ea9f67b90f2227fb7fcc74ea34183d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d0f63cafbeb445408c884727b473667fb479675e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d137b7b6e04e1caf43a62bd6788a75361cfa98f6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1840494c4fa78ff399c0399b3ad7ca3d22d4587.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d18727988e47264b42b4153dc82fc1a750f08db0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1c0dfd19a08d61586758091370acbdc6f267017.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1c25cfc437d8bd803860e39a45b2f3b9fa48393.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d1d3eacc320104100bce46235fe656e5a8223c66.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d20d45aa85c0daa299da98c277cee826fe67bd27.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d257148f457557ea80ca56690e525db3a4b0ff55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d25ce4b3e9cc392ceafebc7fe3bcbe05aaad4bbc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2d08c5470a385d0160b2c1441fd1c30fff1c17c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2daccc4b3a0f90bff39cb4597f8b7e484613d9e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2dfdb42c1b380e860aa5609302f29698dd27923.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d2f4b869ff23874b6bde0aab68c419108b7e69f4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d32c64ef01aa228277d031a74df51363f98aa2b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d34d6cdcd81a456125ab5e0875466c6334d8e5c8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d34fcb56caa8f80404789fba0ffac447483a4d84.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3784fb4c0685d7b651f4113f3c71e050881f3a5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3a23ded424200d0c6f06b1dbd0a7b7b0e7b5d9b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3a2edf232786d458e2125f8dfeda8847f842afa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3af8763f289dace1054bdcb4dfeda28b0aefcae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d3fce1e11aee2273620e75efe4aa0390fcde9ba5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d40569ae9dbd693c0ab3d6ba69704d31e451011b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d41b6a64dd181f2efa65aaed03a3d229b3566c1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d41cd6b60a97e7071518cbd1a63abb8b910df024.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d43715cce8935439f90172d141050d78c7e76fb7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4605b2ad3e3753c5f255678abc1690b949c5abc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4645b713821371161a9925dec8a3d6c157ba1aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4aff499ad527be5fe33b8e92547df57af26d40d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4b99af9a573df50a27fccbec3fa8e350f1854eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d4c9f975891087e6eed6393629b41155deafc509.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d50ac8e8a03f8e7ec2c6e993dd39f09f465dab57.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d54ac01458df3f240e0656d82330f9de23ba9651.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d54b3731883a5f8393d60d27487f8d017aedd3f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d5e82799f4452e148c3e02acd6526cf30757eb52.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d5edfe3e3dc3008b928c8e6dbd50784b905f189e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d600779c17b7b21c18e1308e6d765fe02a7945d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d6149eea92f2c40c11de3b778102fcf9b6a006b8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d623b36cc3f56d1001b2d3abadd8a5628fefd014.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d63c8c746055851217a514321cd735eaf6937263.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d64b8b52f4a98801e185e2f132b2f80c29dd0c37.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d66b79c4ebdcfd239cecec58203606bc123bd6bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d66c30148a6fa816937f2f095802264d3dfa0273.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d703eea8075cacec4d41fee7dc4734f593ee79e8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d712f23ef88ae5d7b161d36f42d22a5ba53b6354.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d713fe25dc90b3511fc259cebf463376dcb55d84.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7145383e39dec0e346b5094401acf85ef3c2075.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d723b191785c97d284675f700a7baeb52a2eb791.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7290cc4c3036c9205e689cbcc60e7d16b97a7d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d733f4c03e338ea7c6d8f759c1132499bdcea059.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d773df9ccfc1ace90fe3afb5c00976deabedf6f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7adde8780b39f1364c572a19c3bfb19417678e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7bda8157fb27d544e049fd7d2ec735725f1bf44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d7fae2c18645d36a181a0bdd2d8ca7a4ac0f6d1d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d82773721479613ad72e334510a248f1436b38d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d867098db97b3f26e71a151c63b74260bfab21f8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d86e4dcbe9c4cac8f7c8c5d97ce384ae0cbdbfbc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d8901a63986cc28ef24cab012b32114851a8c1ec.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d9061c204d8a85c974676f4438994a0be9d69a60.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d924ee32b178b6bffa7a71603d6e2818f66177a5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d937609afa8e21a761dad6b01ff3f26346e450fc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d95835bc6f000d3a3379bbc38d90e83dcaf867ee.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d992eab7de49033f5480c5e86a69e675db0d2a19.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d9c23b7f8fcc4e4f4c81f5f00cfd345b98df2e0f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_d9c3e27b522320dcca5ee84fa534b03aae2bfea9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da07d8b5666423da30a95e3b2cabd3839d200981.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da29a515d14dac02066bcd4701285b9916b43cf5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da6afccdee4107507a64323e17bf12c46da2b92a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da74887afedbd67928fe4d596709f9ff92530611.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da822ea727fb3543e445e4000f7e6ebb946d6a3b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_da9f6e1d59132fe96709490af25bd794f267851c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db0d0cf55d90b3f3c9eecada1db93c420f34b1ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db5016bff9e5dc37184d2b9417eb351c7ea1c322.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db85839ee8d464c5a81b8dad9839f5e0f4b467a8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_db8f0bd93b352d28c5b6d78f4332026993f0bea4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbae1670fac6812b2d2cbad973e4b475509ea504.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbb06b43d5d65429e23cc717448cf1fffb0cfd74.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbc4135fce01e8731fec7a78d0cc0fdeeae28b90.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbcea8f7b5930abf76eecefce92d0db785d2df5d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dbde2ef18e2174ebe13a6e7c8c2a6b05a6612047.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc039d422a57c159ea4dbcc867d766ff1b356a07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc08afbff5def8bcb4e823657ce01f57c9dc77c9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc184767d723f4995791848cdc68bd948408204f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc1a7f9b1afeba6690fdc0d0d1755ea89c805573.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc34b6ef496d4e0d8fbbe10731d4a7b1c136c036.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc3d625c5ad3e871f5a727ac946df642d988b9ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc4d27535b9570b8f4b790470a83c1d0a9a2b6ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc5ba6d73f331c76e696953606c5b347b6a46f3f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc62a8db637d32e7dfdb2521cbdae6e1fbbd5fd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc818f3ce244743cb1dbff9aca399df90742a6d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc91797c1474a368e9cb056b50b4629d7736c3cb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dc9e54273c0ea2358fb573a7d918aa7b09fe07f9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dcf815ef540060cc7ed43e1c57a28e1d080c5621.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd10bbf37503bbc92af82bc3487989b41b20ca85.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd11806cd2d3ef1127f676b2d98bf8fff2a1e5ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd35634440edb25cb095800b882c70aaceca1dbb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd67d442001d2b167e70e8730abde4d4461b8569.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dd9494d9ac35eba6794a4f9120d2db9932596ef8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dda8d021381083bc48b7fb1840729254dd8e5137.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ddcb1cfea1b0dbe50a02252cba99428fd977527e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dde93ffe7fca311e136e42fbcd12b05c9fc7174c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ddf5339054f47d9ed6cc7f9e66ab21ce3bccf3db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de1ff66d2aeb47d2fdccaa4bb6b9d066b380c99e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de26a187c4db06115072a5132e1166b5b03368b0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de36bc309877917a18fd21acb30563c7e2f233c1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de5359f0fba3da9dfed06ddbea8fe2a33a9cf40c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de6683d175affaa5ff261ab8503f64172d8eba8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de7eb562a7eff31d589e12945d80233aac202ae2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_de85901d66dc04b1143bb6404445baf65693b781.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_deb9ec2cccab94920e40f62a1f0f094acd919d07.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df0b2bcba57e77d975ec5304fc50cbd09cddf4bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df4bb75ca79f805a81fbad750ad22f6d22b0d8ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df4c9eb48da49a61957537270d94e56cb4e426be.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df5b1c6758d4b8540158299dd0362297083084c2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df645b3888dc8d1df50c47c0d75822eebd3eb019.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_df66feebc9a0dcc508ce002c255154622875e524.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_dfcd68acfca68d1acac94f493e25be0ef20f209f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e02a198f23c409b715761b702d7b0e6e5992701f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e035773419a9b3631698a3d375d829af55f7731e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e088f0f7363804cf5403adef70828ab32d09a02a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e0966fa1ff013e477b1706928de6cb7f8587c154.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e09d9baa269dfbb30b714389d1733be51cc419b7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e0e48d7edfe9513f24ad9fae68cac3aa940b17dd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e10f47a44400de385ddbeb99475b717c5646fb41.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e11a3b7d4fdfed64e64f7a95dbc64eff541092d6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e13b86fe4e153e0bfa8d1e75f3641fe32b0c5149.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e16075c3a5fcfe63ba12e854bb1fed6873f014ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e16edb824cecf459a8ec51b8dc74b1e06369aceb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e1c1a31a1d8556cbe0b6ea76faacc78855108539.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e1cc934ba7baab1a2eb062df1e4ee5066e9ffbc3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e1d85ad2c9d197f501267fe0804e6985802fbd18.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2762543d3380185e304f84749a70db1b8d3dd8c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e28fd64c2f2b27577109a984e6ab82f5f0fcb296.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2b629c37cf94134693ce455b8c88b72a39df7fe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2bf6805a489739abb77c13173d57723e9304afa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2c9f955f227430c6224ebc347649386be7f01eb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e2deafd2f36cee29109fb824e0135407453adcfe.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e3015c5d50481547aa5754d042d9d7040cf1c7ff.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e307a1b0d5a8f94e0a0f4032f401d20b4b643523.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e334e691714f0b99773c2ac515ed82de0f387065.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e34b7e452a4db74189334697e3a240ad68085f0e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e389d0e4442cd8304081892ddc75043e68a6398c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e465193d97d43237c22c04478ca5833011d8dc8b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e477abef05ff37ec27705eda51896e2aa3a04966.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e4d9a2396ceccdadab24602f30e9070901a76dc7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e502730dea6987e2c038446c448aa08bdcc23113.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e514c6b4bc75d95a150104a17972abae77cb47ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e52e3053f30f780f346fa6b7a836ad2554cb85df.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e56757fb17f5e94a6ba1fb14540a68c36d571159.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e578ec9e09d3b78dca6b5bf0be1538657f02f319.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5935fbda313d3518f142f43d46f56c600f69286.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5b2bb9f8466de1ad5210e4c39ee7b8ecacdffa9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5b65fc519ea7cfcd19f7eddbc3acad6842ff558.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5c5079636a4a31a849ce8a5af89d50330a74628.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e5ccd5f7ddc894b2717112cbfc766804e02b7bd1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e618fb4e529104fc90069c8779ce5463460bd516.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e638053e01268a4c5883620fc6a9901951e2e01a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e639a1e84faa98477b05df71d363b9ff0f9b2760.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e68a9e05debd456a9975953f7b0d510e7a0f6978.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6973d75297bd2c3432a7c88e8a9ee1c9ae693bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6b53fb8d81148ff384d31a703bb4c2e7a5a33af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6e0ec1db1ea308e226f675e68e29b839e41b252.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e6e6b10e73733716e71ebf5a53703fb935fc5e02.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7153f9a9b0b7c54ddf2debbe297efcffbb4fcfa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e73a776ae4ba68c23acab1a5a6381684051738ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e75c757c67aa23cb88e1aced6fcf36b7b28391db.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e75d492ac3a6ab75648056bcf26250a4aa929cfd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e76879f8ff4796f48ad87ff8003f4f6e6adca9a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7ae1294b6dea5c8b93c2b814fa7460c4047105b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7b2eb64b66d46359fab44333c2c484f4c9dd5de.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7c0a99e949baa5f3a7ee2d6e84427982f82f76d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7d37e7ee96c392fa24c02a9143438a3a7d05741.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e7de729aa50c10d8101ef504138c3769e3286753.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e83c604d1b8260958becd1c7c209745ff9151715.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e89bcea4393593313d18a4aa6dcb44cd75bc828d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e8a9427f34bbf5ddb28a39161acc36806e68f2d0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e8d8fe5f4f8641998b8b805a20b2ca92d019ee59.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e8d9b65558398c0c10127b560807578ef117d7ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e907e8d1089557dfcc95a05160be5092e9119a53.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e95e3908479965856843317c8b0c42a6961dfd23.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e986d5f8d5591f3e0f1cdfad19c38c420fd93023.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e9b04e6d5527ba0b8089ba8bdd264e2d5759338b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_e9b53fa68641f45baabf40b7cfb8b35a9a1b9c7f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea077e68dbc1bed2dd20a5f4dd35e0cad6330ee4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea591185b1c5f521023e250a26f742984255b241.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea62567e9ea16771d8445464c38f5a2931cb355a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ea6a6d4cc262ea838dbb83ee747112f95fa297bc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eab6cdc59bf216f7045f0cf5f221bb91ec415cd2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eac353f963c52624cf79e82cc2b2c02eed94b677.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eac5952f46f4f2bf06257b00661774eeed48a323.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eb278488b2cca114adca5e4614d86f92447f937a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ebb241b947a0adfc8e50c5d71765c14af24593ae.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ebb9abf5b09e63cbe76390bb46ff7cbefb3141f0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec171210efd217c07d357fcf42e5372ad7e9abab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec3deb1382003ac010d9bc1c59d1878d3ec7a727.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec51d24ab5f24e003ed6751ae8ae5b327892b15a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec7ec8d547ee9713aa3b5b667f22cdcaa8f62b2d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec7fc24902b1ebd8f2bf8088b0ecf6de8be8362d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ec9f63a538940e5ace02ae5b5ddc01f730adac4d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eca613eaa8471ad7da66d2f8f2b8e07f6e02b467.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ecd7dec90b3c62bf3a30bd75d3c6869529a06b01.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ece60111633db08f765b3c7cd5cd768cbd030255.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ed37ba962e0288e2840eb0925d016b5a7e3b3164.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ed6bdf67720e938d538a867548ac3579b8238169.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ede81dbc4cb208ef6e684c76ba1eb451d37fe10c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee1a43f2210a8d1e5623411c95c33424cee5e747.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee239db5a67c23a383590a651f0d8a0be43a13c7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee8e709eec7aef1fa681053c6d2969a5ff18c45c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ee974931e65d6b16b7c868d462b95dcae20b7513.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eeb0e96b759e18cf703cfab0cda1385726f6e0a1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_eee408cf9456ff977aa7d12345e9b2f1e60639f1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef2ebb4a86e7ed0001de9c5e607b66fe8877409f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef40f0acf1885096efb840ec5600ec421c4db331.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef5421703cbfa63a58ec02701e245d479a1fbfc1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ef7cc2aa1ffd38298b52764a93cd1271b4d92f8d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_efaa0cb33c71cb8ca7b83dd0e7a6c7b01f6b50a9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_efb9e7d9af47cdf79f15f674f8976c05f08b0ce8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_efc6a7b25710f0626c3af534111b161e1459d2e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f01468c62c878295443981662e037ec5213cf7a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f020134822739be6fa0bb3d98e9dec79f025324a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f0209426a8e6bfeef7d8ae7b16db791888142298.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f028af9e5e3c25800dde938e991aaab4fc1d64aa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f053c9c32518b895daaa3521827f37af78836fb8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f069b38b26c30bc770f74c856e47eb498f5818e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f0cad48d9bc80d58705ea60eb2dda4baad68cedb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f1246d1013d954a9316f4432c986d3be9459c548.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f12f1f1b679cabab04218037ef370d2c7e1fe332.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f15c41ddb04ec7f80235bb3db19198dd6b699713.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f18c74becc24a93427d9c0838784e9b6caad6e81.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f1ecc90ad7b86791a9e6f73a582aeff30f393804.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f21596e8c608a795ff971aea8e199db9e72b65d7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f24bd5b92ce6bba640b8ec6b4e53fe35902c5572.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f24d42e820adc1a26a428d59df7ffdd7f8580176.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f24f26e45d5cf567d29fbe375fbf8abdec39186f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f25b87c435bc5d7d85d738f3fdf68947d79f5a77.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f280e1639680ac1e5830a21f921bfe2cf364ef42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f2da112b1e07c44fc8a7f19368da203f6935049c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f30316cfe49323638f71ba688dd8ff9b2266b335.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3193ea266f3718398bc5622f8bc7042c3527a42.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f34fdb8294257d951dcc9c4fa7ecf1192568b91b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f36aaa63ed42a578b953ebd614318d44cf44e8a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f395bec57c3b2e6e169134dd8d20b287d7405134.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3bf7ef503bb026258b3ec3d82d3ef1443046964.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3d0166931e4406873d8f552a5d5b61fde2391a3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3fd08d56f8a9be1a8dd104cdb1ac58e283b5064.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f3ff73f82aee3184849d04c2364eaa45c6d0de9c.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f42cf0e5fe479690883507028748b0cd3dc83cbb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4658c32d562f9d60c5ca1262a2e0df2375063bb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f48f8b681a405bfeba5aadaef40f32367ec5cd2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4900c0a5c0d03dc17d7a907ab40652d9920e756.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4a6438394dd3427f29aa0bbe58ad1f797c3c38d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4b87f983a5e84582efa1663f84da76cf60b5f6f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4c803838f5644ccc6f04f7c8a6233fed0b6639e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f4df1cbfbaf67705820f125b474469ad7ebab0c0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f50fa4ea674a590d0a817367ad9915a5fce20c51.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f51f1a11f778d99a00aa5959a3e58a41fcbfb1e3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f525b59df454ccf53da6cb201e0aa8d09f52a2ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f57f84892e2a8496169b7406e63b0d4f5aa63aaf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f5803aadd93e33567aa6b23100ce4fbb6c040dd6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f5f1797f6b672a55476348571ce17645c8a62869.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f6566441ac3074578cfe45758ba0583c0da0a5ab.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f672bf80a78885428b2c02e522426470653a7351.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f682399cd6412fed6a1141296a7e4d42078f7b29.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f6856ca950bcf173571766c3f04de4163be0402e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f69548d6cced86c21c09c6475237a0cb926df0ed.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f69878f4ca8cfe6b8d8748766f66a1ef8eab20ad.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f6f102a388ffb05c690a20a29cfe0b35a35eed61.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f7035f4bfd8f2f427720a07e3c311bccc1dba683.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f71f96ce4dcc7f789a8ace73c230c203b05ff6dc.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f727911254904ce4341e4ff5f8bafc430b8cfbbf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f731289837f915e2aec1bd01eef1b3c1b099864d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f79def2b4edf6d18f6ef1d6b141f9e0435441f6a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f7aa9c39b06e55bf4bc9f9a2a0fb075c9d4e69ce.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f7cf08242b3fb1c643d4149bec985b667b9d28fa.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f851da732f397624717160f89271514bc334b59b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f861d8693f82d22e2c5b1abbcbae5f30f4433e5e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f87790f260630f312b84888dcbdf849ce130ae59.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f87991cb7787a29d3ce4711b4ce04c5fb6a14ca9.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f90410c26d7649e21e2ae5e32e7af89d84d2ea70.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f92e9a82c879051d6fe3c42108f8a574187704af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f93bc23b8a4f1e0fc5c5756c4e1c835bf59dea09.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f93bf815b520a9d9e17b43bf9d7fb870751b6225.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f974b12e83e214c30995a25631d37df1478927af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f9824fb32933b27501ae8a7f43f460a2dda6a814.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f98a6b193fec3203eaa75819f6b51aa45a48f212.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_f9c58761c927b222112cb5cb6c9acb5d3c915785.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fa16fa84278b489af253b52839786f94aeeac36f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fa62a97675719c2e8e9bb97361b92ff1c7b9d2ef.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fa85f869a92f0482605e52019828244b12e12b44.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fabdc143c29d5ca50ab1e96a814bda6d05b0d5d2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fac5a0f98b94530befd634891e42c424bb86f0e1.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fac99c3c82b77946f6844699d2333cd532a78a26.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_faf56e45b2240515e97fc1bfd552eb03b6de5094.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_faf686067fa433cea5e95dd523846dc881eff635.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb2fbb135d59028afcf867c2cf08edc323565528.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb4c15452f9155c5966990f09432e5eb7e28e785.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb4c5f8fecfbbe16e6648becb3b5ca89fa3d8a94.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb5bb49928ce5515d7b297d5eadd4ec70a22d60b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb79e1f9231692d736dbada062ed6821f34927bf.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fb9477a613665cebcad781389ba7c5a36f51efe2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fba36678d5047ded97ee7a7ba9feb9569afdb6ea.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fba47fa8d9b5375bc408af68b67345ab9dba2eb8.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fbea85b766bf0c918ee0baf24dffc6a5563d5105.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fbeec221cd63adaedceec39db41ea942f99f5133.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc030b61ae20c4b7d9b2d10930a17e01e9e93328.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc1790325b59bd44b0a5f6cf9723a25fd845cba7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc1eb85a00017efdc610e4259d2abe935b85304f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc5841a729099340d608e31023acbeaeade3e886.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc5ebf0f2200f37ccc0849e0c3745f6e2f00111d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc7b0916744b593435d8e1e7b6d874d760cd5e3b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fc86c13e933cba40553ffba31d53aad27415ce4b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fcb0b08e29b2e1bf181fceceb9dc416e54f52b00.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fcb6ef39c3db49f26f736d6c9221dd825409ec4e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fcbe827108d252b2f5847fa8e132c9c3e56a90a0.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fccabea88b8e290688c1b360875d228e6fdf1624.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd10a3b937e9659716925e39a01d794914b08e26.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd19d7614f2ed5da21a52ed172ef62cc07c9c01a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd26e43ca652e6f58ff48c356165aa4349833b55.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd345632e0cae0d549ba79626a08b1885711deb6.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd3558b4c7a667dbc365c4c2ceda646975408f51.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd614df484b263deae3b3c20adb0ce7b62eaa651.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fd9cd1305633b62b68fb8474ce021f639f8492e7.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fde12cd366d6850ce26afce98e5076b695b4875b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe245e9ea974adce2b9807d33b9ba12d916eaffb.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe72cdd69944d2d765478d4aed13066a02b76f6d.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe8b8c3525fe86a20a2d6c69585f3e36c16caabd.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe97b7adcd67ed9bda8831d1f3f1ca7590c6d251.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fe9d98dbec5096a89b116f85675af772f023014a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_feb5e77111fe1e20bafdb83a925b5faeeb6214af.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fecd7501265b4c4dcf015485e63e2324304f70d3.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fecffa403b3631b1957e1a9a06f18fdb3b4eee5f.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ff453e3bdc9752cb7b81f7cc3056325a8b9a8ad4.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ff6862dbdbb20bc63a650e1f93e9ac169bb702b2.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffb5b7349a671b182d73c8016590f26fe06a4cba.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffb8adef0cef91a86f36872407fea35df90e8f2b.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffc6056d9fe125a4dbe08c1d86354e51f7daadd5.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_ffd868d49abdb769ab82c21508d655daf54b8a99.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fff7aa57cca501f221077124359a589b3a6f9d0a.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_ck_autogen_fffbfcac254e33926131a71905e93f9cc0aef89e.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/fmha_fwd.hpp aten/src/ATen/native/transformers/hip/flash_attn/flash_api.h third_party/composable_kernel,https://github.com/pytorch/pytorch/pull/144799,alugorey,jianyuh,,,
dd2a943e145,inductor,not user facing,Fix the AOTI compile failure with ARM CPU for Meta internal (#147204),torch/_inductor/cpu_vec_isa.py,https://github.com/pytorch/pytorch/pull/147204,hl475,houseroad,,,
6f7e67c43c1,skip,Untopiced,Add torch._scaled_mm for CPU (#139975),aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/_inductor/codegen/cpp_prefix.h torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/139975,yanbing-j,jgong5,malfet,mingfeima,
c6b331f7d9c,dynamo,not user facing,Deprecate `skip_code_recursive_on_cache_limit_hit` config flag (#136970),torch/_dynamo/config.py torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/136970,zeshengzong,williamwen42,,,
84abeaad5c1,fx,Untopiced,[export] Log evaluate_expr (#146939),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv test/export/test_draft_export.py torch/fx/experimental/sym_node.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/146939,angelayi,oulgen,,,
57060bebf35,fx,Untopiced,[symbolic shapes] Add replacement for backed symints (#147240),test/export/test_draft_export.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/147240,angelayi,pianpwk,,,
302f56a1f2d,skip,Untopiced,"Revert ""Fix non-bitwise type annotations for Tensor operators (see #145838) (#146845)""",test/typing/pass/arithmetic_ops.py torch/_decomp/decompositions.py torch/_prims/rng_prims.py torch/_tensor.py torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.py torch/distributions/exp_family.py torch/optim/adam.py torch/optim/nadam.py torch/optim/swa_utils.py,,,,,,
d9b3d76b85f,cuda,Untopiced,Fix linter warnings (#147386),aten/src/ATen/native/cuda/SoftMax.cu,https://github.com/pytorch/pytorch/pull/147386,ahmadsharif1,Skylion007,ngimel,,
74682e85953,quantization,not user facing,Fix typo (#147330),torch/ao/nn/quantizable/modules/activation.py,https://github.com/pytorch/pytorch/pull/147330,12v,Skylion007,srinivasreddy,,
166419b9c1a,dynamo,not user facing,dynamo: Don't crash when encountering a object with no __name__ (#147246),test/dynamo/test_compile.py test/dynamo/test_trace_rules.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/147246,c00w,Skylion007,anijain2305,jansel,
1b047d5d7a3,skip,not user facing,Add link to non_blocking/pinmem tutorial in `Tensor.to` docstrings (#145651),torch/_tensor_docs.py,https://github.com/pytorch/pytorch/pull/145651,vmoens,svekars,,,
eb892cd7687,inductor,not user facing,[codegen] enable SORT and TUPLE_REDUCTION for AMD Triton (#147340),test/inductor/test_torchinductor_dynamic_shapes.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/147340,chenyang78,Skylion007,eellison,jansel,
75db0fd8a0b,dynamo,not user facing,"[dynamo] refactor dynamo__custom_eval_frame to C++, refactor SKIP_CODE[_RECURSIVE] (#145603)",build_variables.bzl torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/eval_frame.h torch/csrc/dynamo/eval_frame_cpp.cpp torch/csrc/dynamo/eval_frame_cpp.h torch/csrc/dynamo/extra_state.cpp torch/csrc/dynamo/extra_state.h torch/csrc/dynamo/framelocals_mapping.cpp torch/csrc/dynamo/framelocals_mapping.h,https://github.com/pytorch/pytorch/pull/145603,williamwen42,anijain2305,jansel,,
63e8ad49b86,dynamo,not user facing,[dynamo] replace hardcoded eval frame control flags skip_code_recursive_flag/cache_limit_hit_flag (#146355),test/dynamo/test_frame_init.py torch/_C/_dynamo/eval_frame.pyi torch/_dynamo/convert_frame.py torch/_dynamo/testing.py torch/_dynamo/types.py torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/eval_frame.h torch/csrc/dynamo/eval_frame_cpp.cpp torch/csrc/dynamo/init.cpp,https://github.com/pytorch/pytorch/pull/146355,williamwen42,jansel,,,
3f35664ee8b,inductor,not user facing,More precise check for shared storage check in inductor/reinplace pass (#147050),test/inductor/test_auto_functionalize.py torch/_inductor/fx_passes/reinplace.py,https://github.com/pytorch/pytorch/pull/147050,laithsakka,zou3519,,,
7622e29a374,skip,Untopiced,"Revert ""Nccl update to 2.25.1 for cuda 12.4-12.8  (#146073)""",.ci/docker/ci_commit_pins/nccl-cu11.txt .ci/docker/ci_commit_pins/nccl-cu12.txt .ci/docker/common/install_base.sh .ci/docker/common/install_cuda.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/build-manywheel-images.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml .gitignore .gitmodules cmake/External/nccl.cmake setup.py test/dynamo/test_repros.py third_party/nccl/nccl tools/build_pytorch_libs.py,,,,,,
a21a123fd57,distributed,not user facing,Add fqn_modifier at loading_state_dict and unit test (#146557),test/distributed/checkpoint/test_state_dict.py torch/distributed/checkpoint/state_dict.py torch/testing/_internal/distributed/common_state_dict.py,https://github.com/pytorch/pytorch/pull/146557,mori360,fegin,,,
c8433c2c6c9,cuda,docs,"[BE] correct docs for clock_rate to MHz, fixes #147098 (#147393)",torch/cuda/__init__.py,https://github.com/pytorch/pytorch/pull/147393,janeyx99,andrewor14,,,
c9a15d980f2,distributed,not user facing,[FSDP2] Simplify shard_placement_fn in test (#146847),test/distributed/_composable/fsdp/test_fully_shard_training.py,https://github.com/pytorch/pytorch/pull/146847,tsunghsienlee,awgu,,,
ca397d82a67,inductor,not user facing,[Sigmoid] Fix issues with constant folding and fba_ops (#146948),torch/_inductor/constant_folding.py,https://github.com/pytorch/pytorch/pull/146948,trieuat,zhxchen17,,,
bae049b439c,python_frontend,not user facing,Update addr doc (#146482),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/146482,albanD,janeyx99,,,
5d547d82e6c,fx,not user facing,Add no_data_dependent_graph_break mode (#147342),test/dynamo/test_misc.py torch/_dynamo/variables/tensor.py torch/fx/experimental/_config.py,https://github.com/pytorch/pytorch/pull/147342,bobrenjc93,laithsakka,,,
525ca80f53a,fx,not user facing,add unbacked strict mode (#147333),test/dynamo/test_misc.py torch/_dynamo/decorators.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/147333,bobrenjc93,laithsakka,,,
babb2dc2afd,skip,Untopiced,"Revert ""Add torch._scaled_mm for CPU (#139975)""",aten/src/ATen/native/Blas.cpp aten/src/ATen/native/mkldnn/Linear.cpp aten/src/ATen/native/mkldnn/Linear.h aten/src/ATen/native/mkldnn/MKLDNNCommon.cpp aten/src/ATen/native/native_functions.yaml test/inductor/test_fp8.py test/test_matmul_cuda.py test/test_matmul_fp8.py torch/_inductor/codegen/cpp_prefix.h torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/testing/_internal/common_device_type.py torch/testing/_internal/common_methods_invocations.py,,,,,,
959d79f85f6,onnx,not user facing,[ONNX] Move and improve error reproduction logic in test (#147391),test/onnx/error_reproduction.py test/onnx/torchlib/error_reproduction.py,https://github.com/pytorch/pytorch/pull/147391,justinchuby,titaiwangms,,,
757d7f28d15,releng,not user facing,[CD] Increase timeout for windows binary builds (#147390),.github/templates/common.yml.j2 .github/templates/windows_binary_build_workflow.yml.j2 .github/workflows/generated-windows-binary-libtorch-debug-main.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-main.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/147390,atalman,huydhn,jeanschmidt,malfet,
953f7834cc5,onnx,improvements,[ONNX] Pick up missing types in dynamic shapes renaming (#147407),test/onnx/exporter/test_dynamic_shapes.py test/onnx/exporter/test_small_models_e2e.py torch/onnx/_internal/exporter/_dynamic_shapes.py torch/onnx/_internal/exporter/_ir_passes.py,https://github.com/pytorch/pytorch/pull/147407,titaiwangms,justinchuby,,,
f16d30137c0,distributed,not user facing,[OSS] Update FileSystem methods to properly handle a string argument (#145751),torch/distributed/checkpoint/filesystem.py,https://github.com/pytorch/pytorch/pull/145751,ankitageorge,pradeepfn,,,
5006932cbc7,skip,not user facing,[cutlass backend] forward fix of standalone runner for fbcode (#147158),test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/147158,henrylhtsang,chenyang78,,,
bd370c138a9,quantization,not user facing,fix pt2e block wise quantization unit test (#147406),test/quantization/pt2e/test_quantize_pt2e.py,https://github.com/pytorch/pytorch/pull/147406,cccclai,andrewor14,jerryzh168,,
4ece056791d,skip,not user facing,Nccl update to 2.25.1 for cuda 12.4-12.8  (#146073),.ci/docker/ci_commit_pins/nccl-cu11.txt .ci/docker/ci_commit_pins/nccl-cu12.txt .ci/docker/common/install_base.sh .ci/docker/common/install_cuda.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/build-manywheel-images.yml .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml .gitignore .gitmodules cmake/External/nccl.cmake setup.py test/dynamo/test_repros.py third_party/nccl/nccl tools/build_pytorch_libs.py,https://github.com/pytorch/pytorch/pull/146073,atalman,Skylion007,fduwjj,kwen2501,
465930ee815,inductor,not user facing,"Revert ""[ROCm] ROCm-specific gemm tuning parameters"" (#147388)",torch/_inductor/choices.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_plus_mm.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/kernel/unpack_mixed_mm.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/template_heuristics.py,https://github.com/pytorch/pytorch/pull/147388,jansel,yanboliang,,,
2c3680ce383,export,Untopiced,[apf] Fix input adapter (#147238),torch/export/unflatten.py,https://github.com/pytorch/pytorch/pull/147238,angelayi,pianpwk,,,
454fbd5bbe5,skip,not user facing,realize stride symbols in estimate_runtime (#146752),test/test_dynamic_shapes.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/146752,laithsakka,bdhirsh,bobrenjc93,,
655b061ef07,inductor,not user facing,[inductor] Freeze runtime asserts after shape prop but before codegen (#147331),torch/_inductor/compile_fx.py torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/147331,bobrenjc93,eellison,,,
81eb2a78ad9,inductor,Untopiced,[Inductor] Add autotuning artifact logging (#147222),test/dynamo/test_logging.py torch/_inductor/autotune_process.py torch/_inductor/codegen/cuda/cuda_template.py torch/_logging/_internal.py torch/_logging/_registrations.py,https://github.com/pytorch/pytorch/pull/147222,mlazos,eellison,henrylhtsang,,
e9b3ff05705,inductor,Untopiced,"[Cutlass] Add support for runtime param choices, starting with swizzle (#147223)",torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/147223,mlazos,Chillee,eellison,,
77dbd285359,inductor,Untopiced,[Cutlass] Restore search space for swizzle (#147224),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/147224,mlazos,eellison,,,
303ad1916fb,inductor,not user facing,[FlexAttention] Fix weird generate stride call in flex decode (#147435),torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/147435,drisspg,BoyuanFeng,,,
ed83b0b70be,distributed,Untopiced,[ddp] decouple python reducer from compilation mode (#147123),test/distributed/_composable/test_replicate_with_compiler.py torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/utils.py torch/nn/parallel/distributed.py,https://github.com/pytorch/pytorch/pull/147123,xmfan,fegin,,,
8cbf7d0d6e3,skip,not user facing,[Inductor UT][XPU] Skip fft_c2c case since it's not implemented on XPU. (#147351),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/147351,etaf,jansel,,,
e6c86952c67,releng,not user facing,Add CUDA 12.8 windows nightly build (#147037),.github/scripts/generate_binary_build_matrix.py .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/147037,tinglvv,atalman,,,
5220d402b55,skip,not user facing,[ROCm] TopK optimizations for AMD GPUs (#146387),aten/src/ATen/native/cuda/TensorTopK.cpp aten/src/ATen/native/cuda/TensorTopK.cu,https://github.com/pytorch/pytorch/pull/146387,apakbin,malfet,ngimel,,
9fee408daa9,caffe2,Untopiced,[caffe2] disable warning for unused arguments (#147411),aten/src/ATen/native/quantized/cpu/qnnpack/buckbuild.bzl,https://github.com/pytorch/pytorch/pull/147411,rmaz,kimishpatel,,,
5a3a50c7910,releng,Untopiced,Update Arm Compute Library (ACL) to v25.02 (#147454),.ci/aarch64_linux/aarch64_wheel_ci_build.py .ci/aarch64_linux/build_aarch64_wheel.py .ci/docker/common/install_acl.sh,https://github.com/pytorch/pytorch/pull/147454,fadara01,malfet,,,
4f3c070b254,inductor,not user facing,[inductor] GraphLowering code movement (#147335),torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/147335,bobrenjc93,eellison,,,
279c7f262ea,onnx,not user facing,[ONNX] Refactor dispatcher and registry (#147396),torch/onnx/_internal/exporter/_dispatching.py torch/onnx/_internal/exporter/_ir_passes.py torch/onnx/_internal/exporter/_registration.py torch/onnx/_internal/exporter/_torchlib/__init__.py torch/onnx/_internal/exporter/_torchlib/_torchlib_registry.py torch/onnx/_internal/exporter/_torchlib/ops/hop.py,https://github.com/pytorch/pytorch/pull/147396,justinchuby,titaiwangms,,,
e758d8b4d16,skip,not user facing,[Inductor][Triton] Rework casting logic to avoid illegal bitcast (#147395),torch/_inductor/runtime/triton_helpers.py,https://github.com/pytorch/pytorch/pull/147395,alexbaden,eellison,,,
8bea08e5bc8,skip,not user facing,[BE] Fix tensor stub (#147384),tools/pyi/gen_pyi.py,https://github.com/pytorch/pytorch/pull/147384,vmoens,albanD,atalman,janeyx99,
394676759d3,releng,not user facing,ci: Add h100 nightly perf testing (#146868),.ci/pytorch/test.sh .github/workflows/inductor-perf-test-nightly-h100.yml .github/workflows/upload-torch-dynamo-perf-stats.yml,https://github.com/pytorch/pytorch/pull/146868,seemethere,eellison,huydhn,,
24738768a89,distributed,Untopiced,more dist ops in non strict (#147417),test/export/test_export.py torch/_export/non_strict_utils.py torch/distributed/_functional_collectives.py torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/147417,avikchaudhuri,angelayi,,,
41ae15faa3d,onnx,not user facing,[ONNX] Add scaffolding for onnx decomp and logic for op tests (#147392),test/onnx/torchlib/README.md test/onnx/torchlib/ops_test_common.py test/onnx/torchlib/ops_test_data.py test/onnx/torchlib/test_ops.py torch/onnx/_internal/exporter/_torchlib/_tensor_typing.py torch/onnx/_internal/exporter/_torchlib/ops/__init__.py torch/onnx/_internal/exporter/_torchlib/ops/core.py,https://github.com/pytorch/pytorch/pull/147392,justinchuby,titaiwangms,,,
fb55bac3de7,distributed,not user facing,[fr][fix] Split MatchState and dynamic info for fr analysis downstream (#147439),test/distributed/flight_recorder/test_fr_analysis.py tools/flight_recorder/components/builder.py tools/flight_recorder/components/types.py tools/flight_recorder/components/utils.py,https://github.com/pytorch/pytorch/pull/147439,fduwjj,fegin,,,
f63db6255fd,export,Untopiced,Re-land exclude upsample_bilinear2d.vec and nearest2d.vec from default export decomposition table (#147153),test/export/test_export.py torch/export/decomp_utils.py,https://github.com/pytorch/pytorch/pull/147153,GregoryComer,manuelcandales,,,
48203bec636,inductor,not user facing,"[BE] remove  sysconfig.get_config_var(""LIBDIR"") from cuda lib paths (#147409)",torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/147409,henrylhtsang,chenyang78,,,
004d65aeb04,inductor,not user facing,Add type hints to cuda kernel (#147471),torch/_inductor/codegen/cuda/cuda_kernel.py,https://github.com/pytorch/pytorch/pull/147471,mlazos,eellison,,,
a88d7d42683,skip,not user facing,[util] fetch logical count cpu (#147413),tools/stats/monitor.py,https://github.com/pytorch/pytorch/pull/147413,yangw-dev,clee2000,,,
3986c3e4a64,inductor,not user facing,[reland][cutlass backend] Do not change dtype of GEMM template for cutlass 3x (#147434),torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/147434,henrylhtsang,ColinPeppler,,,
1e94c7aaa46,export,Untopiced,[draft_export] only clear pending unbacked symbols for overwritten kernels (#147427),test/export/test_draft_export.py torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/147427,pianpwk,angelayi,,,
16e202a38e1,dynamo,Untopiced,[dynamo] improved graph break messages for some common graph break sites [1/N] (#146525),.flake8 test/dynamo/test_activation_checkpointing.py test/dynamo/test_autograd_function.py test/dynamo/test_decorators.py test/dynamo/test_exc.py test/dynamo/test_graph_break_messages.py test/dynamo/test_hooks.py test/dynamo/test_misc.py test/test_custom_ops.py torch/_dynamo/decorators.py torch/_dynamo/exc.py torch/_dynamo/output_graph.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/146525,williamwen42,jansel,,,
93316cfe945,inductor,not user facing,Move ir_pre_fusion.txt and ir_post_fusion.txt to TORCH_LOGS (#147248),test/dynamo/test_logging.py test/inductor/test_debug_trace.py torch/_inductor/debug.py torch/_logging/_internal.py torch/_logging/_registrations.py torch/testing/_internal/logging_utils.py,https://github.com/pytorch/pytorch/pull/147248,dulinriley,eellison,,,
f79b352f5a4,inductor,Untopiced,[Intel GPU] qconv_pointwise.binary XPU support (#135189),aten/src/ATen/native/mkldnn/xpu/detail/Attr.h aten/src/ATen/native/mkldnn/xpu/detail/QConv.cpp aten/src/ATen/native/mkldnn/xpu/detail/QMatmul.cpp aten/src/ATen/native/mkldnn/xpu/qconv.cpp test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py torch/ao/quantization/quantizer/xpu_inductor_quantizer.py,https://github.com/pytorch/pytorch/pull/135189,ZhiweiYan-96,EikanWang,guangyey,jerryzh168,
fea718f062b,dynamo,Untopiced,"[BaseHOP] change hop(subgraph, operands) to hop(subgraph, *operands) (#146730)",test/dynamo/test_base_hop.py test/higher_order_ops/test_invoke_quant.py test/inductor/test_compiled_optimizers.py test/inductor/test_foreach.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/_invoke_quant.py torch/_higher_order_ops/base_hop.py torch/_higher_order_ops/foreach_map.py torch/_inductor/fx_passes/joint_graph.py torch/_inductor/lowering.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/146730,zou3519,mlazos,ydwu4,,
de1cb0f351e,distributed,Untopiced,capture the return value in the contract typing (#147488),torch/distributed/_composable/contract.py,https://github.com/pytorch/pytorch/pull/147488,xunnanxu,Skylion007,,,
0b0da81021e,dynamo,Untopiced,Support static method of torchbind attributes in torch.compile with inductor backend (#146927),test/inductor/test_torchbind.py torch/_dynamo/repro/after_aot.py torch/_higher_order_ops/effects.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/wrapper.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/146927,yushangdi,angelayi,,,
0d56b7e665e,fx,not user facing,Support size oblivious max equation (#147344),test/dynamo/test_misc.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/147344,bobrenjc93,angelayi,,,
5f5b44f6bf3,releng,not user facing,[ROCm] Update inductor-periodic.yml to use the correct label (#147473),.github/workflows/inductor-periodic.yml,https://github.com/pytorch/pytorch/pull/147473,amdfaa,jeffdaily,,,
7185ca8348c,inductor,not user facing,[Cutlass] Add test verifying number of precompiles (#147477),test/inductor/test_cutlass_backend.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/147477,mlazos,henrylhtsang,,,
77aa6028715,export,Untopiced,[torchbind] Differentiate ScriptModule and ScriptObject with qualified name (#147399),test/export/test_export.py torch/_export/non_strict_utils.py torch/_higher_order_ops/torchbind.py torch/_library/fake_class_registry.py,https://github.com/pytorch/pytorch/pull/147399,ydwu4,yushangdi,,,
8f6b9403c10,releng,not user facing,[audio hash update] update the pinned audio hash (#147423),.github/ci_commit_pins/audio.txt,https://github.com/pytorch/pytorch/pull/147423,pytorchupdatebot,pytorchbot,,,
76ad19a5492,dynamo,not user facing,[dynamo][codegen] Implement CSE for pre-graph graph-arg bytecode reconstruction (#147425),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_dynamo/codegen.py torch/_dynamo/source.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/147425,anijain2305,StrongerXi,jansel,,
db4ce78d46c,quantization,not user facing,PEP585: More UP006 fixes (#146392),test/distributed/_composable/fsdp/test_fully_shard_ignore_params.py test/distributed/test_c10d_functional_native.py test/functorch/test_ac_logging.py test/onnx/test_onnxscript_runtime.py test/onnx/torchlib/error_reproduction.py torch/_C/_monitor.pyi torch/_decomp/__init__.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/utils.py torch/_dynamo/variables/base.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/functions.py torch/_export/__init__.py torch/_functorch/_activation_checkpointing/ac_logging_utils.py torch/_functorch/_aot_autograd/subclass_parametrization.py torch/_higher_order_ops/aoti_call_delegate.py torch/_higher_order_ops/utils.py torch/_inductor/__init__.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/simd_kernel_features.py torch/_inductor/compile_fx.py torch/_inductor/cudagraph_trees.py torch/_inductor/dependencies.py torch/_inductor/freezing_utils.py torch/_inductor/fuzzer.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/ir.py torch/_inductor/kernel/flex_attention.py torch/_inductor/metrics.py torch/_inductor/mkldnn_ir.py torch/_inductor/output_code.py torch/_inductor/scheduler.py torch/_inductor/utils.py torch/_inductor/wrapper_benchmark.py torch/_jit_internal.py torch/_prims/__init__.py torch/_prims_common/__init__.py torch/_refs/__init__.py torch/_refs/linalg/__init__.py torch/_refs/nn/__init__.py torch/_subclasses/functional_tensor.py torch/_subclasses/meta_utils.py torch/ao/quantization/__init__.py torch/autograd/__init__.py torch/backends/quantized/__init__.py torch/compiler/__init__.py torch/cuda/__init__.py torch/distributed/_composable/checkpoint_activation.py torch/distributed/_serialization.py torch/distributed/_shard/sharded_optim/__init__.py torch/distributed/_shard/sharded_tensor/__init__.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/checkpoint/planner.py torch/distributed/elastic/events/__init__.py torch/distributed/elastic/multiprocessing/__init__.py torch/distributed/elastic/multiprocessing/errors/__init__.py torch/distributed/pipelining/_utils.py torch/distributed/pipelining/schedules.py torch/distributed/rpc/__init__.py torch/export/__init__.py torch/export/passes/__init__.py torch/futures/__init__.py torch/fx/_graph_pickler.py torch/fx/experimental/sym_node.py torch/fx/graph.py torch/jit/frontend.py torch/mtia/__init__.py torch/nested/__init__.py torch/nn/attention/__init__.py torch/onnx/_internal/exporter/_dynamic_shapes.py torch/serialization.py torch/sparse/__init__.py torch/testing/_internal/distributed/common_state_dict.py torch/testing/_internal/opinfo/definitions/__init__.py torch/utils/data/dataset.py torch/utils/model_dump/__init__.py torch/xpu/__init__.py,https://github.com/pytorch/pytorch/pull/146392,aorenste,Skylion007,albanD,justinchuby,
a000c7e6d2f,skip,not user facing,Add hint message for `pack_padded_sequence` (#146747),torch/nn/utils/rnn.py,https://github.com/pytorch/pytorch/pull/146747,zeshengzong,mikaylagawarecki,,,
452315c84f8,inductor,not user facing,Fix RuntimeError: value cannot be converted to type int64_t without overflow (#147492),torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/147492,drisspg,eellison,,,
6a72aaadae5,python_frontend,Untopiced,"Fix `torch.max` optional args `dim`, `keepdim` description (#147177)",torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/147177,zeshengzong,colesbury,,,
9da250aadae,distributed,Untopiced,type `fully_shard` so that the return value can be chained with typing enabled (#147489),test/distributed/_composable/fsdp/test_fully_shard_init.py torch/distributed/_composable/contract.py torch/distributed/fsdp/_fully_shard/_fully_shard.py torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/147489,xunnanxu,Skylion007,,,
f9b8121350d,inductor,not user facing,Make Inductor scheduler aware of _scaled_mm (#146992),torch/_inductor/ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/146992,lw,drisspg,eellison,shunting314,
6beba8dcced,inductor,not user facing,Optimize `graph.py` typing (#147099),torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/147099,zeshengzong,aorenste,cyyever,,
5116b27792d,releng,Untopiced,Add cifllow/riscv64 label,.github/pytorch-probot.yml,,,,,,
ead970c8d03,skip,Untopiced,"Revert ""Add cifllow/riscv64 label""",.github/pytorch-probot.yml,,,,,,
574371d828e,inductor,not user facing,Add current cuda device index to FXGraphCache key (#147464),test/dynamo/test_aot_autograd_cache.py test/inductor/test_codecache.py torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/147464,jamesjwu,anijain2305,bdhirsh,,
382fbcc1e43,quantization,Untopiced,add the `torch.float8_e8m0fnu` dtype to PyTorch (#147466),aten/src/ATen/DLConvertor.cpp aten/src/ATen/Dispatch_v2.h aten/src/ATen/native/Copy.cpp aten/src/ATen/native/TensorCompare.cpp aten/src/ATen/native/cpu/CopyKernel.cpp aten/src/ATen/native/cpu/FillKernel.cpp aten/src/ATen/native/cpu/IndexKernel.cpp aten/src/ATen/native/cuda/Copy.cu aten/src/ATen/native/cuda/Indexing.cu aten/src/ATen/native/cuda/jit_utils.h c10/core/Scalar.h c10/core/ScalarType.cpp c10/core/ScalarType.h c10/util/Float8_e8m0fnu-inl.h c10/util/Float8_e8m0fnu.cpp c10/util/Float8_e8m0fnu.h c10/util/TypeCast.h test/quantization/core/experimental/test_float8.py tools/pyi/gen_pyi.py torch/_tensor_str.py torch/csrc/TypeInfo.cpp torch/csrc/utils/python_scalars.h torch/storage.py torchgen/api/types/types.py torchgen/model.py,https://github.com/pytorch/pytorch/pull/147466,vkuzo,drisspg,,,
83bb921a5ad,inductor,not user facing,[ROCm] Update meta_registration for efficient attention (#146979),test/inductor/test_fused_attention.py test/inductor/test_torchinductor.py torch/_inductor/fx_passes/fuse_attention.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/146979,AmdSampsa,shunting314,,,
863ac20659a,skip,not user facing,[CI] Do not overwrite return code of test file when fails for rerun disabled tests (#147484),torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/147484,clee2000,ZainRizvi,,,
6971b775104,skip,not user facing,[CPU Stream] Add noop for CPU stream record_event() and wait_event() (#145935),torch/cpu/__init__.py,https://github.com/pytorch/pytorch/pull/145935,jvandebon,Skylion007,,,
fb1f7f6a095,caffe2,not user facing,[codemod] Fix unused-value issue in caffe2/aten/src/ATen/native/miopen/Conv_miopen.cpp +1 (#147496),aten/src/ATen/native/miopen/Conv_miopen.cpp,https://github.com/pytorch/pytorch/pull/147496,r-barnes,Skylion007,,,
2565951f8a6,skip,not user facing,[cutlass backend] remove triton from most tests and add an integration test (#147169),test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/147169,henrylhtsang,ColinPeppler,,,
d068141c3b2,skip,not user facing,[cutlass backend] add subproc tests (#147173),test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/147173,henrylhtsang,ColinPeppler,,,
af316403915,skip,not user facing,[cutlass backend] enable mixed mm test (cutlass2x) for H100 (#147474),test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/147474,henrylhtsang,ColinPeppler,alexsamardzic,,
be0df96b504,skip,not user facing,Fix c++ implementation of strip_function_call (#147436),torch/csrc/dynamo/init.cpp,https://github.com/pytorch/pytorch/pull/147436,aorenste,jansel,,,
87e6e2924eb,skip,not user facing,Increase memory for linux binary builds  (#147542),.github/actionlint.yaml .github/workflows/_binary-build-linux.yml,https://github.com/pytorch/pytorch/pull/147542,jeanschmidt,ZainRizvi,atalman,,
fdb1305ace9,export,Untopiced,"reland ""[sigmoid] Test OSS model runner with test_export.py"" (#147535)",test/export/test_export.py test/export/testing.py torch/export/exported_program.py,https://github.com/pytorch/pytorch/pull/147535,zhxchen17,yiming0416,,,
4b35139a462,skip,not user facing,[ROCm][TunableOp] Fix TunableOp warmup environment variable. (#147412),aten/src/ATen/cuda/tunable/TunableOp.h,https://github.com/pytorch/pytorch/pull/147412,naromero77amd,jeffdaily,,,
ac88a6c00d1,fx,Untopiced,[fx] demote node prepend to self log from warning to debug (#147538),torch/fx/node.py,https://github.com/pytorch/pytorch/pull/147538,xmfan,yanboliang,,,
8b818ab58f6,nn_frontend,Untopiced,Use float data type for Half sum in fallback implementation of batchnorm backward on CPU (#147353),aten/src/ATen/native/Normalization.cpp test/test_nn.py,https://github.com/pytorch/pytorch/pull/147353,CaoE,cpuhrsch,leslie-fang-intel,,
8a5265cb37e,quantization,not user facing,[Intel GPU] qlinear_pointwise.binary[_tensor] XPU support (#135337),aten/src/ATen/native/mkldnn/xpu/qlinear.cpp test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py torch/_meta_registrations.py torch/ao/quantization/quantizer/xpu_inductor_quantizer.py,https://github.com/pytorch/pytorch/pull/135337,ZhiweiYan-96,EikanWang,guangyey,jerryzh168,
ba214ab56c5,distributed,Untopiced,TCPStore: soft fail bind when agent store active (#147465),test/distributed/elastic/utils/distributed_test.py test/distributed/test_store.py torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp,https://github.com/pytorch/pytorch/pull/147465,d4l3k,fduwjj,,,
fe100c3c5bc,releng,not user facing,Add libtorch nightly build for CUDA 12.8  (#146265),.ci/manywheel/build_cuda.sh .github/scripts/generate_binary_build_matrix.py .github/templates/linux_binary_build_workflow.yml.j2 .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/146265,tinglvv,atalman,,,
c615b8c174c,skip,not user facing,Build a storage reader/writer to write checkpoints in HF format (#146352),test/distributed/checkpoint/test_hf_storage.py torch/distributed/checkpoint/__init__.py torch/distributed/checkpoint/_fsspec_filesystem.py torch/distributed/checkpoint/_hf_storage.py torch/distributed/checkpoint/filesystem.py,https://github.com/pytorch/pytorch/pull/146352,ankitageorge,saumishr,,,
a2c3a2c5c44,python_frontend,Untopiced,Support serialization for uintx/intx in weights_only (#147500),test/test_serialization.py torch/_weights_only_unpickler.py,https://github.com/pytorch/pytorch/pull/147500,jerryzh168,mikaylagawarecki,,,
533b884870a,skip,not user facing,[cuDNN][SDPA][Nested Tensor] Experimental cuDNN Nested Tensor SDPA Support (forward only) (#141178),aten/src/ATen/cuda/detail/UnpackRaw.cuh aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorTransformerUtils.h aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py test/test_transformers.py torch/nested/_internal/sdpa.py,https://github.com/pytorch/pytorch/pull/141178,eqy,jbschlosser,,,
86ae672b6af,inductor,not user facing,Use has_triton_package in _inductor.runtime.hints (#147442),torch/_inductor/runtime/hints.py,https://github.com/pytorch/pytorch/pull/147442,dilililiwhy,Skylion007,,,
c74b59fc1f4,linalg_frontend,not user facing,[ROCm][TunableOp] resolve the rocBLAS version dynamically (#147363),aten/src/ATen/cuda/tunable/Tunable.cpp test/test_linalg.py,https://github.com/pytorch/pytorch/pull/147363,apakbin,jeffdaily,naromero77amd,pruthvistony,
4986f0f52eb,jit,Untopiced,[PT2]: allow empty dict to pass type check (#147167) (#147480),aten/src/ATen/core/function_schema_inl.h torch/csrc/jit/runtime/static/impl.cpp,https://github.com/pytorch/pytorch/pull/147480,kqfu,henryoier,jeanschmidt,,
e5da9df421f,skip,Untopiced,"Revert ""Increase memory for linux binary builds  (#147542)""",.github/actionlint.yaml .github/workflows/_binary-build-linux.yml,,,,,,
3395da7f7cb,skip,Untopiced,"Revert ""Build a storage reader/writer to write checkpoints in HF format (#146352)""",test/distributed/checkpoint/test_hf_storage.py torch/distributed/checkpoint/__init__.py torch/distributed/checkpoint/_fsspec_filesystem.py torch/distributed/checkpoint/_hf_storage.py torch/distributed/checkpoint/filesystem.py,,,,,,
b61a5564271,jit,Untopiced,Turn onnx functions into static (#147598),torch/csrc/jit/serialization/export.cpp,https://github.com/pytorch/pytorch/pull/147598,cyyever,justinchuby,,,
cffe7183f1a,inductor,not user facing,[cutlass backend] Fix standalone runner test after swizzle became a runtime parameter (#147554),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/147554,henrylhtsang,mlazos,,,
e7bf490c430,skip,not user facing,[ROCm] Implemented dropout usage for RNN with MIOpen backend (#144572),aten/src/ATen/miopen/Descriptors.h aten/src/ATen/native/miopen/RNN_miopen.cpp,https://github.com/pytorch/pytorch/pull/144572,iupaikov-amd,,,,
71d2827eeb4,distributed,Untopiced,Code Refactoring for getting start and stride from global ranks (#147230),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/Utils.cpp torch/csrc/distributed/c10d/Utils.hpp,https://github.com/pytorch/pytorch/pull/147230,shengfukevin,kwen2501,,,
51748a5d1a5,releng,not user facing,Update OpenBLAS to 0.3.29 (#144857),.ci/docker/common/install_conda.sh .ci/docker/common/install_openblas.sh,https://github.com/pytorch/pytorch/pull/144857,michalowski-arm,malfet,,,
654f2666d93,skip,not user facing,Increase memory for linux binary builds  (#147542),.github/actionlint.yaml .github/workflows/_binary-build-linux.yml,https://github.com/pytorch/pytorch/pull/147542,jeanschmidt,ZainRizvi,atalman,,
7ce4974e50f,dataloader_frontend,not user facing,Fix PEP585 update (#147536),torch/utils/data/dataset.py,https://github.com/pytorch/pytorch/pull/147536,aorenste,jeanschmidt,,,
36c461af953,distributed,Untopiced,Support SymmetricMemory's signaling kernels on sm60 and sm70 (#146308),torch/csrc/distributed/c10d/CUDASymmetricMemory-inl.h torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/CUDASymmetricMemoryOps.cu,https://github.com/pytorch/pytorch/pull/146308,lw,yifuwang,,,
d6bb1d7f0a9,skip,not user facing,Delete Mixed MM Special Casing (#147151),test/inductor/test_pattern_matcher.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/kernel/__init__.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/kernel/unpack_mixed_mm.py,https://github.com/pytorch/pytorch/pull/147151,eellison,cpuhrsch,drisspg,,
af1072ffb6e,skip,not user facing,[Intel GPU] Enable BUILD_GRAPH for xpu_mkldnn (#147608),cmake/Modules/FindMKLDNN.cmake,https://github.com/pytorch/pytorch/pull/147608,DDEle,EikanWang,atalman,,
a8ce4d18460,benchmark,Untopiced,Add cachebench (#147537),benchmarks/dynamo/cachebench.py,https://github.com/pytorch/pytorch/pull/147537,oulgen,jamesjwu,,,
6a6de0e09df,export,Untopiced,better error message (#147532),torch/_export/non_strict_utils.py,https://github.com/pytorch/pytorch/pull/147532,tugsbayasgalan,avikchaudhuri,zou3519,,
784f64bb055,inductor,Untopiced,"[inductor] triton support port-#5512, update cpp wrapper for gpu (#146917)",torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/146917,anmyachev,EikanWang,YUNQIUGUO,davidberard98,
a0c7d960285,profiler,improvements,[Easy] Add Delimeter To Show Where Allocation Addr Begins (#147461),torch/utils/viz/MemoryViz.js,https://github.com/pytorch/pytorch/pull/147461,sraikund16,zdevito,,,
f4e4cfcb91d,caffe2,not user facing,[caffe2] Ignore compiler option when building using clang (#147556),aten/src/ATen/cpu/vec/vec_base.h,https://github.com/pytorch/pytorch/pull/147556,Nicoshev,janeyx99,,,
2190ca7f474,skip,not user facing,Use __qualname__ in add_safe_globals and update Unpickling error raised for Unsupported GLOBAL  (#146815),test/test_serialization.py torch/_weights_only_unpickler.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/146815,hanson-hschang,mikaylagawarecki,,,
0295aabf607,skip,not user facing,[trymerge] Post initial starting merge comment on stacked PRs (#147028),.github/scripts/trymerge.py,https://github.com/pytorch/pytorch/pull/147028,clee2000,ZainRizvi,,,
76ce194b8e8,inductor,not user facing,"For addmm and bmm, check if config.autotune_fallback_to_aten before using aten as a fallback. Also fix bmm cutlass backend  (#147148)",torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py,https://github.com/pytorch/pytorch/pull/147148,henrylhtsang,eellison,,,
b5c3bb6185b,releng,Untopiced,Add continuous run for cachebench (#147546),.ci/pytorch/test.sh .github/workflows/inductor-perf-test-nightly.yml,https://github.com/pytorch/pytorch/pull/147546,oulgen,huydhn,,,
c0ee62573ab,optim,Untopiced,[Easy][optim] Add LBFGS params optional desc (#147579),torch/optim/lbfgs.py,https://github.com/pytorch/pytorch/pull/147579,zeshengzong,janeyx99,,,
fd8ae1aa047,inductor,not user facing,[ROCm] gfx940 and gfx941 cleanup (#147394),aten/src/ATen/Context.cpp aten/src/ATen/native/cuda/Blas.cpp aten/src/ATen/native/cuda/KernelUtils.cuh aten/src/ATen/native/cuda/int4mm.cu torch/_inductor/config.py torch/testing/_internal/common_cuda.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/147394,naromero77amd,jeffdaily,jithunnair-amd,,
5ed1e23e3af,distributed,Untopiced,Fix type stubs for SymmetricMemory (#146310),torch/_C/_distributed_c10d.pyi,https://github.com/pytorch/pytorch/pull/146310,lw,yifuwang,,,
6eb795c9e89,dynamo,not user facing,"[associative_scan] compile backend change to ""eager"" (#146973)",test/export/test_export.py test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/associative_scan.py,https://github.com/pytorch/pytorch/pull/146973,bohnstingl,ydwu4,,,
05e6f15966d,skip,Untopiced,"Revert ""[Inductor][Triton] Rework casting logic to avoid illegal bitcast (#147395)""",torch/_inductor/runtime/triton_helpers.py,,,,,,
ef6b16ea9d2,skip,Untopiced,"Revert ""[trymerge] Post initial starting merge comment on stacked PRs (#147028)""",.github/scripts/trymerge.py,,,,,,
d91be786cbe,inductor,not user facing,[cutlass backend] clear_on_fresh_inductor_cache when generatings cutlass ops (#147586),torch/_inductor/codegen/cuda/cutlass_utils.py,https://github.com/pytorch/pytorch/pull/147586,henrylhtsang,ColinPeppler,chenyang78,,
2fb9416e6fe,inductor,not user facing,[inductor][cpu] Move VNNI weight packing into AMX GEMM kernel for contiguous BMM weights (#146843),aten/src/ATen/cpu/vec/vec_half.h aten/src/ATen/native/cpu/FlashAttentionKernel.cpp test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_bmm_template.py torch/_inductor/codegen/cpp_flex_attention_template.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py,https://github.com/pytorch/pytorch/pull/146843,frost-intel,jansel,jgong5,leslie-fang-intel,
698f6f9faef,export,Untopiced,specify only some dimensions in shapes collection (#147534),test/export/test_export.py torch/export/dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/147534,avikchaudhuri,bobrenjc93,,,
7c8c82cd648,skip,not user facing,[trymerge] Post initial starting merge comment on stacked PRs (#147028),.github/scripts/trymerge.py,https://github.com/pytorch/pytorch/pull/147028,clee2000,ZainRizvi,,,
995b125cdd4,releng,Untopiced,[CI] Build sm89 with more procs experiment (#147487),.ci/pytorch/build.sh .github/workflows/_linux-build.yml .github/workflows/pull.yml,https://github.com/pytorch/pytorch/pull/147487,clee2000,huydhn,,,
b1a81a4a650,releng,not user facing,Don't use '-e' when installing Triton (#147228),.ci/docker/common/install_triton.sh,https://github.com/pytorch/pytorch/pull/147228,jayfurmanek,jeffdaily,pruthvistony,,
f95ab46797e,skip,Untopiced,[ROCm] OCP FP8 Support for new GPUs (#146632),aten/src/ATen/Context.cpp aten/src/ATen/cuda/CUDADataType.h aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp cmake/Dependencies.cmake cmake/public/LoadHIP.cmake test/test_linalg.py test/test_matmul_cuda.py torch/_utils_internal.py torch/testing/_internal/common_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/146632,petrex,jeffdaily,,,
5d26b7108f4,distributed,Untopiced,[PP] Remove extra code and docs BE (#147636),torch/distributed/pipelining/stage.py,https://github.com/pytorch/pytorch/pull/147636,H-Huang,awgu,,,
1c334893dc1,releng,Untopiced,[CacheBench] Refactor code to prepare for mode benchmarks (#147641),.ci/pytorch/test.sh benchmarks/dynamo/cachebench.py,https://github.com/pytorch/pytorch/pull/147641,oulgen,huydhn,,,
6e0b09728a5,export,Untopiced,[export] Remove report from draft-export output (#147558),test/export/test_draft_export.py torch/export/_draft_export.py,https://github.com/pytorch/pytorch/pull/147558,angelayi,pianpwk,,,
84fcf1bb11a,skip,not user facing,constexpr all the things in irange.h (#147633),c10/test/util/irange_test.cpp c10/util/irange.h,https://github.com/pytorch/pytorch/pull/147633,swolchok,albanD,,,
2d433cf1ad5,dynamo,not user facing,[Inductor UT][Windows][XPU] Enable Inductor UT on XPU Windows. (#147347),torch/_dynamo/test_case.py,https://github.com/pytorch/pytorch/pull/147347,etaf,EikanWang,jansel,,
b11d5cd5842,inductor,not user facing,[Inductor UT][Windows][XPU] Fix Inductor UT on XPU Windows. (#146481),test/inductor/test_debug_trace.py test/inductor/test_extension_backend.py test/inductor/test_indexing.py test/inductor/test_torchinductor.py test/inductor/test_triton_kernels.py test/inductor/test_xpu_basic.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/triton_bundler.py,https://github.com/pytorch/pytorch/pull/146481,etaf,EikanWang,jansel,,
fecd3f7ecb2,skip,not user facing,[ROCm] change is_hip_clang() to always return True (#147646),tools/amd_build/build_amd.py,https://github.com/pytorch/pytorch/pull/147646,ethanwee1,jeffdaily,petrex,,
fc095a885c7,skip,not user facing,move _strobelight/example to avoid graph breaks (#147547),test/strobelight/examples/cli_function_profiler_example.py test/strobelight/examples/compile_time_profile_example.py torch/_strobelight/examples/cli_function_profiler_example.py torch/_strobelight/examples/compile_time_profile_example.py,https://github.com/pytorch/pytorch/pull/147547,laithsakka,bobrenjc93,,,
77d27806574,skip,not user facing,Enable strobelight profiling  specific compile frame ids using COMPILE_STROBELIGHT_FRAME_FILTER (#147549),test/strobelight/examples/compile_time_profile_example.py torch/_strobelight/compile_time_profiler.py torch/_utils_internal.py,https://github.com/pytorch/pytorch/pull/147549,laithsakka,bobrenjc93,,,
086d146f6ff,onnx,not user facing,Update ruff linter for PEP585 (#147540),benchmarks/dynamo/common.py benchmarks/dynamo/pr_time_benchmarks/benchmarks/basic_modules_benchmarks.py pyproject.toml test/onnx/torchlib/ops_test_common.py test/onnx/torchlib/test_ops.py test/onnx/verify.py test/test_decomp.py test/test_foreach.py test/test_ops.py torch/_guards.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_ops.py torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/exporter/_torchlib/_torchlib_registry.py torch/onnx/_internal/fx/passes/decomp.py torch/onnx/utils.py torch/serialization.py torch/testing/_internal/logging_utils.py torch/utils/data/dataset.py,https://github.com/pytorch/pytorch/pull/147540,aorenste,Skylion007,justinchuby,,
e1bf892d900,distributed,Untopiced,[DDP] Temporarily disable comm mem (#147663),torch/csrc/distributed/c10d/reducer.cpp,https://github.com/pytorch/pytorch/pull/147663,kwen2501,d4l3k,,,
3cc3d7e08fc,linalg_frontend,not user facing,Also support non-contiguous activation for torch._weight_int8pack_mm on CPU (#147588),aten/src/ATen/native/LinearAlgebra.cpp aten/src/ATen/native/cpu/int8mm_kernel.cpp test/test_linalg.py,https://github.com/pytorch/pytorch/pull/147588,sanchitintel,leslie-fang-intel,malfet,mingfeima,
72b4f35cb55,releng,not user facing,[CI] Reduce the AOT target list to reduce build time (#147601),.ci/pytorch/build.sh,https://github.com/pytorch/pytorch/pull/147601,chuanqi129,atalman,,,
3409cbd1770,skip,Untopiced,"Revert ""Delete Mixed MM Special Casing (#147151)""",test/inductor/test_pattern_matcher.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/kernel/__init__.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/kernel/unpack_mixed_mm.py,,,,,,
bea72180ed7,skip,Untopiced,"Revert ""[ROCm] Implemented dropout usage for RNN with MIOpen backend (#144572)""",aten/src/ATen/miopen/Descriptors.h aten/src/ATen/native/miopen/RNN_miopen.cpp,,,,,,
fa8e3a28a7b,skip,Untopiced,"Revert ""[cuDNN][SDPA][Nested Tensor] Experimental cuDNN Nested Tensor SDPA Support (forward only) (#141178)""",aten/src/ATen/cuda/detail/UnpackRaw.cuh aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorTransformerUtils.h aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py test/test_transformers.py torch/nested/_internal/sdpa.py,,,,,,
85ea6798342,distributed,Untopiced,[TorchRec][PT2] disable contextlib in PT2 train pipeline (#147254),torch/nn/parallel/distributed.py,,,,,,
db15cb0988a,skip,not user facing,[Submodule] [Cutlass] Update to 3.8.0 tag (#147655),third_party/cutlass,https://github.com/pytorch/pytorch/pull/147655,drisspg,eqy,henrylhtsang,,
f9c117f859b,skip,not user facing,[mps/inductor] XFAIL adaptive_avg_pool_with_output_size_0. (#147676),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/147676,dcci,malfet,,,
6a5e3917a74,mps,not user facing,[MPS] Add inductor support for spherical_bessel_j0. (#147650),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/147650,dcci,jansel,,,
b084635735a,skip,not user facing,[MPS/inductor] Adjust more tests that depends on non-divisible input sizes (#147681),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/147681,dcci,jansel,,,
8c761ac7e31,dynamo,Untopiced,Handle `is`/`is not` (#146496),test/dynamo/test_functions.py torch/_dynamo/variables/builtin.py,https://github.com/pytorch/pytorch/pull/146496,guilhermeleobas,anijain2305,zou3519,,
d0adff761ea,dynamo,not user facing,Propagate `AttributeError` to user code in user_defined.py (#146497),test/dynamo/test_functions.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/146497,guilhermeleobas,anijain2305,zou3519,,
3e2d9d079e4,skip,Untopiced,"Revert ""[ROCm] OCP FP8 Support for new GPUs (#146632)""",aten/src/ATen/Context.cpp aten/src/ATen/cuda/CUDADataType.h aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp cmake/Dependencies.cmake cmake/public/LoadHIP.cmake test/test_linalg.py test/test_matmul_cuda.py torch/_utils_internal.py torch/testing/_internal/common_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,,,,,,
f03e7f38012,mps,Untopiced,[MPS] Workaround rng bug for 5D tensors (#147667),aten/src/ATen/native/mps/operations/Distributions.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/147667,malfet,dcci,,,
f47573f70d4,distributed,not user facing,Add super().setUp() to some test cases (#147651),test/distributed/test_c10d_functional_native.py test/export/test_draft_export.py,https://github.com/pytorch/pytorch/pull/147651,clee2000,huydhn,,,
b5d7aefa57d,skip,not user facing,[BE] add missing overload annotations for `tree_map_only` (#147699),torch/utils/_cxx_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/147699,XuehaiPan,Skylion007,,,
718cf68aee3,skip,not user facing,[cuBLAS][cuBLASLt] Unify `cuBLASLt` workspaces with `cuBLAS` workspaces (#145130),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDAContextLight.h aten/src/ATen/cuda/CublasHandlePool.cpp benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/145130,eqy,ngimel,,,
4e934ee5a7f,mps,Untopiced,[MPS] Add eager support for xlog1py. (#147687),aten/src/ATen/native/mps/kernels/BinaryKernel.metal aten/src/ATen/native/mps/operations/BinaryKernel.mm aten/src/ATen/native/native_functions.yaml c10/metal/special_math.h test/test_mps.py,https://github.com/pytorch/pytorch/pull/147687,dcci,malfet,,,
7c52ef24245,xpu,not user facing,Add XPU to is_compile_supported to support roi_align op in torchvision (#147541),torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/147541,frost-intel,guangyey,jansel,,
baccadb2f14,xpu,not user facing,xpu: torch.xpu.get_arch_list() to return [] if xpu not compiled (#147431),torch/utils/cpp_extension.py torch/xpu/__init__.py,https://github.com/pytorch/pytorch/pull/147431,dvrogozh,EikanWang,albanD,guangyey,
8b65dbad133,inductor,not user facing,[MPS/Inductor] Add support for xlog1py. (#147709),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/147709,dcci,jansel,,,
cee03b77464,inductor,not user facing,[Inductor] Update should_decompose_mm condition for CPU (#147673),torch/_inductor/fx_passes/decompose_mem_bound_mm.py,https://github.com/pytorch/pytorch/pull/147673,hl475,houseroad,,,
8d618f3da76,inductor,not user facing,[AOTI][XPU] Suppress multi-line comment warning for XPU. (#147710),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/147710,etaf,EikanWang,jansel,,
5b6ad682bc3,skip,Untopiced,"Revert ""[TorchRec][PT2] disable contextlib in PT2 train pipeline (#147254)""",torch/nn/parallel/distributed.py,,,,,,
dc9a03d30c4,dynamo,not user facing,[Window] Fix invalid file path on windows. (#147708),torch/_dynamo/pgo.py,https://github.com/pytorch/pytorch/pull/147708,etaf,jansel,,,
a50af71fb65,fx,Untopiced,[FX] Refactor immutable collections implementation (#144640),test/dynamo/test_functions.py test/test_fx.py torch/_higher_order_ops/aoti_call_delegate.py torch/_higher_order_ops/executorch_call_delegate.py torch/_inductor/pattern_matcher.py torch/fx/__init__.py torch/fx/immutable_collections.py,https://github.com/pytorch/pytorch/pull/144640,XuehaiPan,jansel,,,
cba14212e6f,fx,not user facing,[FX] micro-optimization `map_aggregate(immutable_dict)` (#147691),torch/fx/node.py,https://github.com/pytorch/pytorch/pull/147691,XuehaiPan,Skylion007,jansel,,
d0f08dc3ebc,skip,not user facing,Update slow tests (#147728),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/147728,pytorchupdatebot,pytorchbot,,,
80d3afc6985,fx,not user facing,[inductor] Improve type annotations in _inductor/pattern_matcher.py (#146626),torch/_inductor/pattern_matcher.py torch/fx/operator_schemas.py,https://github.com/pytorch/pytorch/pull/146626,rec,Skylion007,,,
576ed1e400d,skip,not user facing,Upgrade submodule oneDNN to v3.7 (#147498),test/inductor/test_binary_folding.py third_party/ideep third_party/mkl-dnn.BUILD,https://github.com/pytorch/pytorch/pull/147498,yanbing-j,atalman,fadara01,mingfeima,
cde12207a08,skip,not user facing,[Intel GPU] Add SDPA implementation on XPU with OneDNN (#147612),aten/src/ATen/native/mkldnn/xpu/Attention.cpp aten/src/ATen/native/mkldnn/xpu/detail/Attention.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.h aten/src/ATen/native/mkldnn/xpu/detail/oneDNN.h aten/src/ATen/native/mkldnn/xpu/detail/oneDNNContext.cpp aten/src/ATen/native/mkldnn/xpu/detail/oneDNNContext.h,https://github.com/pytorch/pytorch/pull/147612,DDEle,EikanWang,,,
900a7747813,skip,Untopiced,"Revert ""[ROCm] Update periodic.yml to use 2GPU runners (#146839)""",.github/workflows/periodic.yml,,,,,,
ffa19b90245,skip,not user facing,[ROCm][Windows] Fix unrecognized constexpr std::memcpy for HIP-clang (#147316),c10/util/Half.h,https://github.com/pytorch/pytorch/pull/147316,m-gallus,jeffdaily,,,
075b91bef17,quantization,Untopiced,[Intel GPU] qconv.pointwise with mixed dtype XPU support (#135465),aten/src/ATen/native/mkldnn/xpu/qconv.cpp test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/135465,ZhiweiYan-96,EikanWang,desertfire,guangyey,
b9b1fd9b930,skip,not user facing,[Intel GPU] qlinear.pointwise with mixed dtype support (#136753),aten/src/ATen/native/mkldnn/xpu/qlinear.cpp test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/136753,ZhiweiYan-96,EikanWang,desertfire,guangyey,
69c4f6ff13d,fx,Untopiced,[Minor] Fix minor mistake in docstring of replace_pattern (#147611),torch/fx/subgraph_rewriter.py,https://github.com/pytorch/pytorch/pull/147611,xwu-intel,soulitzer,,,
9605c5063b2,linalg_frontend,not user facing,[ROCm][TunableOp] Speed-up matmul_small_brute_force_tunableop unit test (#147659),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/147659,naromero77amd,jeffdaily,,,
52f6d4aa309,onnx,not user facing,[BE][CI][Easy] bump `ruff` to 0.9.0: long statements in docstrings (#146509),torch/hub.py torch/onnx/_internal/fx/fx_symbolic_graph_extractor.py torch/onnx/_internal/fx/passes/modularization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/146509,XuehaiPan,Skylion007,justinchuby,,
754fb834db2,releng,not user facing,[BE][CI] bump `ruff` to 0.9.0: string quote styles (#144569),.ci/aarch64_linux/aarch64_wheel_ci_build.py .ci/aarch64_linux/build_aarch64_wheel.py .ci/pytorch/smoke_test/max_autotune.py .github/scripts/github_utils.py .github/scripts/trymerge.py .github/scripts/trymerge_explainer.py aten/src/ATen/native/transformers/cuda/flash_attn/kernels/generate_kernels.py benchmarks/dynamo/check_accuracy.py benchmarks/dynamo/check_csv.py benchmarks/dynamo/check_graph_breaks.py benchmarks/dynamo/check_memory_compression_ratio.py benchmarks/dynamo/check_perf_csv.py benchmarks/instruction_counts/core/api.py docs/source/scripts/exportdb/generate_example_rst.py functorch/examples/dp_cifar10/cifar10_opacus.py functorch/examples/dp_cifar10/cifar10_transforms.py scripts/compile_tests/failures_histogram.py scripts/release_notes/categorize.py setup.py test/run_test.py test/test_dispatch.py test/test_jit_fuser_te.py tools/autograd/gen_inplace_or_view_type.py tools/autograd/gen_python_functions.py tools/autograd/gen_trace_type.py tools/autograd/gen_variable_factories.py tools/autograd/gen_variable_type.py tools/github/github_utils.py tools/iwyu/fixup.py tools/lite_interpreter/gen_selected_mobile_ops_header.py tools/packaging/build_wheel.py torch/__init__.py torch/_jit_internal.py torch/_tensor_str.py torch/onnx/_internal/diagnostics/infra/context.py torch/onnx/_internal/exporter/_onnx_program.py torch/onnx/symbolic_helper.py torchgen/api/python.py torchgen/api/types/signatures.py torchgen/api/types/types_base.py torchgen/dest/lazy_ir.py torchgen/dest/native_functions.py torchgen/dest/register_dispatch_key.py torchgen/dest/ufunc.py torchgen/gen.py torchgen/gen_aoti_c_shim.py torchgen/gen_backend_stubs.py torchgen/gen_executorch.py torchgen/gen_functionalization_type.py torchgen/gen_vmap_plumbing.py torchgen/model.py torchgen/static_runtime/generator.py,https://github.com/pytorch/pytorch/pull/144569,XuehaiPan,Skylion007,,,
19fd21fe7e3,inductor,not user facing,[Inductor] Hot fix after #146917 (#147639),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/147639,anmyachev,davidberard98,etaf,,
8eb400ef66f,distributed,Untopiced,[BE] TCPStore: use typed errors for assertions (#147647),torch/csrc/distributed/c10d/TCPStore.cpp torch/csrc/distributed/c10d/TCPStoreLibUvBackend.cpp torch/csrc/distributed/c10d/exception.h,https://github.com/pytorch/pytorch/pull/147647,d4l3k,fduwjj,,,
89b9c12de8f,skip,not user facing,remove prints from partitioner (#147749),torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/147749,bdhirsh,Skylion007,laithsakka,,
96acb566260,cuda,not user facing,[ROCm] Optimize the stride one indexing backwards kernel (#146420),aten/src/ATen/native/cuda/Indexing.cu,https://github.com/pytorch/pytorch/pull/146420,doru1004,jeffdaily,pruthvistony,,
0b52d801d22,skip,not user facing,[AOTI][refactor] Rename use_absolute_path to use_relative_path (#147679),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/147679,desertfire,angelayi,,,
22fae0d948a,skip,not user facing,[AOTI][refactor] Replace run_command_and_check with CppBuilder.build (#147680),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/147680,desertfire,angelayi,yushangdi,,
a71d8b72460,skip,not user facing,Fix `ReferenceError: weakly-referenced object no longer exists` in cycle detector (#146922),torch/utils/viz/_cycles.py,https://github.com/pytorch/pytorch/pull/146922,zdevito,Chillee,tianfengfrank,,
81dccd706b3,skip,Untopiced,[ROCm] OCP FP8 Support for new GPUs (#146632),aten/src/ATen/Context.cpp aten/src/ATen/cuda/CUDADataType.h aten/src/ATen/cuda/tunable/GemmHipblaslt.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp cmake/Dependencies.cmake cmake/public/LoadHIP.cmake test/test_linalg.py test/test_matmul_cuda.py torch/_utils_internal.py torch/testing/_internal/common_cuda.py torch/utils/hipify/cuda_to_hip_mappings.py,https://github.com/pytorch/pytorch/pull/146632,petrex,jeffdaily,,,
e72b4c61bf1,skip,Untopiced,"Revert ""Upgrade submodule oneDNN to v3.7 (#147498)""",test/inductor/test_binary_folding.py third_party/ideep third_party/mkl-dnn.BUILD,,,,,,
55bf3ff3a5b,skip,not user facing,[Docs] Add `OpDTypes.any_common_cpu_cuda_one` (#147605),torch/testing/_internal/common_device_type.py,https://github.com/pytorch/pytorch/pull/147605,shink,soulitzer,,,
97557b98334,inductor,not user facing,[Inductor] Update `set_driver_to_gpu` code to avoid backend re-initialization with new Triton (#147621),torch/_inductor/runtime/triton_helpers.py,https://github.com/pytorch/pytorch/pull/147621,anmyachev,jansel,,,
43074680b5e,releng,not user facing,[ROCm] Add support for gfx1102 arch to wheel builds. (#147761),.ci/docker/libtorch/build.sh .ci/docker/manywheel/build.sh,https://github.com/pytorch/pytorch/pull/147761,naromero77amd,jeffdaily,,,
2c8cd41c1fa,releng,not user facing,Delete unused conda-aws-upload environment (#147792),.github/workflows/_binary-upload.yml .github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/147792,malfet,atalman,,,
c644f4c5feb,inductor,not user facing,[Inductor] Fix the decompositions of torch isin (#147519),test/inductor/test_torchinductor.py torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/147519,leslie-fang-intel,FFFrog,jgong5,peterbell10,
dacdc9782b2,inductor,not user facing,[Inductor] Add input value checking to randint meta function (#147191),test/test_tensor_creation_ops.py torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/147191,DDEle,jansel,leslie-fang-intel,,
33ff96b3f97,inductor,not user facing,cpp_builder: unbreak clang++ detection (#147775),torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/147775,benjaminglass1,desertfire,,,
b0fa92042ba,composability,not user facing,Fix torch.mean out dtype check (#147188),test/test_ops.py torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/147188,FFFrog,albanD,,,
8f728e28ddb,distributed,not user facing,Enable ASAN in CUDA tests (#147512),test/distributed/optim/test_zero_redundancy_optimizer.py test/test_ops.py,https://github.com/pytorch/pytorch/pull/147512,cyyever,soulitzer,,,
e34c15a05b0,skip,not user facing,torch._scaled_mm with MXFP8 (#147548),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDABlas.h aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp test/test_matmul_cuda.py torch/testing/_internal/common_cuda.py,https://github.com/pytorch/pytorch/pull/147548,vkuzo,drisspg,,,
20295c017ee,inductor,not user facing,Fix import of getArtifactLogger for ir_pre_fusion and ir_post_fusion (#147560),torch/_inductor/debug.py torch/_logging/_registrations.py,https://github.com/pytorch/pytorch/pull/147560,dulinriley,Skylion007,,,
9478c90e2b1,skip,not user facing,[Quant] flip: throw runtime error for QUInt4x2 and QUInt2x4 input (#147430),aten/src/ATen/native/TensorTransformations.cpp test/test_shape_ops.py,https://github.com/pytorch/pytorch/pull/147430,Xia-Weiwen,mingfeima,ngimel,,
9b06b304686,skip,Untopiced,"Revert ""[AOTI][refactor] Replace run_command_and_check with CppBuilder.build (#147680)""",torch/_inductor/codecache.py,,,,,,
890213f65f9,skip,Untopiced,"Revert ""[AOTI][refactor] Rename use_absolute_path to use_relative_path (#147679)""",torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,,,,,,
4b7604ec101,skip,not user facing,Delete Mixed MM Special Casing (#147151),test/inductor/test_cutlass_backend.py test/inductor/test_pattern_matcher.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/kernel/__init__.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py torch/_inductor/kernel/unpack_mixed_mm.py,https://github.com/pytorch/pytorch/pull/147151,eellison,cpuhrsch,drisspg,,
ab3b814af39,releng,Untopiced,[CacheBench] Add ciflow/trunk test (#147688),.ci/pytorch/test.sh .github/workflows/trunk.yml benchmarks/dynamo/cachebench.py,https://github.com/pytorch/pytorch/pull/147688,oulgen,huydhn,,,
60d4cbfc062,releng,not user facing,[CacheBench] Add repeat option so that we can have more accurate cache results (#147780),.ci/pytorch/test.sh benchmarks/dynamo/cachebench.py,https://github.com/pytorch/pytorch/pull/147780,oulgen,huydhn,,,
c4fb6ae55dd,releng,not user facing,[CacheBench] Separate dynamic into its own option (#147781),.ci/pytorch/test.sh benchmarks/dynamo/cachebench.py,https://github.com/pytorch/pytorch/pull/147781,oulgen,huydhn,,,
895564d6b6d,releng,Untopiced,[CacheBench] Add huggingface (#147782),.ci/pytorch/test.sh .github/workflows/inductor-perf-test-nightly.yml benchmarks/dynamo/cachebench.py,https://github.com/pytorch/pytorch/pull/147782,oulgen,huydhn,,,
bb7e8fbd668,releng,not user facing,[CacheBench] Add hf_T5 llama moco to cachebench (#147783),.ci/pytorch/test.sh benchmarks/dynamo/cachebench.py,https://github.com/pytorch/pytorch/pull/147783,oulgen,huydhn,,,
fb73b0c7c55,skip,Untopiced,"Revert ""use copy2d in h2d/d2h copy when possible (#146256)""",aten/src/ATen/native/cuda/Copy.cu c10/cuda/CUDAFunctions.h test/test_cuda.py,,,,,,
d73b9276628,distributed,Untopiced,[DSD] Fixes issue when there is a PG without parameters (#147730),test/distributed/checkpoint/test_state_dict.py torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/147730,fegin,mori360,,,
866dc45d3ca,inductor,not user facing,[Inductor][ROCm][CK] Unhardedcoded kernel shapes for ck_conv_template codegen (#147504),torch/_inductor/codegen/rocm/ck_conv_template.py,https://github.com/pytorch/pytorch/pull/147504,AviralGoelAMD,eellison,jansel,tenpercent,
7bd2e3bca18,skip,not user facing,Update torch-xpu-ops commit pin (#147743),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/147743,xytintel,EikanWang,,,
94969d0a40e,inductor,improvements,[inductor][user triton] Handle scf.yield more accurately (#147762),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/147762,davidberard98,oulgen,zou3519,,
af2d63637ed,cuda,performance,Add option to limit number of SMs used by matmul kernels (#144974),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/native/cuda/RowwiseScaledMM.cu test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/144974,lw,albanD,eqy,,
b63c6016140,releng,not user facing,Update merge rules for oneDNN part (#147615),.github/merge_rules.yaml,https://github.com/pytorch/pytorch/pull/147615,EikanWang,atalman,,,
7e37fb0a4c0,mps,improvements,[MPS] faster integer matmul for mps (#147526),aten/src/ATen/native/mps/kernels/LinearAlgebra.metal aten/src/ATen/native/mps/operations/LinearAlgebra.mm,https://github.com/pytorch/pytorch/pull/147526,Isalia20,malfet,,,
2680e835c87,inductor,bc breaking,[AOTI][refactor] Rename use_absolute_path to use_relative_path (#147805),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/147805,desertfire,malfet,,,
7ed0670e219,inductor,not user facing,[AOTI][refactor] Replace run_command_and_check with CppBuilder.build (#147806),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/147806,desertfire,malfet,,,
cc1c9826d4a,inductor,not user facing,[AOTI][refactor] Fix a typo (#147807),aten/src/ATen/cpu/vec/vec_base.h torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/147807,desertfire,malfet,,,
0b9da1ae0ad,skip,not user facing,[AOTI][refactor] Consolidate CppBuilder.build and CppBuilder.build_fbcode_cpu_re (#147803),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/147803,desertfire,malfet,,,
a695aae89b6,mps,bug fixes,[MPS] fix attention for >4d tensors (#147545),aten/src/ATen/native/mps/operations/Attention.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/147545,Isalia20,malfet,,,
810d2a3dbd4,releng,not user facing,[ARM] Fix bug in _ref_test_helper in test_ops and fix failing test on Aarch64 (#146597),.ci/pytorch/test.sh test/test_ops.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/146597,robert-hardwick,digantdesai,,,
346bbefa630,skip,not user facing,[BE] Parameterize TestSDPA in test_mps.py (#147856),test/test_mps.py,https://github.com/pytorch/pytorch/pull/147856,malfet,Skylion007,,,
e5a13410cd0,skip,not user facing,Fix the tiny doc descriptions (#147319),torch/_library/triton.py,https://github.com/pytorch/pytorch/pull/147319,FFFrog,zou3519,,,
651e6aacf9b,releng,not user facing,[ROCm] Remove benign warning about missing amdgpu.ids (#147791),.ci/docker/common/install_rocm_drm.sh,https://github.com/pytorch/pytorch/pull/147791,ethanwee1,jeffdaily,,,
6061664266a,skip,not user facing,Enabled force_shape_pad for triton tests in test_kernel_benchmark (#147620),test/inductor/test_kernel_benchmark.py,https://github.com/pytorch/pytorch/pull/147620,iupaikov-amd,chenyang78,jeffdaily,shunting314,
580f1183b4d,releng,Untopiced,Enable ruff rule S324 (#147665),.github/scripts/pytest_caching_utils.py benchmarks/instruction_counts/applications/ci.py pyproject.toml tools/stats/upload_dynamo_perf_stats.py torch/_logging/_internal.py torch/distributed/distributed_c10d.py torch/fx/passes/graph_drawer.py torch/utils/_config_module.py torch/utils/_content_store.py torchgen/utils.py,https://github.com/pytorch/pytorch/pull/147665,zeshengzong,Skylion007,,,
ea6938a1f77,skip,not user facing,Add XuehaiPan to CODEOWNERS for C++ PyTree utilities (#137408),CODEOWNERS,https://github.com/pytorch/pytorch/pull/137408,XuehaiPan,zou3519,,,
17766b7aad0,skip,not user facing,[inductor][triton] Ignore block ptr advances for removed buffers (#147193),test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/codegen/common.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/147193,kundaMwiza,jansel,,,
14b9f7f7bc1,skip,not user facing,Remove link to search survey (#147751),docs/source/_templates/layout.html,https://github.com/pytorch/pytorch/pull/147751,svekars,malfet,,,
9740d69e782,inductor,not user facing,[logging] Add toplevel dynamo_compile / tlparse logging for AOTI (#147760),torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/147760,masnesral,desertfire,,,
46d1422afd3,inductor,not user facing,cpp_wrapper: fix inductor triton tests (#146109),test/inductor/test_triton_extension_backend.py test/inductor/test_triton_kernels.py test/inductor/test_triton_syntax.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/146109,benjaminglass1,desertfire,,,
7c515b2da4c,skip,not user facing,cpp_wrapper: fix test_torchinductor* tests (#146424),test/inductor/test_torchinductor_codegen_config_overrides.py test/inductor/test_torchinductor_dynamic_shapes.py torch/utils/_sympy/printers.py,https://github.com/pytorch/pytorch/pull/146424,benjaminglass1,desertfire,,,
361b6c97cd2,inductor,not user facing,cpp_wrapper: Fixup output code indentation (#147215),torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/147215,benjaminglass1,desertfire,,,
de80b6f0d39,skip,not user facing,Updated test_cuda.py to rerun tests (#147040),test/test_cuda.py,https://github.com/pytorch/pytorch/pull/147040,BLOrange-AMD,jeffdaily,pruthvistony,,
824474cb357,dynamo,not user facing,[cond] support output sizes mismatch in front end (#147130),test/dynamo/test_export.py test/dynamo/test_higher_order_ops.py test/functorch/test_control_flow.py test/inductor/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/cond.py,https://github.com/pytorch/pytorch/pull/147130,ydwu4,zou3519,,,
adf0f4ffd24,inductor,not user facing,[custom op] fix inductor cpp codegen when returning a list of single tensor (#147649),test/inductor/test_aot_inductor_custom_ops.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/147649,ydwu4,angelayi,,,
68ddca94498,skip,Untopiced,[ca] trace saved variable unpacking (#147242),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/autograd/engine.cpp torch/csrc/autograd/python_saved_variable_hooks.cpp torch/csrc/autograd/python_saved_variable_hooks.h torch/csrc/autograd/saved_variable.cpp torch/csrc/autograd/saved_variable.h torch/csrc/autograd/saved_variable_hooks.h torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/147242,xmfan,jansel,,,
5758743f3c9,skip,Untopiced,[ca] side-effect free initial trace: GraphTask (#147796),torch/csrc/autograd/engine.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/147796,xmfan,jansel,,,
ec768d8dc04,skip,Untopiced,[ca] side-effect free inital trace: compiled_args (#147804),tools/autograd/gen_autograd_functions.py torch/csrc/autograd/custom_function.h torch/csrc/autograd/function.h torch/csrc/autograd/function_hook.h torch/csrc/autograd/functions/accumulate_grad.cpp torch/csrc/autograd/functions/accumulate_grad.h torch/csrc/autograd/functions/basic_ops.cpp torch/csrc/autograd/functions/basic_ops.h torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/autograd/python_hook.cpp torch/csrc/autograd/python_hook.h torch/csrc/autograd/utils/lambda_post_hook.h torch/csrc/distributed/c10d/reducer.cpp torch/csrc/dynamo/compiled_autograd.h,https://github.com/pytorch/pytorch/pull/147804,xmfan,jansel,,,
687fe64667a,jit,Untopiced,Fix crash in -[PTMCoreMLCompiler _compileModel:atPath:] (#147809),torch/csrc/jit/backends/coreml/objc/PTMCoreMLCompiler.mm,https://github.com/pytorch/pytorch/pull/147809,dinhvh,mcr229,,,
8d921eb97ff,export,Untopiced,export method (#147573),test/export/test_export.py torch/_export/utils.py,https://github.com/pytorch/pytorch/pull/147573,avikchaudhuri,tugsbayasgalan,,,
6eb3d1e7627,distributed,not user facing,[DCP] Cache save plans in default planner (#147343),test/distributed/checkpoint/test_file_system_checkpoint.py test/distributed/checkpoint/test_planner.py torch/distributed/checkpoint/default_planner.py torch/distributed/checkpoint/planner.py torch/distributed/checkpoint/planner_helpers.py,https://github.com/pytorch/pytorch/pull/147343,saumishr,MeetVadakkanchery,,,
0d31c621a37,skip,Untopiced,"Revert ""[inductor][triton] Ignore block ptr advances for removed buffers (#147193)""",test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/codegen/common.py torch/_inductor/codegen/triton.py,,,,,,
a821d69d925,fx,Untopiced,Fix register constant to be usable in exportz (#147533),test/export/test_export.py test/test_proxy_tensor.py torch/fx/_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/147533,tugsbayasgalan,zou3519,,,
cc444e75d54,skip,not user facing,"follow up to #147548, fix regression on MI300 (#147878)",aten/src/ATen/cuda/CUDABlas.cpp,https://github.com/pytorch/pytorch/pull/147878,jeffdaily,drisspg,,,
1e894d26352,skip,Untopiced,"Revert ""Add option to limit number of SMs used by matmul kernels (#144974)""",aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/native/cuda/RowwiseScaledMM.cu test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/csrc/Module.cpp,,,,,,
2df9a8d72de,inductor,not user facing,[Inductor][Tests] Update `get_divisible_by_16` function in `test_torchinductor.py` to work correctly with new Triton (#147865),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/147865,anmyachev,davidberard98,,,
05bc8fe62e3,skip,Untopiced,"Revert ""follow up to #147548, fix regression on MI300 (#147878)""",aten/src/ATen/cuda/CUDABlas.cpp,,,,,,
0633f63f0d7,skip,not user facing,[cutlass backend] try fix standlone runner test (#147811),test/inductor/test_cutlass_backend.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/147811,henrylhtsang,chenyang78,,,
c82c1411c61,skip,Untopiced,"Revert ""torch._scaled_mm with MXFP8 (#147548)""",aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDABlas.h aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp test/test_matmul_cuda.py torch/testing/_internal/common_cuda.py,,,,,,
8e7e5ba1827,skip,not user facing,Add sparse tensors constructed via legacy constructor to _sparse_tensors_to_validate (#147759),test/test_serialization.py torch/_weights_only_unpickler.py,https://github.com/pytorch/pytorch/pull/147759,mikaylagawarecki,albanD,,,
276dfe8150f,dynamo,not user facing,[dynamo][cpp-guards] Disable dict-tag optim if the guard_manager has child accessors (#147694),torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/147694,anijain2305,isuruf,,,
3ecfe6be256,autograd_frontend,not user facing,[Submodule] Turning flash-attention integration into 3rd party submod (#144120) (#146372),CMakeLists.txt aten/src/ATen/CMakeLists.txt aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/flash_attn/alibi.h aten/src/ATen/native/transformers/cuda/flash_attn/block_info.h aten/src/ATen/native/transformers/cuda/flash_attn/dropout.h aten/src/ATen/native/transformers/cuda/flash_attn/flash.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.cpp aten/src/ATen/native/transformers/cuda/flash_attn/flash_api.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_kernel.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_launch_template.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_bwd_preprocess_kernel.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_kernel.h aten/src/ATen/native/transformers/cuda/flash_attn/flash_fwd_launch_template.h aten/src/ATen/native/transformers/cuda/flash_attn/kernel_traits.h aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim128_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim128_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim160_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim160_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim192_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim192_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim224_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim224_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim256_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim32_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim32_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim64_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim64_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim96_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_bwd_hdim96_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim128_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim128_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim160_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim160_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim192_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim192_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim224_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim224_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim256_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim256_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim32_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim32_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim64_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim64_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim96_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_hdim96_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim128_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim128_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim160_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim160_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim192_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim192_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim224_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim224_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim256_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim256_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim32_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim32_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim64_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim64_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim96_bf16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/flash_fwd_split_hdim96_fp16_sm80.cu aten/src/ATen/native/transformers/cuda/flash_attn/kernels/generate_kernels.py aten/src/ATen/native/transformers/cuda/flash_attn/mask.h aten/src/ATen/native/transformers/cuda/flash_attn/rotary.h aten/src/ATen/native/transformers/cuda/flash_attn/softmax.h aten/src/ATen/native/transformers/cuda/flash_attn/utils.h aten/src/ATen/native/transformers/hip/flash_attn/flash_api.h caffe2/CMakeLists.txt tools/autograd/derivatives.yaml torch/_meta_registrations.py torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h,https://github.com/pytorch/pytorch/pull/146372,drisspg,jbschlosser,,,
143f0f0006f,skip,Untopiced,"Revert ""[ca] side-effect free inital trace: compiled_args (#147804)""",tools/autograd/gen_autograd_functions.py torch/csrc/autograd/custom_function.h torch/csrc/autograd/function.h torch/csrc/autograd/function_hook.h torch/csrc/autograd/functions/accumulate_grad.cpp torch/csrc/autograd/functions/accumulate_grad.h torch/csrc/autograd/functions/basic_ops.cpp torch/csrc/autograd/functions/basic_ops.h torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/autograd/python_hook.cpp torch/csrc/autograd/python_hook.h torch/csrc/autograd/utils/lambda_post_hook.h torch/csrc/distributed/c10d/reducer.cpp torch/csrc/dynamo/compiled_autograd.h,,,,,,
4d614baa301,skip,Untopiced,"Revert ""[ca] side-effect free initial trace: GraphTask (#147796)""",torch/csrc/autograd/engine.h torch/csrc/dynamo/python_compiled_autograd.cpp,,,,,,
90e3a3d86d6,skip,Untopiced,"Revert ""[ca] trace saved variable unpacking (#147242)""",test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/autograd/engine.cpp torch/csrc/autograd/python_saved_variable_hooks.cpp torch/csrc/autograd/python_saved_variable_hooks.h torch/csrc/autograd/saved_variable.cpp torch/csrc/autograd/saved_variable.h torch/csrc/autograd/saved_variable_hooks.h torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,,,,,,
8de6fe8c0bb,python_frontend,docs,[docs] fix numpy docs reference (#147697),docs/source/tensor_view.rst,https://github.com/pytorch/pytorch/pull/147697,martin-kokos,soulitzer,,,
12112fd1987,distributed,Untopiced,Fix bug in FSDP wrapped module with zero argument  (#147771),test/distributed/fsdp/test_wrap.py torch/distributed/fsdp/_runtime_utils.py,https://github.com/pytorch/pytorch/pull/147771,mori360,awgu,,,
b533bb4b133,skip,not user facing,optimize the decomposition of aten.native_group_norm (#144733),test/inductor/test_cpu_repro.py torch/_refs/__init__.py,https://github.com/pytorch/pytorch/pull/144733,jiayisunx,jansel,leslie-fang-intel,,
805f7d97f7d,inductor,Untopiced,[Inductor][Optimus] Fix a corner case in split cat aten pass (#147784),test/inductor/test_split_cat_fx_aten_passes.py torch/_inductor/fx_passes/split_cat.py,https://github.com/pytorch/pytorch/pull/147784,mengluy0125,Microve,,,
9ad0ad64979,mps,not user facing,[MPS] Introduce a shader for `entr()`. (#147914),c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/147914,dcci,malfet,,,
d3fc583ff06,skip,not user facing,[cutlass backend] force_disable_caches for test_number_mm_precompiles (#147901),test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/147901,henrylhtsang,ColinPeppler,,,
f211818bc0d,distributed,Untopiced,[c10d] Restrict use condition of NCCL mem pool (#147764),build_variables.bzl torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/CUDASymmetricMemory.cu torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/cuda/utils.cpp torch/csrc/distributed/c10d/cuda/utils.hpp torch/csrc/distributed/c10d/reducer.cpp,https://github.com/pytorch/pytorch/pull/147764,kwen2501,syed-ahmed,yifuwang,,
ba25e26baa5,cuda,not user facing,[ROCm] Use IPT=8 for block radix sort (#147657),aten/src/ATen/native/cuda/Sort.cu,https://github.com/pytorch/pytorch/pull/147657,jerrymannil,jeffdaily,,,
c839fa4dd2c,inductor,Untopiced,"[Resubmit] Record input strides at time of tracing, constrain to them for triton fn (#147861)",test/inductor/test_triton_kernels.py torch/_inductor/graph.py torch/_inductor/lowering.py torch/fx/experimental/proxy_tensor.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/147861,eellison,zou3519,,,
a2399c9b44d,python_frontend,not user facing,[BE] Switch `index_variable` to `torch.testing.make_tensor` (#147892),torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/147892,malfet,dcci,,,
9ed40af917e,skip,not user facing,[BE][EZ] Delete MacOS-12.3 xfail list (#147905),test/test_mps.py,https://github.com/pytorch/pytorch/pull/147905,malfet,dcci,,,
12b9674cb60,skip,not user facing,torch._scaled_mm with MXFP8 (#147548),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDABlas.h aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp test/test_matmul_cuda.py torch/testing/_internal/common_cuda.py,https://github.com/pytorch/pytorch/pull/147548,vkuzo,drisspg,,,
acca9b9cb0a,skip,Untopiced,"Revert ""[AOTI][refactor] Consolidate CppBuilder.build and CppBuilder.build_fbcode_cpu_re (#147803)""",torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,,,,,,
4ec6c1d1ec4,skip,not user facing,Fix test_halide.py report invocation to re-run failed tests (#147640),test/inductor/test_halide.py,https://github.com/pytorch/pytorch/pull/147640,isuruf,jansel,,,
4216478250e,releng,not user facing,Fix the benchmark config name from H100 benchmark (#147947),.github/workflows/inductor-perf-test-nightly-h100.yml,https://github.com/pytorch/pytorch/pull/147947,huydhn,wdvr,,,
a84db75e1ba,skip,Untopiced,"Revert ""torch._scaled_mm with MXFP8 (#147548)""",aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDABlas.h aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp test/test_matmul_cuda.py torch/testing/_internal/common_cuda.py,,,,,,
7a06bfdd1c7,inductor,not user facing,[inductor][ck] kBatch parametrized (#147885),torch/_inductor/codegen/rocm/ck_template.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py,https://github.com/pytorch/pytorch/pull/147885,coconutruben,ColinPeppler,,,
3fd68e4e2f5,dynamo,not user facing,[dynamo] make some more graph break messages readable in English [2/N] (#147385),test/custom_operator/test_custom_ops.py torch/_dynamo/codegen.py torch/_dynamo/comptime.py torch/_dynamo/convert_frame.py torch/_dynamo/guards.py torch/_dynamo/output_graph.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/147385,williamwen42,jansel,,,
cf6d1e68244,dynamo,not user facing,[dynamo] add generic graph break hints (#147429),torch/_dynamo/codegen.py torch/_dynamo/convert_frame.py torch/_dynamo/graph_break_hints.py torch/_dynamo/guards.py torch/_dynamo/output_graph.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/147429,williamwen42,jansel,zou3519,,
7ffae2c028a,skip,not user facing,Split test_transformers.py (#147441),test/run_test.py test/test_transformers.py test/test_transformers_privateuse1.py,https://github.com/pytorch/pytorch/pull/147441,Zhenbin-8,drisspg,,,
60d94ea22b0,cuda,performance,Add option to limit number of SMs used by matmul kernels (#147966),aten/src/ATen/Context.cpp aten/src/ATen/Context.h aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/native/cuda/RowwiseScaledMM.cu test/test_matmul_cuda.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/csrc/Module.cpp,https://github.com/pytorch/pytorch/pull/147966,lw,danthe3rd,,,
f522d899fb2,skip,not user facing,"Add MSVC version condition to ""Fix for MSVC problem on Windows Arm64 (#136765)"" (#145076)",aten/src/ATen/cpu/vec/vec_base.h,https://github.com/pytorch/pytorch/pull/145076,iremyux,alinpahontu2912,malfet,,
be830c8b1c4,inductor,not user facing,[Inductor][CPP] fix store mode atomic add (#147961),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_prefix.h,https://github.com/pytorch/pytorch/pull/147961,leslie-fang-intel,malfet,,,
edaf9ddeb5b,skip,not user facing,Add basic Gaudi support to benchmarks/dynamo (#145920),benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/145920,kfojcik-intel,eellison,,,
08f4c1a2332,skip,not user facing,[dynamo] add sourceless builder for `types.MethodType` (#147880),test/dynamo/test_functions.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/147880,XuehaiPan,jansel,,,
0a2da008f82,skip,Untopiced,[ca] trace saved variable unpacking (#147242),test/inductor/test_compiled_autograd.py torch/_dynamo/compiled_autograd.py torch/csrc/autograd/engine.cpp torch/csrc/autograd/python_saved_variable_hooks.cpp torch/csrc/autograd/python_saved_variable_hooks.h torch/csrc/autograd/saved_variable.cpp torch/csrc/autograd/saved_variable.h torch/csrc/autograd/saved_variable_hooks.h torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/147242,xmfan,jansel,,,
5e3069dde89,skip,Untopiced,[ca] side-effect free initial trace: GraphTask (#147796),torch/csrc/autograd/engine.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/147796,xmfan,jansel,,,
fd1220e3861,skip,Untopiced,[ca] side-effect free inital trace: compiled_args (#147804),tools/autograd/gen_autograd_functions.py torch/csrc/autograd/custom_function.h torch/csrc/autograd/function.h torch/csrc/autograd/function_hook.h torch/csrc/autograd/functions/accumulate_grad.cpp torch/csrc/autograd/functions/accumulate_grad.h torch/csrc/autograd/functions/basic_ops.cpp torch/csrc/autograd/functions/basic_ops.h torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/autograd/python_hook.cpp torch/csrc/autograd/python_hook.h torch/csrc/autograd/utils/lambda_post_hook.h torch/csrc/distributed/c10d/reducer.cpp torch/csrc/dynamo/compiled_autograd.h,https://github.com/pytorch/pytorch/pull/147804,xmfan,jansel,,,
ac926f81ccf,skip,not user facing,[Inductor][Triton] Rework casting logic to avoid illegal bitcast (#147395),torch/_inductor/runtime/triton_helpers.py,https://github.com/pytorch/pytorch/pull/147395,alexbaden,eellison,,,
ba9ed856e0a,inductor,not user facing,[FlexAttention] Improve error msg for embedding < 16 (#147765),test/inductor/test_flex_attention.py test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/147765,BoyuanFeng,drisspg,,,
5ef94ca8162,mps,not user facing,[BE] Do not copy arguments in variadic template (#147977),aten/src/ATen/native/mps/OperationUtils.h,https://github.com/pytorch/pytorch/pull/147977,malfet,Skylion007,dcci,,
ef61c290e11,distributed (dtensor),bug fixes,[DTensor][random] defer DTensor RNG state sync until first random op call or manual_seed call; support more flexible OffsetBasedRNGTracker init (#147025),test/distributed/tensor/parallel/test_tp_random_state.py test/distributed/tensor/parallel/test_tp_style.py test/distributed/tensor/test_random_ops.py torch/distributed/tensor/_api.py torch/distributed/tensor/_dispatch.py torch/distributed/tensor/_random.py,https://github.com/pytorch/pytorch/pull/147025,XilunWu,kwen2501,,,
3f7e242c864,releng,not user facing,[CI] Checkout with more processes (#147652),.github/actions/checkout-pytorch/action.yml,https://github.com/pytorch/pytorch/pull/147652,clee2000,ZainRizvi,,,
678435c4439,inductor,Untopiced,[FlexAttention] Fix IMA bug (#147918),torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/147918,drisspg,BoyuanFeng,Skylion007,,
fb566c5aead,composability,Untopiced,Fix auto_functionalize x inference_mode (#147925),test/inductor/test_auto_functionalize.py torch/_higher_order_ops/auto_functionalize.py torch/_subclasses/functional_tensor.py,https://github.com/pytorch/pytorch/pull/147925,zou3519,bdhirsh,,,
4e4191854ba,skip,not user facing,[logs][qol] Print log options alphabetically (#147888),torch/_logging/_internal.py,https://github.com/pytorch/pytorch/pull/147888,anijain2305,jansel,,,
0ea5d1067bc,skip,not user facing,ROCm: Remove static specifier for allow_tf32 variable. (#147186),aten/src/ATen/Context.cpp test/test_cuda.py,https://github.com/pytorch/pytorch/pull/147186,jagadish-amd,jeffdaily,naromero77amd,,
cfb293ee022,inductor,not user facing,[inductor] Add logs for precompile and autotuning (#147923),torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/147923,henrylhtsang,Skylion007,,,
ebf6b9839c5,mps,performance,[MPS] faster integer batched matmul (#147877),aten/src/ATen/native/mps/kernels/LinearAlgebra.metal aten/src/ATen/native/mps/operations/LinearAlgebra.mm,https://github.com/pytorch/pytorch/pull/147877,Isalia20,Skylion007,,,
00732c3f7e4,mps,bug fixes,[MPS] Implemented `masked_fill_scalar` as shader (#147369),aten/src/ATen/native/mps/OperationUtils.h aten/src/ATen/native/mps/kernels/Indexing.metal aten/src/ATen/native/mps/kernels/UnfoldBackward.metal aten/src/ATen/native/mps/operations/Indexing.mm c10/metal/indexing.h test/test_mps.py,https://github.com/pytorch/pytorch/pull/147369,malfet,dcci,,,
30db64bf519,inductor,not user facing,[PT2] Support add/remove passes in pre_grad (#146064),torch/_inductor/compile_fx.py torch/_inductor/config.py torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/146064,huxintong,frank-wei,,,
8594856651f,skip,not user facing,[aotd] Alias of intermediate unwrap TensorAlias (#147638),test/functorch/test_aotdispatch.py test/inductor/test_torchinductor.py torch/_functorch/_aot_autograd/runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/147638,IvanKobzarev,bdhirsh,,,
bab84f0bd90,skip,not user facing,[hop] Support more output types for `flat_apply` (#146714),test/dynamo/test_flat_apply.py torch/_higher_order_ops/flat_apply.py,https://github.com/pytorch/pytorch/pull/146714,StrongerXi,zou3519,,,
f46f0e465c8,dynamo,Untopiced,[dynamo] Initial support for `nonstrict_trace` (#146367),test/dynamo/test_decorators.py test/dynamo/test_flat_apply.py torch/_dynamo/__init__.py torch/_dynamo/decorators.py torch/_dynamo/output_graph.py torch/_dynamo/trace_rules.py torch/_dynamo/utils.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py torch/_higher_order_ops/flat_apply.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/146367,StrongerXi,zou3519,,,
7e0ef2c8449,dynamo,not user facing,[dynamo] Use the new `get_unique_name_wrt` helper when applicable (#146950),torch/_dynamo/output_graph.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/146950,StrongerXi,zou3519,,,
73e963459e1,dynamo,not user facing,[dynamo] Support `nonstrict_trace` on class method (#147571),test/dynamo/test_decorators.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/147571,StrongerXi,zou3519,,,
eb08ada5d31,dynamo,not user facing,[dynamo] Support reads to global/captured tensors in `nonstrict_trace`-ed function (#147572),test/dynamo/test_decorators.py test/dynamo/test_flat_apply.py torch/_dynamo/variables/torch.py torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/147572,StrongerXi,zou3519,,,
683e083e8dd,mps,Untopiced,[MPS] Add support for `entr()` in eager. (#147948),aten/src/ATen/native/mps/kernels/SpecialOps.metal aten/src/ATen/native/mps/operations/SpecialOps.mm aten/src/ATen/native/native_functions.yaml c10/metal/special_math.h test/test_mps.py,https://github.com/pytorch/pytorch/pull/147948,dcci,malfet,,,
7c71ab1d409,dynamo,not user facing,[scan] User-facing reverse flag handling (#147886),test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/scan.py,https://github.com/pytorch/pytorch/pull/147886,bohnstingl,ydwu4,,,
6a658d983e8,skip,not user facing,Build a storage reader/writer to write checkpoints in HF format (#147622),test/distributed/checkpoint/test_hf_storage.py torch/distributed/checkpoint/__init__.py torch/distributed/checkpoint/_hf_storage.py torch/distributed/checkpoint/filesystem.py,https://github.com/pytorch/pytorch/pull/147622,ankitageorge,saumishr,,,
976ff5cf01b,build_frontend,build,Add cmake hints to USE_SYSTEM_NVTX for nvtx3 include dir (#147418),CMakeLists.txt cmake/public/cuda.cmake,https://github.com/pytorch/pytorch/pull/147418,xwang233,malfet,nWEIdia,,
201666d77dd,inductor,not user facing,[cutlass backend] turn autotuning logs off by default + rename log to autotuning log (#147922),torch/_inductor/autotune_process.py torch/_inductor/codegen/cuda/cuda_template.py torch/_logging/_registrations.py,https://github.com/pytorch/pytorch/pull/147922,henrylhtsang,eellison,,,
a1ee2c3a08c,skip,not user facing,[do not merge yet] update grammar  (#147996),docs/source/notes/autograd.rst,https://github.com/pytorch/pytorch/pull/147996,sokkaofthewatertribe,,,,
dc7556f1bdb,skip,Untopiced,"Revert ""[do not merge yet] update grammar  (#147996)""",docs/source/notes/autograd.rst,,,,,,
6e129a697f8,skip,not user facing,[do not merge yet] update grammar  (#147996),docs/source/notes/autograd.rst,https://github.com/pytorch/pytorch/pull/147996,sokkaofthewatertribe,,,,
7e7d05bf85f,skip,Untopiced,"Revert ""[do not merge yet] update grammar  (#147996)""",docs/source/notes/autograd.rst,,,,,,
b13ad1a193f,skip,not user facing,[ROCm][TunableOp] Remove extra transpose characters in hipBLASLt signature. (#147900),aten/src/ATen/cuda/tunable/GemmHipblaslt.h,https://github.com/pytorch/pytorch/pull/147900,naromero77amd,jeffdaily,,,
84e60eece83,linalg_frontend,not user facing,[ROCm] [TunableOp] Unit tests for scaled GEMM and GEMM with bias (#147890),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/147890,naromero77amd,jeffdaily,,,
915eb012e1d,skip,Untopiced,"Revert ""[dynamo] add sourceless builder for `types.MethodType` (#147880)""",test/dynamo/test_functions.py torch/_dynamo/variables/builder.py,,,,,,
723f3a9eabe,skip,not user facing,torch.utils._content_store: fix error in hash_storage on XPU (#147785),test/test_content_store.py torch/utils/_content_store.py,https://github.com/pytorch/pytorch/pull/147785,benjaminglass1,jansel,,,
f98cd84b046,skip,not user facing,cpp_wrapper: use largeTensorTest for test memory checks (#146991),test/inductor/test_codecache.py test/inductor/test_metrics.py test/inductor/test_torchinductor.py test/test_torch.py torch/testing/_internal/common_device_type.py,https://github.com/pytorch/pytorch/pull/146991,benjaminglass1,desertfire,,,
f104ef12482,inductor,not user facing,[AOTI][refactor] Consolidate CppBuilder.build and CppBuilder.build_fbcode (#147975),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/147975,desertfire,angelayi,yushangdi,,
ea5d40db73c,skip,not user facing,Address source code building command for Intel GPU support (#143476),README.md,https://github.com/pytorch/pytorch/pull/143476,ZailiWang,EikanWang,malfet,,
9ca871f32bd,skip,not user facing,Remove binaries/benchmark_args.h (#147920),binaries/benchmark_args.h,https://github.com/pytorch/pytorch/pull/147920,cyyever,Skylion007,,,
20ce67cd068,skip,not user facing,"Udpate hw requirement for FP64 on ""Getting Started on Intel GPU"" (#147802)",docs/source/notes/get_start_xpu.rst,https://github.com/pytorch/pytorch/pull/147802,ZhaoqiongZ,malfet,,,
784902983e7,releng,not user facing,Remove +PTX from cuda 12.6 builds (#148000),.ci/manywheel/build_cuda.sh,https://github.com/pytorch/pytorch/pull/148000,atalman,clee2000,ngimel,tinglvv,
ad0c879e220,skip,Untopiced,Support torch.compile rng selective activation checkpointing with cudagraph (#146878),test/dynamo/test_activation_checkpointing.py test/inductor/test_cudagraph_trees.py test/test_hop_infra.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/utils.py torch/_functorch/config.py torch/_functorch/partitioners.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/_inductor/cudagraph_trees.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/output_code.py torch/_inductor/utils.py torch/_prims/rng_prims.py torch/fx/experimental/symbolic_shapes.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/146878,eellison,anijain2305,bdhirsh,,
97ebccaa915,skip,not user facing,Add _fft_r2c as core ATen (#147998),aten/src/ATen/native/native_functions.yaml,https://github.com/pytorch/pytorch/pull/147998,larryliu0820,tugsbayasgalan,,,
84c89a45276,inductor,not user facing,[cutlass backend] cache_clear algorithm select cache on fresh inductor cache (#147590),torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/147590,henrylhtsang,chenyang78,eellison,,
07b7b3ed4e9,skip,not user facing,torch._scaled_mm with MXFP8 (#147548),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDABlas.h aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/TunableGemm.h aten/src/ATen/native/cuda/Blas.cpp test/test_matmul_cuda.py torch/testing/_internal/common_cuda.py,https://github.com/pytorch/pytorch/pull/147548,vkuzo,drisspg,,,
9d3636283bd,inductor,not user facing,[Inductor] Use generic GPU device in test_preserves_strides (#148006),test/inductor/test_triton_kernels.py,https://github.com/pytorch/pytorch/pull/148006,alexbaden,eellison,etaf,,
17358ce7788,skip,Untopiced,"Revert ""Support torch.compile rng selective activation checkpointing with cudagraph (#146878)""",test/dynamo/test_activation_checkpointing.py test/inductor/test_cudagraph_trees.py test/test_hop_infra.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/utils.py torch/_functorch/config.py torch/_functorch/partitioners.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/_inductor/cudagraph_trees.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/output_code.py torch/_inductor/utils.py torch/_prims/rng_prims.py torch/fx/experimental/symbolic_shapes.py torch/testing/_internal/hop_db.py,,,,,,
8cb87229797,skip,not user facing,[inductor][triton] Ignore block ptr advances for removed buffers (#147193),test/inductor/test_torchinductor_strided_blocks.py torch/_inductor/codegen/common.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/147193,kundaMwiza,jansel,,,
e0b93082f1b,distributed,not user facing,Remove HuggingFace reader and writer from __init__.py (#148030),torch/distributed/checkpoint/__init__.py,https://github.com/pytorch/pytorch/pull/148030,ankitageorge,hl475,,,
b6fe28ff02d,inductor,new features,[Inductor] Graph Partition (#147038),test/inductor/test_torchinductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_fx.py torch/_inductor/config.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/147038,BoyuanFeng,eellison,,,
21bd5fe2030,skip,not user facing,Update torch-xpu-ops commit pin (#147968),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/147968,xytintel,Skylion007,,,
c622796cde9,skip,Untopiced,"Revert ""Build a storage reader/writer to write checkpoints in HF format (#147622)""",test/distributed/checkpoint/test_hf_storage.py torch/distributed/checkpoint/_hf_storage.py torch/distributed/checkpoint/filesystem.py,,,,,,
9017becf1d8,inductor,not user facing,Add unique kernel name support for user defined triton kernel (#147587),torch/_inductor/codegen/wrapper.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/147587,muchulee8,desertfire,,,
fd43c36aa94,skip,not user facing,[ca] side-effect free initial trace: RAII PyCompilerInterface (#147891),torch/csrc/dynamo/compiled_autograd.cpp torch/csrc/dynamo/compiled_autograd.h torch/csrc/dynamo/python_compiled_autograd.cpp,https://github.com/pytorch/pytorch/pull/147891,xmfan,jansel,,,
8ee84aa7031,onnx,bug fixes,[ONNX] Fix missed None type support in dyamic shapes string cases (#148025),test/onnx/exporter/test_dynamic_shapes.py torch/onnx/_internal/exporter/_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/148025,titaiwangms,justinchuby,,,
91e7c7945ce,skip,not user facing,[Intel GPU] Avoid unnecessary copy when the dst of Matmul is non-contiguous (#144759),aten/src/ATen/native/mkldnn/xpu/Blas.cpp aten/src/ATen/native/mkldnn/xpu/detail/Attr.h aten/src/ATen/native/mkldnn/xpu/detail/Matmul.cpp aten/src/ATen/native/mkldnn/xpu/detail/QMatmul.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.cpp aten/src/ATen/native/mkldnn/xpu/detail/Utils.h,https://github.com/pytorch/pytorch/pull/144759,jianyizh,EikanWang,,,
26f19539ad8,inductor,improvements,[triton 3.3] cpp_wrapper: add a global_scratch arg (#148051),test/inductor/test_aot_inductor.py tools/amd_build/build_amd.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/device_op_overrides.py torch/_inductor/codegen/xpu/device_op_overrides.py,https://github.com/pytorch/pytorch/pull/148051,davidberard98,YUNQIUGUO,,,
0489a349e72,inductor,not user facing,Skip the logging if the pass cannot be pickled (#148053),torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/148053,houseroad,chenyang78,,,
c5bf9aaf1cc,dynamo,not user facing,Log graph breaks (#146537),test/dynamo/test_metrics_context.py test/dynamo/test_utils.py torch/_dynamo/convert_frame.py torch/_dynamo/metrics_context.py torch/_dynamo/utils.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/146537,Raymo111,c00w,,,
7ae0e0b2eae,skip,not user facing,[aotd] Log torch._functorch.config in tlparse (#147883),test/dynamo/test_structured_trace.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/147883,IvanKobzarev,bdhirsh,jamesjwu,,
71ee17baa1e,releng,not user facing,Smoke Test skip cuda.gds on windows (#148060),.ci/pytorch/smoke_test/smoke_test.py,https://github.com/pytorch/pytorch/pull/148060,atalman,mikaylagawarecki,,,
2c35af4def7,skip,not user facing,[Intel GPU] Avoid including CPU oneDNN header files for Intel GPU (#147969),aten/src/ATen/CMakeLists.txt aten/src/ATen/native/mkldnn/xpu/detail/Utils.cpp cmake/Modules/FindMKLDNN.cmake,https://github.com/pytorch/pytorch/pull/147969,EikanWang,ZhiweiYan-96,atalman,liangan1,
6ccbff1450b,inductor,not user facing,[Inductor] Fix `inductor/test_kernel_benchmark.py` for new Triton; do not duplicate parameters in `_dump_launch_params` (#147746),torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/147746,anmyachev,jansel,,,
e64441915fd,skip,not user facing,Fix overflow in checkInBoundsForStorage (#147352),aten/src/ATen/native/Resize.h test/test_view_ops.py,https://github.com/pytorch/pytorch/pull/147352,mikaylagawarecki,albanD,,,
536bce5a048,skip,not user facing,Make Tensor.set_ validate storage_offset when sizes/strides are unchanged (#147354),aten/src/ATen/native/Resize.h aten/src/ATen/native/TensorShape.cpp c10/core/SymBool.h test/functorch/test_aotdispatch.py test/test_torch.py torch/csrc/autograd/python_torch_functions_manual.cpp,https://github.com/pytorch/pytorch/pull/147354,mikaylagawarecki,albanD,,,
9a1f720a72d,skip,not user facing,Validate inputs to _nested_view_from_buffer to prevent overflows (#147356),aten/src/ATen/native/nested/NestedTensorMath.cpp test/test_nestedtensor.py,https://github.com/pytorch/pytorch/pull/147356,mikaylagawarecki,albanD,jbschlosser,,
ce805a5ba5a,skip,not user facing,[BE/metal] Rename REGISTER_I0_I1 to REGISTER_SPECIAL. (#148036),aten/src/ATen/native/mps/kernels/SpecialOps.metal,https://github.com/pytorch/pytorch/pull/148036,dcci,jansel,malfet,,
f0d00421cfc,inductor,not user facing,[inductor][ck] kBatch filtering with gen_ops (#148004),torch/_inductor/codegen/rocm/ck_universal_gemm_template.py,https://github.com/pytorch/pytorch/pull/148004,coconutruben,ColinPeppler,tenpercent,,
c73a92fbf5b,releng,not user facing,[BE][CI] bump `ruff` to 0.9.2: multiline `assert` statements (#144546),.github/scripts/label_utils.py .lintrunner.toml benchmarks/dynamo/cachebench.py benchmarks/dynamo/common.py benchmarks/dynamo/microbenchmarks/bench_mm_fusion.py benchmarks/dynamo/microbenchmarks/operator_inp_utils.py benchmarks/dynamo/runner.py benchmarks/functional_autograd_benchmark/torchaudio_models.py benchmarks/functional_autograd_benchmark/torchvision_models.py benchmarks/gpt_fast/model.py benchmarks/instruction_counts/core/expand.py benchmarks/operator_benchmark/benchmark_core.py benchmarks/sparse/triton_ops.py benchmarks/transformer/attention_bias_benchmarks.py benchmarks/transformer/score_mod.py functorch/dim/reference.py scripts/compile_tests/download_reports.py scripts/export/update_schema.py test/onnx/dynamo/test_dynamo_with_onnxruntime_backend.py test/onnx/exporter/test_hf_models_e2e.py test/onnx/onnx_test_common.py test/onnx/pytorch_test_common.py test/onnx/test_fx_passes.py test/onnx/test_pytorch_onnx_onnxruntime.py test/onnx/torchlib/ops_test_common.py test/onnx/torchlib/ops_test_data.py test/run_test.py test/test_dataloader.py test/test_foreach.py test/test_nestedtensor.py test/test_optim.py tools/autograd/gen_python_functions.py tools/autograd/gen_variable_type.py tools/autograd/load_derivatives.py tools/code_coverage/package/tool/parser/llvm_coverage_segment.py tools/flight_recorder/components/builder.py tools/flight_recorder/components/config_manager.py tools/flight_recorder/components/loader.py tools/flight_recorder/components/types.py tools/flight_recorder/components/utils.py tools/onnx/gen_diagnostics.py tools/setup_helpers/generate_code.py tools/test/gen_operators_yaml_test.py tools/testing/target_determination/heuristics/interface.py tools/testing/test_run.py tools/testing/test_selections.py torch/_jit_internal.py torch/_lobpcg.py torch/_meta_registrations.py torch/_ops.py torch/_utils.py torch/onnx/_internal/diagnostics/_diagnostic.py torch/onnx/_internal/exporter/_building.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_schemas.py torch/onnx/_internal/fx/fx_onnx_interpreter.py torch/onnx/_internal/fx/passes/readability.py torch/onnx/_internal/fx/passes/type_promotion.py torch/onnx/_internal/fx/passes/virtualization.py torch/onnx/_internal/io_adapter.py torch/onnx/symbolic_helper.py torch/onnx/symbolic_opset14.py torch/onnx/symbolic_opset9.py torch/onnx/verification.py torch/overrides.py torch/serialization.py torchgen/_autoheuristic/ah_tree.py torchgen/_autoheuristic/train_decision.py torchgen/_autoheuristic/train_regression.py torchgen/api/cpp.py torchgen/api/lazy.py torchgen/api/types/signatures.py torchgen/dest/lazy_ir.py torchgen/dest/ufunc.py torchgen/executorch/api/custom_ops.py torchgen/executorch/api/et_cpp.py torchgen/executorch/model.py torchgen/gen.py torchgen/gen_backend_stubs.py torchgen/gen_executorch.py torchgen/local.py torchgen/model.py torchgen/utils.py torchgen/yaml_utils.py,https://github.com/pytorch/pytorch/pull/144546,XuehaiPan,malfet,,,
1845e7d1f59,releng,not user facing,Use nightly-wheel-upload env for triton wheel publishing (#148108),.github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/148108,atalman,malfet,,,
644d84d594b,skip,Untopiced,"Revert ""optimize the decomposition of aten.native_group_norm (#144733)""",test/inductor/test_cpu_repro.py torch/_refs/__init__.py,,,,,,
d48eb58d1db,skip,not user facing,[BE][CI] bump ruff to 0.9.8 (#145606),.lintrunner.toml tools/testing/clickhouse.py,https://github.com/pytorch/pytorch/pull/145606,XuehaiPan,malfet,,,
783d83c5d8b,inductor,not user facing,[PT2] Port fuse_split_getitem_squeeze to PT2 pre_grad passes (#148059),torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/148059,huxintong,frank-wei,,,
10ffd942160,releng,not user facing,Reference the commit explicitly (#148026),.github/scripts/trymerge.py,https://github.com/pytorch/pytorch/pull/148026,ZainRizvi,atalman,malfet,seemethere,
871b3909fca,releng,not user facing,ci: Remove manylinux 2014 remnants (#148028),.ci/docker/manywheel/Dockerfile_2014 .github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/148028,seemethere,atalman,malfet,wdvr,
915b9c80ab5,export,Untopiced,[export] Sync aoti schema to schema.py (#148017),torch/_export/serde/export_schema.thrift torch/_export/serde/schema.py torch/_export/serde/schema.yaml torch/csrc/utils/generated_serialization_types.h,https://github.com/pytorch/pytorch/pull/148017,zhxchen17,yiming0416,,,
f4235310e8d,composability,not user facing,[BE][Ez]: Remove redundant empty tensor copies in meta-reg (#147978),torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/147978,Skylion007,jansel,,,
fc78192b1da,releng,not user facing,ci: Only run CI specific things when in CI (#148126),.ci/docker/manywheel/build.sh,https://github.com/pytorch/pytorch/pull/148126,seemethere,Camyll,atalman,malfet,
2978771c9d7,skip,not user facing,[CI] test upload: better check for if job is rerun disabled tests (#148027),tools/stats/check_disabled_tests.py tools/stats/upload_stats_lib.py,https://github.com/pytorch/pytorch/pull/148027,clee2000,huydhn,,,
40ad5e01dff,distributed (torchelastic),not user facing,Remove NO_MULTIPROCESSING_SPAWN checks (#146705),test/distributed/algorithms/quantization/test_quantization.py test/distributed/elastic/multiprocessing/api_test.py test/distributed/test_c10d_spawn.py test/distributed/test_distributed_spawn.py test/test_cuda.py test/test_dataloader.py test/test_multiprocessing.py test/test_multiprocessing_spawn.py test/test_torch.py torch/testing/_internal/common_utils.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/146705,cyyever,colesbury,,,
5a14ff8aced,releng,bug fixes,Add cufile to list of libraries to preload (#148137),torch/__init__.py,https://github.com/pytorch/pytorch/pull/148137,atalman,malfet,,,
c6d1038aaae,fx,Untopiced,only print GraphModule during fx.Interpreter errors if valid (#148090),torch/fx/interpreter.py,https://github.com/pytorch/pytorch/pull/148090,bdhirsh,jamesjwu,zou3519,,
481a57bc37f,skip,Untopiced,Support torch.compile rng selective activation checkpointing with cudagraph (#146878),test/dynamo/test_activation_checkpointing.py test/inductor/test_cudagraph_trees.py test/test_hop_infra.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py torch/_functorch/_aot_autograd/runtime_wrappers.py torch/_functorch/_aot_autograd/schemas.py torch/_functorch/_aot_autograd/utils.py torch/_functorch/config.py torch/_functorch/partitioners.py torch/_inductor/codegen/wrapper.py torch/_inductor/config.py torch/_inductor/cudagraph_trees.py torch/_inductor/graph.py torch/_inductor/ir.py torch/_inductor/lowering.py torch/_inductor/output_code.py torch/_inductor/utils.py torch/_prims/rng_prims.py torch/fx/experimental/symbolic_shapes.py torch/testing/_internal/hop_db.py,https://github.com/pytorch/pytorch/pull/146878,eellison,anijain2305,bdhirsh,,
30375cb326b,skip,not user facing,Fix minor typo in python_nccl (#148088),torch/csrc/cuda/python_nccl.cpp,https://github.com/pytorch/pytorch/pull/148088,x41lakazam,Skylion007,,,
0edb2da4a43,skip,not user facing,[dynamo] add sourceless builder for `types.MethodType` (#147880),test/dynamo/test_functions.py torch/_dynamo/variables/builder.py,https://github.com/pytorch/pytorch/pull/147880,XuehaiPan,jansel,,,
edc5bf91d2e,skip,not user facing,[Intel GPU] Add synchronize() in torch.utils.benchmark (#147835),torch/utils/benchmark/utils/timer.py,https://github.com/pytorch/pytorch/pull/147835,DDEle,EikanWang,desertfire,,
3ce352e389a,composability,not user facing,[BE][PYFMT] migrate PYFMT for `torch._dynamo` to `ruff format` (#144549),tools/linter/adapters/pyfmt_linter.py torch/_decomp/__init__.py torch/_decomp/decompositions.py torch/_decomp/decompositions_for_jvp.py torch/_dispatch/python.py torch/_dynamo/backends/distributed.py torch/_dynamo/backends/registry.py torch/_dynamo/callback.py torch/_dynamo/compiled_autograd.py torch/_dynamo/comptime.py torch/_dynamo/config.py torch/_dynamo/convert_frame.py torch/_dynamo/debug_utils.py torch/_dynamo/decorators.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/_dynamo/guards.py torch/_dynamo/output_graph.py torch/_dynamo/pgo.py torch/_dynamo/polyfills/itertools.py torch/_dynamo/polyfills/operator.py torch/_dynamo/repro/after_aot.py torch/_dynamo/resume_execution.py torch/_dynamo/side_effects.py torch/_dynamo/source.py torch/_dynamo/symbolic_convert.py torch/_dynamo/testing.py torch/_dynamo/trace_rules.py torch/_dynamo/types.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/constant.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/dicts.py torch/_dynamo/variables/functions.py torch/_dynamo/variables/higher_order_ops.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/nn_module.py torch/_dynamo/variables/optimizer.py torch/_dynamo/variables/tensor.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/torch_function.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/144549,XuehaiPan,jansel,,,
926b7b5027c,skip,Untopiced,"Revert ""Remove NO_MULTIPROCESSING_SPAWN checks (#146705)""",test/distributed/algorithms/quantization/test_quantization.py test/distributed/elastic/multiprocessing/api_test.py test/distributed/test_c10d_spawn.py test/distributed/test_distributed_spawn.py test/test_cuda.py test/test_dataloader.py test/test_multiprocessing.py test/test_multiprocessing_spawn.py test/test_torch.py torch/testing/_internal/common_utils.py torch/testing/_internal/distributed/distributed_test.py,,,,,,
eb9c1273415,dynamo,not user facing,[dynamo][optimizers] Install ID_GUARDED tensors into the Fx graph (#147824),test/dynamo/test_decorators.py test/dynamo/test_subclasses.py torch/_dynamo/output_graph.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/optimizer.py torch/_functorch/aot_autograd.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/147824,anijain2305,jansel,,,
760921a7d88,mps,not user facing,[MPS] Add inductor support for the `entr()` operator. (#148128),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/148128,dcci,jansel,malfet,,
9b7130b8db6,distributed,not user facing,Clean temporary directory at exit (#147813),torch/distributed/nn/jit/instantiator.py,https://github.com/pytorch/pytorch/pull/147813,arthurlw,H-Huang,,,
3b4b23ab0b4,distributed,not user facing,[BE][Ez]: Remove extra copy in dtensor parallel loss (#148096),torch/distributed/tensor/parallel/loss.py,https://github.com/pytorch/pytorch/pull/148096,Skylion007,jansel,wconstab,,
b0dfd242fa1,distributed (torchelastic),not user facing,Remove NO_MULTIPROCESSING_SPAWN checks (#146705),test/distributed/algorithms/quantization/test_quantization.py test/distributed/elastic/multiprocessing/api_test.py test/distributed/test_c10d_spawn.py test/distributed/test_distributed_spawn.py test/test_cuda.py test/test_dataloader.py test/test_multiprocessing.py test/test_multiprocessing_spawn.py test/test_torch.py torch/testing/_internal/common_utils.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/146705,cyyever,colesbury,,,
bc362cc15a1,inductor,not user facing,Move expanded dim require_exact_stride handling to api from sdpa lowering (#148101),torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/148101,eellison,zou3519,,,
4caeede7993,dynamo,not user facing,[dynamo] more better error messages [3/N] (#147494),test/dynamo/test_exceptions.py test/dynamo/test_export.py test/dynamo/test_graph_break_messages.py test/dynamo/test_logging.py torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/lists.py torch/_dynamo/variables/nn_module.py,https://github.com/pytorch/pytorch/pull/147494,williamwen42,jansel,zou3519,,
baba7beed2b,dynamo,not user facing,[dynamo] add context manager debug information to graph breaks (#147872),test/dynamo/test_graph_break_messages.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/ctx_manager.py,https://github.com/pytorch/pytorch/pull/147872,williamwen42,zou3519,,,
ea12fc8a9ff,autograd_frontend,Untopiced,Revert D70262395 (#148164),tools/autograd/gen_autograd_functions.py torch/csrc/autograd/custom_function.h torch/csrc/autograd/function.h torch/csrc/autograd/function_hook.h torch/csrc/autograd/functions/accumulate_grad.cpp torch/csrc/autograd/functions/accumulate_grad.h torch/csrc/autograd/functions/basic_ops.cpp torch/csrc/autograd/functions/basic_ops.h torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/autograd/python_hook.cpp torch/csrc/autograd/python_hook.h torch/csrc/autograd/utils/lambda_post_hook.h torch/csrc/distributed/c10d/reducer.cpp torch/csrc/dynamo/compiled_autograd.h,https://github.com/pytorch/pytorch/pull/148164,wdvr,xmfan,,,
4e160d5fd9c,inductor,not user facing,[triton 3.3] Fix aoti cpp wrapper remaining 5 issue. (following #148051) (#148117),torch/_inductor/codegen/cpp_wrapper_gpu.py,https://github.com/pytorch/pytorch/pull/148117,YUNQIUGUO,chenyang78,davidberard98,,
995df34b193,distributed,not user facing,"[BE][PYFMT] migrate PYFMT for `torch.{distributed,distributions}` to `ruff format` (#144547)",tools/linter/adapters/pyfmt_linter.py torch/distributed/_composable/contract.py torch/distributed/_functional_collectives.py torch/distributed/_shard/sharded_tensor/api.py torch/distributed/_shard/sharded_tensor/reshard.py torch/distributed/_sharded_tensor/__init__.py torch/distributed/_state_dict_utils.py torch/distributed/_symmetric_memory/__init__.py torch/distributed/_tensor/__init__.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/distributed/_tools/mem_tracker.py torch/distributed/_tools/mod_tracker.py torch/distributed/_tools/runtime_estimator.py torch/distributed/_tools/sac_estimator.py torch/distributed/algorithms/ddp_comm_hooks/ddp_zero_hook.py torch/distributed/algorithms/ddp_comm_hooks/default_hooks.py torch/distributed/algorithms/join.py torch/distributed/algorithms/model_averaging/utils.py torch/distributed/checkpoint/default_planner.py torch/distributed/checkpoint/filesystem.py torch/distributed/checkpoint/optimizer.py torch/distributed/checkpoint/planner.py torch/distributed/checkpoint/state_dict.py torch/distributed/checkpoint/state_dict_loader.py torch/distributed/checkpoint/state_dict_saver.py torch/distributed/checkpoint/utils.py torch/distributed/device_mesh.py torch/distributed/distributed_c10d.py torch/distributed/elastic/control_plane.py torch/distributed/elastic/events/__init__.py torch/distributed/elastic/metrics/__init__.py torch/distributed/elastic/metrics/api.py torch/distributed/elastic/multiprocessing/__init__.py torch/distributed/elastic/multiprocessing/api.py torch/distributed/elastic/multiprocessing/errors/__init__.py torch/distributed/elastic/rendezvous/__init__.py torch/distributed/elastic/rendezvous/api.py torch/distributed/elastic/rendezvous/etcd_rendezvous.py torch/distributed/elastic/rendezvous/registry.py torch/distributed/elastic/utils/store.py torch/distributed/fsdp/_common_utils.py torch/distributed/fsdp/_exec_order_utils.py torch/distributed/fsdp/_flat_param.py torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py torch/distributed/fsdp/_fully_shard/_fsdp_common.py torch/distributed/fsdp/_fully_shard/_fsdp_param.py torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py torch/distributed/fsdp/_fully_shard/_fully_shard.py torch/distributed/fsdp/_init_utils.py torch/distributed/fsdp/_optim_utils.py torch/distributed/fsdp/_shard_utils.py torch/distributed/fsdp/_state_dict_utils.py torch/distributed/fsdp/_trace_utils.py torch/distributed/fsdp/_unshard_param_utils.py torch/distributed/fsdp/api.py torch/distributed/fsdp/fully_sharded_data_parallel.py torch/distributed/fsdp/sharded_grad_scaler.py torch/distributed/fsdp/wrap.py torch/distributed/launch.py torch/distributed/nn/api/remote_module.py torch/distributed/nn/jit/instantiator.py torch/distributed/optim/__init__.py torch/distributed/optim/apply_optimizer_in_backward.py torch/distributed/optim/named_optimizer.py torch/distributed/optim/zero_redundancy_optimizer.py torch/distributed/pipelining/_IR.py torch/distributed/pipelining/_backward.py torch/distributed/pipelining/microbatch.py torch/distributed/pipelining/schedules.py torch/distributed/pipelining/stage.py torch/distributed/rendezvous.py torch/distributed/rpc/api.py torch/distributed/rpc/options.py torch/distributed/rpc/server_process_global_profiler.py torch/distributed/run.py torch/distributed/tensor/_api.py torch/distributed/tensor/_dispatch.py torch/distributed/tensor/_op_schema.py torch/distributed/tensor/_ops/_einsum_strategy.py torch/distributed/tensor/_ops/_embedding_ops.py torch/distributed/tensor/_ops/_math_ops.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/_ops/_pointwise_ops.py torch/distributed/tensor/_ops/_view_ops.py torch/distributed/tensor/_ops/utils.py torch/distributed/tensor/_redistribute.py torch/distributed/tensor/_sharding_prop.py torch/distributed/tensor/_utils.py torch/distributed/tensor/debug/_comm_mode.py torch/distributed/tensor/debug/_visualize_sharding.py torch/distributed/tensor/examples/comm_mode_features_example.py torch/distributed/tensor/examples/convnext_example.py torch/distributed/tensor/examples/torchrec_sharding_example.py torch/distributed/tensor/experimental/_attention.py torch/distributed/tensor/experimental/_func_map.py torch/distributed/tensor/experimental/_tp_transform.py torch/distributed/tensor/parallel/fsdp.py torch/distributed/tensor/parallel/input_reshard.py torch/distributed/tensor/parallel/style.py torch/distributed/tensor/placement_types.py torch/distributed/utils.py torch/distributions/bernoulli.py torch/distributions/beta.py torch/distributions/binomial.py torch/distributions/categorical.py torch/distributions/cauchy.py torch/distributions/chi2.py torch/distributions/constraint_registry.py torch/distributions/constraints.py torch/distributions/continuous_bernoulli.py torch/distributions/dirichlet.py torch/distributions/exponential.py torch/distributions/fishersnedecor.py torch/distributions/gamma.py torch/distributions/geometric.py torch/distributions/gumbel.py torch/distributions/half_cauchy.py torch/distributions/half_normal.py torch/distributions/independent.py torch/distributions/inverse_gamma.py torch/distributions/kumaraswamy.py torch/distributions/laplace.py torch/distributions/lkj_cholesky.py torch/distributions/log_normal.py torch/distributions/logistic_normal.py torch/distributions/lowrank_multivariate_normal.py torch/distributions/mixture_same_family.py torch/distributions/multinomial.py torch/distributions/multivariate_normal.py torch/distributions/negative_binomial.py torch/distributions/normal.py torch/distributions/one_hot_categorical.py torch/distributions/pareto.py torch/distributions/poisson.py torch/distributions/relaxed_bernoulli.py torch/distributions/relaxed_categorical.py torch/distributions/studentT.py torch/distributions/transformed_distribution.py torch/distributions/transforms.py torch/distributions/uniform.py torch/distributions/utils.py torch/distributions/weibull.py torch/distributions/wishart.py,https://github.com/pytorch/pytorch/pull/144547,XuehaiPan,kwen2501,,,
3a58a04898c,distributed,not user facing,Build a storage reader/writer to write checkpoints in HF format (#148089),test/distributed/checkpoint/test_hf_storage.py torch/distributed/checkpoint/_hf_storage.py torch/distributed/checkpoint/filesystem.py,https://github.com/pytorch/pytorch/pull/148089,ankitageorge,saumishr,,,
af720cd5a7e,skip,not user facing,[Intel GPU] Decompule Intel GPU oneDNN from other backends (#147926),cmake/Modules/FindMKLDNN.cmake,https://github.com/pytorch/pytorch/pull/147926,ZhiweiYan-96,EikanWang,,,
4106aa33eb2,distributed,not user facing,[dtensor][fix] fix _scaled_dot_product_flash_attention sharding (#148125),test/distributed/tensor/test_attention.py torch/distributed/tensor/_ops/_matrix_ops.py,https://github.com/pytorch/pytorch/pull/148125,XilunWu,fegin,tianyu-l,,
34d726011f4,dynamo,not user facing,[dynamo] update data-dependent branching graph break messages (#147912),test/dynamo/test_export.py test/dynamo/test_graph_break_messages.py test/dynamo/test_repros.py torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/147912,williamwen42,jansel,zou3519,,
1cb4e2df659,inductor,not user facing,[BE][PYFMT] migrate PYFMT for `torch._inductor` to `ruff format` (#144550),tools/linter/adapters/pyfmt_linter.py torch/_inductor/__init__.py torch/_inductor/analyze_preserves_zero_mask.py torch/_inductor/async_compile.py torch/_inductor/choices.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_flex_attention_template.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_grouped_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/codegen/cpp_template.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/cuda_cpp_scheduling.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/cuda_template.py torch/_inductor/codegen/cuda/cutlass_lib_extensions/gemm_operation_extensions.py torch/_inductor/codegen/cuda/cutlass_utils.py torch/_inductor/codegen/cuda/gemm_template.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/memory_planning.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/rocm/ck_conv_template.py torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/codegen/rocm/rocm_cpp_scheduling.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/rocm/rocm_template.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/codegen/triton_utils.py torch/_inductor/codegen/wrapper.py torch/_inductor/comms.py torch/_inductor/compile_fx.py torch/_inductor/compiler_bisector.py torch/_inductor/config.py torch/_inductor/constant_folding.py torch/_inductor/cpp_builder.py torch/_inductor/cudagraph_trees.py torch/_inductor/cudagraph_utils.py torch/_inductor/debug.py torch/_inductor/decomposition.py torch/_inductor/dependencies.py torch/_inductor/dtype_propagation.py torch/_inductor/fuzzer.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/fx_passes/ddp_fusion.py torch/_inductor/fx_passes/efficient_conv_bn_eval.py torch/_inductor/fx_passes/fuse_attention.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/micro_pipeline_tp.py torch/_inductor/fx_passes/mkldnn_fusion.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/pre_grad.py torch/_inductor/fx_passes/quantization.py torch/_inductor/fx_passes/split_cat.py torch/_inductor/fx_utils.py torch/_inductor/graph.py torch/_inductor/index_propagation.py torch/_inductor/ir.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/_inductor/kernel/mm_common.py torch/_inductor/loop_body.py torch/_inductor/lowering.py torch/_inductor/memory.py torch/_inductor/metrics.py torch/_inductor/mkldnn_ir.py torch/_inductor/mkldnn_lowerings.py torch/_inductor/ops_handler.py torch/_inductor/package/package.py torch/_inductor/pattern_matcher.py torch/_inductor/runtime/benchmarking.py torch/_inductor/runtime/runtime_utils.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/_inductor/sizevars.py torch/_inductor/utils.py torch/_inductor/virtualized.py torch/_inductor/wrapper_benchmark.py,https://github.com/pytorch/pytorch/pull/144550,XuehaiPan,jansel,,,
1db3c58faba,releng,not user facing,Remove manylinux 2014 artifacts (#148135),.ci/magma/Makefile .circleci/scripts/binary_populate_env.sh .github/workflows/build-manywheel-images.yml,https://github.com/pytorch/pytorch/pull/148135,atalman,malfet,,,
0a948f705bd,dynamo,not user facing,[Dynamo] Fix `AssertionError` when dynamo traces `torch.functional.xxx()` functions (#148075),test/dynamo/test_functions.py torch/_dynamo/trace_rules.py,https://github.com/pytorch/pytorch/pull/148075,shink,yanboliang,,,
4708cfdbd93,dynamo,not user facing,Support whitelist of dynamic sources (#147979),test/dynamo/test_misc.py torch/_dynamo/variables/builder.py torch/compiler/config.py,https://github.com/pytorch/pytorch/pull/147979,bobrenjc93,Mingming-Ding,,,
e593288859f,releng,not user facing,"ci: Remove manylinux builds for triton, except for XPU (#148129)",.github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/148129,seemethere,Camyll,atalman,huydhn,
b5cd4ac9505,skip,not user facing,[torchgen] Add support for schema with namespace (#148038),tools/test/test_codegen_model.py torchgen/model.py,https://github.com/pytorch/pytorch/pull/148038,larryliu0820,bdhirsh,,,
e5e31050d31,mps,performance,[MPS] Implement linear1d as shader (#148154),aten/src/ATen/native/mps/kernels/UpSample.metal aten/src/ATen/native/mps/operations/UpSample.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/148154,malfet,dcci,,,
790ec756eea,inductor,not user facing,[cutlass backend] Check if len(timings) == len(choices) before skipping precompile (#148050),torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/148050,henrylhtsang,coconutruben,,,
d7650770044,inductor,not user facing,[cutlass backend] Sort the list of ops for better repro (#148047),torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/148047,henrylhtsang,chenyang78,coconutruben,,
2d2f60bdda8,inductor,not user facing,[cond] support mismatched output in inductor (#147567),test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_arrayref.py test/inductor/test_control_flow.py torch/_inductor/codegen/wrapper.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/147567,ydwu4,jansel,,,
982d7ba3ef1,inductor,not user facing,[while_loop][inductor] relax the constraint that all inputs must be on the same device (#148019),test/inductor/test_aot_inductor.py test/inductor/test_aot_inductor_arrayref.py test/inductor/test_control_flow.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/148019,ydwu4,eellison,jansel,,
945e359fc1a,skip,Untopiced,Initial implementation of host memory stats (#147660),aten/src/ATen/core/CachingHostAllocator.h aten/src/ATen/cuda/CachingHostAllocator.cpp aten/src/ATen/cuda/CachingHostAllocator.h aten/src/ATen/test/cuda_caching_host_allocator_test.cpp c10/core/Allocator.h c10/core/CachingDeviceAllocator.h c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDAMallocAsyncAllocator.cpp c10/xpu/XPUCachingAllocator.cpp docs/source/conf.py docs/source/cuda.rst test/test_cuda.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/csrc/cuda/Module.cpp torch/csrc/xpu/Module.cpp torch/cuda/__init__.py torch/cuda/memory.py,https://github.com/pytorch/pytorch/pull/147660,mradmila,ngimel,,,
6e037ac41c0,skip,Untopiced,"[async TP] insert reshape node to handle ""reshape -> scaled mm -> reshape pattern"" in async TP with rowwise scales (#148001)",test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py,https://github.com/pytorch/pytorch/pull/148001,danielvegamyhre,yifuwang,,,
42aeb5d2596,releng,not user facing,Resolve zip file permission issue when uploading artifacts on ROCm MI300 CI runners (#145504),.github/workflows/_rocm-test.yml,https://github.com/pytorch/pytorch/pull/145504,amdfaa,jeffdaily,,,
ebc3f27bf41,skip,Untopiced,"Revert ""[async TP] insert reshape node to handle ""reshape -> scaled mm -> reshape pattern"" in async TP with rowwise scales (#148001)""",test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py,,,,,,
297c00264e5,skip,not user facing,stage 1 of depreate silent fallback of tuning gemm (#147798),torch/_inductor/config.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py,https://github.com/pytorch/pytorch/pull/147798,henrylhtsang,eellison,,,
e0e516c5549,dynamo,not user facing,Don't crash when we call __qualname__ on torch._C.ScriptFunction (#147894),test/dynamo/test_functions.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/147894,c00w,jansel,,,
5a1954eb938,inductor,not user facing,[Inductor-CPU] Fix broken int8 WoQ GEMM AMX implementation in main (#147895),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_micro_gemm.py,https://github.com/pytorch/pytorch/pull/147895,sanchitintel,jgong5,leslie-fang-intel,,
67780845313,inductor,not user facing,[inductor][cutlass] Environment variables for allow/denylist (#148161),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/148161,bertmaher,eellison,henrylhtsang,,
48c55a66ec7,releng,not user facing,[ROCm] Move ROCm unstable MI300 jobs back to stable (#146675),.github/workflows/inductor-rocm-mi300.yml .github/workflows/rocm-mi300.yml .github/workflows/unstable.yml,https://github.com/pytorch/pytorch/pull/146675,amdfaa,jeffdaily,jithunnair-amd,,
6f91720e1c5,inductor,not user facing,[inductor][ck] manual kBatch heuristic (#148118),torch/_inductor/codegen/rocm/ck_universal_gemm_template.py,https://github.com/pytorch/pytorch/pull/148118,coconutruben,ColinPeppler,,,
4995e058bf6,skip,not user facing,[user-triton] handle inline_asm_case (#148043),test/inductor/test_triton_kernels.py torch/_higher_order_ops/triton_kernel_wrap.py torch/testing/_internal/triton_utils.py,https://github.com/pytorch/pytorch/pull/148043,sijiac,zou3519,,,
d174562487c,mps,not user facing,[MPS][BE][EZ] Aggregate macros (#148187),aten/src/ATen/native/mps/kernels/UpSample.metal,https://github.com/pytorch/pytorch/pull/148187,malfet,Skylion007,,,
e74fdbe6d08,inductor,not user facing,[inductor] ignore block ptr advancements for removed buffers (#148087),test/inductor/test_torchinductor.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/148087,kundaMwiza,eellison,jansel,,
40b3e4a358c,dynamo,not user facing,[dynamo] expose code execution strategy to python (#148020),torch/_C/_dynamo/eval_frame.pyi torch/_dynamo/decorators.py torch/_dynamo/eval_frame.py torch/_dynamo/variables/functions.py torch/csrc/dynamo/eval_frame.c torch/csrc/dynamo/eval_frame_cpp.cpp torch/csrc/dynamo/eval_frame_cpp.h,https://github.com/pytorch/pytorch/pull/148020,williamwen42,jansel,,,
83ec7cdcd4a,dynamo,not user facing,Fix recompile reason logging (#148200),torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/148200,bobrenjc93,xmfan,,,
3a69dee955f,skip,not user facing,[Submodule][FlashAttention] Bump to 2.7.4 (#148147),third_party/flash-attention,https://github.com/pytorch/pytorch/pull/148147,drisspg,Skylion007,,,
1ae7cc41ca3,python_frontend,Untopiced,Define `__all__` for `torch.utils.tensorboard` (#147550),torch/utils/tensorboard/__init__.py,https://github.com/pytorch/pytorch/pull/147550,ringohoffman,albanD,,,
83fb974b5d1,jit,not user facing,scriptfunction: Make sure we have valid __name__ and __qualname__ (#147906),test/test_jit.py torch/jit/_script.py,https://github.com/pytorch/pytorch/pull/147906,c00w,jansel,,,
5d297f7a34c,mps,Untopiced,[MPS][BE] Combine two `upsample_kernel_out_template` into one (#148211),aten/src/ATen/native/mps/kernels/UpSample.metal aten/src/ATen/native/mps/operations/UpSample.mm,https://github.com/pytorch/pytorch/pull/148211,malfet,dcci,,,
2544afaa1aa,distributed,Untopiced,[DeviceMesh] Add some documentation for `from_group` API and add a 2D test (#146364),test/distributed/test_device_mesh.py torch/distributed/device_mesh.py,https://github.com/pytorch/pytorch/pull/146364,wz337,fduwjj,,,
230a3b0f839,skip,not user facing,Add cuda 11.8 guard for cufile preload (#148184),torch/__init__.py,https://github.com/pytorch/pytorch/pull/148184,atalman,mikaylagawarecki,,,
338ed67a1e7,inductor,Untopiced,[inductor] Implement max_pool2d_with_indices as a reduction for large window sizes (#147876),test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/ir.py torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/147876,isuruf,eellison,,,
02c5f21541b,inductor,not user facing,[Inductor] fix `AOTInductorTestABICompatibleGpu.test_triton_kernel_weird_param_order` with new Triton (#148011),torch/_inductor/codegen/cpp_wrapper_gpu.py,https://github.com/pytorch/pytorch/pull/148011,anmyachev,davidberard98,,,
08434df1f2f,mps,bug fixes,[MPS] fix empty place holder error for smooth l1 loss (#148133),aten/src/ATen/native/mps/operations/LossOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/148133,Isalia20,malfet,,,
6eff6b28e4d,fx,not user facing,introduce dynamism library (#147981),test/fx/test_dynamism.py torch/fx/experimental/_dynamism.py,https://github.com/pytorch/pytorch/pull/147981,bobrenjc93,pianpwk,wdvr,,
493cd97af5f,skip,not user facing,add skips to test_notifies_oom and test_set_per_process_memory_fraction (#148134),test/test_cuda.py,https://github.com/pytorch/pytorch/pull/148134,Fuzzkatt,eqy,,,
baf1c8fcdcc,skip,Untopiced,"Revert ""introduce dynamism library (#147981)""",test/fx/test_dynamism.py torch/fx/experimental/_dynamism.py,,,,,,
9377a32cd18,inductor,not user facing,[Inductor][NFC] Remove unused functions from `compile_tasks.py` (#147564),torch/_inductor/runtime/compile_tasks.py,https://github.com/pytorch/pytorch/pull/147564,anmyachev,Skylion007,davidberard98,,
c87097e74a2,skip,not user facing,[triton 3.3] Fix inductor/test_profiler.py test (#148230),test/inductor/test_profiler.py,https://github.com/pytorch/pytorch/pull/148230,davidberard98,YUNQIUGUO,drisspg,,
fd16311e7fd,inductor,not user facing,[inductor][subgraph] Plumbing to get ShapeAsConstantBuffer from subgraph to main graph output (#147559),torch/_inductor/graph.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/147559,anijain2305,eellison,,,
b8efebe57d0,skip,Untopiced,"[async TP] insert reshape node to handle ""reshape -> scaled mm -> reshape pattern"" in async TP with rowwise scales (#148001)",test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py,https://github.com/pytorch/pytorch/pull/148001,danielvegamyhre,yifuwang,,,
fe3b9e3764f,inductor,Untopiced,[Inductor] optimize the heuristics of outer loop fusion (#147523),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/147523,jiayisunx,jansel,jgong5,leslie-fang-intel,
191c9bd013c,skip,Untopiced,"Revert ""[async TP] insert reshape node to handle ""reshape -> scaled mm -> reshape pattern"" in async TP with rowwise scales (#148001)""",test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py,,,,,,
2f1b8e0fe2f,distributed,not user facing,[DTensor][Test] Add a test to demonstrate current dtensor view behavior if redistribution happens (#148015),test/distributed/tensor/test_view_ops.py,https://github.com/pytorch/pytorch/pull/148015,wz337,XilunWu,,,
ab78bf5c66d,skip,not user facing,[Break XPU][Inductor UT] Avoid custom op registration conflicts in test_auto_functionalize.py. (#148155),test/inductor/test_auto_functionalize.py,https://github.com/pytorch/pytorch/pull/148155,etaf,EikanWang,jansel,,
762724f3d08,inductor,not user facing,[Break XPU][Inductor] Generalize device-bias code and fix test_graph_partition for XPU (#148178),test/inductor/test_kernel_benchmark.py test/inductor/test_pattern_matcher.py test/inductor/test_torchinductor.py torch/_inductor/fx_passes/post_grad.py,https://github.com/pytorch/pytorch/pull/148178,etaf,EikanWang,benjaminglass1,jansel,
735d7b1af69,skip,not user facing,[EZ][BE] Increase tolerances for interpolate op (#148224),test/test_mps.py,https://github.com/pytorch/pytorch/pull/148224,malfet,Skylion007,dcci,,
3a0c9f7f9d2,mps,Untopiced,[MPS] Fix SDPA crash (#148239),aten/src/ATen/native/mps/operations/Attention.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/148239,malfet,dcci,,,
1c1bf410ecd,fx,not user facing,introduce dynamism library (#147981),test/fx/test_dynamism.py torch/fx/experimental/_dynamism.py,https://github.com/pytorch/pytorch/pull/147981,bobrenjc93,pianpwk,wdvr,,
8bf39202798,skip,not user facing,Remove unneeded Clang-tidy suppression (#148246),aten/src/ATen/core/Tensor.h aten/src/ATen/core/TensorBase.h aten/src/ATen/core/stack.h torch/custom_class.h,https://github.com/pytorch/pytorch/pull/148246,cyyever,Skylion007,,,
d23051f29ba,inductor,Untopiced,[Inductor] Support parallel reduction for GroupNorm (#144020),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py torch/_inductor/metrics.py,https://github.com/pytorch/pytorch/pull/144020,jiayisunx,jansel,jgong5,leslie-fang-intel,
a983b2b11ad,skip,Untopiced,"Revert ""Initial implementation of host memory stats (#147660)""",aten/src/ATen/core/CachingHostAllocator.h aten/src/ATen/cuda/CachingHostAllocator.cpp aten/src/ATen/cuda/CachingHostAllocator.h aten/src/ATen/test/cuda_caching_host_allocator_test.cpp c10/core/Allocator.h c10/core/CachingDeviceAllocator.h c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDAMallocAsyncAllocator.cpp c10/xpu/XPUCachingAllocator.cpp docs/source/conf.py docs/source/cuda.rst test/test_cuda.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/csrc/cuda/Module.cpp torch/csrc/xpu/Module.cpp torch/cuda/__init__.py torch/cuda/memory.py,,,,,,
8e004865dd5,skip,Untopiced,"Revert ""introduce dynamism library (#147981)""",test/fx/test_dynamism.py torch/fx/experimental/_dynamism.py,,,,,,
2b86309da37,skip,not user facing,separate f16 vectorized class from bf16 (#146596),aten/src/ATen/cpu/vec/vec256/vec256.h aten/src/ATen/cpu/vec/vec256/vec256_16bit_float.h aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h aten/src/ATen/cpu/vec/vec256/vec256_half.h,https://github.com/pytorch/pytorch/pull/146596,Ryo-not-rio,malfet,,,
0ff2e6a85a3,inductor,not user facing,Fix None and equal_to_1 arguments issue in Triton kernel generated by AOTI (#148102),test/inductor/test_aot_inductor.py torch/_inductor/ir.py torch/testing/_internal/triton_utils.py,https://github.com/pytorch/pytorch/pull/148102,renganxu,chenyang78,davidberard98,,
5301710b153,cpp_frontend,not user facing,[codemod] Fix unused-value issue in caffe2/aten/src/ATen/cuda/detail/CUDAHooks.cpp +4 (#147555),aten/src/ATen/cuda/detail/CUDAHooks.cpp aten/src/ATen/native/cuda/jit_utils.cpp aten/src/ATen/native/cudnn/RNN.cpp torch/csrc/StorageSharing.cpp torch/csrc/cuda/shared/nvtx.cpp torch/csrc/distributed/c10d/NCCLUtils.cpp torch/csrc/jit/codegen/fuser/cuda/fused_kernel.cpp,https://github.com/pytorch/pytorch/pull/147555,r-barnes,Skylion007,eqy,,
82603fd7d21,fx,not user facing,introduce dynamism library (#147981),test/fx/test_dynamism.py torch/fx/experimental/_dynamism.py,https://github.com/pytorch/pytorch/pull/147981,bobrenjc93,pianpwk,wdvr,,
1919e0de9aa,skip,Untopiced,"Revert ""stage 1 of depreate silent fallback of tuning gemm (#147798)""",torch/_inductor/config.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py,,,,,,
1a6883759d1,skip,not user facing,Fix macro for bit_cast in c10/util/bit_cast.h - one line change (#148265),c10/util/bit_cast.h,https://github.com/pytorch/pytorch/pull/148265,wschin,Skylion007,malfet,,
19de523de65,mps,performance,[MPS] metal unary kernel for sqrt (#148272),aten/src/ATen/native/mps/kernels/UnaryKernel.metal aten/src/ATen/native/mps/operations/UnaryKernel.mm aten/src/ATen/native/mps/operations/UnaryOps.mm aten/src/ATen/native/native_functions.yaml test/test_mps.py,https://github.com/pytorch/pytorch/pull/148272,Isalia20,malfet,,,
ce2f680e000,skip,not user facing,[fr] Added protection against missing stack frames in fr (#148203),tools/flight_recorder/components/types.py,https://github.com/pytorch/pytorch/pull/148203,VieEeEw,fduwjj,,,
de7af81f188,skip,Untopiced,"[async TP] insert reshape node to handle ""reshape -> scaled mm -> reshape pattern"" in async TP with rowwise scales (#148001)",test/distributed/tensor/parallel/test_micro_pipeline_tp.py torch/_inductor/fx_passes/micro_pipeline_tp.py,https://github.com/pytorch/pytorch/pull/148001,danielvegamyhre,yifuwang,,,
6e10471966e,skip,not user facing,[ci] disable cudagraph for tts_angular on dashboard (#148221),benchmarks/dynamo/common.py benchmarks/dynamo/torchbench.py benchmarks/dynamo/torchbench.yaml,https://github.com/pytorch/pytorch/pull/148221,BoyuanFeng,eellison,,,
dae3fbfe972,skip,Untopiced,[c10d] Add hccl distributed backend to c10d data structures (#146478),torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/146478,ankurneog,H-Huang,,,
b59776d8572,skip,not user facing,[import][inductor] Simplify grid handling (#147583),test/inductor/test_aot_inductor.py test/inductor/test_cpp_wrapper_hipify.py test/inductor/test_cuda_repro.py test/inductor/test_kernel_benchmark.py test/inductor/test_max_autotune.py test/inductor/test_profiler.py test/inductor/test_select_algorithm.py test/inductor/test_torchinductor.py test/inductor/test_triton_kernels.py torch/_inductor/autotune_process.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/device_op_overrides.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/codegen/wrapper.py torch/_inductor/codegen/xpu/device_op_overrides.py torch/_inductor/compile_fx.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/ir.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/_inductor/kernel/mm_common.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/utils/_get_clean_triton.py,https://github.com/pytorch/pytorch/pull/147583,jansel,eellison,shunting314,,
26358fa2d83,skip,not user facing,Add AppendingByteSerializer class (#148226),test/test_appending_byte_serializer.py torch/utils/_appending_byte_serializer.py,https://github.com/pytorch/pytorch/pull/148226,oulgen,aorenste,,,
9c506aa8a62,inductor,not user facing,[aotinductor] add option to disable runtime assertions (#146462),test/inductor/test_aot_inductor.py torch/_inductor/config.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/146462,ColinPeppler,22quinn,desertfire,,
1d7397a2d04,skip,not user facing,[Inductor] Avoid tensor slice overflow for large step (#147433),aten/src/ATen/native/TensorShape.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_torchinductor.py torch/_decomp/__init__.py torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/147433,DDEle,,,,
9aa897b9928,sparse_frontend,Untopiced,Remove unnecessary tensor clone (#148159),aten/src/ATen/functorch/BatchRulesScatterOps.cpp aten/src/ATen/native/mps/operations/Inverse.mm aten/src/ATen/native/sparse/SparseCsrTensorMath.cpp aten/src/ATen/native/sparse/SparseTensor.cpp torch/csrc/api/include/torch/nn/module.h torch/csrc/distributed/c10d/reducer.cpp torch/csrc/jit/python/python_ir.cpp torch/csrc/jit/runtime/argument_spec.h torch/csrc/jit/runtime/graph_executor.cpp torch/csrc/jit/tensorexpr/kernel.cpp torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/148159,cyyever,Skylion007,,,
95d81d21a66,mps,Untopiced,[MPS] Speedup interpolation (#148277),aten/src/ATen/native/mps/kernels/UpSample.metal,https://github.com/pytorch/pytorch/pull/148277,malfet,Skylion007,,,
6d70b42810b,skip,not user facing,[BE][Ez]: Update fmt submodule to 11.1.4 (#148264),third_party/fmt,https://github.com/pytorch/pytorch/pull/148264,Skylion007,cyyever,jansel,,
1106eb0212e,skip,not user facing,[BE] Fix extra semicolon warning (#148284),aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h aten/src/ATen/cpu/vec/vec256/vec256_half.h,https://github.com/pytorch/pytorch/pull/148284,malfet,Skylion007,,,
94afb165d94,skip,Untopiced,"Revert ""[c10d] Add hccl distributed backend to c10d data structures (#146478)""",torch/distributed/distributed_c10d.py,,,,,,
edaff88f69f,skip,Untopiced,[fx] Move map_aggregate to C++ (#148243),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_C/__init__.pyi.in torch/_dynamo/polyfills/fx.py torch/_dynamo/polyfills/loader.py torch/csrc/fx/node.cpp torch/fx/graph.py torch/fx/node.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/148243,jansel,oulgen,,,
0135f57f4aa,skip,Untopiced,[fx] Move Node._update_args_kwargs to C++ (#148260),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_C/__init__.pyi.in torch/csrc/fx/node.cpp torch/fx/node.py,https://github.com/pytorch/pytorch/pull/148260,jansel,oulgen,,,
29c2de9ae16,skip,Untopiced,[fx] Move Node._prepend/Node._remove_from_list to C++ (#148261),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_C/__init__.pyi.in torch/csrc/fx/node.cpp torch/fx/node.py,https://github.com/pytorch/pytorch/pull/148261,jansel,oulgen,,,
608377d3418,skip,Untopiced,"Revert ""[import][inductor] Simplify grid handling (#147583)""",test/inductor/test_aot_inductor.py test/inductor/test_cpp_wrapper_hipify.py test/inductor/test_cuda_repro.py test/inductor/test_kernel_benchmark.py test/inductor/test_max_autotune.py test/inductor/test_profiler.py test/inductor/test_select_algorithm.py test/inductor/test_torchinductor.py test/inductor/test_triton_kernels.py torch/_inductor/autotune_process.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/device_op_overrides.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/codegen/wrapper.py torch/_inductor/codegen/xpu/device_op_overrides.py torch/_inductor/compile_fx.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/ir.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/_inductor/kernel/mm_common.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/utils/_get_clean_triton.py,,,,,,
ab81ca50534,inductor,not user facing,[Inductor][CPU] Add GEMM templates for _weight_int4pack_mm_for_cpu with AVX512 (#146756),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/quantized_lowerings.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/146756,Xia-Weiwen,jansel,jgong5,leslie-fang-intel,
b3bb73e11cc,skip,not user facing,Separate transpose from memory load/store and add load size support for convert_to_int32 (#147067),aten/src/ATen/cpu/vec/vec256/vec256_float.h aten/src/ATen/cpu/vec/vec256/vec256_int.h aten/src/ATen/cpu/vec/vec512/vec512_float.h aten/src/ATen/cpu/vec/vec512/vec512_int.h,https://github.com/pytorch/pytorch/pull/147067,CaoE,jansel,leslie-fang-intel,,
6a3a1f96ceb,skip,not user facing,Enable XPU for Inductor MM Triton Kernel Benchmark (#148237),test/inductor/test_kernel_benchmark.py test/inductor/test_select_algorithm.py,https://github.com/pytorch/pytorch/pull/148237,EikanWang,jansel,,,
118a165ac58,inductor,not user facing,[Inductor][CPP] Add transposed B matrix support for CppMicroGemmFP32Vec (#147068),aten/src/ATen/cpu/vec/vec256/vec256_16bit_float.h torch/_inductor/codegen/cpp_micro_gemm.py,https://github.com/pytorch/pytorch/pull/147068,CaoE,jansel,jgong5,leslie-fang-intel,
165e33531c4,inductor,not user facing,[Inductor][CPP] Fix the vec codegen for tanh (#148254),test/inductor/test_cpu_repro.py torch/_inductor/codegen/cpp.py,https://github.com/pytorch/pytorch/pull/148254,leslie-fang-intel,jgong5,malfet,,
6c089f5da34,releng,not user facing,ci: move xpu triton build to manylinux 2.28 (#148195),.github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/148195,chuanqi129,seemethere,,,
d57f6178443,inductor,not user facing,[Inductor][CPP] Avoid transpose with cpp micro-gemm for FlexAttention (#147069),test/inductor/test_flex_attention.py torch/_inductor/codegen/cpp_flex_attention_template.py torch/_inductor/codegen/cpp_micro_gemm.py torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/147069,CaoE,drisspg,jgong5,leslie-fang-intel,
84502baafff,mps,bug fixes,[MPS] Fix sqrt and other for `torch.chalf` (#148285),aten/src/ATen/native/mps/kernels/UnaryKernel.metal test/test_mps.py,https://github.com/pytorch/pytorch/pull/148285,malfet,dcci,,,
3ca1a2564d5,mps,Untopiced,[BE][MPS] Use `copysign` for imaginary part of sqrt (#148286),aten/src/ATen/native/mps/kernels/UnaryKernel.metal,https://github.com/pytorch/pytorch/pull/148286,malfet,dcci,,,
57addfcd580,skip,not user facing,Significantly speed up save_cache_artifacts (#148227),test/inductor/test_codecache.py torch/compiler/_cache.py,https://github.com/pytorch/pytorch/pull/148227,oulgen,jamesjwu,,,
1c544a9ddde,inductor,not user facing,"[Inductor-CPP] If all of the activation scale dims are 1, make it a 0D tensor (#147033)",test/inductor/test_cpu_select_algorithm.py test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/mkldnn_lowerings.py,https://github.com/pytorch/pytorch/pull/147033,sanchitintel,jansel,leslie-fang-intel,,
09291817b29,skip,not user facing,Fix extra semicolon warning (#148291),c10/core/CPUAllocator.cpp c10/core/SymInt.cpp c10/core/TensorImpl.cpp c10/core/impl/PyInterpreter.cpp c10/core/impl/alloc_cpu.cpp c10/core/thread_pool.cpp c10/cuda/CUDACachingAllocator.cpp c10/cuda/impl/CUDAGuardImpl.cpp c10/util/Logging.cpp c10/util/flags_use_no_gflags.cpp c10/util/numa.cpp c10/util/typeid.cpp,https://github.com/pytorch/pytorch/pull/148291,cyyever,Skylion007,,,
a929e11e4ff,export,Untopiced,[dynamic shapes][export] ignore when real-tensor fallback fails (#147779),test/export/test_draft_export.py torch/_subclasses/fake_tensor.py,https://github.com/pytorch/pytorch/pull/147779,pianpwk,bobrenjc93,,,
aade4fbd55a,distributed,Untopiced,Expose the rendezvous keepalive arguments (#145228),torch/distributed/elastic/rendezvous/dynamic_rendezvous.py,https://github.com/pytorch/pytorch/pull/145228,carmocca,wconstab,,,
40c2505f16d,dynamo,not user facing,[logging] Log individual Triton kernel compilation times to dynamo_compile (#147022),test/dynamo/test_metrics_context.py test/dynamo/test_utils.py torch/_dynamo/metrics_context.py torch/_dynamo/utils.py torch/_inductor/async_compile.py torch/_inductor/runtime/compile_tasks.py,https://github.com/pytorch/pytorch/pull/147022,masnesral,jamesjwu,,,
302c6602985,onnx,Untopiced,Consistently use load_torchbind_test_lib in tests (#148082),test/export/test_export.py test/export/test_lift_unlift.py test/jit/test_torchbind.py test/test_weak.py torch/testing/_internal/torchbind_impls.py,https://github.com/pytorch/pytorch/pull/148082,Flamefire,angelayi,,,
d260d4fc554,distributed,not user facing,HSDP custom hook UTs are multi-threaded - can't set device rank (#148099),test/distributed/_composable/fsdp/test_fully_shard_init.py,https://github.com/pytorch/pytorch/pull/148099,pragupta,jeffdaily,,,
b162b1600b7,inductor,not user facing,[Inductor] Hot fix after #148011 (#148270),torch/_inductor/codegen/cpp_wrapper_gpu.py,https://github.com/pytorch/pytorch/pull/148270,anmyachev,davidberard98,,,
1bbe57336bc,dynamo,not user facing,Replace unimplemented with unimplemented_v2 for dynamo (#148158),torch/_dynamo/variables/constant.py,https://github.com/pytorch/pytorch/pull/148158,FFFrog,Skylion007,williamwen42,,
8f361c808b1,dynamo,not user facing,[dynamo] run-only recursively on recompile limit exceeded (#148021),test/dynamo/test_recompile_ux.py torch/_dynamo/convert_frame.py,https://github.com/pytorch/pytorch/pull/148021,williamwen42,anijain2305,,,
07f876e9602,skip,Untopiced,Subprocess compile (#146134),test/fx/test_graph_pickler.py test/inductor/test_compile_subprocess.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/compile_fx_ext.py torch/_inductor/compile_fx_subproc.py torch/_inductor/lowering.py torch/_inductor/virtualized.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/146134,aorenste,jamesjwu,,,
52078154f2f,inductor,not user facing,Add support for no-op concat with padded output (#146866),test/inductor/test_padding.py torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/146866,nandesuka,shunting314,,,
e45040b1d37,skip,Untopiced,[c10d] Add hccl distributed backend to c10d data structures (#146478),torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/146478,ankurneog,H-Huang,,,
a41413829c9,releng,not user facing,Use release notes label for module: distributed_checkpoint (#148352),.github/labeler.yml,https://github.com/pytorch/pytorch/pull/148352,janeyx99,fegin,,,
d0b23e661d1,skip,not user facing,"[cutlass backend] Add main tests for mm, addmm and bmm - step 1 (#148229)",test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/148229,henrylhtsang,chenyang78,,,
5887a2d8dee,build_frontend,bug fixes,[BE] Use `C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED` (#148354),aten/src/ATen/cpu/vec/vec256/vec256_16bit_float.h aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h aten/src/ATen/cpu/vec/vec256/vec256_half.h,https://github.com/pytorch/pytorch/pull/148354,malfet,izaitsevfb,,,
586d8df6510,build_frontend,bug fixes,Fix condition for `CONVERT_NON_VECTORIZED_INIT` invocation (#148362),aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h aten/src/ATen/cpu/vec/vec256/vec256_half.h,https://github.com/pytorch/pytorch/pull/148362,malfet,izaitsevfb,,,
216a108aaf3,releng,not user facing,[ROCm] Add rocm-mi300 and inductor-rocm-mi300 to upload-test-stats.yml (#148365),.github/workflows/upload-test-stats.yml,https://github.com/pytorch/pytorch/pull/148365,ethanwee1,jeffdaily,,,
d43c6f00333,skip,not user facing,[invoke_subgraph] Run joint passes on the hop graphs (#139325),test/dynamo/test_graph_deduplication.py test/higher_order_ops/test_invoke_subgraph.py torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py,https://github.com/pytorch/pytorch/pull/139325,anijain2305,bdhirsh,zou3519,,
0bd2caac55a,releng,not user facing,Docker release - pin buildkit to v0.19.0 (#148372),.github/workflows/docker-release.yml,https://github.com/pytorch/pytorch/pull/148372,atalman,seemethere,,,
b17f5223a45,inductor,Untopiced,Generate AOTI input check by default (#148005),docs/source/torch.compiler_aot_inductor.rst test/inductor/test_aot_inductor.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py,https://github.com/pytorch/pytorch/pull/148005,yushangdi,desertfire,hl475,jingsh,
c21dc11a179,inductor,not user facing,[Intel GPU] Enable SDPA on XPU (#147614),aten/src/ATen/native/mkldnn/xpu/Attention.cpp aten/src/ATen/native/native_functions.yaml test/test_transformers.py torch/_inductor/lowering.py torch/_meta_registrations.py torch/csrc/inductor/aoti_torch/generated/c_shim_cpu.h torch/csrc/inductor/aoti_torch/generated/c_shim_cuda.h torch/csrc/inductor/aoti_torch/generated/c_shim_xpu.h torchgen/aoti/fallback_ops.py torchgen/gen_aoti_c_shim.py,https://github.com/pytorch/pytorch/pull/147614,DDEle,EikanWang,jansel,,
17518007b2b,skip,not user facing,[cutlass backend] Benchmark compared to aten and triton (#148347),benchmarks/inductor_backends/cutlass.py,https://github.com/pytorch/pytorch/pull/148347,henrylhtsang,chenyang78,drisspg,,
a3d69e6e1a5,skip,not user facing,Better log message to update pr_time_benchmarks/expected_results.csv (#148303),benchmarks/dynamo/pr_time_benchmarks/check_results.py,https://github.com/pytorch/pytorch/pull/148303,jansel,Skylion007,,,
5eb0337cfd5,skip,Untopiced,[fx] Optimizations for node name generation (#148288),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/148288,jansel,oulgen,,,
8531d247ba4,skip,Untopiced,[fx] Optimize TracerBase.create_arg and Graph._gen_python_code (#148292),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/fx/graph.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/148292,jansel,oulgen,,,
b7832f03392,skip,not user facing,Enable ASAN in CUDA tests (#147812),test/inductor/test_binary_folding.py test/inductor/test_cudagraph_trees.py test/inductor/test_cudagraph_trees_expandable_segments.py test/inductor/test_gpu_cpp_wrapper.py test/inductor/test_inductor_freezing.py test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/147812,cyyever,janeyx99,,,
98bf2f1170f,distributed (checkpoint),not user facing,Use Python 3.9 typing (#148157),torch/_C/_VariableFunctions.pyi.in torch/_C/_nn.pyi.in torch/_C/return_types.pyi.in torch/distributed/_tools/mem_tracker.py torch/distributed/_tools/sac_estimator.py torch/distributed/checkpoint/state_dict.py torch/distributed/checkpoint/state_dict_loader.py torch/distributed/fsdp/_flat_param.py torch/distributed/fsdp/wrap.py torch/distributed/optim/zero_redundancy_optimizer.py torch/nn/functional.pyi.in torch/nn/modules/module.py torch/overrides.py,https://github.com/pytorch/pytorch/pull/148157,cyyever,janeyx99,,,
ec2805ada81,distributed,Untopiced,Remove outdated CUDA version check (#148142),test/distributed/test_c10d_nccl.py test/test_cuda.py test/test_torch.py torch/cuda/__init__.py torch/nn/parallel/distributed.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/148142,cyyever,eqy,janeyx99,,
70410f93f26,xpu,Untopiced,doc/xpu: align description of SyclExtension with CPP/CUDA (#147988),torch/utils/cpp_extension.py,https://github.com/pytorch/pytorch/pull/147988,dvrogozh,guangyey,janeyx99,,
d54cab78e19,caffe2,not user facing,[codemod] Fix missing field initializer in caffe2/torch/lib/libshm/manager.cpp +1 (#148393),torch/lib/libshm/manager.cpp,https://github.com/pytorch/pytorch/pull/148393,r-barnes,Skylion007,,,
7ab6749ec7d,skip,not user facing,Bump onnxscript to 0.2.2 in CI (#148388),.ci/docker/common/install_onnx.sh .ci/docker/requirements-ci.txt,https://github.com/pytorch/pytorch/pull/148388,justinchuby,malfet,,,
ac99fc7e575,inductor,not user facing,Updates to build rowwise scaled mm kernel on SM10.0a (#148274),aten/src/ATen/native/cuda/RowwiseScaledMM.cu cmake/Codegen.cmake test/inductor/test_fp8.py torch/_inductor/kernel/mm_scaled.py,https://github.com/pytorch/pytorch/pull/148274,danielvegamyhre,drisspg,,,
e0f0db01050,skip,not user facing,updates to benchmarks (#144831),benchmarks/transformer/sdpa.py,https://github.com/pytorch/pytorch/pull/144831,drisspg,danielvegamyhre,,,
da2688f6242,dynamo,Untopiced,Introduce delayed compile via `eager_then_compile` stance (#147983),test/dynamo/test_decorators.py torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/source.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builder.py torch/_guards.py,https://github.com/pytorch/pytorch/pull/147983,bobrenjc93,williamwen42,,,
0b0d28accdc,nn_frontend,not user facing,Optimize param `prepend` class reference `torch.nn.Module` (#148304),torch/nn/modules/module.py,https://github.com/pytorch/pytorch/pull/148304,zeshengzong,Skylion007,,,
f1cce0951b1,skip,not user facing,Create unique test report files for distributed tests (#148325),test/run_test.py,https://github.com/pytorch/pytorch/pull/148325,Flamefire,clee2000,,,
e3e45d90d85,skip,not user facing,[Inductor] Record Triton’s Base32 Cache Key in `.best_config` for Debugging (#147019),test/inductor/test_best_config.py test/run_test.py torch/_inductor/runtime/autotune_cache.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/147019,fulvius31,davidberard98,,,
16d07988fc8,skip,Untopiced,add supports_coalescing property in c10d::Backend  to determine whether backend supports coalescing (#135338),torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/ProcessGroup.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/135338,taozhiwei,albanD,kwen2501,,
fdee60769ac,distributed (checkpoint),not user facing,[DCP] Introduce process based async checkpointing (#147039),docs/source/distributed.checkpoint.rst test/distributed/checkpoint/e2e/test_e2e_save_and_load.py torch/distributed/checkpoint/_async_executor.py torch/distributed/checkpoint/_async_process_executor.py torch/distributed/checkpoint/_async_thread_executor.py torch/distributed/checkpoint/logger.py torch/distributed/checkpoint/state_dict_saver.py,https://github.com/pytorch/pytorch/pull/147039,MeetVadakkanchery,saumishr,,,
f339e41a382,inductor,not user facing,[inductor][triton] Fix average pool nd for int64 dtype (#146061),torch/_inductor/lowering.py,https://github.com/pytorch/pytorch/pull/146061,kundaMwiza,eellison,,,
f2f25a5444b,skip,not user facing,Upgrade submodule oneDNN to v3.7.1 (#148293),test/inductor/test_binary_folding.py third_party/ideep third_party/mkl-dnn.BUILD,https://github.com/pytorch/pytorch/pull/148293,yanbing-j,atalman,mingfeima,,
7fcbaff206d,mps,not user facing,[BE] Remove stale arg for complex ops (#148398),aten/src/ATen/native/mps/kernels/UnaryKernel.metal,https://github.com/pytorch/pytorch/pull/148398,malfet,dcci,,,
67937be6732,mps,not user facing,[BE] Move `sinc` kernels to the same OP family (#148399),aten/src/ATen/native/mps/kernels/UnaryKernel.metal,https://github.com/pytorch/pytorch/pull/148399,malfet,dcci,,,
ed9055c303e,skip,Untopiced,"Revert ""[fx] Optimize TracerBase.create_arg and Graph._gen_python_code (#148292)""",benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/fx/graph.py torch/fx/proxy.py,,,,,,
611b0e9bc4e,skip,Untopiced,"Revert ""[fx] Optimizations for node name generation (#148288)""",benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/fx/graph.py,,,,,,
eea54a55f63,releng,not user facing,ci: Switch manywheel build.sh to just use dev (#148310),.ci/docker/manywheel/build.sh,https://github.com/pytorch/pytorch/pull/148310,seemethere,Camyll,atalman,,
f859722f70d,distributed,Untopiced,[dtensor] refactor sharding prop to handle cross mesh computation (#147869),test/distributed/tensor/test_dtensor.py test/distributed/tensor/test_math_ops.py test/distributed/tensor/test_op_strategy.py torch/distributed/tensor/_dispatch.py torch/distributed/tensor/_op_schema.py torch/distributed/tensor/_ops/_embedding_ops.py torch/distributed/tensor/_ops/_experimental_ops.py torch/distributed/tensor/_ops/_math_ops.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/_ops/_pointwise_ops.py torch/distributed/tensor/_ops/_random_ops.py torch/distributed/tensor/_ops/_tensor_ops.py torch/distributed/tensor/_ops/_view_ops.py torch/distributed/tensor/_sharding_prop.py torch/distributed/tensor/experimental/_attention.py torch/distributed/tensor/experimental/_register_sharding.py,https://github.com/pytorch/pytorch/pull/147869,wanchaol,tianyu-l,,,
e8900fbe4f3,mps,not user facing,[MPS] Add some useful utils (#148448),c10/metal/utils.h,https://github.com/pytorch/pytorch/pull/148448,malfet,Skylion007,dcci,,
c219c5ca38c,skip,not user facing,Fix code descriptions in the test package. (#148145),torch/testing/_comparison.py torch/testing/_internal/common_modules.py torch/testing/_utils.py,https://github.com/pytorch/pytorch/pull/148145,threewebcode,janeyx99,,,
9d196edb7d1,skip,Untopiced,"Revert ""Bump onnxscript to 0.2.2 in CI (#148388)""",.ci/docker/common/install_onnx.sh .ci/docker/requirements-ci.txt,,,,,,
63778cb8a03,skip,Untopiced,"Revert ""[Inductor] Record Triton’s Base32 Cache Key in `.best_config` for Debugging (#147019)""",test/inductor/test_best_config.py test/run_test.py torch/_inductor/runtime/autotune_cache.py torch/_inductor/runtime/triton_heuristics.py,,,,,,
6fb18ff6850,skip,Untopiced,"Revert ""Better log message to update pr_time_benchmarks/expected_results.csv (#148303)""",benchmarks/dynamo/pr_time_benchmarks/check_results.py,,,,,,
97b9e68bc65,skip,Untopiced,"Revert ""[fx] Move Node._prepend/Node._remove_from_list to C++ (#148261)""",benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_C/__init__.pyi.in torch/csrc/fx/node.cpp torch/fx/node.py,,,,,,
17d003fe75c,skip,Untopiced,"Revert ""[fx] Move Node._update_args_kwargs to C++ (#148260)""",benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_C/__init__.pyi.in torch/csrc/fx/node.cpp torch/fx/node.py,,,,,,
92beda54c87,skip,Untopiced,"Revert ""[fx] Move map_aggregate to C++ (#148243)""",benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_C/__init__.pyi.in torch/_dynamo/polyfills/fx.py torch/_dynamo/polyfills/loader.py torch/csrc/fx/node.cpp torch/fx/graph.py torch/fx/node.py torch/fx/proxy.py,,,,,,
439395c0ae0,mps,improvements,[MPS] add slogdet and logdet implementations to mps (#148287),aten/src/ATen/native/native_functions.yaml test/test_mps.py,https://github.com/pytorch/pytorch/pull/148287,Isalia20,malfet,,,
e4c558be1d7,dynamo,not user facing,[scan] Corrections for scan (#146110),test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/scan.py,https://github.com/pytorch/pytorch/pull/146110,bohnstingl,ydwu4,,,
60205b0eb26,fx,Untopiced,[export] Fix logging so that it doesn't result in max recursion error (#148231),torch/export/_draft_export.py torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/148231,angelayi,yiming0416,,,
f30776c37a5,releng,Untopiced,[BE] Upgrade to mypy 1.14 (#145966),.ci/docker/requirements-ci.txt .lintrunner.toml tools/code_coverage/package/util/setting.py tools/pyi/gen_pyi.py torch/_C/__init__.pyi.in torch/_C/_distributed_c10d.pyi torch/_C/_dynamo/eval_frame.pyi torch/_C/_functorch.pyi torch/_export/utils.py torch/ao/pruning/_experimental/pruner/base_structured_sparsifier.py torch/cuda/gds.py torch/distributed/optim/zero_redundancy_optimizer.pyi,https://github.com/pytorch/pytorch/pull/145966,ZainRizvi,Skylion007,,,
d1abde11ec7,dynamo,not user facing,[dynamo] Support passing arguments to `DeviceMesh.get_group` (#147741),test/distributed/tensor/test_dtensor_compile.py torch/_dynamo/variables/distributed.py,https://github.com/pytorch/pytorch/pull/147741,danthe3rd,StrongerXi,,,
9be8f741563,releng,not user facing,ci: Consolidate commit hash updates into a matrix (#148466),.github/workflows/nightly.yml,https://github.com/pytorch/pytorch/pull/148466,seemethere,Camyll,atalman,clee2000,
d290186ed32,releng,not user facing,ci: Add triton to update hash workflow (#148472),.github/workflows/nightly.yml,https://github.com/pytorch/pytorch/pull/148472,seemethere,Camyll,atalman,,
84961a0c175,releng,not user facing,ci: Add workflow dispatch for commit hash update (#148486),.github/workflows/nightly.yml,https://github.com/pytorch/pytorch/pull/148486,seemethere,clee2000,,,
c677f3251f4,fx,Untopiced,[export] don't use unbacked_renamings in export (#147574),test/export/test_export.py test/export/test_serialize.py torch/_export/serde/serialize.py torch/export/_trace.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/147574,pianpwk,avikchaudhuri,,,
70d0e1b96a9,skip,not user facing,Bump onnxscript to 0.2.2 in CI (#148388),.ci/docker/common/install_onnx.sh .ci/docker/requirements-ci.txt,https://github.com/pytorch/pytorch/pull/148388,justinchuby,malfet,,,
842ffea4455,mps,Untopiced,[MPS][BE] Towards strided unary ops support (#148449),aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/kernels/SpecialOps.metal aten/src/ATen/native/mps/kernels/UnaryKernel.metal c10/metal/indexing.h,https://github.com/pytorch/pytorch/pull/148449,malfet,Skylion007,dcci,,
5f47b7e268b,linalg_frontend,not user facing,[ROCm][TunableOp] Unit test for offline tuning of GEMM with bias (#148371),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/148371,naromero77amd,jeffdaily,,,
d789c22712a,releng,not user facing,Upgrade github ubuntu-20.04 runners to ubuntu-24.04 (#148469),.github/actionlint.yaml .github/workflows/tryrebase.yml .github/workflows/update-viablestrict.yml,https://github.com/pytorch/pytorch/pull/148469,clee2000,malfet,seemethere,,
93e9daed542,skip,not user facing,[cuDNN][SDPA][Nested Tensor] Experimental cuDNN Nested Tensor SDPA Support (forward only) (#141178),aten/src/ATen/cuda/detail/UnpackRaw.cuh aten/src/ATen/native/cudnn/MHA.cpp aten/src/ATen/native/cudnn/MHA.h aten/src/ATen/native/native_functions.yaml aten/src/ATen/native/nested/NestedTensorTransformerUtils.h aten/src/ATen/native/nested/cuda/NestedTensorTransformerFunctions.cpp aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/test_nestedtensor.py test/test_transformers.py torch/nested/_internal/sdpa.py,https://github.com/pytorch/pytorch/pull/141178,eqy,jbschlosser,,,
ade4af8c954,mps,not user facing,[MPS][BE] Fix `c10::metal::sinc` implementation (#148471),aten/src/ATen/native/mps/kernels/UnaryKernel.metal c10/metal/special_math.h,https://github.com/pytorch/pytorch/pull/148471,malfet,dcci,,,
0c0a4baddd7,mps,performance,[MPS] unary kernels - avoid copying tensors if they have same stride (#148350),aten/src/ATen/native/mps/OperationUtils.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/148350,Isalia20,janeyx99,,,
c98c3af421d,inductor,not user facing,Add a couple config options to compiler bisector (#148450),torch/_inductor/compiler_bisector.py,https://github.com/pytorch/pytorch/pull/148450,eellison,shunting314,,,
84b58bd63e7,distributed,Untopiced,Enable FSDP tests on XPU device (#147518),test/distributed/fsdp/test_fsdp_apply.py test/distributed/fsdp/test_fsdp_checkpoint.py test/distributed/fsdp/test_fsdp_clip_grad_norm.py test/distributed/fsdp/test_fsdp_comm.py test/distributed/fsdp/test_fsdp_core.py test/distributed/fsdp/test_fsdp_dtensor_state_dict.py test/distributed/fsdp/test_fsdp_exec_order.py test/distributed/fsdp/test_fsdp_fine_tune.py test/distributed/fsdp/test_fsdp_fx.py test/distributed/fsdp/test_fsdp_input.py test/distributed/fsdp/test_fsdp_multiple_forward.py test/distributed/fsdp/test_fsdp_multiple_wrapping.py test/distributed/fsdp/test_fsdp_pure_fp16.py test/distributed/fsdp/test_fsdp_traversal.py test/distributed/fsdp/test_fsdp_uneven.py test/distributed/fsdp/test_hsdp_dtensor_state_dict.py test/distributed/fsdp/test_utils.py torch/testing/_internal/common_distributed.py torch/testing/_internal/common_fsdp.py torch/testing/_internal/distributed/_tensor/common_dtensor.py,https://github.com/pytorch/pytorch/pull/147518,zhangxiaoli73,weifengpy,,,
e02a2ca07a6,distributed,Untopiced,Fix dist.init_process_group on windows (#148266),torch/csrc/distributed/c10d/TCPStore.cpp torch/distributed/rendezvous.py,https://github.com/pytorch/pytorch/pull/148266,H-Huang,d4l3k,,,
a907b6abae0,dynamo,Untopiced,[compiled_autograd] workaround windows compilation issue (#148454),torch/csrc/dynamo/compiled_autograd.h,https://github.com/pytorch/pytorch/pull/148454,zou3519,atalman,xmfan,,
cf5e3f3cea7,cuda,Untopiced,Add cutlass kernel for rowwise scaled mm on sm100 (#148421),aten/src/ATen/native/cuda/RowwiseScaledMM.cu,https://github.com/pytorch/pytorch/pull/148421,danielvegamyhre,drisspg,,,
b5873292c65,profiler,bug fixes,Add overload names to profiler trace (#143114),aten/src/ATen/record_function.cpp aten/src/ATen/record_function.h test/profiler/test_profiler.py torch/_C/_autograd.pyi torch/autograd/profiler.py torch/autograd/profiler_util.py torch/csrc/autograd/init.cpp torch/csrc/autograd/profiler_kineto.cpp torch/csrc/autograd/profiler_kineto.h torch/csrc/profiler/collection.cpp torch/csrc/profiler/collection.h torch/csrc/profiler/orchestration/observer.cpp torch/csrc/profiler/orchestration/observer.h torch/csrc/profiler/python/init.cpp torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/143114,kundaMwiza,sraikund16,,,
713a504a828,dynamo,not user facing,[dynamo][guards] Fix mem leak caused be refcount increment (#148480),torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/148480,anijain2305,StrongerXi,williamwen42,xmfan,
2927a64357b,inductor,not user facing,[inductor][cpu] Fix error with FlexibleLayout weights in  BMM (#148188),test/inductor/test_cpu_select_algorithm.py torch/_inductor/codegen/cpp_gemm_template.py,https://github.com/pytorch/pytorch/pull/148188,frost-intel,jgong5,leslie-fang-intel,,
b28cbe5db39,dynamo,not user facing,[dynamo] remove internal stack trace for fullgraph=True graph breaks (#148205),test/dynamo/test_graph_break_messages.py torch/_dynamo/eval_frame.py torch/_dynamo/exc.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/148205,williamwen42,zou3519,,,
3985ce0b886,dynamo,not user facing,[dynamo] rename test_graph_break_messages -> test_error_messages (#148220),.flake8 test/dynamo/test_error_messages.py test/dynamo/test_graph_break_messages.py,https://github.com/pytorch/pytorch/pull/148220,williamwen42,jansel,zou3519,,
edc3ca577e1,profiler,not user facing,[Profiler] Add profiler activity for HPU devices (#148182),torch/_C/_profiler.pyi torch/autograd/profiler.py torch/csrc/autograd/init.cpp torch/csrc/profiler/kineto_shim.cpp torch/csrc/profiler/orchestration/observer.h torch/csrc/profiler/python/init.cpp torch/profiler/profiler.py,https://github.com/pytorch/pytorch/pull/148182,wdziurdz,sraikund16,,,
1673bc76107,inductor,not user facing,[mm_logs][ez] dump tuned mm info at lowering stage (#148363),torch/_inductor/kernel/bmm.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_scaled.py,https://github.com/pytorch/pytorch/pull/148363,YUNQIUGUO,henrylhtsang,,,
e0ea5939747,releng,not user facing,[CD] Upgrade Windows xpu support package to 2025.0.1 for binary compression (#148313),.ci/pytorch/windows/internal/xpu_install.bat,https://github.com/pytorch/pytorch/pull/148313,chuanqi129,atalman,,,
ed8ec0cb985,inductor,not user facing,[cutlass backend][BE] Fix two small things in cutlass backend standalone debugger (#148493),torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/148493,henrylhtsang,chenyang78,,,
913356fb414,fx,not user facing,Fix recent regression in evaluate_expr that effect cache lookups (#147836),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/fx/experimental/sym_node.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/147836,laithsakka,oulgen,,,
b020d166f2e,skip,not user facing,stage 1 of depreate silent fallback of tuning gemm (#147798),test/inductor/test_cutlass_backend.py torch/_inductor/config.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_scaled.py,https://github.com/pytorch/pytorch/pull/147798,henrylhtsang,eellison,,,
df7e43e5d47,inductor,not user facing,[AOTI] Fix aot_inductor_package test errors (#148279),setup.py torch/_inductor/codecache.py torch/_inductor/cpp_builder.py torch/_inductor/package/package.py torch/csrc/inductor/aoti_package/model_package_loader.cpp,https://github.com/pytorch/pytorch/pull/148279,desertfire,zoranzhao,,,
fb1b7ec173a,optim,Untopiced,Remove deprecate method and attirbute in `LRScheduler` (#147301),test/optim/test_lrscheduler.py torch/optim/lr_scheduler.py,https://github.com/pytorch/pytorch/pull/147301,zeshengzong,janeyx99,,,
2295efa1b35,inductor,not user facing,Fix only logging ir_post_fusion with torch_compile_debug enabled (#148499),torch/_inductor/debug.py torch/_inductor/scheduler.py,https://github.com/pytorch/pytorch/pull/148499,eellison,shunting314,,,
6c3492b4911,releng,not user facing,[ROCm] Enable mi300-specific workflows to be triggered on PRs (#147904),.github/pytorch-probot.yml .github/workflows/inductor-perf-test-nightly-rocm.yml .github/workflows/inductor-rocm-mi300.yml .github/workflows/rocm-mi300.yml,https://github.com/pytorch/pytorch/pull/147904,jithunnair-amd,huydhn,,,
0ef2e938d0a,skip,not user facing,[ROCm] [TunableOp] Track top solutions during tuning process (#147243),aten/src/ATen/cuda/tunable/TunableOp.h,https://github.com/pytorch/pytorch/pull/147243,naromero77amd,jeffdaily,,,
96afa8a2bb7,skip,not user facing,[TEST][SPARSE] Simplify branching in test_cusparselt_backend (#148318),test/test_sparse_semi_structured.py,https://github.com/pytorch/pytorch/pull/148318,Aidyn-A,jcaip,,,
19a6cf35f6b,skip,not user facing,add input shape check for _local_scalar_dense (#145717),aten/src/ATen/native/Scalar.cpp test/test_torch.py,https://github.com/pytorch/pytorch/pull/145717,jiayisunx,malfet,,,
8274da93126,distributed,Untopiced,[c10d][PGNCCL] Fix capturability of isend and irecv (#148462),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/148462,Aidyn-A,kwen2501,,,
864b75dd501,mps,bug fixes,[MPS] Fix unary_kernel_strided logic (#148512),aten/src/ATen/native/mps/OperationUtils.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/148512,malfet,janeyx99,,,
70c5edb697b,skip,not user facing,[ROCm] fix CK compile for gfx1200 (#148496),aten/src/ATen/native/hip/ck_types.h,https://github.com/pytorch/pytorch/pull/148496,alugorey,jeffdaily,,,
c65ee728f06,skip,Untopiced,Initial implementation of host memory stats (#147660),aten/src/ATen/core/CachingHostAllocator.h aten/src/ATen/cuda/CachingHostAllocator.cpp aten/src/ATen/cuda/CachingHostAllocator.h aten/src/ATen/test/cuda_caching_host_allocator_test.cpp c10/core/Allocator.h c10/core/CachingDeviceAllocator.h c10/cuda/CUDACachingAllocator.cpp c10/cuda/CUDAMallocAsyncAllocator.cpp c10/xpu/XPUCachingAllocator.cpp docs/source/conf.py docs/source/cuda.rst test/test_cuda.py torch/_C/__init__.pyi.in torch/_dynamo/trace_rules.py torch/csrc/cuda/Module.cpp torch/csrc/xpu/Module.cpp torch/cuda/__init__.py torch/cuda/memory.py,https://github.com/pytorch/pytorch/pull/147660,mradmila,ngimel,,,
38479e495e4,skip,not user facing,Add note to get start xpu (#148168),docs/source/notes/get_start_xpu.rst,https://github.com/pytorch/pytorch/pull/148168,ZhaoqiongZ,janeyx99,,,
e555c4d8ae6,inductor,not user facing,Fix bug in AOTI lowering (#148364),test/inductor/test_aot_inductor.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/148364,tugsbayasgalan,desertfire,,,
9db9593bbaf,composability,not user facing,Add some more meta kernels (#147862),torch/_meta_registrations.py,https://github.com/pytorch/pytorch/pull/147862,angelayi,zou3519,,,
3f4311d5897,releng,not user facing,[CD] Upgrade xpu runtime pypi packages version and enable windows kineto again (#148319),.circleci/scripts/binary_windows_build.sh .github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/148319,chuanqi129,atalman,xuhancn,,
9dd46a9233e,releng,not user facing,Deprecate sm70 for cuda 12.8 binary (#147607),.ci/manywheel/build_cuda.sh,https://github.com/pytorch/pytorch/pull/147607,tinglvv,atalman,,,
9eef457c024,context parallel,not user facing,[dtensor] add aten._scaled_dot_product_cudnn_attention.default op support (#148377),test/distributed/tensor/test_attention.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/experimental/_attention.py,https://github.com/pytorch/pytorch/pull/148377,XilunWu,drisspg,,,
8af79b7ec81,skip,not user facing,[ROCm] Bump AOTriton to 0.9.1b (#148433),aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/hip/aotriton_adapter.h aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip cmake/External/aotriton.cmake test/test_transformers.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/148433,xinyazhang,jeffdaily,,,
50e827b3dfb,onnx,not user facing,[ONNX] Create VerificationInterpreter (#148396),test/onnx/exporter/test_verification.py torch/onnx/_internal/exporter/_onnx_program.py torch/onnx/_internal/exporter/_verification.py,https://github.com/pytorch/pytorch/pull/148396,justinchuby,titaiwangms,,,
c5d92edd5ac,dynamo,not user facing,[dynamo] WeakRefVar reconstruct (#148083),test/dynamo/test_repros.py torch/_dynamo/variables/misc.py,https://github.com/pytorch/pytorch/pull/148083,IvanKobzarev,anijain2305,,,
c9edd37ffb8,skip,Untopiced,"Revert ""[dtensor] add aten._scaled_dot_product_cudnn_attention.default op support (#148377)""",test/distributed/tensor/test_attention.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/experimental/_attention.py,,,,,,
c6a05df1743,onnx,not user facing,[ONNX] Use onnxscript apis for 2.7 (#148453),torch/onnx/_internal/_exporter_legacy.py torch/onnx/_internal/_lazy_import.py torch/onnx/_internal/exporter/_compat.py torch/onnx/_internal/exporter/_registration.py torch/onnx/_internal/fx/fx_onnx_interpreter.py,https://github.com/pytorch/pytorch/pull/148453,justinchuby,shubhambhokare1,titaiwangms,,
98458e5c81e,releng,Untopiced,Add a docstring to build.sh (#144566),.ci/docker/build.sh,,,,,,
9efa9c73f6d,distributed,not user facing,[Dyamo] Replace unimplemented with unimplemented_v2 for variables/distributed (#148500),torch/_dynamo/variables/distributed.py,https://github.com/pytorch/pytorch/pull/148500,yanboliang,williamwen42,,,
dd6ec8706e7,releng,not user facing,[BE] Relax sympy dependency to 1.13.3 or newer (#148575),requirements.txt setup.py,https://github.com/pytorch/pytorch/pull/148575,malfet,ZainRizvi,atalman,,
ed9624ee600,fx,Untopiced,[export] Fix AttrProxy slicing (#148507),test/export/test_export.py torch/_logging/_registrations.py torch/fx/experimental/proxy_tensor.py,https://github.com/pytorch/pytorch/pull/148507,angelayi,zhxchen17,,,
4aeca28137d,skip,not user facing,[cutlass backend] fix assertion that prevent self multiplication  (#148233),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cuda_kernel.py,https://github.com/pytorch/pytorch/pull/148233,henrylhtsang,ColinPeppler,,,
a10f577ee09,dynamo,not user facing,[dynamo] Account for function id reuse in relevant Dynamo decorators (#148385),test/dynamo/test_decorators.py torch/_dynamo/decorators.py,https://github.com/pytorch/pytorch/pull/148385,StrongerXi,zou3519,,,
ad9a10aff05,dynamo,not user facing,[dynamo] Make `nonstrict_trace` work with some `pytree.register_constant`-ed instances (#148007),test/dynamo/test_decorators.py test/dynamo/test_flat_apply.py test/export/test_export.py test/test_pytree.py torch/_dynamo/guards.py torch/_dynamo/variables/base.py torch/_dynamo/variables/torch.py torch/_dynamo/variables/user_defined.py torch/_higher_order_ops/flat_apply.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/148007,StrongerXi,zou3519,,,
87bd3471fff,distributed,Untopiced,[c10d] Move record param for init to the right place (#148571),torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/148571,fduwjj,kwen2501,,,
10354e146fa,skip,not user facing,Re-enable test_torchinductor:test_buffer_batch_norm (#148573),test/inductor/test_torchinductor.py,https://github.com/pytorch/pytorch/pull/148573,masnesral,StrongerXi,,,
2a639ce1d7a,distributed (checkpoint),not user facing,Add new hf storage class  to torch.distributed package (#148361),torch/distributed/checkpoint/__init__.py,https://github.com/pytorch/pytorch/pull/148361,ankitageorge,MeetVadakkanchery,,,
9e755a1c035,releng,not user facing,[ROCm] add gfx12 to nightly wheels (#148562),.ci/docker/libtorch/build.sh .ci/docker/manywheel/build.sh,https://github.com/pytorch/pytorch/pull/148562,alugorey,jeffdaily,,,
5ccd659c0e5,composability,not user facing,Fix decomp for linspace (#147997),torch/_refs/__init__.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/147997,tugsbayasgalan,zou3519,,,
e1dee4ccb3d,onnx,not user facing,[ONNX] Assert capture strategy in tests (#148348),test/onnx/exporter/test_api.py test/onnx/exporter/test_hf_models_e2e.py test/onnx/exporter/test_small_models_e2e.py torch/onnx/_internal/exporter/_capture_strategies.py torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_onnx_program.py torch/onnx/_internal/exporter/_testing.py,https://github.com/pytorch/pytorch/pull/148348,justinchuby,shubhambhokare1,titaiwangms,,
897fd9b5142,skip,Untopiced,"Revert ""Subprocess compile (#146134)""",test/fx/test_graph_pickler.py test/inductor/test_compile_subprocess.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/compile_fx_ext.py torch/_inductor/compile_fx_subproc.py torch/_inductor/lowering.py torch/_inductor/virtualized.py torch/testing/_internal/common_utils.py,,,,,,
d6d670ab4d7,inductor,not user facing,"[AOTI] build CPU CPP kernels at O3, and all other code at O1 (#148587)",test/inductor/test_aot_inductor_custom_ops.py test/inductor/test_cpu_repro.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_utils.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/wrapper.py torch/_inductor/compile_fx.py torch/_inductor/cpp_builder.py torch/_inductor/graph.py torch/_inductor/scheduler.py torch/_inductor/utils.py torch/csrc/inductor/aoti_package/model_package_loader.cpp,https://github.com/pytorch/pytorch/pull/148587,benjaminglass1,desertfire,,,
690fc2c876e,dynamo,not user facing,Add aot_eager_then_compile stance (#148509),test/dynamo/test_decorators.py torch/_dynamo/eval_frame.py,https://github.com/pytorch/pytorch/pull/148509,bobrenjc93,williamwen42,,,
703176e538e,skip,not user facing,[ROCm] Fix sort for non-standard bool (#147459),aten/src/ATen/native/cuda/Sort.cpp torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/147459,pragupta,jeffdaily,pruthvistony,,
6cc3e691038,inductor,not user facing,[inductor] use eager stride for custom op if no tags (#148367),test/inductor/test_torchinductor.py torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/148367,shunting314,eellison,zou3519,,
23441492f6f,dynamo,Untopiced,[scan] Refactoring of input checking and dynamo invocation (#142125),test/functorch/test_control_flow.py torch/_dynamo/variables/higher_order_ops.py torch/_higher_order_ops/scan.py torch/_higher_order_ops/utils.py torch/_higher_order_ops/while_loop.py,https://github.com/pytorch/pytorch/pull/142125,bohnstingl,ydwu4,,,
63fbc738dca,profiler,Untopiced,[Easy/Profiler] Add last entry to truncated values (#148576),torch/csrc/profiler/util.cpp,https://github.com/pytorch/pytorch/pull/148576,sraikund16,davidberard98,,,
32715a23111,inductor,not user facing,[inductor][ck] add kBatch_sweep to config.rocm (#148223),torch/_inductor/codegen/rocm/ck_universal_gemm_template.py torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/148223,coconutruben,ColinPeppler,,,
38b3375a81d,inductor,not user facing,"[MTIA] Use ""ieee"" instead of ""tf32"" for MTIA's default precision in FlexAttention (#148565)",torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/148565,PatriceVignola,mortzur,,,
43e1284c969,composability,not user facing,Fix empty matrix handling of addmv in inductor (#143792),test/inductor/test_torchinductor.py torch/_decomp/decompositions.py,https://github.com/pytorch/pytorch/pull/143792,maybeLee,jansel,jgong5,leslie-fang-intel,
1433bc14552,skip,not user facing,Remove CAFFE2_USE_EXCEPTION_PTR (#147247),build.bzl caffe2/core/macros.h.in cmake/MiscCheck.cmake,https://github.com/pytorch/pytorch/pull/147247,cyyever,janeyx99,,,
8728d4b8159,inductor,not user facing,Clear triton kernels after parent make_launcher (#148604),test/dynamo/test_aot_autograd_cache.py torch/_inductor/async_compile.py,https://github.com/pytorch/pytorch/pull/148604,jamesjwu,masnesral,,,
02e1580e390,mps,bug fixes,[MPS] fix crash for mse loss with 0 numel inputs (#148608),aten/src/ATen/native/mps/operations/LossOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/148608,Isalia20,malfet,,,
ad49cfc9f0a,skip,not user facing,[cutlass backend] Forward fix for less aligned gemm shapes (#148521),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/148521,henrylhtsang,ColinPeppler,,,
8b65d522e10,dynamo,not user facing,refactor delayed compile to use code context (#148530),torch/_dynamo/convert_frame.py torch/_dynamo/eval_frame.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/148530,bobrenjc93,williamwen42,,,
f057206fca1,onnx,Untopiced,[ONNX] Support complex comparison when verify=True (#148619),torch/onnx/_internal/exporter/_verification.py,https://github.com/pytorch/pytorch/pull/148619,titaiwangms,justinchuby,,,
1fac47702e8,skip,not user facing,[Break XPU][Inductor UT] Generalize device-bias code introduced by #146866. (#148534),test/inductor/test_padding.py,https://github.com/pytorch/pytorch/pull/148534,etaf,nandesuka,,,
4dc956a1d81,inductor,not user facing,[Inductor][Triton] Fix test_autotune_inplace_kernel to work with newer Triton version (#148595),test/inductor/test_cuda_repro.py,https://github.com/pytorch/pytorch/pull/148595,PaulZhang12,davidberard98,,,
ae6bb584838,skip,Untopiced,"Revert ""[cutlass backend] Forward fix for less aligned gemm shapes (#148521)""",test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/gemm_template.py,,,,,,
e7bc1d1791c,onnx,improvements,[ONNX] Update saved exported program in debugging report if the exporting passes run_decomposition() (#148617),torch/onnx/_internal/exporter/_core.py,https://github.com/pytorch/pytorch/pull/148617,titaiwangms,justinchuby,,,
79aa17489c3,dynamo,Untopiced,[dynamo] ctx_manager.py: replace unimplemented with unimplemented_v2 (#148570),torch/_dynamo/variables/ctx_manager.py,https://github.com/pytorch/pytorch/pull/148570,zou3519,williamwen42,,,
bdcc1b579b0,python_frontend,bug fixes,Allow torch.load under FakeTensorMode to load FakeTensors with correct devices (for plain Tensors) (#147786),test/test_fake_tensor.py torch/_utils.py torch/serialization.py torch/storage.py,https://github.com/pytorch/pytorch/pull/147786,mikaylagawarecki,albanD,,,
209977e6e5b,skip,not user facing,Add information about checkpoint offset to untyped storages when torch.load under FakeTensorMode (#147787),test/test_fake_tensor.py torch/serialization.py torch/storage.py,https://github.com/pytorch/pytorch/pull/147787,mikaylagawarecki,albanD,,,
be0ceee1c37,jit,Untopiced,Make record/storage alignment in torch.save configurable (#147788),caffe2/serialize/inline_container.cc caffe2/serialize/inline_container.h docs/source/notes/serialization.rst test/test_serialization.py torch/_C/__init__.pyi.in torch/csrc/jit/python/init.cpp torch/serialization.py torch/utils/serialization/config.py,https://github.com/pytorch/pytorch/pull/147788,mikaylagawarecki,albanD,,,
d5184901c4f,skip,not user facing,Make torch.serialization.skip_data work with torch.load (#148018),test/test_serialization.py torch/serialization.py,https://github.com/pytorch/pytorch/pull/148018,mikaylagawarecki,albanD,,,
5fb0f45d3be,skip,not user facing,[triton 3.3] test_triton_kernel_constants fix (#148626),test/inductor/test_triton_kernels.py,https://github.com/pytorch/pytorch/pull/148626,davidberard98,FindHao,,,
b160dda7432,inductor,not user facing,cpp_wrapper: reduce memory usage by removing unneeded temporaries (#147403),test/inductor/test_memory_planning.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/wrapper.py torch/csrc/inductor/aoti_include/common.h torch/csrc/inductor/aoti_runtime/arrayref_tensor.h torch/csrc/inductor/aoti_runtime/utils.h torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp torch/csrc/inductor/cpp_wrapper/common.h,https://github.com/pytorch/pytorch/pull/147403,benjaminglass1,desertfire,,,
679e7d257e6,inductor,not user facing,[mm_logs] follow up to add count info based on shape for inductor `aten.mm`s (#148623),torch/_inductor/kernel/bmm.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_scaled.py,https://github.com/pytorch/pytorch/pull/148623,YUNQIUGUO,henrylhtsang,,,
841451af9f8,skip,Untopiced,"Revert ""[Inductor] Avoid tensor slice overflow for large step (#147433)""",aten/src/ATen/native/TensorShape.cpp test/expect/HasDecompTest.test_has_decomposition.expect test/inductor/test_torchinductor.py torch/_decomp/__init__.py torch/_decomp/decompositions.py,,,,,,
3cde4c30690,skip,not user facing,[BE] Remove `onlyCPU` decorator from test_local_scalar_dense (#148559),aten/src/ATen/native/cuda/CUDAScalar.cu aten/src/ATen/native/mps/operations/Scalar.mm test/test_torch.py,https://github.com/pytorch/pytorch/pull/148559,malfet,ZainRizvi,atalman,seemethere,
28b68b46bc9,skip,Untopiced,"Revert ""[cutlass backend] fix assertion that prevent self multiplication  (#148233)""",test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cuda_kernel.py,,,,,,
097b0d372ae,skip,not user facing,[pytree] fix previously failed dynamo tests (#148669),test/dynamo_expected_failures/TestGenericPytree.test_flatten_unflatten_deque_cxx test/dynamo_expected_failures/TestGenericPytree.test_flatten_unflatten_deque_py test/test_pytree.py,https://github.com/pytorch/pytorch/pull/148669,XuehaiPan,zou3519,,,
d4cf0e5af40,build_frontend,improvements,Enable onednn in pytorch for ppc64le architecture (#143743),CMakeLists.txt aten/src/ATen/native/CPUBlas.cpp aten/src/ATen/native/quantized/cpu/OnednnUtils.h,https://github.com/pytorch/pytorch/pull/143743,Tiwari-Avanish,malfet,,,
d91a634edf8,distributed,Untopiced,[c10d] Make getDefaultBackend more fault tolerant (#148596),torch/csrc/distributed/c10d/ProcessGroup.hpp,https://github.com/pytorch/pytorch/pull/148596,kwen2501,LucasLLC,fduwjj,,
d10bacd4cef,skip,not user facing,[AOTI][dashboard] Skip torchbench models not supported by export (#148359),benchmarks/dynamo/common.py benchmarks/dynamo/torchbench.py benchmarks/dynamo/torchbench.yaml,https://github.com/pytorch/pytorch/pull/148359,desertfire,angelayi,ysiraichi,,
b85ae06bedf,skip,not user facing,Update CPU tolerance for f16 triplet margin loss (#147742),test/inductor/test_torchinductor_opinfo.py,https://github.com/pytorch/pytorch/pull/147742,GeorgeWigley,jansel,leslie-fang-intel,,
96176e32a92,skip,Untopiced,"Revert ""[ROCm] Bump AOTriton to 0.9.1b (#148433)""",aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/hip/aotriton_adapter.h aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip cmake/External/aotriton.cmake test/test_transformers.py torch/testing/_internal/common_methods_invocations.py,,,,,,
f08146b67ba,skip,not user facing,[pytree] add APIs to determine a class is a namedtuple or PyStructSequence (#113257),benchmarks/dynamo/common.py test/test_pytree.py torch/_dynamo/polyfills/pytree.py torch/_export/serde/serialize.py torch/autograd/forward_ad.py torch/testing/_internal/composite_compliance.py torch/utils/_cxx_pytree.py torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/113257,XuehaiPan,zou3519,,,
29b28e9d9f9,skip,Untopiced,Fix `torch.nn.functional.hardswish` gradients corner case (#148049),aten/src/ATen/native/cpu/Activation.cpp aten/src/ATen/native/cuda/ActivationHardswishKernel.cu test/test_nn.py torch/csrc/jit/runtime/symbolic_script.cpp,https://github.com/pytorch/pytorch/pull/148049,zeshengzong,soulitzer,,,
65dbc3b4543,mps,not user facing,[BE][MPS] Remove redundant `handle_tensor_scalar_binary_op` (#148685),aten/src/ATen/native/mps/operations/BitwiseOps.mm,https://github.com/pytorch/pytorch/pull/148685,malfet,dcci,,,
edd640a95a3,quantization,not user facing,[BE][Ez]: Use itertools.chain.from_iterable when possible (#148190),torch/_dynamo/variables/torch.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/simd_kernel_features.py torch/_inductor/loop_body.py torch/_logging/_internal.py torch/ao/quantization/_equalize.py torch/distributed/_tools/memory_tracker.py torch/distributed/fsdp/_common_utils.py torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py torch/distributed/fsdp/_optim_utils.py torch/distributed/optim/zero_redundancy_optimizer.py torch/functional.py torch/fx/_symbolic_trace.py torch/nn/parallel/_functions.py torch/profiler/_memory_profiler.py torch/testing/_internal/common_modules.py torch/utils/benchmark/utils/compare.py torch/utils/benchmark/utils/fuzzer.py,https://github.com/pytorch/pytorch/pull/148190,Skylion007,jansel,malfet,,
1add61c2428,dynamo,not user facing,Replace `unimplemented` with `unimplemented_v2' in `codegen.py` (#148069),torch/_dynamo/codegen.py,https://github.com/pytorch/pytorch/pull/148069,zeshengzong,Skylion007,williamwen42,,
cf9efbdf16c,skip,Untopiced,"Revert ""Enable onednn in pytorch for ppc64le architecture (#143743)""",CMakeLists.txt aten/src/ATen/native/CPUBlas.cpp aten/src/ATen/native/quantized/cpu/OnednnUtils.h,,,,,,
262411e48bf,inductor,Untopiced,[inductor] online softmax (#127011),benchmarks/dynamo/huggingface.py test/inductor/test_cooperative_reductions.py test/inductor/test_online_softmax.py torch/_dynamo/utils.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/config.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/inductor_prims.py torch/_inductor/ir.py torch/_inductor/loop_body.py torch/_inductor/lowering.py torch/_inductor/runtime/triton_helpers.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/127011,shunting314,jansel,,,
1d7fc0c6818,dynamo,not user facing,[dynamo] Remove dead code path around `functools.partial` objects (#148683),torch/_dynamo/guards.py torch/_dynamo/variables/user_defined.py,https://github.com/pytorch/pytorch/pull/148683,StrongerXi,yanboliang,,,
3d62e81a1ee,distributed (checkpoint),not user facing,[DCP] fix dcp gather_object/scatter_object_list (#147675),test/distributed/checkpoint/test_utils.py torch/distributed/checkpoint/utils.py,https://github.com/pytorch/pytorch/pull/147675,Ghost-LZW,MeetVadakkanchery,,,
5a5ac989184,releng,not user facing,[aarch64] add libcufile for cu126 and cu128 (#148465),.ci/aarch64_linux/aarch64_wheel_ci_build.py,https://github.com/pytorch/pytorch/pull/148465,tinglvv,abhilash1910,atalman,,
d35a4ddae23,skip,not user facing,[cutlass backend] Forward fix for less aligned gemm shapes (#148521),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/148521,henrylhtsang,ColinPeppler,,,
2fb654676f6,skip,not user facing,[cutlass backend] fix assertion that prevent self multiplication  (#148233),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cuda_kernel.py,https://github.com/pytorch/pytorch/pull/148233,henrylhtsang,ColinPeppler,,,
75d29443e7d,python_frontend,Untopiced,[Docs] update bucketize documentaion (#148400),torch/_torch_docs.py,https://github.com/pytorch/pytorch/pull/148400,blaine-rister,albanD,benjaminglass1,eellison,
e6800bda7fa,linalg_frontend,not user facing,[Test][Linalg][CUDA] Increase niter in test_svd_lowrank_cuda_float64 (#145930),test/test_linalg.py,https://github.com/pytorch/pytorch/pull/145930,Aidyn-A,ngimel,,,
ed9c8a5d136,skip,not user facing,ROCm: Disable torch check for Multiplication of two Float8_e5m2 matrices (#148228),aten/src/ATen/native/cuda/Blas.cpp,https://github.com/pytorch/pytorch/pull/148228,jagadish-amd,jeffdaily,petrex,,
3960f978325,skip,not user facing,Documents torch.cuda.MemPool API (#148374),docs/source/notes/cuda.rst,https://github.com/pytorch/pytorch/pull/148374,syed-ahmed,eqy,ngimel,,
e2a0296e80b,distributed,Untopiced,[dtensor] add CuDNN SDPA op support to DTensor (#148537),test/distributed/tensor/test_attention.py torch/distributed/tensor/_ops/_matrix_ops.py torch/distributed/tensor/experimental/_attention.py,https://github.com/pytorch/pytorch/pull/148537,XilunWu,drisspg,fegin,,
a0bc6d81bbb,releng,not user facing,"[CI][CUDA] Move away from cuda12.4, Add cuda12.6 eager CI tests (#148602)",.ci/docker/build.sh .github/workflows/docker-builds.yml .github/workflows/periodic.yml .github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml test/test_foreach.py,https://github.com/pytorch/pytorch/pull/148602,tinglvv,atalman,malfet,,
33a285379af,cpp_frontend,not user facing,[codemod] Remove unused-variable in caffe2/torch/csrc/distributed/c10d/cuda/AsyncMM.cu (#148501),torch/csrc/distributed/c10d/cuda/AsyncMM.cu,https://github.com/pytorch/pytorch/pull/148501,r-barnes,Skylion007,,,
50eb4f39905,quantization,not user facing,Enable UBSAN test (#147511),test/quantization/core/test_quantized_op.py,https://github.com/pytorch/pytorch/pull/147511,cyyever,colesbury,,,
e3087f6d761,onnx,not user facing,[ONNX] Improve verify_onnx_program to use VerificationInterpreter (#148706),test/onnx/exporter/test_verification.py torch/onnx/_internal/exporter/_verification.py,https://github.com/pytorch/pytorch/pull/148706,justinchuby,titaiwangms,,,
a7fe685be8b,inductor,not user facing,Add cpp wrapper skip to cudagraph logs (#148700),test/inductor/test_cudagraph_trees.py torch/_inductor/compile_fx.py,https://github.com/pytorch/pytorch/pull/148700,eellison,jbschlosser,,,
c8cd8f68bd5,dynamo,not user facing,[dynamo] Properly account for non-list instances in list comparison (#148470),test/dynamo/test_functions.py torch/_dynamo/polyfills/__init__.py torch/_dynamo/utils.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/148470,StrongerXi,anijain2305,,,
b4430c3a6d5,skip,not user facing,[Intel GPU][pt2e]: Collapse 3D input to 2D for matmul in qlinear_pointwise_binary fusion (#148423),aten/src/ATen/native/mkldnn/xpu/detail/QMatmul.cpp aten/src/ATen/native/mkldnn/xpu/qlinear.cpp test/inductor/test_mkldnn_pattern_matcher.py,https://github.com/pytorch/pytorch/pull/148423,ZhiweiYan-96,EikanWang,jansel,,
127bd5a02da,skip,not user facing,Add sparsity (#148513),benchmarks/transformer/sdpa.py,https://github.com/pytorch/pytorch/pull/148513,drisspg,danielvegamyhre,,,
00cd6c07b98,inductor,not user facing,[Intel GPU][pt2e] Enable quantized grouped convolution at XPU (#148522),aten/src/ATen/native/mkldnn/xpu/detail/QConv.cpp test/inductor/test_mkldnn_pattern_matcher.py torch/_inductor/fx_passes/quantization.py,https://github.com/pytorch/pytorch/pull/148522,ZhiweiYan-96,EikanWang,jansel,liangan1,
dfb4094b9c4,skip,not user facing,Skip buffer in dense update (#148533),torch/csrc/inductor/aoti_runtime/model_container.h,https://github.com/pytorch/pytorch/pull/148533,zoranzhao,22quinn,jingsh,,
9c9b05bc4f3,skip,not user facing,Expose functions used in custom backend in torch_python dll (#148213),torch/csrc/tensor/python_tensor.h torch/csrc/utils/python_arg_parser.h,https://github.com/pytorch/pytorch/pull/148213,wschin,malfet,,,
389b496062b,skip,not user facing,[XPU] Add test/kernel.errors.txt to .gitignore. (#148538),.gitignore,https://github.com/pytorch/pytorch/pull/148538,etaf,desertfire,jansel,,
cdb4fd0d29a,releng,not user facing,Update win-vs2022-cuda12.1-py3 -> win-vs2022-cuda12.6-py3 (#148717),.github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/148717,atalman,ZainRizvi,malfet,,
33f8ab2f583,linalg_frontend,not user facing,[ROCm][TunableOp] Add support for rowwise scaling on scaled GEMM. (#148238),aten/src/ATen/cuda/tunable/GemmCommon.h test/test_linalg.py torch/cuda/tunable.py,https://github.com/pytorch/pytorch/pull/148238,naromero77amd,jeffdaily,,,
fe4b88f6aab,foreach_frontend,Untopiced,[HPU] Add hpu to fused kernels supported devices (#148666),torch/utils/_foreach_utils.py,https://github.com/pytorch/pytorch/pull/148666,Nitin-x2a,albanD,,,
c0f15572858,distributed,Untopiced,[FSDP2][doc] highlight equivalence of set_requires_gradient_sync and no_sync (#148715),torch/distributed/fsdp/_fully_shard/_fully_shard.py,https://github.com/pytorch/pytorch/pull/148715,weifengpy,mori360,,,
5f392ae560d,skip,not user facing,Throws error when using torch.cuda.MemPool with expandable segments (#148378),c10/cuda/CUDACachingAllocator.cpp test/test_cuda.py,https://github.com/pytorch/pytorch/pull/148378,syed-ahmed,eqy,ngimel,,
3f069e76795,inductor,not user facing,[mm_logs] enhance the printing for overview info (#148716),torch/_inductor/compile_fx.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/mm.py torch/_inductor/kernel/mm_scaled.py,https://github.com/pytorch/pytorch/pull/148716,YUNQIUGUO,henrylhtsang,,,
3d854ea9bd5,skip,not user facing,Remove deprecated std::aligned_storage_t (#148660),aten/src/ATen/core/boxing/impl/boxing.h aten/src/ATen/core/dispatch/Dispatcher.h,https://github.com/pytorch/pytorch/pull/148660,cyyever,swolchok,,,
4075646bd8a,xpu,Untopiced,Use oneDNN v3.7.1 for Intel GPU (#148403),cmake/Modules/FindMKLDNN.cmake,https://github.com/pytorch/pytorch/pull/148403,ZhiweiYan-96,EikanWang,,,
416ea1c71c8,inductor,not user facing,Code Clean: Remove unnecessary code (#148735),torch/_inductor/graph.py,https://github.com/pytorch/pytorch/pull/148735,FFFrog,cyyever,jingsh,,
243b47e2ec8,skip,not user facing,[Intel GPU] Fix SDPA dummy LSE output to match meta function (#148652),aten/src/ATen/native/mkldnn/xpu/Attention.cpp,https://github.com/pytorch/pytorch/pull/148652,DDEle,EikanWang,,,
bb84a23c227,skip,not user facing,[ROCm] [TunableOp] Enable logging of BLAS parameters (#147034),aten/src/ATen/cuda/tunable/GemmCommon.h aten/src/ATen/cuda/tunable/README.md aten/src/ATen/cuda/tunable/Tunable.cpp aten/src/ATen/cuda/tunable/Tunable.h aten/src/ATen/cuda/tunable/TunableOp.h,https://github.com/pytorch/pytorch/pull/147034,naromero77amd,jeffdaily,,,
6cf360be04d,export,Untopiced,fix lost input mutations with export_tracepoint (#148709),test/export/test_export.py torch/_export/wrappers.py,https://github.com/pytorch/pytorch/pull/148709,avikchaudhuri,angelayi,,,
d4d7d813fa7,releng,not user facing,Update CURL url for manywheel images (#148343),.ci/docker/manywheel/build_scripts/build_utils.sh,https://github.com/pytorch/pytorch/pull/148343,AlekseiNikiforovIBM,atalman,dr4gon01,janeyx99,
f80aad62fae,functorch,Untopiced,Improve Pareto frontier plot for AutoAC (#148678),torch/_functorch/config.py torch/_functorch/partitioners.py,https://github.com/pytorch/pytorch/pull/148678,lw,Chillee,fmassa,,
81847d08cf8,skip,not user facing,[Intel GPU][quant] Refine zero-point memory creation (#148640),aten/src/ATen/native/mkldnn/xpu/detail/QConv.cpp aten/src/ATen/native/mkldnn/xpu/detail/QMatmul.cpp,https://github.com/pytorch/pytorch/pull/148640,ZhiweiYan-96,EikanWang,liangan1,,
372ad7b1818,distributed,Untopiced,Enable FSDP2 on HPU device (#148667),torch/distributed/fsdp/_fully_shard/_fsdp_collectives.py torch/distributed/fsdp/_fully_shard/_fsdp_param.py,https://github.com/pytorch/pytorch/pull/148667,AnantGulati,wconstab,,,
d54b2b7fa7b,releng,not user facing,[BE] Delete split builds (#148739),.github/workflows/_linux-build.yml .github/workflows/periodic.yml .github/workflows/pull.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/148739,malfet,atalman,,,
17302b4bc83,python_frontend,bug fixes,Move get accelerator to use build time flags when possible (#146098),aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/cuda/detail/CUDAHooks.h aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/xpu/detail/XPUHooks.cpp aten/src/ATen/xpu/detail/XPUHooks.h docs/source/torch.rst test/test_cuda.py torch/accelerator/__init__.py torch/accelerator/_utils.py torch/csrc/DeviceAccelerator.cpp torch/serialization.py torch/utils/data/dataloader.py,https://github.com/pytorch/pytorch/pull/146098,albanD,EikanWang,jeromean,malfet,
abcca2fcbbb,skip,Untopiced,"Revert ""Fix `torch.nn.functional.hardswish` gradients corner case (#148049)""",aten/src/ATen/native/cpu/Activation.cpp aten/src/ATen/native/cuda/ActivationHardswishKernel.cu test/test_nn.py torch/csrc/jit/runtime/symbolic_script.cpp,,,,,,
136b8165d13,distributed,Untopiced,[DCP] Save Plan Caching: Fix the missing all_plans update in the cache. (#148577),test/distributed/checkpoint/test_planner.py torch/distributed/checkpoint/default_planner.py,https://github.com/pytorch/pytorch/pull/148577,saumishr,MeetVadakkanchery,,,
d05694807d3,inductor,not user facing,[XPU][Inductor] Update Intel triton for release 2.7. (#147727),.ci/docker/ci_commit_pins/triton-xpu.txt,https://github.com/pytorch/pytorch/pull/147727,etaf,EikanWang,Skylion007,,
50c9f6d83b1,inductor,not user facing,[Windows][Inductor][XPU] Unload triton pyd files to be able to remove them on Windows. (#148323),torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/148323,etaf,jansel,,,
a3b77d434a3,inductor,not user facing,Subprocess compile (attempt 2) (#148635),test/fx/test_graph_pickler.py test/inductor/test_compile_subprocess.py test/inductor/test_torchinductor.py torch/_inductor/codecache.py torch/_inductor/compile_fx.py torch/_inductor/compile_fx_ext.py torch/_inductor/compile_fx_subproc.py torch/_inductor/lowering.py torch/_inductor/virtualized.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/148635,aorenste,jamesjwu,,,
8059ead8237,inductor,Untopiced,[ROCm] Incorporate ROCm triton specific tuning parameters (#148437),torch/_inductor/autotune_process.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/mm_common.py torch/_inductor/runtime/coordinate_descent_tuner.py torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/148437,jataylo,eellison,jansel,,
116c1e42c55,skip,not user facing,Re-enable tests (#148732),test/functorch/test_aotdispatch.py,https://github.com/pytorch/pytorch/pull/148732,cyyever,Skylion007,,,
f84710aef42,mps,bug fixes,[MPS] Fix scalar to tensors bitshifts (#148686),aten/src/ATen/native/mps/operations/BitwiseOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/148686,malfet,albanD,,,
da923afdc78,mps,improvements,[MPS][BE] Align bitshift behavior with CPU (#148719),aten/src/ATen/native/mps/operations/BitwiseOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/148719,malfet,Skylion007,,,
9769618d359,inductor,not user facing,[CI] [inductor] Add cu126 inductor jobs and move away cu124 (#148612),.ci/docker/build.sh .github/workflows/docker-builds.yml .github/workflows/inductor-micro-benchmark.yml .github/workflows/inductor-perf-compare.yml .github/workflows/inductor-perf-test-nightly-h100.yml .github/workflows/inductor-perf-test-nightly.yml .github/workflows/inductor-periodic.yml .github/workflows/inductor-unittest.yml .github/workflows/inductor.yml benchmarks/dynamo/ci_expected_accuracy/aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/aot_eager_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_aot_eager_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_torchbench_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_torchbench_training.csv,https://github.com/pytorch/pytorch/pull/148612,tinglvv,atalman,nWEIdia,,
18c6e00c7b0,distributed,not user facing,[CUDA Graphs][NCCL] Set event queries to happen under thread-local mode in `ProcessGroupNCCL.cpp` (#148594),aten/src/ATen/cuda/CUDAGraph.cpp aten/src/ATen/cuda/CUDAGraph.h torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp,https://github.com/pytorch/pytorch/pull/148594,eqy,kwen2501,,,
0a7ccee1e05,skip,not user facing,[ROCm][Windows] Disable Composable Kernels and Triton for Windows builds (#147334),aten/src/ATen/CMakeLists.txt,https://github.com/pytorch/pytorch/pull/147334,m-gallus,jeffdaily,,,
e839e4f5bd0,skip,not user facing,Fix Wc++98-compat-extra-semi (#148757),aten/src/ATen/cpu/vec/vec256/vec256_16bit_float.h aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h,https://github.com/pytorch/pytorch/pull/148757,cyyever,Skylion007,,,
61c4074df79,releng,not user facing,Add Windows Arm64 Nightly Builds (#139760),.ci/pytorch/windows/arm64/bootstrap_buildtools.bat .ci/pytorch/windows/arm64/bootstrap_python.bat .ci/pytorch/windows/arm64/bootstrap_tests.bat .ci/pytorch/windows/arm64/smoke_test.bat .circleci/scripts/binary_windows_arm64_build.sh .circleci/scripts/binary_windows_arm64_test.sh .github/actionlint.yaml .github/scripts/generate_binary_build_matrix.py .github/scripts/generate_ci_workflows.py .github/templates/windows_arm64_binary_build_workflow.yml.j2 .github/workflows/generated-windows-arm64-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-arm64-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-arm64-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/139760,iremyux,malfet,,,
1239176fe71,skip,not user facing,Remove Cuda 12.4 from nightly Binaries  (#148625),.github/scripts/generate_binary_build_matrix.py .github/scripts/generate_ci_workflows.py .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/148625,tinglvv,atalman,,,
b246cd7b820,skip,Untopiced,"Revert ""Move get accelerator to use build time flags when possible (#146098)""",aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/cuda/detail/CUDAHooks.h aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/xpu/detail/XPUHooks.cpp aten/src/ATen/xpu/detail/XPUHooks.h docs/source/torch.rst test/test_cuda.py torch/accelerator/__init__.py torch/accelerator/_utils.py torch/csrc/DeviceAccelerator.cpp torch/serialization.py torch/utils/data/dataloader.py,,,,,,
6b44a91a62e,inductor,not user facing,use statically_known_true instead of guard_size_oblivious in pattern matcher (#147557),torch/_inductor/pattern_matcher.py,https://github.com/pytorch/pytorch/pull/147557,bobrenjc93,eellison,,,
aebd2e411f2,skip,not user facing,[pytree][easy] lock global registry containers properly for thread-safety (#148750),torch/utils/_pytree.py,https://github.com/pytorch/pytorch/pull/148750,XuehaiPan,StrongerXi,,,
d36391307f9,onnx,not user facing,[ONNX] Handle error in verification interpreter (#148730),torch/onnx/_internal/exporter/_verification.py,https://github.com/pytorch/pytorch/pull/148730,justinchuby,titaiwangms,,,
6602e632cdd,jit,Untopiced,Suppress build warnings when gcc-11 is used (#148763),torch/csrc/jit/tensorexpr/llvm_codegen.cpp,https://github.com/pytorch/pytorch/pull/148763,malfet,Skylion007,ZainRizvi,atalman,
99da439d109,skip,Untopiced,"Revert ""Remove Cuda 12.4 from nightly Binaries  (#148625)""",.github/scripts/generate_binary_build_matrix.py .github/scripts/generate_ci_workflows.py .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,,,,,,
d8dc700e253,releng,not user facing,Delete duplicate entry from `docker-builds.yml` (#148782),.github/workflows/docker-builds.yml,https://github.com/pytorch/pytorch/pull/148782,malfet,atalman,,,
bb94b65da77,skip,Untopiced,"Revert ""[cutlass backend] fix assertion that prevent self multiplication  (#148233)""",test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cuda_kernel.py,,,,,,
08baaa7d636,skip,not user facing,[Docs][TunableOp] TunableOp documentation update (#148384),docs/source/cuda.tunable.rst torch/cuda/tunable.py,https://github.com/pytorch/pytorch/pull/148384,naromero77amd,jeffdaily,svekars,,
7b79e172758,releng,not user facing,[BE] Move cuda12.6 builds to gcc11 (#148740),.ci/docker/build.sh .github/workflows/docker-builds.yml .github/workflows/periodic.yml .github/workflows/pull.yml .github/workflows/slow.yml .github/workflows/trunk.yml,https://github.com/pytorch/pytorch/pull/148740,malfet,Skylion007,ZainRizvi,atalman,
67742128b76,skip,not user facing,[ROCm] Bump AOTriton to 0.9.2b (#148433),aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/cuda/sdp_utils.cpp aten/src/ATen/native/transformers/hip/aotriton_adapter.h aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip cmake/External/aotriton.cmake test/test_transformers.py torch/testing/_internal/common_methods_invocations.py,https://github.com/pytorch/pytorch/pull/148433,xinyazhang,jeffdaily,,,
755965d2e4c,inductor,bug fixes,[inductor] fix matmul w/ torch.bucketize epilogue (#148769),test/inductor/test_cuda_repro.py torch/_inductor/codegen/triton.py torch/_inductor/config.py torch/_inductor/runtime/triton_helpers.py,https://github.com/pytorch/pytorch/pull/148769,davidberard98,aaronenyeshi,benjaminglass1,,
0f852641c29,skip,Untopiced,"Revert ""[cutlass backend] Forward fix for less aligned gemm shapes (#148521)""",test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/gemm_template.py,,,,,,
f2dfe2d99c5,inductor,Untopiced,[Triton 3.3] [ROCm] Enabled split_scan support for ROCm builds (#147619),torch/_inductor/ir.py,https://github.com/pytorch/pytorch/pull/147619,iupaikov-amd,davidberard98,,,
187d5c0eb15,dynamo,not user facing,[logging] Log cudagraphify timings to dynamo_timed (#143220),test/dynamo/test_structured_trace.py torch/_dynamo/utils.py torch/_inductor/compile_fx.py torch/_inductor/cudagraph_trees.py,https://github.com/pytorch/pytorch/pull/143220,masnesral,eellison,,,
26f8d810374,build_frontend,improvements,Enable onednn in pytorch for ppc64le architecture (#143743),CMakeLists.txt aten/src/ATen/native/CPUBlas.cpp aten/src/ATen/native/quantized/cpu/OnednnUtils.h,https://github.com/pytorch/pytorch/pull/143743,Tiwari-Avanish,malfet,,,
d96c85558a1,onnx,improvements,[ONNX] Use torch export to get dynamic shapes for JIT convert strategy (#148627),test/onnx/exporter/test_capture_strategies.py torch/onnx/_internal/exporter/_capture_strategies.py,https://github.com/pytorch/pytorch/pull/148627,justinchuby,titaiwangms,,,
24085db082e,inductor,not user facing,Don't clear feedback_saver_fns after cache clear (#148723),torch/_inductor/select_algorithm.py,https://github.com/pytorch/pytorch/pull/148723,exclamaforte,eellison,henrylhtsang,,
179b7a0abc2,inductor,not user facing,Do not crash when compiling quantized LORA models (#148435),torch/_inductor/fx_passes/quantization.py,https://github.com/pytorch/pytorch/pull/148435,Whadup,Valentine233,leslie-fang-intel,,
a89e7c2da9f,inductor,not user facing,[Upstream] Wrap log_2_e in tl.constexpr for new 3.3 bump (#148785),torch/_inductor/runtime/triton_helpers.py,https://github.com/pytorch/pytorch/pull/148785,drisspg,davidberard98,,,
9f170d9d132,inductor,not user facing,[Triton 3.3] Remove ROCm specific mm gemm template (#148662),torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/148662,AmdSampsa,davidberard98,,,
85467ed063d,inductor,improvements,Fix for AOTI + CUDAGraphs when calling from Python (#148601),test/inductor/test_aot_inductor.py torch/_inductor/__init__.py torch/_inductor/codegen/aoti_runtime/interface.cpp torch/_inductor/package/package.py torch/csrc/inductor/aoti_eager/kernel_holder.cpp torch/csrc/inductor/aoti_package/model_package_loader.cpp torch/csrc/inductor/aoti_package/model_package_loader.h torch/csrc/inductor/aoti_package/pybind.cpp torch/csrc/inductor/aoti_runner/model_container_runner.cpp torch/csrc/inductor/aoti_runner/model_container_runner.h torch/csrc/inductor/aoti_runner/model_container_runner_cpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cpu.h torch/csrc/inductor/aoti_runner/model_container_runner_cuda.cpp torch/csrc/inductor/aoti_runner/model_container_runner_cuda.h torch/csrc/inductor/aoti_runner/model_container_runner_xpu.cpp torch/csrc/inductor/aoti_runner/model_container_runner_xpu.h torch/csrc/inductor/aoti_runtime/interface.h torch/csrc/inductor/aoti_runtime/model.h torch/csrc/inductor/aoti_runtime/model_container.h,https://github.com/pytorch/pytorch/pull/148601,jbschlosser,desertfire,,,
8f71d4563ea,nn_frontend,not user facing,Fix rms_norm in fp16/bf16 (#147203),aten/src/ATen/native/layer_norm.cpp test/test_nn.py torch/testing/_internal/common_modules.py,https://github.com/pytorch/pytorch/pull/147203,riccardofelluga,eqy,,,
75179fd6e69,caffe2,not user facing,[Codemod][AddExplicitStrictExportArg] caffe2/test/inductor (#148781),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/148781,gmagogsfm,SherlockNoMad,,,
f7c0c230b0c,distributed,Untopiced,Fix compile errors (#148758),aten/src/ATen/native/cudnn/RNN.cpp cmake/public/utils.cmake torch/csrc/distributed/c10d/intra_node_comm.cpp torch/csrc/distributed/c10d/reducer_cuda.cpp torch/csrc/distributed/rpc/message.cpp torch/csrc/dynamo/guards.cpp torch/csrc/jit/codegen/fuser/cpu/fused_kernel.h torch/csrc/jit/ir/alias_analysis.cpp,https://github.com/pytorch/pytorch/pull/148758,cyyever,Skylion007,,,
5f1c79ba2b6,releng,not user facing,[CD] Enable triton xpu windows build (#147637),.github/scripts/windows/build_triton.bat .github/scripts/windows/install_vs2022.ps1 .github/workflows/build-triton-wheel.yml,https://github.com/pytorch/pytorch/pull/147637,chuanqi129,atalman,,,
c16cd25cf58,skip,not user facing,[ca] remove compiled_autograd_tracing (#148381),torch/_functorch/_aot_autograd/runtime_wrappers.py torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h,https://github.com/pytorch/pytorch/pull/148381,xmfan,jansel,,,
666508eb170,skip,not user facing,[aot cache][ca] remove restriction on caching ca's aot inference graph (#148491),test/dynamo/test_aot_autograd_cache.py torch/_functorch/_aot_autograd/autograd_cache.py torch/csrc/autograd/python_function.h,https://github.com/pytorch/pytorch/pull/148491,xmfan,jamesjwu,,,
3745da18f45,inductor,not user facing,[AOTI] Swith to local cpp compile for fbcode (#148592),torch/_inductor/codecache.py torch/_inductor/cpp_builder.py,https://github.com/pytorch/pytorch/pull/148592,zoranzhao,hl475,,,
118c9e501ae,onnx,not user facing,[ONNX] Remove inaccurate test comment (#148813),test/onnx/exporter/test_capture_strategies.py,https://github.com/pytorch/pytorch/pull/148813,justinchuby,cyyever,titaiwangms,,
c3b05c4a276,skip,not user facing,[triton 3.3] support both specialize_impl and create_specialize_impl (#148806),torch/_higher_order_ops/triton_kernel_wrap.py,https://github.com/pytorch/pytorch/pull/148806,davidberard98,drisspg,,,
849cc058eef,skip,not user facing,[CUDA][TF32] Account for tf32 in `test_efficient_conv_bn_eval` (#148802),test/inductor/test_efficient_conv_bn_eval.py,https://github.com/pytorch/pytorch/pull/148802,eqy,Skylion007,,,
439782960c6,skip,not user facing,Fix typos in SpectralOps.cpp (#148818),aten/src/ATen/native/SpectralOps.cpp,https://github.com/pytorch/pytorch/pull/148818,csukuangfj,Skylion007,,,
9841f0ddcfa,distributed,Untopiced,Add support for non functional collectives under FakeTensorMode and fake_pg for memory tracking (#147566),test/distributed/_tools/test_fake_collectives.py test/distributed/_tools/test_mod_tracker.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/FakeProcessGroup.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/_tools/common_utils.py torch/distributed/_tools/fake_collectives.py torch/distributed/_tools/fsdp2_mem_tracker.py torch/distributed/_tools/mem_tracker.py torch/distributed/_tools/mod_tracker.py torch/distributed/_tools/sac_estimator.py,https://github.com/pytorch/pytorch/pull/147566,sanketpurandare,weifengpy,,,
7ffadff2868,distributed,Untopiced,c10d/ProcessGroup: cleanup abort and shutdown (#148798),test/distributed/test_c10d_common.py test/distributed/test_c10d_pypg.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Backend.hpp torch/csrc/distributed/c10d/ProcessGroup.hpp torch/csrc/distributed/c10d/ProcessGroupMPI.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/PyProcessGroup.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py,https://github.com/pytorch/pytorch/pull/148798,d4l3k,kwen2501,,,
148eb735ee2,skip,not user facing,Change nvcc arch flags for sm100 (#148774),cmake/Codegen.cmake,https://github.com/pytorch/pytorch/pull/148774,danielvegamyhre,Skylion007,,,
5245304f1ec,composability,not user facing,Update decompositions_for_jvp.py (#148821),torch/_decomp/decompositions_for_jvp.py,https://github.com/pytorch/pytorch/pull/148821,NinoRisteski,Skylion007,,,
17dbeb11db7,skip,Untopiced,[PGNCCL] Launch kernel on current stream & remove `record_stream` entirely (#148590),test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Ops.cpp torch/csrc/distributed/c10d/ProcessGroup.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/148590,kwen2501,Aidyn-A,eqy,fduwjj,
9cb25f0ea28,skip,Untopiced,"Revert ""[PGNCCL] Launch kernel on current stream & remove `record_stream` entirely (#148590)""",test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Ops.cpp torch/csrc/distributed/c10d/ProcessGroup.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py torch/testing/_internal/distributed/distributed_test.py,,,,,,
85fe576ee3e,skip,not user facing,[set_linter] allow x in {...} (#148422),tools/linter/adapters/set_linter.py,https://github.com/pytorch/pytorch/pull/148422,jansel,Skylion007,,,
2149f6c6845,skip,Untopiced,[PGNCCL] Launch kernel on current stream & remove `record_stream` entirely (#148590),test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp test/forward_backward_compatibility/check_forward_backward_compatibility.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Ops.cpp torch/csrc/distributed/c10d/ProcessGroup.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/148590,kwen2501,Aidyn-A,eqy,fduwjj,
6566d67bd32,dynamo,not user facing,[dynamo] show stack above dynamo in graph break user tracebacks (#148401),test/dynamo/test_error_messages.py test/dynamo/test_exc.py torch/_dynamo/bytecode_transformation.py torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py,https://github.com/pytorch/pytorch/pull/148401,williamwen42,jansel,zou3519,,
685fb377131,skip,not user facing,[dynamo] allow global import `from collections import deque` in user code (#148676),test/dynamo/test_deque_reconstruct.py test/test_pytree.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/148676,XuehaiPan,jansel,,,
327e07ac1dc,skip,Untopiced,Add a stable TORCH_LIBRARY to C shim (#148124),build_variables.bzl docs/cpp/source/Doxyfile setup.py test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/__init__.py test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/csrc/libtorch_agnostic_kernel.cpp test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/ops.py test/cpp_extensions/libtorch_agnostic_extension/setup.py test/cpp_extensions/libtorch_agnostic_extension/test/test_libtorch_agnostic.py test/run_test.py test/test_cpp_extensions_aot.py tools/amd_build/build_amd.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp torch/csrc/stable/library.h,https://github.com/pytorch/pytorch/pull/148124,janeyx99,albanD,atalman,zou3519,
bb9c4260249,mobile,Untopiced,Typo Errors fixed in multiple files (#148262),setup.py test/mobile/test_lite_script_module.py torch/_library/triton.py torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/dynamo/compiled_autograd.h torch/distributed/checkpoint/state_dict_loader.py torch/distributed/distributed_c10d.py torch/export/_draft_export.py torch/export/_unlift.py,https://github.com/pytorch/pytorch/pull/148262,ENUMERA8OR,janeyx99,,,
a8e3d1984a7,skip,not user facing,[Inductor UT][XPU] Skip test case test_cat_max_autotune_triton  for known issue. (#148734),test/inductor/test_max_autotune.py,https://github.com/pytorch/pytorch/pull/148734,etaf,jansel,,,
9a1a2e15165,skip,not user facing,Better log message to update pr_time_benchmarks/expected_results.csv (#148303),benchmarks/dynamo/pr_time_benchmarks/check_results.py,https://github.com/pytorch/pytorch/pull/148303,jansel,Skylion007,,,
f1444f006c4,caffe2,Untopiced,[caffe2/torch] Fixup upstream LLVM (major version 21) API changes (#148833),torch/csrc/jit/tensorexpr/llvm_codegen.cpp torch/csrc/jit/tensorexpr/llvm_jit.cpp,https://github.com/pytorch/pytorch/pull/148833,HighW4y2H3ll,Skylion007,,,
19a39a7a064,skip,Untopiced,"Revert ""[dynamo] allow global import `from collections import deque` in user code (#148676)""",test/dynamo/test_deque_reconstruct.py test/test_pytree.py torch/_dynamo/variables/lists.py,,,,,,
275a7c5dbb2,skip,Untopiced,"Revert ""Add a stable TORCH_LIBRARY to C shim (#148124)""",build_variables.bzl docs/cpp/source/Doxyfile setup.py test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/__init__.py test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/csrc/libtorch_agnostic_kernel.cpp test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/ops.py test/cpp_extensions/libtorch_agnostic_extension/setup.py test/cpp_extensions/libtorch_agnostic_extension/test/test_libtorch_agnostic.py test/run_test.py test/test_cpp_extensions_aot.py tools/amd_build/build_amd.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp torch/csrc/stable/library.h,,,,,,
b95889042c3,mps,performance,[MPS] Introduce strides unary op (#148468),aten/src/ATen/native/mps/OperationUtils.mm aten/src/ATen/native/mps/kernels/UnaryKernel.metal c10/metal/indexing.h test/test_mps.py,https://github.com/pytorch/pytorch/pull/148468,malfet,dcci,,,
aac230a511f,mps,Untopiced,[MPS] Fix Wreorder-init-list (#148839),aten/src/ATen/native/mps/OperationUtils.mm,https://github.com/pytorch/pytorch/pull/148839,cyyever,Skylion007,,,
b47d81682df,skip,not user facing,[cutlass backend] Forward fix for less aligned gemm shapes (#148521),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/gemm_template.py,https://github.com/pytorch/pytorch/pull/148521,henrylhtsang,ColinPeppler,,,
7ae0ce6360b,skip,not user facing,[cutlass backend] fix assertion that prevent self multiplication  (#148233),test/inductor/test_cutlass_backend.py torch/_inductor/codegen/cuda/cuda_kernel.py,https://github.com/pytorch/pytorch/pull/148233,henrylhtsang,ColinPeppler,,,
3680e666d8c,skip,Untopiced,Move aoti_torch_cpu__weight_int4pack_mm_cpu_tensor to not be mangled (#148834),torch/csrc/inductor/aoti_torch/c/shim.h,https://github.com/pytorch/pytorch/pull/148834,janeyx99,desertfire,,,
00199acdb85,inductor,not user facing,[inductor][triton] Block ptr analysis fix assert on matched index expression  (#148446),test/inductor/test_block_analysis.py test/inductor/test_torchinductor.py test/inductor/test_torchinductor_codegen_dynamic_shapes.py torch/_inductor/codegen/block_analysis.py,https://github.com/pytorch/pytorch/pull/148446,kundaMwiza,jansel,,,
74da76f67cc,skip,not user facing,[ROCm][Windows] Fix ROCm/HIP version header (#148560),aten/src/ATen/cuda/tunable/Tunable.cpp,https://github.com/pytorch/pytorch/pull/148560,m-gallus,jeffdaily,,,
526524b4895,skip,not user facing,Update slow tests (#148873),test/slow_tests.json,https://github.com/pytorch/pytorch/pull/148873,pytorchupdatebot,pytorchbot,,,
ea86b8d3154,distributed,not user facing,Fix redistribution cost for all-reduce (#148761),torch/distributed/tensor/_collective_utils.py,https://github.com/pytorch/pytorch/pull/148761,fmassa,Skylion007,fegin,lw,
59f14d19aea,autograd_frontend,Untopiced,Implement gradient for the `residuals` of `torch.linalg.lstsq` (#148526),tools/autograd/derivatives.yaml torch/csrc/autograd/FunctionsManual.cpp torch/csrc/autograd/FunctionsManual.h torch/linalg/__init__.py torch/testing/_internal/opinfo/definitions/linalg.py,https://github.com/pytorch/pytorch/pull/148526,Bichidian,lezcano,,,
098494e9cb5,skip,not user facing,[dynamo] allow global import `from collections import deque` in user code (#148676),test/dynamo/test_deque_reconstruct.py test/test_pytree.py torch/_dynamo/variables/lists.py,https://github.com/pytorch/pytorch/pull/148676,XuehaiPan,jansel,,,
68c12ecfe26,python_frontend,bug fixes,Move get accelerator to use build time flags when possible (#146098),aten/src/ATen/DeviceAccelerator.cpp aten/src/ATen/cuda/detail/CUDAHooks.h aten/src/ATen/detail/AcceleratorHooksInterface.h aten/src/ATen/mps/MPSHooks.h aten/src/ATen/xpu/detail/XPUHooks.cpp aten/src/ATen/xpu/detail/XPUHooks.h docs/source/torch.rst test/distributed/test_c10d_common.py test/test_cuda.py torch/accelerator/__init__.py torch/accelerator/_utils.py torch/csrc/DeviceAccelerator.cpp torch/csrc/Module.cpp torch/serialization.py torch/utils/data/dataloader.py,https://github.com/pytorch/pytorch/pull/146098,albanD,EikanWang,jeromean,malfet,
2068235c0ad,skip,not user facing,Add timm_efficientnet to flaky models after cuda 12.6 update in CI/CD (#148788),benchmarks/dynamo/check_accuracy.py,https://github.com/pytorch/pytorch/pull/148788,atalman,izaitsevfb,malfet,,
31625b08b8a,skip,not user facing,Add ccode for FloorDiv (#148727),torch/utils/_sympy/functions.py,https://github.com/pytorch/pytorch/pull/148727,kalpit-meta-1,bobrenjc93,,,
4fdd0769072,releng,not user facing,[CD] Add triton xpu as dependency of torch xpu windows whl (#148755),.ci/pytorch/windows/internal/smoke_test.bat .circleci/scripts/binary_populate_env.sh,https://github.com/pytorch/pytorch/pull/148755,chuanqi129,atalman,,,
a81751d8b7c,releng,not user facing,[CD] Annotate linux/arm64 cuda wheels with consistent nvidia dependencies (#145021),.github/scripts/generate_binary_build_matrix.py .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml,https://github.com/pytorch/pytorch/pull/145021,tmm1,atalman,seemethere,,
b8b1b364c9d,mps,Untopiced,Fix invalid format string in libfmt calls (#148855),aten/src/ATen/native/mps/OperationUtils.mm,https://github.com/pytorch/pytorch/pull/148855,cyyever,Skylion007,,,
bec7bdad47a,skip,Untopiced,[fx] Move map_aggregate to C++ (#148243),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_C/__init__.pyi.in torch/_dynamo/polyfills/fx.py torch/_dynamo/polyfills/loader.py torch/csrc/fx/node.cpp torch/fx/graph.py torch/fx/node.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/148243,jansel,oulgen,,,
bf752c36da0,skip,Untopiced,[fx] Move Node._update_args_kwargs to C++ (#148260),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_C/__init__.pyi.in torch/csrc/fx/node.cpp torch/fx/node.py,https://github.com/pytorch/pytorch/pull/148260,jansel,oulgen,,,
5d4e7d58b42,skip,Untopiced,[fx] Move Node._prepend/Node._remove_from_list to C++ (#148261),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/_C/__init__.pyi.in torch/csrc/fx/node.cpp torch/fx/node.py,https://github.com/pytorch/pytorch/pull/148261,jansel,oulgen,,,
8f858e226ba,skip,Untopiced,[fx] Optimizations for node name generation (#148288),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/148288,jansel,oulgen,,,
a60b4ed6236,skip,Untopiced,[fx] Optimize TracerBase.create_arg and Graph._gen_python_code (#148292),benchmarks/dynamo/pr_time_benchmarks/expected_results.csv torch/fx/graph.py torch/fx/interpreter.py torch/fx/proxy.py,https://github.com/pytorch/pytorch/pull/148292,jansel,oulgen,,,
9dbc2527dc8,cpp_frontend,bug fixes,Disable some SVE autovec  (#148489),aten/src/ATen/cpu/vec/vec_base.h,https://github.com/pytorch/pytorch/pull/148489,Nicoshev,malfet,,,
2ec9aceaeb7,skip,Untopiced,"Revert ""Move aoti_torch_cpu__weight_int4pack_mm_cpu_tensor to not be mangled (#148834)""",torch/csrc/inductor/aoti_torch/c/shim.h,,,,,,
ebd087e4b59,skip,Untopiced,"Revert ""[pytree] add APIs to determine a class is a namedtuple or PyStructSequence (#113257)""",benchmarks/dynamo/common.py test/test_pytree.py torch/_dynamo/polyfills/pytree.py torch/_export/serde/serialize.py torch/autograd/forward_ad.py torch/testing/_internal/composite_compliance.py torch/utils/_cxx_pytree.py torch/utils/_pytree.py,,,,,,
203dd18c5cc,skip,not user facing,Bump Clang-tidy to 19.1.4 (#148648),.clang-tidy aten/src/ATen/MapAllocator.cpp aten/src/ATen/core/IListRef_test.cpp c10/core/SymInt.cpp c10/core/WrapDimMinimal.cpp c10/cuda/CUDAAllocatorConfig.cpp c10/test/core/DispatchKeySet_test.cpp c10/test/util/lazy_test.cpp c10/util/string_view.h tools/linter/adapters/s3_init_config.json,https://github.com/pytorch/pytorch/pull/148648,cyyever,Skylion007,,,
4c13a859e59,inductor,Untopiced,Workaround no triton float8_e8m0fnu support in inductor (#148722),test/inductor/test_cuda_repro.py torch/_inductor/lowering.py torch/_inductor/utils.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/148722,eellison,vkuzo,,,
00cabd42354,inductor,not user facing,[Inductor][Windows] add env_var switch to turn all Windows inductor UTs. (#148733),torch/_dynamo/test_case.py,https://github.com/pytorch/pytorch/pull/148733,xuhancn,jansel,,,
d1f21d8ec33,skip,not user facing,Enable Direct Use of Arm Compute Library (ACL) in ATen (#148584),aten/src/ATen/CMakeLists.txt caffe2/CMakeLists.txt cmake/Modules/FindACL.cmake,https://github.com/pytorch/pytorch/pull/148584,fadara01,malfet,,,
ed7e964f2bc,inductor,not user facing,codecache.py: use str.format rather than % formatting (#148691),torch/_inductor/codecache.py,https://github.com/pytorch/pytorch/pull/148691,benjaminglass1,desertfire,,,
3129faf8be3,distributed,Untopiced,Optimize shard_dim_alltoall to use alltoall_single (#148868),torch/csrc/distributed/c10d/Functional.cpp,https://github.com/pytorch/pytorch/pull/148868,wanchaol,tianyu-l,,,
6b0fd741d11,dynamo,not user facing,dynamo: Count number of opcodes processes (#147149),test/dynamo/test_utils.py torch/_dynamo/symbolic_convert.py torch/_dynamo/utils.py,https://github.com/pytorch/pytorch/pull/147149,c00w,jansel,,,
2e4874e48d0,skip,not user facing,Update RELEASE.md with latest changes to release process and release 2.7 information (#148888),RELEASE.md,https://github.com/pytorch/pytorch/pull/148888,atalman,ZainRizvi,albanD,,
494abeff8a0,distributed,Untopiced,"CUDACachingAllocator,c10d: fixes for IPC release performance (#148805)",c10/cuda/CUDACachingAllocator.cpp torch/csrc/StorageSharing.cpp torch/csrc/distributed/c10d/init.cpp,https://github.com/pytorch/pytorch/pull/148805,d4l3k,Skylion007,fegin,zdevito,
ed969d12360,distributed,Untopiced,[DSD] Fix the shared parameter mismatch for optimizer state_dict when flattening FQNs are used (#148825),test/distributed/checkpoint/test_state_dict.py torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/148825,fegin,fduwjj,mori360,,
e9c12e819dd,skip,not user facing,Update torch-xpu-ops commit pin (#148881),third_party/xpu.txt,https://github.com/pytorch/pytorch/pull/148881,chunhuanMeng,EikanWang,,,
4a2173d9a05,skip,not user facing,[cutlass backend][ez] Incorporate AOTI dynamic shape test into main test of MM (#148786),test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/148786,henrylhtsang,jingsh,,,
2a1eeaeed89,releng,not user facing,Remove 12.4 x86 builds and 12.6 sbsa builds from nightly  (#148895),.github/scripts/generate_binary_build_matrix.py .github/scripts/generate_ci_workflows.py .github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml .github/workflows/generated-linux-binary-libtorch-cxx11-abi-nightly.yml .github/workflows/generated-linux-binary-manywheel-main.yml .github/workflows/generated-linux-binary-manywheel-nightly.yml .github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml .github/workflows/generated-macos-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-arm64-binary-wheel-nightly.yml .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/148895,tinglvv,atalman,,,
b706044cca5,skip,not user facing,[ROCm][Windows] Enable hipblaslt for Windows (#148563),cmake/Dependencies.cmake cmake/public/LoadHIP.cmake,https://github.com/pytorch/pytorch/pull/148563,m-gallus,jeffdaily,,,
c652772af70,releng,not user facing,[aarch64] install ninja for docker to build triton on arm (#148768),.ci/docker/common/install_ninja.sh .ci/docker/manywheel/Dockerfile_2_28_aarch64 .ci/docker/manywheel/build.sh,https://github.com/pytorch/pytorch/pull/148768,tinglvv,Skylion007,atalman,,
8701b302cc4,skip,not user facing,setuptools pinning (#148879),requirements.txt,https://github.com/pytorch/pytorch/pull/148879,ozanMSFT,seemethere,,,
6ef15c7f464,inductor,not user facing,[pytorch] Update flexattention bwd config generation (#148600),torch/_inductor/kernel/flex_attention.py,https://github.com/pytorch/pytorch/pull/148600,mandroid6,Skylion007,,,
98b3f1db9fc,inductor,not user facing,[Flex Attention] support num_heads > 1 in block_mask (#148857),test/inductor/test_flex_decoding.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py,https://github.com/pytorch/pytorch/pull/148857,BoyuanFeng,drisspg,,,
fcb633fafae,cpp_frontend,Untopiced,Introduce TORCH_ABI_VERSION and a runtime aoti_torch_abi_version C shim ABI (#148892),torch/csrc/api/include/torch/version.h.in torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/148892,janeyx99,albanD,,,
12a95390ae3,fx,Untopiced,[Minimizer] allow overriding of ShapeProp logic by subclasses of _MinimizerBase (#148784),torch/fx/passes/net_min_base.py torch/fx/passes/split_utils.py,https://github.com/pytorch/pytorch/pull/148784,qcyuan,blaine-rister,,,
a95eb0c0a7d,skip,Untopiced,"Revert ""[PGNCCL] Launch kernel on current stream & remove `record_stream` entirely (#148590)""",test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp test/forward_backward_compatibility/check_forward_backward_compatibility.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Ops.cpp torch/csrc/distributed/c10d/ProcessGroup.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py torch/testing/_internal/distributed/distributed_test.py,,,,,,
5bbca7d328f,skip,not user facing,[ROCm][Windows] Fix OpenMP Flags for clang-cl (#148097),cmake/Modules/FindOpenMP.cmake,https://github.com/pytorch/pytorch/pull/148097,m-gallus,jeffdaily,,,
492f3fd5cff,inductor,Untopiced,replace usages of upload_graph in inductor with tlparse (v2) (#148720),test/dynamo/test_structured_trace.py test/inductor/test_group_batch_fusion.py test/inductor/test_split_cat_fx_passes.py torch/_inductor/fx_passes/group_batch_fusion.py torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/pre_grad.py,https://github.com/pytorch/pytorch/pull/148720,bdhirsh,mengluy0125,,,
b215841ebb3,inductor,not user facing,[MM] Add sm carevout to lowerings (#148793),test/inductor/test_max_autotune.py torch/_inductor/utils.py,https://github.com/pytorch/pytorch/pull/148793,drisspg,eellison,lw,,
a6e71dbc88c,skip,not user facing,Enable ASAN on inductor CUDA tests (#148749),test/inductor/test_benchmark_fusion.py test/inductor/test_efficient_conv_bn_eval.py,https://github.com/pytorch/pytorch/pull/148749,cyyever,jansel,,,
295f2ed4d10,export,Untopiced,"Fix ""invalid application of 'sizeof' to an incomplete type"" (#148854)",torch/_export/serde/schema_check.py,https://github.com/pytorch/pytorch/pull/148854,cyyever,Skylion007,,,
c297c09a373,skip,not user facing,Fix invalid nested int guarding in broadcast_shapes() (#145957),test/test_nestedtensor.py torch/functional.py,https://github.com/pytorch/pytorch/pull/145957,jbschlosser,bobrenjc93,,,
0fa0a740958,skip,not user facing,Refactor `test/test_torch.py` by moving testcase to `test_indexing.py` (#148875),test/test_indexing.py test/test_torch.py,https://github.com/pytorch/pytorch/pull/148875,zeshengzong,,,,
9fddbf34171,inductor,not user facing,Update the comment (#148726),torch/_inductor/config.py,https://github.com/pytorch/pytorch/pull/148726,Microve,yf225,,,
457ff9b7aef,autograd_frontend,Untopiced,[reland][ca] side-effect free inital trace: compiled_args (#148376),tools/autograd/gen_autograd_functions.py torch/csrc/autograd/custom_function.h torch/csrc/autograd/function.h torch/csrc/autograd/function_hook.h torch/csrc/autograd/functions/accumulate_grad.cpp torch/csrc/autograd/functions/accumulate_grad.h torch/csrc/autograd/functions/basic_ops.cpp torch/csrc/autograd/functions/basic_ops.h torch/csrc/autograd/functions/tensor.cpp torch/csrc/autograd/functions/tensor.h torch/csrc/autograd/python_function.cpp torch/csrc/autograd/python_function.h torch/csrc/autograd/python_hook.cpp torch/csrc/autograd/python_hook.h torch/csrc/autograd/utils/lambda_post_hook.h torch/csrc/distributed/c10d/reducer.cpp torch/csrc/dynamo/compiled_autograd.h,https://github.com/pytorch/pytorch/pull/148376,xmfan,jansel,,,
8c45d44abba,distributed,not user facing,Skip distributed subprocess test internally as they don't work (#148909),test/distributed/test_c10d_common.py,https://github.com/pytorch/pytorch/pull/148909,albanD,janeyx99,,,
621dadd4caa,fx,Untopiced,"partitioner: when materializing unbacked tensor intermediates, apply hint to symbol, not expr (#144097)",test/dynamo/test_repros.py test/test_dynamic_shapes.py torch/_functorch/partitioners.py torch/fx/experimental/sym_node.py,https://github.com/pytorch/pytorch/pull/144097,bdhirsh,laithsakka,,,
ff29791ed8f,skip,Untopiced,[WIP] Initial implementation of Grouped Gemm API (#148531),aten/src/ATen/native/cuda/Blas.cpp aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/cuda/ScaledGroupMM.cu aten/src/ATen/native/cuda/ScaledGroupMM.h aten/src/ATen/native/cuda/cutlass_utils.hpp aten/src/ATen/native/native_functions.yaml cmake/Codegen.cmake test/expect/HasDecompTest.test_has_decomposition.expect test/test_matmul_cuda.py,https://github.com/pytorch/pytorch/pull/148531,ngimel,drisspg,,,
940b60db974,quantization,not user facing,Use the device interface for detecting Triton availability (#139171),tools/dynamo/verify_dynamo.py torch/_dynamo/device_interface.py torch/_dynamo/eval_frame.py torch/_dynamo/logging.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/triton.py torch/_inductor/compile_fx.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/runtime/autotune_cache.py torch/_inductor/scheduler.py torch/_inductor/utils.py torch/sparse/_triton_ops.py torch/utils/_content_store.py torch/utils/_triton.py,https://github.com/pytorch/pytorch/pull/139171,galexite,,,,
ecfbfe16039,skip,not user facing,[AOTI] Remove aoti_torch_cpu__weight_int4pack_mm_cpu_tensor (#148907),test/inductor/test_cpu_cpp_wrapper.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/148907,desertfire,janeyx99,,,
bed92a85233,skip,not user facing,[Window][Inductor UT] Fix for tempfile.NamedTemporaryFile(delete=True) not work on Windows. (#148632),test/inductor/test_codecache.py,https://github.com/pytorch/pytorch/pull/148632,etaf,jansel,,,
1fcc4bc109c,inductor,not user facing,Don't look at TESTING_ONLY in fuzzer (#146870),torch/_inductor/fuzzer.py,https://github.com/pytorch/pytorch/pull/146870,exclamaforte,masnesral,,,
41e4728f74b,dynamo,not user facing,update types on dynamo configs (#146873),torch/_dynamo/config.py,https://github.com/pytorch/pytorch/pull/146873,exclamaforte,williamwen42,,,
2bcc3acb90b,skip,not user facing,Update low prec codegen for div/mod (#142350),test/inductor/test_op_dtype_prop.py test/inductor/test_pattern_matcher.py torch/_inductor/codegen/triton.py,https://github.com/pytorch/pytorch/pull/142350,eellison,blaine-rister,,,
9ad64ce7954,inductor,not user facing,[triton 3.3] Forward-fix mm template selection logic (#148924),torch/_inductor/kernel/mm.py,https://github.com/pytorch/pytorch/pull/148924,davidberard98,atalman,drisspg,eellison,
da4bb72a716,skip,not user facing,Backout D70075331 (#148824),aten/src/ATen/cuda/CUDABlas.cpp aten/src/ATen/cuda/CUDAContextLight.h aten/src/ATen/cuda/CublasHandlePool.cpp benchmarks/dynamo/common.py,https://github.com/pytorch/pytorch/pull/148824,renganxu,eqy,,,
ee21ccc8162,quantization,not user facing,Skip ao_sparsity TestComposability for missing FBGEMM (#144146),test/ao/sparsity/test_composability.py,https://github.com/pytorch/pytorch/pull/144146,Flamefire,jcaip,jerryzh168,,
992838e7025,dynamo,not user facing,[dynamo][guards] Do not ID_MATCH on numpy tensors (#148923),torch/_dynamo/guards.py torch/_dynamo/output_graph.py torch/csrc/dynamo/guards.cpp,https://github.com/pytorch/pytorch/pull/148923,anijain2305,jansel,,,
f1787ee0f77,dynamo,not user facing,[dynamo] Remove L scoping for recompilation messages (#148917),test/dynamo/test_aot_autograd.py test/dynamo/test_misc.py test/dynamo/test_recompile_ux.py test/inductor/test_torchinductor.py torch/_dynamo/guards.py,https://github.com/pytorch/pytorch/pull/148917,anijain2305,williamwen42,,,
c983e1124cd,skip,Untopiced,"Revert ""[WIP] Initial implementation of Grouped Gemm API (#148531)""",aten/src/ATen/native/cuda/Blas.cpp aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/cuda/ScaledGroupMM.cu aten/src/ATen/native/cuda/ScaledGroupMM.h aten/src/ATen/native/cuda/cutlass_utils.hpp aten/src/ATen/native/native_functions.yaml cmake/Codegen.cmake test/expect/HasDecompTest.test_has_decomposition.expect test/test_matmul_cuda.py,,,,,,
3945954741e,releng,not user facing,Bump triton pin. Add aarch64 triton build (#148705),.ci/docker/ci_commit_pins/triton-xpu.txt .ci/docker/ci_commit_pins/triton.txt .ci/docker/triton_version.txt .github/scripts/build_triton_wheel.py .github/workflows/build-triton-wheel.yml test/inductor/test_torchinductor_codegen_dynamic_shapes.py,https://github.com/pytorch/pytorch/pull/148705,atalman,EikanWang,Skylion007,drisspg,
16560d4e8f8,skip,Untopiced,"Revert ""Refactor `test/test_torch.py` by moving testcase to `test_indexing.py` (#148875)""",test/test_indexing.py test/test_torch.py,,,,,,
09029010e5e,inductor,Untopiced,[inductor] Fix create_specialize_impl error in latest Triton (#148933),torch/_higher_order_ops/triton_kernel_wrap.py torch/_inductor/codegen/cpp_wrapper_cpu.py,https://github.com/pytorch/pytorch/pull/148933,jansel,davidberard98,yanboliang,,
e0d4c43ad10,releng,Untopiced,Add env for disabling meta reference on functionalization. (#148822),.ci/pytorch/test.sh aten/src/ATen/templates/RegisterFunctionalization.cpp torchgen/gen_functionalization_type.py,https://github.com/pytorch/pytorch/pull/148822,ysiraichi,bdhirsh,,,
52acc1f9552,distributed,Untopiced,[DSD] Update the document to mention the limitation of set_optimizer_state_dict (#148918),docs/source/distributed.checkpoint.rst torch/distributed/checkpoint/state_dict.py,https://github.com/pytorch/pytorch/pull/148918,fegin,fduwjj,mori360,,
dcc502f3767,linalg_frontend,not user facing,[ROCm][TunableOp] Add bias data type to params signature. (#146227),aten/src/ATen/cuda/tunable/GemmCommon.h test/test_linalg.py torch/cuda/tunable.py,https://github.com/pytorch/pytorch/pull/146227,naromero77amd,jeffdaily,,,
b366f336069,inductor,not user facing,[MPSInductor] Prep for mutlistage reductions (#148969),torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/148969,malfet,dcci,,,
ef6296e7f20,skip,Untopiced,[PGNCCL] Launch kernel on current stream & remove `record_stream` entirely (#148590),test/cpp/c10d/ProcessGroupNCCLErrorsTest.cpp test/forward_backward_compatibility/check_forward_backward_compatibility.py torch/_C/_distributed_c10d.pyi torch/csrc/distributed/c10d/Ops.cpp torch/csrc/distributed/c10d/ProcessGroup.hpp torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp torch/csrc/distributed/c10d/Types.hpp torch/csrc/distributed/c10d/init.cpp torch/distributed/distributed_c10d.py torch/testing/_internal/distributed/distributed_test.py,https://github.com/pytorch/pytorch/pull/148590,kwen2501,Aidyn-A,eqy,fduwjj,
a5f6b24d87b,skip,not user facing,Remove outdated skipIfRocmVersionLessThan decorations (#148941),test/nn/test_convolution.py test/test_cuda_primary_ctx.py,https://github.com/pytorch/pytorch/pull/148941,cyyever,jeffdaily,,,
5ee9dbc0a19,releng,Untopiced,Bump jinja2 from 3.1.5 to 3.1.6 in /.ci/docker (#148812),.ci/docker/requirements-ci.txt,,,,,,
883fb78c7eb,releng,Untopiced,Update jinja2 version in requirements-gha-cache.txt,.github/requirements-gha-cache.txt,,,,,,
57ee821a41b,dynamo,not user facing,fix dynamo ide (#148849),torch/_dynamo/__init__.py,https://github.com/pytorch/pytorch/pull/148849,drisspg,bobrenjc93,,,
c916a8efc54,skip,Untopiced,"Revert ""Use the device interface for detecting Triton availability (#139171)""",tools/dynamo/verify_dynamo.py torch/_dynamo/device_interface.py torch/_dynamo/eval_frame.py torch/_dynamo/logging.py torch/_dynamo/utils.py torch/_dynamo/variables/builder.py torch/_higher_order_ops/triton_kernel_wrap.py torch/_inductor/codegen/cuda_combined_scheduling.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/triton.py torch/_inductor/compile_fx.py torch/_inductor/fx_passes/pad_mm.py torch/_inductor/runtime/autotune_cache.py torch/_inductor/scheduler.py torch/_inductor/utils.py torch/sparse/_triton_ops.py torch/utils/_content_store.py torch/utils/_triton.py,,,,,,
8d08b490158,skip,not user facing,Reland: [inductor] Simplify grid handling (#148305),test/inductor/test_aot_inductor.py test/inductor/test_cpp_wrapper_hipify.py test/inductor/test_cuda_repro.py test/inductor/test_kernel_benchmark.py test/inductor/test_max_autotune.py test/inductor/test_profiler.py test/inductor/test_select_algorithm.py test/inductor/test_torchinductor.py test/inductor/test_triton_kernels.py torch/_inductor/autotune_process.py torch/_inductor/codecache.py torch/_inductor/codegen/common.py torch/_inductor/codegen/cpp.py torch/_inductor/codegen/cpp_template_kernel.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_gpu.py torch/_inductor/codegen/cuda/cuda_kernel.py torch/_inductor/codegen/cuda/device_op_overrides.py torch/_inductor/codegen/debug_utils.py torch/_inductor/codegen/halide.py torch/_inductor/codegen/mps.py torch/_inductor/codegen/multi_kernel.py torch/_inductor/codegen/rocm/rocm_kernel.py torch/_inductor/codegen/simd.py torch/_inductor/codegen/triton.py torch/_inductor/codegen/triton_combo_kernel.py torch/_inductor/codegen/triton_split_scan.py torch/_inductor/codegen/wrapper.py torch/_inductor/codegen/xpu/device_op_overrides.py torch/_inductor/compile_fx.py torch/_inductor/fx_passes/b2b_gemm.py torch/_inductor/ir.py torch/_inductor/kernel/bmm.py torch/_inductor/kernel/conv.py torch/_inductor/kernel/flex_attention.py torch/_inductor/kernel/flex_decoding.py torch/_inductor/kernel/mm_common.py torch/_inductor/runtime/triton_heuristics.py torch/_inductor/scheduler.py torch/_inductor/select_algorithm.py torch/utils/_get_clean_triton.py,https://github.com/pytorch/pytorch/pull/148305,jansel,eellison,shunting314,,
4e7d264cf8e,dynamo,Untopiced,Introduce `UserDefinedExceptionClassVariable` (#146504),test/dynamo/test_ctx_manager.py test/dynamo/test_exceptions.py test/dynamo_expected_failures/TestNN.test_RNN_cell_forward_zero_hidden_size test/dynamo_expected_failures/TestNN.test_unflatten test/dynamo_expected_failures/TestNestedTensorSubclassCPU.test_chunk_cpu test/dynamo_skips/TestNestedTensorSubclassCPU.test_composite_op_with_custom_mode_cpu_float32 test/test_nestedtensor.py torch/_dynamo/exc.py torch/_dynamo/side_effects.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/__init__.py torch/_dynamo/variables/builder.py torch/_dynamo/variables/user_defined.py torch/fx/graph.py,https://github.com/pytorch/pytorch/pull/146504,guilhermeleobas,anijain2305,,,
fb53e9e514e,dynamo,Untopiced,Add `__context/cause/suppress_context/traceback__` to Exception (#146499),test/dynamo/test_ctx_manager.py test/dynamo/test_exceptions.py test/dynamo/test_generator.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/builtin.py torch/_dynamo/variables/ctx_manager.py torch/_dynamo/variables/misc.py torch/_dynamo/variables/user_defined.py torch/testing/_internal/common_utils.py,https://github.com/pytorch/pytorch/pull/146499,guilhermeleobas,anijain2305,zou3519,,
daff65d671b,dynamo,Untopiced,Correctly propagate exception to parent tx (#146502),test/dynamo/test_exceptions.py test/dynamo/test_generator.py torch/_dynamo/convert_frame.py torch/_dynamo/exc.py torch/_dynamo/symbolic_convert.py torch/_dynamo/variables/functions.py,https://github.com/pytorch/pytorch/pull/146502,guilhermeleobas,anijain2305,williamwen42,zou3519,
a1cb67b69eb,rocm,Untopiced,[ROCm] Improve backwards indexing when stride is not one (#147630),aten/src/ATen/native/cuda/Indexing.cu test/test_indexing.py,https://github.com/pytorch/pytorch/pull/147630,doru1004,jeffdaily,,,
4d10da731b0,skip,not user facing,[ROCm] CK Memory-Efficient Attention (attention bias support) (#147778),aten/src/ATen/native/transformers/cuda/attention.cu aten/src/ATen/native/transformers/cuda/attention_backward.cu aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/me_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/me_ck_api.h aten/src/ATen/native/transformers/hip/flash_attn/ck/me_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_bwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/mha_varlen_fwd_ck.hip aten/src/ATen/native/transformers/hip/flash_attn/ck/rename_ck_autogen_files.output.txt aten/src/ATen/native/transformers/hip/flash_attn/ck/rename_ck_autogen_files.sh aten/src/ATen/native/transformers/hip/flash_attn/flash_api.h,https://github.com/pytorch/pytorch/pull/147778,alugorey,houseroad,,,
971606befac,skip,Untopiced,Add a stable TORCH_LIBRARY to C shim (#148124),.gitignore build_variables.bzl docs/cpp/source/Doxyfile setup.py test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/__init__.py test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/csrc/libtorch_agnostic_kernel.cpp test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/ops.py test/cpp_extensions/libtorch_agnostic_extension/setup.py test/cpp_extensions/libtorch_agnostic_extension/test/test_libtorch_agnostic.py test/run_test.py test/test_cpp_extensions_aot.py tools/amd_build/build_amd.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp torch/csrc/stable/library.h,https://github.com/pytorch/pytorch/pull/148124,janeyx99,albanD,atalman,zou3519,
61f9b50e099,skip,not user facing,[ROCm] Fix TORCH_CHECK for hdim 512 support added in AOTriton 0.9b (#148967),aten/src/ATen/native/transformers/hip/flash_attn/aot/mha_all_aot.hip,https://github.com/pytorch/pytorch/pull/148967,xinyazhang,jeffdaily,jithunnair-amd,,
7b2ecb80eb1,caffe2,not user facing,[Codemod][AddExplicitStrictExportArg] caffe2/test/inductor (#148928),test/inductor/test_aot_inductor.py,https://github.com/pytorch/pytorch/pull/148928,gmagogsfm,angelayi,,,
5b8da176813,skip,not user facing,[cutlass backend] Add addmm and bmm tests for AOTI (#148929),test/inductor/test_cutlass_backend.py,https://github.com/pytorch/pytorch/pull/148929,henrylhtsang,ColinPeppler,jingsh,,
73c8068cf88,skip,not user facing,[logging] Set compile_id in the CachingAutotuner during compilation so we have it for dynamo_timed logging (#148693),test/dynamo/test_structured_trace.py torch/_dynamo/utils.py torch/_inductor/async_compile.py torch/_inductor/runtime/triton_heuristics.py,https://github.com/pytorch/pytorch/pull/148693,masnesral,eellison,,,
abcec555322,dynamo,not user facing,gracefully handle `tokenize.TokenError` in funcname parser. Adds support for non-Python source (#148737),torch/_dynamo/funcname_cache.py,https://github.com/pytorch/pytorch/pull/148737,cat-state,williamwen42,,,
c18858d6331,mps,improvements,[MPS] Make `torch.mps.compile_shader` public (#148972),docs/source/mps.rst test/test_mps.py torch/_inductor/runtime/runtime_utils.py torch/mps/__init__.py,https://github.com/pytorch/pytorch/pull/148972,malfet,Skylion007,albanD,atalman,
b54cf1a2814,skip,Untopiced,"Revert ""[logging] Set compile_id in the CachingAutotuner during compilation so we have it for dynamo_timed logging (#148693)""",test/dynamo/test_structured_trace.py torch/_dynamo/utils.py torch/_inductor/async_compile.py torch/_inductor/runtime/triton_heuristics.py,,,,,,
f69e58e8e88,skip,not user facing,[CI] Update crossvit_9_240 as pass (#148989),benchmarks/dynamo/ci_expected_accuracy/dynamic_inductor_timm_training.csv benchmarks/dynamo/ci_expected_accuracy/inductor_timm_training.csv,https://github.com/pytorch/pytorch/pull/148989,desertfire,ZainRizvi,,,
cf19efd3d9e,inductor,Untopiced,Support basic TorchBind in aot_compile and aoti_compile_and_package (#148506),test/export/test_serialize.py test/inductor/test_torchbind.py torch/_C/__init__.pyi.in torch/_export/serde/serialize.py torch/_higher_order_ops/torchbind.py torch/_inductor/codecache.py torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/ir.py torch/_inductor/package/package.py,https://github.com/pytorch/pytorch/pull/148506,yushangdi,angelayi,,,
e6ef0620cca,cpp_frontend,Untopiced,Add shim.h C API to call dispatcher on our own aten ops (#148832),test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/csrc/libtorch_agnostic_kernel.cpp test/cpp_extensions/libtorch_agnostic_extension/libtorch_agnostic/ops.py test/cpp_extensions/libtorch_agnostic_extension/test/test_libtorch_agnostic.py test/test_cpp_extensions_aot.py torch/csrc/inductor/aoti_torch/c/shim.h torch/csrc/inductor/aoti_torch/shim_common.cpp,https://github.com/pytorch/pytorch/pull/148832,janeyx99,albanD,atalman,,
72d9f88ef28,releng,not user facing,[release] Move triton pin to latest triton release/3.3.x (#148971),.ci/docker/ci_commit_pins/triton.txt,https://github.com/pytorch/pytorch/pull/148971,atalman,danzimm,,,
e5fef8a08eb,skip,not user facing,[CI] Don't clean workspace when fetching repo (#147994),.github/actions/checkout-pytorch/action.yml,https://github.com/pytorch/pytorch/pull/147994,clee2000,atalman,malfet,,
6119ffc7114,skip,not user facing,[ROCm][TunableOp] Fix TunableOp BLAS logging for online tuning case. (#148979),aten/src/ATen/cuda/tunable/TunableOp.h,https://github.com/pytorch/pytorch/pull/148979,naromero77amd,jeffdaily,,,
b98af954017,distributed,not user facing,Fix DCP link (#148974),docs/source/distributed.checkpoint.rst,https://github.com/pytorch/pytorch/pull/148974,H-Huang,svekars,,,
53a1a022a9b,skip,Untopiced,[WIP] Initial implementation of Grouped Gemm API (#148531),aten/src/ATen/native/cuda/Blas.cpp aten/src/ATen/native/cuda/RowwiseScaledMM.cu aten/src/ATen/native/cuda/ScaledGroupMM.cu aten/src/ATen/native/cuda/ScaledGroupMM.h aten/src/ATen/native/cuda/cutlass_utils.cuh aten/src/ATen/native/native_functions.yaml cmake/Codegen.cmake test/expect/HasDecompTest.test_has_decomposition.expect test/test_matmul_cuda.py,https://github.com/pytorch/pytorch/pull/148531,ngimel,drisspg,,,
a6459afb0e9,fx,Untopiced,[dynamic shapes] add backed_size_oblivious option (#148696),test/export/test_export.py test/test_dynamic_shapes.py torch/_guards.py torch/fx/experimental/_config.py torch/fx/experimental/symbolic_shapes.py,https://github.com/pytorch/pytorch/pull/148696,pianpwk,bobrenjc93,,,
2dcdb4ba782,skip,not user facing,[ez] include config as part of __all__ in torch.compiler (#148978),torch/compiler/__init__.py,https://github.com/pytorch/pytorch/pull/148978,bobrenjc93,bdhirsh,laithsakka,,
98a2d905bf9,mps,improvements,[MPSInductor] Fix large prod and sum reductions (#148975),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/148975,malfet,dcci,jansel,,
5b60749e9e6,dynamo,not user facing,[cudagraph] add log for skip reasons (#148797),torch/_dynamo/utils.py torch/_inductor/cudagraph_utils.py,https://github.com/pytorch/pytorch/pull/148797,BoyuanFeng,masnesral,,,
46f096bba6f,releng,not user facing,Explicitly set use-ephemeral runners for windows nightly cpu test jobs (#149001),.github/templates/windows_binary_build_workflow.yml.j2 .github/workflows/generated-windows-binary-libtorch-debug-nightly.yml .github/workflows/generated-windows-binary-libtorch-release-nightly.yml .github/workflows/generated-windows-binary-wheel-nightly.yml,https://github.com/pytorch/pytorch/pull/149001,atalman,malfet,,,
60576419a2a,skip,Untopiced,Make dynamism code robust to NotImplementedException (#148823),test/fx/test_dynamism.py torch/fx/experimental/_dynamism.py,https://github.com/pytorch/pytorch/pull/148823,bobrenjc93,laithsakka,,,
e40a9e602b8,releng,Untopiced,Add the max_autotune tests in the periodic jobs. (#143560),.ci/pytorch/test.sh .github/workflows/inductor-nightly.yml benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_max_autotune_inductor_amp_freezing_huggingface_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_max_autotune_inductor_amp_freezing_timm_inference.csv benchmarks/dynamo/ci_expected_accuracy/dynamic_cpu_max_autotune_inductor_amp_freezing_torchbench_inference.csv,https://github.com/pytorch/pytorch/pull/143560,LifengWang,desertfire,leslie-fang-intel,,
2a7e997b3f1,skip,not user facing,test/dynamo/test_utils: Fix one broken test on different python versions (#148987),test/dynamo/test_utils.py,https://github.com/pytorch/pytorch/pull/148987,c00w,jansel,,,
fe22db9cc3b,mps,bug fixes,[MPSInductor] Fix `min`/`max` reductions over large dims (#149004),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/149004,malfet,jansel,,,
758522d56ac,mps,bug fixes,[MPSInductor][EZ] Fix argmin/max signatures (#149020),c10/metal/reduction_utils.h,https://github.com/pytorch/pytorch/pull/149020,malfet,jansel,,,
7b78a2c415b,inductor,bug fixes,[MPSInductor] Fix `argmin`/`argmax` long reductions (#149021),test/inductor/test_mps_basic.py torch/_inductor/codegen/mps.py,https://github.com/pytorch/pytorch/pull/149021,malfet,jansel,,,
924a247fbbe,mps,bug fixes,[MPS] Enable angle and atan2 for `torch.long` (#149017),aten/src/ATen/native/mps/operations/BinaryOps.mm aten/src/ATen/native/mps/operations/UnaryOps.mm test/test_mps.py,https://github.com/pytorch/pytorch/pull/149017,malfet,dcci,,,
